{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PennyLane tutorial\n",
    "\n",
    "In this notebook, we walk through the main features of PennyLane. \n",
    "\n",
    "First, we import `pennylane` itself, as well as a wrapped version of `numpy` which is provided via PennyLane. We don't need `numpy` yet, but it is good practice to import this from the start. The wrapped version of `numpy` can be used pretty much the exact same way as regular `numpy`, while providing additional support for computing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to instantiate a `device`, which indicates where we want computations to be run (e.g., on a local simulator or a remote hardware backend). \n",
    "\n",
    "For this tutorial, we'll work with the `'default.qubit'` device provided by PennyLane â€” a simple pure-state qubit simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all devices, `device()` accepts the following arguments:\n",
    "\n",
    "`name`: the name of the device to be loaded\n",
    "\n",
    "`wires`: the number of subsystems to initialize the device with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating quantum circuits\n",
    "\n",
    "As a working example, let's consider the following two qubit variational circuit:\n",
    "\n",
    "1. Begin in the state $|0\\rangle\\otimes|0\\rangle$ (this is assumed by default in PennyLane)\n",
    "2. Apply local rotations about the $x-$axis to each qubit, with rotation angles $\\theta_1$ and $\\theta_2$\n",
    "3. Apply a CNOT gate\n",
    "4. Measure the Pauli-Z observable $P_z$ on the first qubit\n",
    "\n",
    "We create this circuit in PennyLane the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane.ops import Hadamard, RX, CNOT\n",
    "\n",
    "# Declare quantum circuit\n",
    "@qml.qnode(dev)\n",
    "def circuit(theta1, theta2):\n",
    "    Hadamard(wires=1)\n",
    "    RX(theta1, wires=0)\n",
    "    RX(theta2, wires=1)\n",
    "    CNOT(wires=[0,1])\n",
    "    return qml.expval.PauliZ(wires=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999998\n",
      "0.954870304606541\n",
      "0.5834591942372576\n",
      "0.9298432134377467\n",
      "0.6856097555111197\n",
      "0.6835463281375614\n"
     ]
    }
   ],
   "source": [
    "# We can evaluate the circuit at any value of the parameters\n",
    "print(circuit(0, 0))\n",
    "for theta1, theta2 in np.random.random((5,2)):\n",
    "    print(circuit(theta1, theta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through the different pieces in the circuit above.\n",
    "\n",
    "1. A quantum circuit is declared using standard Python function notation (`def fn(...)`)\n",
    "2. This function contains operators (e.g., gates) found in `pennylane.ops`, which specify the computation\n",
    "3. The function returns one or more `expval` objects, indicating measurements which are made at the end of the circuit\n",
    "4. We use the `qnode` decorator to indicate that this is not a typical Python function. This prevents the function from being run as usual by the Python interpretor. Instead, the function is evaluated on the device `dev` (which may be quantum hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes about circuits:\n",
    "- Final measurements are considered as *expectation values* $\\langle \\cdots \\rangle$ -- i.e., averages -- not single-shot results. Expectation values are deterministic, whereas single-shot measurements are stochastic. This is what allows us to do machine learning on the circuit (Note: the same principle holds for deep learning models).\n",
    "- Since circuits are meant to be run on quantum hardware, there is limited support for classical computation *inside* the circuit function. On the other hand, classical processing of circuit inputs/outputs is fully supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pennylane.ops.qubit.Hadamard,\n",
       " pennylane.ops.qubit.PauliX,\n",
       " pennylane.ops.qubit.PauliY,\n",
       " pennylane.ops.qubit.PauliZ,\n",
       " pennylane.ops.qubit.CNOT,\n",
       " pennylane.ops.qubit.CZ,\n",
       " pennylane.ops.qubit.SWAP,\n",
       " pennylane.ops.qubit.RX,\n",
       " pennylane.ops.qubit.RY,\n",
       " pennylane.ops.qubit.RZ,\n",
       " pennylane.ops.qubit.PhaseShift,\n",
       " pennylane.ops.qubit.Rot,\n",
       " pennylane.ops.qubit.BasisState,\n",
       " pennylane.ops.qubit.QubitStateVector,\n",
       " pennylane.ops.qubit.QubitUnitary]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PennyLane supports a number of common gates used in variational circuits\n",
    "qml.ops.qubit.all_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pennylane.expval.qubit.PauliX,\n",
       " pennylane.expval.qubit.PauliY,\n",
       " pennylane.expval.qubit.PauliZ,\n",
       " pennylane.expval.qubit.Hadamard,\n",
       " pennylane.expval.qubit.Hermitian]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And several choices of measurement operators\n",
    "qml.expval.qubit.all_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients of quantum circuits\n",
    "\n",
    "To do quantum machine learning, we will want to use a *gradient descent* strategy. For this, we define a final objective (or cost) function $C$ which is to be optimized with respect to the free parameters $\\alpha=(\\theta_1, \\theta_2)$. \n",
    "\n",
    "In this case, we'll just use the expectation value itself as cost function, i.e., $C=\\langle P_Z \\rangle$.\n",
    "\n",
    "The core feature of PennyLane is that it performs *automatic differentiation* of quantum circuits. This means that it can automatically compute $\\nabla_\\mathbf{\\alpha}C$ without any intervention from the user. \n",
    "\n",
    "Internally, PennyLane leverages the 'parameter shift' trick for computing derivatives, i.e., it evaluates derivatives as the difference of two circuit evaluations with shifted parameters:\n",
    "\n",
    "$$C(\\theta_1, \\theta_2) = \\texttt{circuit}(\\theta_1, \\theta_2)$$\n",
    "\n",
    "$$\\partial_{\\theta_1} C = a\\big[ \\texttt{circuit}(\\theta_1+s, \\theta_2) - \\texttt{circuit}(\\theta_1 - s, \\theta_2) \\big]$$\n",
    "\n",
    "The values of the shift and scale parameters $s$ and $a$ typically depend only on the *type* of gate, and not its location, i.e., they are the same no matter where the gate appears in the circuit.\n",
    "\n",
    "By using this method, PennyLane provides a hardware-scalable way to compute gradients and to optimize quantum circuits for QML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: (0.327827,0.425986); Gradient=[-0.32198629  0.        ]\n",
      "Value: (0.528242,0.818192); Gradient=[-5.04015582e-01 -5.55111512e-17]\n",
      "Value: (0.464655,0.988004); Gradient=[-4.48114080e-01 -1.66533454e-16]\n",
      "Value: (0.515879,0.723477); Gradient=[-4.93299420e-01 -2.22044605e-16]\n",
      "Value: (0.112378,0.868087); Gradient=[-1.12141621e-01 -5.55111512e-17]\n"
     ]
    }
   ],
   "source": [
    "grad_circuit = qml.grad(circuit, argnum=[0,1])\n",
    "# We can evaluate the circuit gradient at any value of the parameters\n",
    "for theta1, theta2 in np.random.random((5,2)):\n",
    "    print(\"Value: ({:3f},{:3f}); Gradient={}\".format(theta1, theta2, \n",
    "                                             np.stack(grad_circuit(theta1, theta2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing quantum circuits\n",
    "\n",
    "Being able to compute the gradient of a variational quantum circuit with respect to any of its parameters gives us great power. This opens up the door to using the gradient descent procedure to optimize the cost function. \n",
    "\n",
    "The gradient descent algorithm has two steps:\n",
    "\n",
    "1. Compute the gradient $\\nabla_\\alpha C$\n",
    "2. Update the parameters $\\alpha$ in proportion to this gradient, i.e., \n",
    "$$\\alpha \\mapsto \\alpha - \\eta \\nabla_\\alpha C.$$\n",
    "\n",
    "The scaling factor $\\eta$ is known as the *learning rate*.\n",
    "\n",
    "This procedure is carried out automatically by the PennyLane `GradientDescentOptimizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial value: [0.35113739 0.57624772]\n",
      "Value after one step: [0.38553399 0.57624772]\n"
     ]
    }
   ],
   "source": [
    "from pennylane.optimize import GradientDescentOptimizer\n",
    "\n",
    "eta = 0.1\n",
    "opt = GradientDescentOptimizer(eta)\n",
    "\n",
    "init_val = np.random.random(2)\n",
    "new_val = opt.step(circuit, init_val)\n",
    "print(\"Initial value:\", init_val)\n",
    "print(\"Value after one step:\", new_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that automatic update does what we expect\n",
    "new_val_manual = init_val - eta * np.stack(qml.grad(circuit, argnum=[0,1])(*init_val))\n",
    "assert np.allclose(new_val, new_val_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdagradOptimizer',\n",
       " 'AdamOptimizer',\n",
       " 'GradientDescentOptimizer',\n",
       " 'MomentumOptimizer',\n",
       " 'NesterovMomentumOptimizer',\n",
       " 'RMSPropOptimizer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: There are a number of other optimizers in the \n",
    "# gradient descent family which are available in PennyLane\n",
    "qml.optimize.__all__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PennyLane allows us to string together quantum and classical computations in highly-structured ways, and the combined hybrid computation can be differentiated and trained end-to-end. \n",
    "\n",
    "This opens up the possibility of doing many interesting things. For example:\n",
    "\n",
    "- Pre-processing (or post-processing) the input (output) of a quantum circuit using a neural network. Both the classical and quantum components can have trainable weights!\n",
    "- Combining the power of several quantum circuits, either in series or in parallel. These circuits can even be running on different hardware!\n",
    "- Training a model using both GPUs and QPUs.\n",
    "\n",
    "Here is a simple example of a classical function post-processing the output of a quantum circuit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.347766597206169"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a classical cost function which post-processes the circuit's output\n",
    "target = 0.33\n",
    "def cost(weights):\n",
    "    expval = circuit(weights[0], weights[1])\n",
    "    error = np.abs(expval - target) ** 2 + weights[2] ** 2\n",
    "    return error\n",
    "\n",
    "# Evaluate cost at a random starting point\n",
    "weights = np.random.random(3)\n",
    "cost(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Cost=0.24214835271129972\n",
      "Step 10: Cost=0.005679402718735217\n",
      "Step 20: Cost=0.00010721359205829371\n",
      "Step 30: Cost=1.985619107470657e-06\n",
      "Step 40: Cost=3.7464187704847966e-08\n",
      "Step 50: Cost=7.188160531284856e-10\n",
      "Step 60: Cost=1.3945716066782688e-11\n",
      "Step 70: Cost=2.72413352873267e-13\n",
      "Step 80: Cost=5.343083225142393e-15\n",
      "Step 90: Cost=1.0505272028291982e-16\n"
     ]
    }
   ],
   "source": [
    "# Implement gradient descent over 100 steps\n",
    "for step in range(100):\n",
    "    weights = opt.step(cost, weights)\n",
    "    if step % 10 == 0:\n",
    "        print(\"Step {}: Cost={}\".format(step, cost(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm we are near the optimum \n",
    "# (some random initial values may take more steps to reach)\n",
    "assert np.allclose(circuit(weights[0], weights[1]), target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Quantum Generative Adversarial Networks (QGANs)\n",
    "\n",
    "A QGAN is the quantum version of a *Generative Adversarial Network*. These are a particular kind of deep learning model used to generate real-looking synthetic data (such as images).\n",
    "\n",
    "![GAN images](biggan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANs\n",
    "GANs have a unique structure, consisting of two models:\n",
    "\n",
    "- the **generator**: its goal is to produce realistic-looking data samples\n",
    "- the **discriminator**: its goal is distinguish fake data produced by the discriminator from real data\n",
    "\n",
    "Training of GANs proceeds as follows:\n",
    "\n",
    "1. \"Real\" data is captured in some *training dataset*.\n",
    "\n",
    "2. The generator produces \"fake\" data by starting with a random input vector and transforming it into an output.\n",
    "\n",
    "3. The discriminator is fed samples of both real and fake data and must decide which label to assign ('real' or 'fake').\n",
    "\n",
    "4. Training consists of alternating steps: (i) the generator is frozen and the discriminator is trained to distinguish real from fake. (ii) the discriminator is frozen and the generator is trained to fool the discriminator. The gradient of the discriminator's output provides a training signal for the generator to improve its fake generated data. \n",
    "\n",
    "5. Eventually, this training method should converge to a stage where the generator is producing realistic data and the discriminator can't tell real from fake.\n",
    "\n",
    "**Note:** Training is done via the *gradient descent* algorithm, updating only the weights associated to the generator (or vice versa) at each step. There is no internal structure imposed on the generator or discriminator models except that they are differentiable.\n",
    "\n",
    "\n",
    "![GAN structure](gan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Quantum GANs\n",
    " \n",
    "This demo walks through how to build and train a simple Quantum Generative Adversarial Network (QGAN) using PennyLane.\n",
    "\n",
    "References: \n",
    "- [Lloyd and Weedbrook (2018)](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.121.040502) \n",
    "- [Dallaire-Demers and Killoran (2018)](https://journals.aps.org/pra/abstract/10.1103/PhysRevA.98.012324)) \n",
    "\n",
    "Like classical GANs, QGANs consist of a generator and a discriminator. *QGANs use variational quantum circuits for both generator and discriminator.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a device for the QGAN example\n",
    "gan_dev = qml.device('default.qubit', wires=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classical GANs, the starting point is to draw samples either from some \"real data\" distribution, or from the generator, and feed them to the discriminator. In this QGAN example, we will use a quantum circuit to generate the real data.\n",
    "\n",
    "**Note:** One of the areas where QGANs can provide a clear advantage is in modelling *quanum data*, i.e., data that is created by quantum systems. Quantum data distributions should be hard for classical models to emulate efficiently.\n",
    "\n",
    "For this simple example, our real data will be a qubit that has been rotated (from the starting state $\\left|0\\right\\rangle$) to some arbitrary, but fixed, state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real(phi, theta, omega):\n",
    "    qml.Rot(phi, theta, omega, wires=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the generator and discriminator, we will choose the same basic circuit structure, but acting on different wires.\n",
    "\n",
    "Both the real data circuit and the generator will output on wire 0, which will be connected as an input to the discriminator. Wire 1 is provided as a workspace for the generator, while the discriminator's output will be on wire 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the structure of these circuits is more-or-less chosen arbitrarily. There is \n",
    "# nothing particular about these choices of generator or discriminator\n",
    "\n",
    "def generator(w):\n",
    "    qml.RX(w[0], wires=0)\n",
    "    qml.RX(w[1], wires=1)\n",
    "    qml.RY(w[2], wires=0)\n",
    "    qml.RY(w[3], wires=1)\n",
    "    qml.RZ(w[4], wires=0)\n",
    "    qml.RZ(w[5], wires=1)\n",
    "    qml.CNOT(wires=[0,1])\n",
    "    qml.RX(w[6], wires=0)\n",
    "    qml.RY(w[7], wires=0)\n",
    "    qml.RZ(w[8], wires=0)\n",
    "    \n",
    "def discriminator(w):\n",
    "    qml.RX(w[0], wires=0)\n",
    "    qml.RX(w[1], wires=2)\n",
    "    qml.RY(w[2], wires=0)\n",
    "    qml.RY(w[3], wires=2)\n",
    "    qml.RZ(w[4], wires=0)\n",
    "    qml.RZ(w[5], wires=2)\n",
    "    qml.CNOT(wires=[1,2])\n",
    "    qml.RX(w[6], wires=2)\n",
    "    qml.RY(w[7], wires=2)\n",
    "    qml.RZ(w[8], wires=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two QNodes. One where the real data source is wired up to the discriminator, and one where the generator is connected to the discriminator. They can both be run on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(gan_dev)\n",
    "def real_disc_circuit(phi, theta, omega, disc_weights):\n",
    "    real(phi, theta, omega)\n",
    "    discriminator(disc_weights)\n",
    "    return qml.expval.PauliZ(2)\n",
    "\n",
    "@qml.qnode(gan_dev)\n",
    "def gen_disc_circuit(gen_weights, disc_weights):\n",
    "    generator(gen_weights)\n",
    "    discriminator(disc_weights)\n",
    "    return qml.expval.PauliZ(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cost function\n",
    "\n",
    "There are two ingredients to the cost here. \n",
    "- The first is the probability that the discriminator correctly classifies real data as real. \n",
    "- The second ingredient is the probability that the discriminator classifies fake data (i.e., a state prepared by the generator) as real. \n",
    "\n",
    "The discriminator's objective is to maximize the probability of correctly classifying real data, while minimizing the probability of mistakenly classifying fake data.\n",
    "\n",
    "The generator's objective is to maximize the probability that the discriminator accepts fake data as real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_real_true(disc_weights):\n",
    "    true_disc_output = real_disc_circuit(phi, theta, omega, disc_weights)\n",
    "    # convert to probability\n",
    "    prob_real_true = (true_disc_output + 1) / 2\n",
    "    return prob_real_true\n",
    "\n",
    "def prob_fake_true(gen_weights, disc_weights):\n",
    "    fake_disc_output = gen_disc_circuit(gen_weights, disc_weights)\n",
    "    # convert to probability\n",
    "    prob_fake_true = (fake_disc_output + 1) / 2\n",
    "    return prob_fake_true # generator wants to minimize this prob\n",
    "\n",
    "def disc_cost(disc_weights):\n",
    "    cost = prob_fake_true(gen_weights, disc_weights) - prob_real_true(disc_weights) \n",
    "    return cost\n",
    "\n",
    "def gen_cost(gen_weights):\n",
    "    return -prob_fake_true(gen_weights, disc_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimization \n",
    "We initialize the fixed angles of the \"real data\" circuit, as well as the initial parameters for both generator and discriminator. These are chosen so that the generator initially prepares a state on wire 0 that is very close to the $\\left| 1 \\right\\rangle$ state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = np.pi / 6\n",
    "theta = np.pi / 2\n",
    "omega = np.pi / 7\n",
    "np.random.seed(0)\n",
    "eps = 1e-2\n",
    "gen_weights = np.array([np.pi] + [0] * 8) + np.random.normal(scale=eps, size=[9])\n",
    "disc_weights = np.random.normal(size=[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer\n",
    "opt = GradientDescentOptimizer(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first stage of training, we optimize the discriminator while keeping the generator parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: cost = -0.10942017805789145\n",
      "Step 6: cost = -0.38998842264903094\n",
      "Step 11: cost = -0.6660191175815626\n",
      "Step 16: cost = -0.8550839212078475\n",
      "Step 21: cost = -0.9454459581664485\n",
      "Step 26: cost = -0.9805878247866396\n",
      "Step 31: cost = -0.993137132834275\n",
      "Step 36: cost = -0.9974896764916588\n",
      "Step 41: cost = -0.9989863506630721\n",
      "Step 46: cost = -0.9995000463932007\n"
     ]
    }
   ],
   "source": [
    "for it in range(50):\n",
    "    disc_weights = opt.step(disc_cost, disc_weights) \n",
    "    cost = disc_cost(disc_weights)\n",
    "    if it % 5 == 0:\n",
    "        print(\"Step {}: cost = {}\".format(it+1, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the discriminator's optimum, the probability for the discriminator to correctly classify the real data should be close to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998971951842257"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_real_true(disc_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we check how the discriminator classifies the generator's (still unoptimized) fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00024278396180049677"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_fake_true(gen_weights, disc_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the adverserial training method, we have to now train the generator to better fool the discriminator \n",
    "\n",
    "In more complex settings, we can continue training the models in an alternating fashion until we reach the optimum point of the two-player adversarial game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: cost = 0.00026646913829986296\n",
      "Step 5: cost = 0.0004266200858938918\n",
      "Step 10: cost = 0.0006872486146986545\n",
      "Step 15: cost = 0.0011111626380138073\n",
      "Step 20: cost = 0.0018000510248334378\n",
      "Step 25: cost = 0.0029179304125449557\n",
      "Step 30: cost = 0.0047277175397743565\n",
      "Step 35: cost = 0.007646628881032402\n",
      "Step 40: cost = 0.012325866735736601\n",
      "Step 45: cost = 0.019754518934527232\n",
      "Step 50: cost = 0.03136834673567207\n",
      "Step 55: cost = 0.049097345993078245\n",
      "Step 60: cost = 0.07520378135265515\n",
      "Step 65: cost = 0.11169015288702183\n",
      "Step 70: cost = 0.15917286333740988\n",
      "Step 75: cost = 0.21566031343947323\n",
      "Step 80: cost = 0.2763735721045267\n",
      "Step 85: cost = 0.3354169186527466\n",
      "Step 90: cost = 0.3883501266928629\n",
      "Step 95: cost = 0.43371772120148583\n",
      "Step 100: cost = 0.4728490188392819\n",
      "Step 105: cost = 0.5087778323625831\n",
      "Step 110: cost = 0.5451977336157656\n",
      "Step 115: cost = 0.5856632916397881\n",
      "Step 120: cost = 0.6327897835085201\n",
      "Step 125: cost = 0.6872469221106605\n",
      "Step 130: cost = 0.7468453348018416\n",
      "Step 135: cost = 0.8066413637587635\n",
      "Step 140: cost = 0.8607353038575819\n",
      "Step 145: cost = 0.9048413990478825\n",
      "Step 150: cost = 0.9376687441862337\n",
      "Step 155: cost = 0.9604104860258083\n",
      "Step 160: cost = 0.975371147847833\n",
      "Step 165: cost = 0.9848746701785983\n",
      "Step 170: cost = 0.990776500847925\n",
      "Step 175: cost = 0.9943898535953364\n",
      "Step 180: cost = 0.9965827747752587\n",
      "Step 185: cost = 0.9979065439048094\n",
      "Step 190: cost = 0.9987030608439527\n",
      "Step 195: cost = 0.999181393560638\n"
     ]
    }
   ],
   "source": [
    "for it in range(200):\n",
    "    gen_weights = opt.step(gen_cost, gen_weights)\n",
    "    cost = -gen_cost(gen_weights)\n",
    "    if it % 5 == 0:\n",
    "        print(\"Step {}: cost = {}\".format(it, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the optimum of the generator, the probability for the discriminator to be fooled should be close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998971951842257"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_real_true(disc_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999422032442016"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_fake_true(gen_weights, disc_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the joint optimum the overall cost will be close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00047516274220971155"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc_cost(disc_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator has successfully learned how to simulate the real data enough to fool the discriminator!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (CS269Q)",
   "language": "python",
   "name": "cs269q"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
