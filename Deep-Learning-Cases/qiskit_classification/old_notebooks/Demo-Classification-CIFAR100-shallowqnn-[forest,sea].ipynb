{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forest', 'sea']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"hybridqnn_shallow\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "LR = 0.001\n",
    "n_samples_train = 200 #128\n",
    "n_samples_test = 50 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"CIFAR100\" # MNIST / CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = [\"forest\",\"sea\"] # ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d74424240e4abf81aa9773e415585a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "if network == 'hybridqnn_shallow' or network == 'QSVM':\n",
    "    # Set preprocessing transforms\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "else:\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    \n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: 0.0006926810019649565 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZ),\n",
      "  CircuitStateFn(\n",
      "       ┌──────────────────────────┐┌──────────────────────────────────────┐\n",
      "  q_0: ┤0                         ├┤0                                     ├\n",
      "       │  ZZFeatureMap(x[0],x[1]) ││  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
      "  q_1: ┤1                         ├┤1                                     ├\n",
      "       └──────────────────────────┘└──────────────────────────────────────┘\n",
      "  )\n",
      "])\n",
      "HybridQNN_Shallow(\n",
      "  (conv1): Conv2d(3, 2, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (qnn): TorchConnector()\n",
      "  (fc3): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Network init elapsed time: -0.0005611740052700043 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        #n_filts_fc1 = int(((((input_resolution[0])/2)-4)/2)**2)*16 # +7\n",
    "        n_filts_fc1 = 256\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: -1.3623929023742676\n",
      "Batch 1, Loss: -1.3720736503601074\n",
      "Batch 2, Loss: -1.3887619972229004\n",
      "Batch 3, Loss: 0.36625126004219055\n",
      "Batch 4, Loss: -1.4028269052505493\n",
      "Batch 5, Loss: -1.3941388130187988\n",
      "Batch 6, Loss: -1.416565179824829\n",
      "Batch 7, Loss: -1.3887298107147217\n",
      "Batch 8, Loss: 0.44357624650001526\n",
      "Batch 9, Loss: -1.4529292583465576\n",
      "Batch 10, Loss: 0.4527019262313843\n",
      "Batch 11, Loss: -1.391471266746521\n",
      "Batch 12, Loss: -1.4209485054016113\n",
      "Batch 13, Loss: -1.4367257356643677\n",
      "Batch 14, Loss: 0.4491250813007355\n",
      "Batch 15, Loss: -1.4491956233978271\n",
      "Batch 16, Loss: 0.47512340545654297\n",
      "Batch 17, Loss: -1.4568201303482056\n",
      "Batch 18, Loss: 0.49237602949142456\n",
      "Batch 19, Loss: -1.459964394569397\n",
      "Batch 20, Loss: 0.4859585762023926\n",
      "Batch 21, Loss: -1.5049482583999634\n",
      "Batch 22, Loss: 0.4602780342102051\n",
      "Batch 23, Loss: 0.4934549331665039\n",
      "Batch 24, Loss: 0.513362467288971\n",
      "Batch 25, Loss: 0.45359301567077637\n",
      "Batch 26, Loss: 0.5058041214942932\n",
      "Batch 27, Loss: -1.4709432125091553\n",
      "Batch 28, Loss: -1.442136287689209\n",
      "Batch 29, Loss: -1.4551339149475098\n",
      "Batch 30, Loss: -1.4639694690704346\n",
      "Batch 31, Loss: 0.47156232595443726\n",
      "Batch 32, Loss: 0.45873409509658813\n",
      "Batch 33, Loss: -1.4695172309875488\n",
      "Batch 34, Loss: 0.47930222749710083\n",
      "Batch 35, Loss: 0.4720025658607483\n",
      "Batch 36, Loss: -1.4673826694488525\n",
      "Batch 37, Loss: 0.4643245339393616\n",
      "Batch 38, Loss: 0.4533138871192932\n",
      "Batch 39, Loss: 0.4521746039390564\n",
      "Batch 40, Loss: 0.469032347202301\n",
      "Batch 41, Loss: -1.4366883039474487\n",
      "Batch 42, Loss: 0.44149231910705566\n",
      "Batch 43, Loss: -1.4403135776519775\n",
      "Batch 44, Loss: -1.4356937408447266\n",
      "Batch 45, Loss: 0.42824628949165344\n",
      "Batch 46, Loss: -1.438746690750122\n",
      "Batch 47, Loss: 0.4449746012687683\n",
      "Batch 48, Loss: -1.43878173828125\n",
      "Batch 49, Loss: -1.4341564178466797\n",
      "Batch 50, Loss: -1.4350745677947998\n",
      "Batch 51, Loss: -1.4334971904754639\n",
      "Batch 52, Loss: -1.4533662796020508\n",
      "Batch 53, Loss: -1.451078176498413\n",
      "Batch 54, Loss: 0.44437211751937866\n",
      "Batch 55, Loss: -1.4475765228271484\n",
      "Batch 56, Loss: -1.4351887702941895\n",
      "Batch 57, Loss: 0.44891631603240967\n",
      "Batch 58, Loss: -1.4592854976654053\n",
      "Batch 59, Loss: 0.4621766209602356\n",
      "Batch 60, Loss: 0.47223567962646484\n",
      "Batch 61, Loss: 0.46278268098831177\n",
      "Batch 62, Loss: -1.4591031074523926\n",
      "Batch 63, Loss: -1.4520267248153687\n",
      "Batch 64, Loss: -1.4581600427627563\n",
      "Batch 65, Loss: 0.4904276430606842\n",
      "Batch 66, Loss: 0.44283807277679443\n",
      "Batch 67, Loss: -1.444367527961731\n",
      "Batch 68, Loss: -1.4785525798797607\n",
      "Batch 69, Loss: 0.46233922243118286\n",
      "Batch 70, Loss: 0.5022156834602356\n",
      "Batch 71, Loss: -1.4621750116348267\n",
      "Batch 72, Loss: -1.492896556854248\n",
      "Batch 73, Loss: 0.444543719291687\n",
      "Batch 74, Loss: 0.48007041215896606\n",
      "Batch 75, Loss: -1.4536405801773071\n",
      "Batch 76, Loss: 0.4640405774116516\n",
      "Batch 77, Loss: 0.4923941195011139\n",
      "Batch 78, Loss: 0.4791804552078247\n",
      "Batch 79, Loss: -1.4843814373016357\n",
      "Batch 80, Loss: 0.4777625501155853\n",
      "Batch 81, Loss: 0.4453064799308777\n",
      "Batch 82, Loss: 0.4352237582206726\n",
      "Batch 83, Loss: 0.4381527006626129\n",
      "Batch 84, Loss: 0.42426714301109314\n",
      "Batch 85, Loss: -1.4348831176757812\n",
      "Batch 86, Loss: -1.433072566986084\n",
      "Batch 87, Loss: 0.45139554142951965\n",
      "Batch 88, Loss: -1.4141199588775635\n",
      "Batch 89, Loss: 0.43191805481910706\n",
      "Batch 90, Loss: 0.42006707191467285\n",
      "Batch 91, Loss: -1.4548399448394775\n",
      "Batch 92, Loss: -1.438671350479126\n",
      "Batch 93, Loss: 0.41920286417007446\n",
      "Batch 94, Loss: 0.42016342282295227\n",
      "Batch 95, Loss: -1.423251986503601\n",
      "Batch 96, Loss: -1.4335426092147827\n",
      "Batch 97, Loss: -1.4232807159423828\n",
      "Batch 98, Loss: -1.4174883365631104\n",
      "Batch 99, Loss: 0.4173598289489746\n",
      "Batch 100, Loss: 0.4119965732097626\n",
      "Batch 101, Loss: 0.41933342814445496\n",
      "Batch 102, Loss: -1.4294383525848389\n",
      "Batch 103, Loss: -1.4450984001159668\n",
      "Batch 104, Loss: 0.4267590343952179\n",
      "Batch 105, Loss: 0.402785986661911\n",
      "Batch 106, Loss: 0.4332839548587799\n",
      "Batch 107, Loss: -1.465752363204956\n",
      "Batch 108, Loss: -1.4616066217422485\n",
      "Batch 109, Loss: 0.4137515723705292\n",
      "Batch 110, Loss: 0.4705613851547241\n",
      "Batch 111, Loss: 0.4135631322860718\n",
      "Batch 112, Loss: 0.43666478991508484\n",
      "Batch 113, Loss: 0.4522187113761902\n",
      "Batch 114, Loss: -1.462409496307373\n",
      "Batch 115, Loss: 0.4646282494068146\n",
      "Batch 116, Loss: -1.4617199897766113\n",
      "Batch 117, Loss: 0.4087539613246918\n",
      "Batch 118, Loss: 0.4384424686431885\n",
      "Batch 119, Loss: 0.44222310185432434\n",
      "Batch 120, Loss: 0.3794719874858856\n",
      "Batch 121, Loss: 0.3996797800064087\n",
      "Batch 122, Loss: 0.39582154154777527\n",
      "Batch 123, Loss: -1.3871841430664062\n",
      "Batch 124, Loss: 0.36269041895866394\n",
      "Batch 125, Loss: 0.39886900782585144\n",
      "Batch 126, Loss: 0.4384089708328247\n",
      "Batch 127, Loss: 0.431190550327301\n",
      "Batch 128, Loss: -1.349513292312622\n",
      "Batch 129, Loss: -1.397161602973938\n",
      "Batch 130, Loss: 0.3694424629211426\n",
      "Batch 131, Loss: -1.3444970846176147\n",
      "Batch 132, Loss: -1.3907458782196045\n",
      "Batch 133, Loss: 0.3916884660720825\n",
      "Batch 134, Loss: -1.4158786535263062\n",
      "Batch 135, Loss: 0.35305893421173096\n",
      "Batch 136, Loss: 0.2925468981266022\n",
      "Batch 137, Loss: -1.31080961227417\n",
      "Batch 138, Loss: -1.2658402919769287\n",
      "Batch 139, Loss: -1.326478362083435\n",
      "Batch 140, Loss: -1.350451946258545\n",
      "Batch 141, Loss: 0.29710328578948975\n",
      "Batch 142, Loss: -1.402543544769287\n",
      "Batch 143, Loss: 0.37975364923477173\n",
      "Batch 144, Loss: 0.3425441086292267\n",
      "Batch 145, Loss: 0.2950790226459503\n",
      "Batch 146, Loss: 0.3406502902507782\n",
      "Batch 147, Loss: -1.343787431716919\n",
      "Batch 148, Loss: 0.328802227973938\n",
      "Batch 149, Loss: -1.3434367179870605\n",
      "Batch 150, Loss: -1.3167169094085693\n",
      "Batch 151, Loss: -1.2820537090301514\n",
      "Batch 152, Loss: -1.3054014444351196\n",
      "Batch 153, Loss: 0.30269724130630493\n",
      "Batch 154, Loss: -1.3263146877288818\n",
      "Batch 155, Loss: 0.36545100808143616\n",
      "Batch 156, Loss: -1.2898783683776855\n",
      "Batch 157, Loss: 0.3284759819507599\n",
      "Batch 158, Loss: -1.3318421840667725\n",
      "Batch 159, Loss: -1.3983914852142334\n",
      "Batch 160, Loss: 0.3121581971645355\n",
      "Batch 161, Loss: -1.3298801183700562\n",
      "Batch 162, Loss: -1.3461259603500366\n",
      "Batch 163, Loss: 0.3380209803581238\n",
      "Batch 164, Loss: 0.34335094690322876\n",
      "Batch 165, Loss: -1.3435920476913452\n",
      "Batch 166, Loss: 0.37400609254837036\n",
      "Batch 167, Loss: 0.37336331605911255\n",
      "Batch 168, Loss: -1.3657692670822144\n",
      "Batch 169, Loss: -1.413031816482544\n",
      "Batch 170, Loss: -1.3208205699920654\n",
      "Batch 171, Loss: -1.4174538850784302\n",
      "Batch 172, Loss: 0.41728994250297546\n",
      "Batch 173, Loss: -1.3022618293762207\n",
      "Batch 174, Loss: -1.309044361114502\n",
      "Batch 175, Loss: -1.363806962966919\n",
      "Batch 176, Loss: -1.404062271118164\n",
      "Batch 177, Loss: -1.4071732759475708\n",
      "Batch 178, Loss: -1.3660173416137695\n",
      "Batch 179, Loss: 0.37518417835235596\n",
      "Batch 180, Loss: 0.3943970501422882\n",
      "Batch 181, Loss: 0.3857874572277069\n",
      "Batch 182, Loss: 0.399733304977417\n",
      "Batch 183, Loss: -1.408235788345337\n",
      "Batch 184, Loss: -1.437902808189392\n",
      "Batch 185, Loss: 0.3915078639984131\n",
      "Batch 186, Loss: 0.4042346477508545\n",
      "Batch 187, Loss: 0.4256874918937683\n",
      "Batch 188, Loss: -1.392390251159668\n",
      "Batch 189, Loss: -1.398282766342163\n",
      "Batch 190, Loss: 0.404238760471344\n",
      "Batch 191, Loss: -1.412280797958374\n",
      "Batch 192, Loss: 0.40923434495925903\n",
      "Batch 193, Loss: -1.4205039739608765\n",
      "Batch 194, Loss: 0.40872764587402344\n",
      "Batch 195, Loss: -1.4631949663162231\n",
      "Batch 196, Loss: -1.4242238998413086\n",
      "Batch 197, Loss: 0.40094611048698425\n",
      "Batch 198, Loss: 0.46098247170448303\n",
      "Batch 199, Loss: -1.4153889417648315\n",
      "Batch 200, Loss: 0.4309062063694\n",
      "Batch 201, Loss: 0.414686381816864\n",
      "Batch 202, Loss: 0.415752112865448\n",
      "Batch 203, Loss: 0.427050918340683\n",
      "Batch 204, Loss: -1.4034677743911743\n",
      "Batch 205, Loss: -1.4232981204986572\n",
      "Batch 206, Loss: 0.3974757790565491\n",
      "Batch 207, Loss: 0.4015618562698364\n",
      "Batch 208, Loss: 0.38389721512794495\n",
      "Batch 209, Loss: -1.401411533355713\n",
      "Batch 210, Loss: -1.3851284980773926\n",
      "Batch 211, Loss: -1.372796893119812\n",
      "Batch 212, Loss: -1.4017637968063354\n",
      "Batch 213, Loss: 0.3776057958602905\n",
      "Batch 214, Loss: 0.40788641571998596\n",
      "Batch 215, Loss: 0.4293406009674072\n",
      "Batch 216, Loss: 0.42758041620254517\n",
      "Batch 217, Loss: -1.3816648721694946\n",
      "Batch 218, Loss: 0.38508331775665283\n",
      "Batch 219, Loss: 0.38338005542755127\n",
      "Batch 220, Loss: -1.3831416368484497\n",
      "Batch 221, Loss: -1.3798518180847168\n",
      "Batch 222, Loss: 0.4146275520324707\n",
      "Batch 223, Loss: 0.37468963861465454\n",
      "Batch 224, Loss: 0.43003901839256287\n",
      "Batch 225, Loss: -1.3857154846191406\n",
      "Batch 226, Loss: 0.4256440997123718\n",
      "Batch 227, Loss: 0.42178234457969666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 228, Loss: 0.4188058376312256\n",
      "Batch 229, Loss: -1.4038524627685547\n",
      "Batch 230, Loss: -1.3746132850646973\n",
      "Batch 231, Loss: -1.3907124996185303\n",
      "Batch 232, Loss: -1.3856308460235596\n",
      "Batch 233, Loss: -1.3708367347717285\n",
      "Batch 234, Loss: -1.3821921348571777\n",
      "Batch 235, Loss: 0.39564380049705505\n",
      "Batch 236, Loss: 0.3846701979637146\n",
      "Batch 237, Loss: 0.38608527183532715\n",
      "Batch 238, Loss: 0.3986636996269226\n",
      "Batch 239, Loss: 0.37997469305992126\n",
      "Batch 240, Loss: 0.4023094177246094\n",
      "Batch 241, Loss: -1.3743582963943481\n",
      "Batch 242, Loss: -1.3822643756866455\n",
      "Batch 243, Loss: 0.3765608072280884\n",
      "Batch 244, Loss: -1.4019594192504883\n",
      "Batch 245, Loss: -1.3737740516662598\n",
      "Batch 246, Loss: 0.38643068075180054\n",
      "Batch 247, Loss: 0.3836946487426758\n",
      "Batch 248, Loss: -1.394130825996399\n",
      "Batch 249, Loss: 0.3905475437641144\n",
      "Batch 250, Loss: 0.3929651975631714\n",
      "Batch 251, Loss: -1.3728986978530884\n",
      "Batch 252, Loss: 0.38278859853744507\n",
      "Batch 253, Loss: 0.38345569372177124\n",
      "Batch 254, Loss: 0.3764747977256775\n",
      "Batch 255, Loss: 0.38061779737472534\n",
      "Batch 256, Loss: 0.38259702920913696\n",
      "Batch 257, Loss: 0.38429391384124756\n",
      "Batch 258, Loss: 0.3733745515346527\n",
      "Batch 259, Loss: -1.3650524616241455\n",
      "Batch 260, Loss: -1.3606278896331787\n",
      "Batch 261, Loss: -1.3693791627883911\n",
      "Batch 262, Loss: 0.3706774115562439\n",
      "Batch 263, Loss: 0.37058448791503906\n",
      "Batch 264, Loss: -1.3721494674682617\n",
      "Batch 265, Loss: 0.3689146041870117\n",
      "Batch 266, Loss: 0.3712504506111145\n",
      "Batch 267, Loss: 0.3590417802333832\n",
      "Batch 268, Loss: 0.3825492560863495\n",
      "Batch 269, Loss: 0.356338769197464\n",
      "Batch 270, Loss: 0.3606429398059845\n",
      "Batch 271, Loss: -1.3635791540145874\n",
      "Batch 272, Loss: 0.37064385414123535\n",
      "Batch 273, Loss: -1.3497618436813354\n",
      "Batch 274, Loss: -1.3485760688781738\n",
      "Batch 275, Loss: 0.35584330558776855\n",
      "Batch 276, Loss: 0.35722464323043823\n",
      "Batch 277, Loss: 0.3179851770401001\n",
      "Batch 278, Loss: -1.3487787246704102\n",
      "Batch 279, Loss: -1.3333203792572021\n",
      "Batch 280, Loss: 0.3509463369846344\n",
      "Batch 281, Loss: 0.3292933702468872\n",
      "Batch 282, Loss: -1.3480908870697021\n",
      "Batch 283, Loss: -1.3299108743667603\n",
      "Batch 284, Loss: -1.3317742347717285\n",
      "Batch 285, Loss: 0.357083797454834\n",
      "Batch 286, Loss: -1.3479610681533813\n",
      "Batch 287, Loss: -1.3442871570587158\n",
      "Batch 288, Loss: -1.3331737518310547\n",
      "Batch 289, Loss: 0.3546300530433655\n",
      "Batch 290, Loss: -1.357041835784912\n",
      "Batch 291, Loss: -1.3494735956192017\n",
      "Batch 292, Loss: 0.3443550169467926\n",
      "Batch 293, Loss: -1.3743946552276611\n",
      "Batch 294, Loss: -1.3754053115844727\n",
      "Batch 295, Loss: -1.3373029232025146\n",
      "Batch 296, Loss: 0.3703147768974304\n",
      "Batch 297, Loss: -1.3415043354034424\n",
      "Batch 298, Loss: -1.3509892225265503\n",
      "Batch 299, Loss: -1.3432495594024658\n",
      "Batch 300, Loss: 0.3538822531700134\n",
      "Batch 301, Loss: 0.37046489119529724\n",
      "Batch 302, Loss: -1.3802430629730225\n",
      "Batch 303, Loss: -1.3841744661331177\n",
      "Batch 304, Loss: 0.3641010820865631\n",
      "Batch 305, Loss: 0.3575257360935211\n",
      "Batch 306, Loss: -1.3668181896209717\n",
      "Batch 307, Loss: 0.3815121054649353\n",
      "Batch 308, Loss: -1.360537052154541\n",
      "Batch 309, Loss: 0.3665236830711365\n",
      "Batch 310, Loss: -1.3650366067886353\n",
      "Batch 311, Loss: -1.3659405708312988\n",
      "Batch 312, Loss: 0.38028988242149353\n",
      "Batch 313, Loss: 0.3693411648273468\n",
      "Batch 314, Loss: -1.3678721189498901\n",
      "Batch 315, Loss: -1.3675382137298584\n",
      "Batch 316, Loss: -1.369490623474121\n",
      "Batch 317, Loss: 0.373409628868103\n",
      "Batch 318, Loss: -1.3716293573379517\n",
      "Batch 319, Loss: 0.37492963671684265\n",
      "Batch 320, Loss: 0.3828321397304535\n",
      "Batch 321, Loss: -1.3722178936004639\n",
      "Batch 322, Loss: 0.38053634762763977\n",
      "Batch 323, Loss: 0.38519173860549927\n",
      "Batch 324, Loss: 0.3853287398815155\n",
      "Batch 325, Loss: -1.379227876663208\n",
      "Batch 326, Loss: -1.3821892738342285\n",
      "Batch 327, Loss: -1.3829526901245117\n",
      "Batch 328, Loss: -1.3740723133087158\n",
      "Batch 329, Loss: -1.3750061988830566\n",
      "Batch 330, Loss: -1.3858988285064697\n",
      "Batch 331, Loss: -1.3933892250061035\n",
      "Batch 332, Loss: 0.37781232595443726\n",
      "Batch 333, Loss: 0.3801122009754181\n",
      "Batch 334, Loss: -1.3803030252456665\n",
      "Batch 335, Loss: 0.3892574906349182\n",
      "Batch 336, Loss: -1.3821845054626465\n",
      "Batch 337, Loss: 0.3815934658050537\n",
      "Batch 338, Loss: -1.389339566230774\n",
      "Batch 339, Loss: 0.40087443590164185\n",
      "Batch 340, Loss: -1.3838940858840942\n",
      "Batch 341, Loss: -1.3833873271942139\n",
      "Batch 342, Loss: -1.393640398979187\n",
      "Batch 343, Loss: -1.3881443738937378\n",
      "Batch 344, Loss: -1.384448528289795\n",
      "Batch 345, Loss: -1.3950668573379517\n",
      "Batch 346, Loss: -1.391791820526123\n",
      "Batch 347, Loss: 0.3885219693183899\n",
      "Batch 348, Loss: 0.39382022619247437\n",
      "Batch 349, Loss: 0.3944934904575348\n",
      "Batch 350, Loss: 0.390553742647171\n",
      "Batch 351, Loss: 0.39269331097602844\n",
      "Batch 352, Loss: -1.388694405555725\n",
      "Batch 353, Loss: -1.392731785774231\n",
      "Batch 354, Loss: -1.389930248260498\n",
      "Batch 355, Loss: 0.3999837040901184\n",
      "Batch 356, Loss: -1.4015077352523804\n",
      "Batch 357, Loss: -1.3959324359893799\n",
      "Batch 358, Loss: 0.39384880661964417\n",
      "Batch 359, Loss: -1.3998106718063354\n",
      "Batch 360, Loss: -1.4072288274765015\n",
      "Batch 361, Loss: -1.4032750129699707\n",
      "Batch 362, Loss: -1.4111323356628418\n",
      "Batch 363, Loss: 0.39598631858825684\n",
      "Batch 364, Loss: 0.4016711115837097\n",
      "Batch 365, Loss: 0.4094233512878418\n",
      "Batch 366, Loss: -1.397845983505249\n",
      "Batch 367, Loss: 0.40988439321517944\n",
      "Batch 368, Loss: -1.3974884748458862\n",
      "Batch 369, Loss: -1.4027073383331299\n",
      "Batch 370, Loss: 0.3992522358894348\n",
      "Batch 371, Loss: -1.4166666269302368\n",
      "Batch 372, Loss: -1.408968448638916\n",
      "Batch 373, Loss: 0.40646374225616455\n",
      "Batch 374, Loss: 0.39989173412323\n",
      "Batch 375, Loss: 0.40523388981819153\n",
      "Batch 376, Loss: -1.4159284830093384\n",
      "Batch 377, Loss: -1.4004613161087036\n",
      "Batch 378, Loss: -1.4018313884735107\n",
      "Batch 379, Loss: 0.3999579846858978\n",
      "Batch 380, Loss: -1.4222915172576904\n",
      "Batch 381, Loss: 0.4151851534843445\n",
      "Batch 382, Loss: 0.42006716132164\n",
      "Batch 383, Loss: -1.4196125268936157\n",
      "Batch 384, Loss: 0.42121759057044983\n",
      "Batch 385, Loss: 0.41197094321250916\n",
      "Batch 386, Loss: -1.4184355735778809\n",
      "Batch 387, Loss: 0.4127485454082489\n",
      "Batch 388, Loss: 0.4080413579940796\n",
      "Batch 389, Loss: 0.40549352765083313\n",
      "Batch 390, Loss: 0.40469691157341003\n",
      "Batch 391, Loss: 0.40895482897758484\n",
      "Batch 392, Loss: -1.395108938217163\n",
      "Batch 393, Loss: -1.4079678058624268\n",
      "Batch 394, Loss: -1.3977762460708618\n",
      "Batch 395, Loss: -1.395416021347046\n",
      "Batch 396, Loss: -1.3961728811264038\n",
      "Batch 397, Loss: 0.399846613407135\n",
      "Batch 398, Loss: -1.3964080810546875\n",
      "Batch 399, Loss: -1.4057246446609497\n",
      "Training [10%]\tLoss: -0.4951\n",
      "Batch 0, Loss: 0.40564557909965515\n",
      "Batch 1, Loss: -1.4089949131011963\n",
      "Batch 2, Loss: 0.40043920278549194\n",
      "Batch 3, Loss: -1.4043046236038208\n",
      "Batch 4, Loss: -1.402860403060913\n",
      "Batch 5, Loss: -1.3987232446670532\n",
      "Batch 6, Loss: 0.40142953395843506\n",
      "Batch 7, Loss: -1.3999971151351929\n",
      "Batch 8, Loss: -1.4015618562698364\n",
      "Batch 9, Loss: -1.4121019840240479\n",
      "Batch 10, Loss: 0.41199514269828796\n",
      "Batch 11, Loss: -1.4084980487823486\n",
      "Batch 12, Loss: -1.4238290786743164\n",
      "Batch 13, Loss: 0.41069263219833374\n",
      "Batch 14, Loss: 0.4160667657852173\n",
      "Batch 15, Loss: 0.4146098494529724\n",
      "Batch 16, Loss: 0.4248175323009491\n",
      "Batch 17, Loss: 0.4090164303779602\n",
      "Batch 18, Loss: -1.4151966571807861\n",
      "Batch 19, Loss: -1.4214214086532593\n",
      "Batch 20, Loss: -1.4101603031158447\n",
      "Batch 21, Loss: 0.41148170828819275\n",
      "Batch 22, Loss: 0.413103312253952\n",
      "Batch 23, Loss: -1.4123286008834839\n",
      "Batch 24, Loss: -1.4074339866638184\n",
      "Batch 25, Loss: -1.410334825515747\n",
      "Batch 26, Loss: -1.4120478630065918\n",
      "Batch 27, Loss: -1.4076143503189087\n",
      "Batch 28, Loss: -1.4181126356124878\n",
      "Batch 29, Loss: -1.4150722026824951\n",
      "Batch 30, Loss: -1.416520357131958\n",
      "Batch 31, Loss: 0.42086493968963623\n",
      "Batch 32, Loss: 0.4251609444618225\n",
      "Batch 33, Loss: 0.41810309886932373\n",
      "Batch 34, Loss: 0.430253267288208\n",
      "Batch 35, Loss: 0.4235645532608032\n",
      "Batch 36, Loss: 0.423840194940567\n",
      "Batch 37, Loss: 0.4202970266342163\n",
      "Batch 38, Loss: -1.4343866109848022\n",
      "Batch 39, Loss: -1.4161763191223145\n",
      "Batch 40, Loss: 0.4135831594467163\n",
      "Batch 41, Loss: 0.4170563817024231\n",
      "Batch 42, Loss: 0.42049410939216614\n",
      "Batch 43, Loss: 0.43105894327163696\n",
      "Batch 44, Loss: 0.4278579354286194\n",
      "Batch 45, Loss: 0.41378533840179443\n",
      "Batch 46, Loss: 0.4061785936355591\n",
      "Batch 47, Loss: 0.40435561537742615\n",
      "Batch 48, Loss: 0.4156603217124939\n",
      "Batch 49, Loss: 0.40270698070526123\n",
      "Batch 50, Loss: -1.4093424081802368\n",
      "Batch 51, Loss: 0.41151052713394165\n",
      "Batch 52, Loss: -1.4046496152877808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 53, Loss: -1.4020564556121826\n",
      "Batch 54, Loss: 0.4079696238040924\n",
      "Batch 55, Loss: 0.4024907350540161\n",
      "Batch 56, Loss: -1.393786907196045\n",
      "Batch 57, Loss: -1.3965513706207275\n",
      "Batch 58, Loss: -1.3915069103240967\n",
      "Batch 59, Loss: -1.3936184644699097\n",
      "Batch 60, Loss: 0.3938482403755188\n",
      "Batch 61, Loss: -1.3994624614715576\n",
      "Batch 62, Loss: 0.3961449861526489\n",
      "Batch 63, Loss: -1.3939099311828613\n",
      "Batch 64, Loss: 0.39533939957618713\n",
      "Batch 65, Loss: -1.4029643535614014\n",
      "Batch 66, Loss: 0.3930857181549072\n",
      "Batch 67, Loss: 0.3933814465999603\n",
      "Batch 68, Loss: 0.3940165638923645\n",
      "Batch 69, Loss: 0.3927984833717346\n",
      "Batch 70, Loss: 0.39175698161125183\n",
      "Batch 71, Loss: -1.395878553390503\n",
      "Batch 72, Loss: -1.391056776046753\n",
      "Batch 73, Loss: 0.4017525911331177\n",
      "Batch 74, Loss: 0.40757372975349426\n",
      "Batch 75, Loss: -1.3891041278839111\n",
      "Batch 76, Loss: -1.3911046981811523\n",
      "Batch 77, Loss: -1.3871310949325562\n",
      "Batch 78, Loss: 0.39309170842170715\n",
      "Batch 79, Loss: -1.3915469646453857\n",
      "Batch 80, Loss: -1.3870888948440552\n",
      "Batch 81, Loss: 0.3879121243953705\n",
      "Batch 82, Loss: -1.388391375541687\n",
      "Batch 83, Loss: -1.3978056907653809\n",
      "Batch 84, Loss: -1.397158145904541\n",
      "Batch 85, Loss: 0.3915438652038574\n",
      "Batch 86, Loss: -1.3892418146133423\n",
      "Batch 87, Loss: -1.3999873399734497\n",
      "Batch 88, Loss: -1.3923871517181396\n",
      "Batch 89, Loss: 0.39877620339393616\n",
      "Batch 90, Loss: -1.4040592908859253\n",
      "Batch 91, Loss: 0.40437835454940796\n",
      "Batch 92, Loss: -1.4016538858413696\n",
      "Batch 93, Loss: 0.4043589234352112\n",
      "Batch 94, Loss: -1.4075901508331299\n",
      "Batch 95, Loss: 0.4135667085647583\n",
      "Batch 96, Loss: -1.4115122556686401\n",
      "Batch 97, Loss: 0.3934504985809326\n",
      "Batch 98, Loss: 0.40846559405326843\n",
      "Batch 99, Loss: 0.4028995633125305\n",
      "Batch 100, Loss: 0.39633914828300476\n",
      "Batch 101, Loss: 0.39753881096839905\n",
      "Batch 102, Loss: 0.39257189631462097\n",
      "Batch 103, Loss: 0.3935129940509796\n",
      "Batch 104, Loss: -1.3899458646774292\n",
      "Batch 105, Loss: -1.4050366878509521\n",
      "Batch 106, Loss: 0.3977409303188324\n",
      "Batch 107, Loss: 0.39277565479278564\n",
      "Batch 108, Loss: 0.3868846297264099\n",
      "Batch 109, Loss: -1.395444631576538\n",
      "Batch 110, Loss: 0.3933391571044922\n",
      "Batch 111, Loss: -1.4008457660675049\n",
      "Batch 112, Loss: -1.3930647373199463\n",
      "Batch 113, Loss: -1.3957557678222656\n",
      "Batch 114, Loss: -1.3867266178131104\n",
      "Batch 115, Loss: -1.3816825151443481\n",
      "Batch 116, Loss: 0.38766220211982727\n",
      "Batch 117, Loss: -1.389134168624878\n",
      "Batch 118, Loss: -1.4003535509109497\n",
      "Batch 119, Loss: 0.38967373967170715\n",
      "Batch 120, Loss: -1.401202917098999\n",
      "Batch 121, Loss: 0.3981262147426605\n",
      "Batch 122, Loss: -1.3847415447235107\n",
      "Batch 123, Loss: 0.40046554803848267\n",
      "Batch 124, Loss: 0.4031422436237335\n",
      "Batch 125, Loss: -1.4045674800872803\n",
      "Batch 126, Loss: 0.3911203145980835\n",
      "Batch 127, Loss: -1.4044408798217773\n",
      "Batch 128, Loss: 0.4032001197338104\n",
      "Batch 129, Loss: -1.3889074325561523\n",
      "Batch 130, Loss: -1.4028640985488892\n",
      "Batch 131, Loss: -1.3992927074432373\n",
      "Batch 132, Loss: 0.392211377620697\n",
      "Batch 133, Loss: 0.40685415267944336\n",
      "Batch 134, Loss: 0.39396265149116516\n",
      "Batch 135, Loss: -1.3926273584365845\n",
      "Batch 136, Loss: -1.3927137851715088\n",
      "Batch 137, Loss: -1.406546711921692\n",
      "Batch 138, Loss: 0.4010356664657593\n",
      "Batch 139, Loss: 0.39789068698883057\n",
      "Batch 140, Loss: -1.4035429954528809\n",
      "Batch 141, Loss: -1.4019156694412231\n",
      "Batch 142, Loss: 0.4026724696159363\n",
      "Batch 143, Loss: -1.3969500064849854\n",
      "Batch 144, Loss: 0.40096282958984375\n",
      "Batch 145, Loss: 0.3973899483680725\n",
      "Batch 146, Loss: -1.396539330482483\n",
      "Batch 147, Loss: 0.3954731225967407\n",
      "Batch 148, Loss: 0.3994072675704956\n",
      "Batch 149, Loss: -1.3936256170272827\n",
      "Batch 150, Loss: 0.3946624994277954\n",
      "Batch 151, Loss: 0.39570945501327515\n",
      "Batch 152, Loss: -1.4133505821228027\n",
      "Batch 153, Loss: -1.3940373659133911\n",
      "Batch 154, Loss: -1.406196117401123\n",
      "Batch 155, Loss: 0.3924374282360077\n",
      "Batch 156, Loss: 0.3904227316379547\n",
      "Batch 157, Loss: 0.3907660245895386\n",
      "Batch 158, Loss: 0.40163615345954895\n",
      "Batch 159, Loss: -1.4117317199707031\n",
      "Batch 160, Loss: 0.3946305215358734\n",
      "Batch 161, Loss: -1.397453784942627\n",
      "Batch 162, Loss: 0.3942667543888092\n",
      "Batch 163, Loss: 0.39235788583755493\n",
      "Batch 164, Loss: 0.39168399572372437\n",
      "Batch 165, Loss: 0.39275187253952026\n",
      "Batch 166, Loss: -1.390730619430542\n",
      "Batch 167, Loss: -1.3932909965515137\n",
      "Batch 168, Loss: 0.38559573888778687\n",
      "Batch 169, Loss: 0.39439713954925537\n",
      "Batch 170, Loss: 0.3840869069099426\n",
      "Batch 171, Loss: 0.38679954409599304\n",
      "Batch 172, Loss: 0.3814151883125305\n",
      "Batch 173, Loss: 0.38596412539482117\n",
      "Batch 174, Loss: -1.4028140306472778\n",
      "Batch 175, Loss: -1.3861802816390991\n",
      "Batch 176, Loss: 0.38514244556427\n",
      "Batch 177, Loss: -1.3804274797439575\n",
      "Batch 178, Loss: -1.4003691673278809\n",
      "Batch 179, Loss: -1.377819538116455\n",
      "Batch 180, Loss: -1.3855074644088745\n",
      "Batch 181, Loss: -1.379966378211975\n",
      "Batch 182, Loss: -1.3807114362716675\n",
      "Batch 183, Loss: 0.38521188497543335\n",
      "Batch 184, Loss: 0.37889909744262695\n",
      "Batch 185, Loss: -1.3798141479492188\n",
      "Batch 186, Loss: 0.3903570771217346\n",
      "Batch 187, Loss: -1.3772163391113281\n",
      "Batch 188, Loss: 0.37659913301467896\n",
      "Batch 189, Loss: 0.3844817876815796\n",
      "Batch 190, Loss: 0.4008082449436188\n",
      "Batch 191, Loss: -1.397036075592041\n",
      "Batch 192, Loss: -1.3756544589996338\n",
      "Batch 193, Loss: 0.3837282955646515\n",
      "Batch 194, Loss: -1.3758974075317383\n",
      "Batch 195, Loss: -1.380843997001648\n",
      "Batch 196, Loss: -1.3770198822021484\n",
      "Batch 197, Loss: -1.382561206817627\n",
      "Batch 198, Loss: -1.3781459331512451\n",
      "Batch 199, Loss: 0.38628605008125305\n",
      "Batch 200, Loss: -1.3865301609039307\n",
      "Batch 201, Loss: 0.3902895748615265\n",
      "Batch 202, Loss: 0.39119887351989746\n",
      "Batch 203, Loss: -1.3882087469100952\n",
      "Batch 204, Loss: -1.4051501750946045\n",
      "Batch 205, Loss: -1.3948956727981567\n",
      "Batch 206, Loss: 0.39171358942985535\n",
      "Batch 207, Loss: 0.38613927364349365\n",
      "Batch 208, Loss: -1.4104821681976318\n",
      "Batch 209, Loss: 0.3976247012615204\n",
      "Batch 210, Loss: 0.390633761882782\n",
      "Batch 211, Loss: 0.3931378126144409\n",
      "Batch 212, Loss: -1.400622844696045\n",
      "Batch 213, Loss: 0.3897388279438019\n",
      "Batch 214, Loss: -1.3867539167404175\n",
      "Batch 215, Loss: -1.3987070322036743\n",
      "Batch 216, Loss: -1.3937581777572632\n",
      "Batch 217, Loss: -1.395331859588623\n",
      "Batch 218, Loss: -1.3913949728012085\n",
      "Batch 219, Loss: -1.3933879137039185\n",
      "Batch 220, Loss: 0.41714775562286377\n",
      "Batch 221, Loss: 0.3929262161254883\n",
      "Batch 222, Loss: 0.3896057605743408\n",
      "Batch 223, Loss: -1.3890876770019531\n",
      "Batch 224, Loss: 0.3899655342102051\n",
      "Batch 225, Loss: 0.3932516574859619\n",
      "Batch 226, Loss: -1.3915506601333618\n",
      "Batch 227, Loss: 0.39717897772789\n",
      "Batch 228, Loss: -1.396700143814087\n",
      "Batch 229, Loss: -1.3955332040786743\n",
      "Batch 230, Loss: -1.3934718370437622\n",
      "Batch 231, Loss: -1.3973760604858398\n",
      "Batch 232, Loss: -1.3927900791168213\n",
      "Batch 233, Loss: -1.3963708877563477\n",
      "Batch 234, Loss: 0.394553542137146\n",
      "Batch 235, Loss: 0.39349061250686646\n",
      "Batch 236, Loss: 0.41797804832458496\n",
      "Batch 237, Loss: -1.392657995223999\n",
      "Batch 238, Loss: 0.39651328325271606\n",
      "Batch 239, Loss: -1.410226583480835\n",
      "Batch 240, Loss: 0.40351414680480957\n",
      "Batch 241, Loss: 0.4037986397743225\n",
      "Batch 242, Loss: -1.3986594676971436\n",
      "Batch 243, Loss: 0.3982453942298889\n",
      "Batch 244, Loss: -1.3963618278503418\n",
      "Batch 245, Loss: 0.4017629027366638\n",
      "Batch 246, Loss: 0.39727362990379333\n",
      "Batch 247, Loss: 0.40023043751716614\n",
      "Batch 248, Loss: 0.40145552158355713\n",
      "Batch 249, Loss: -1.4023487567901611\n",
      "Batch 250, Loss: -1.390647292137146\n",
      "Batch 251, Loss: 0.3980160057544708\n",
      "Batch 252, Loss: -1.4106969833374023\n",
      "Batch 253, Loss: 0.3931781053543091\n",
      "Batch 254, Loss: -1.3894786834716797\n",
      "Batch 255, Loss: 0.41759341955184937\n",
      "Batch 256, Loss: -1.3891183137893677\n",
      "Batch 257, Loss: 0.3948209583759308\n",
      "Batch 258, Loss: -1.3951839208602905\n",
      "Batch 259, Loss: -1.4175429344177246\n",
      "Batch 260, Loss: -1.3980250358581543\n",
      "Batch 261, Loss: 0.3933887183666229\n",
      "Batch 262, Loss: -1.4008270502090454\n",
      "Batch 263, Loss: -1.3907047510147095\n",
      "Batch 264, Loss: -1.4093477725982666\n",
      "Batch 265, Loss: 0.3934454321861267\n",
      "Batch 266, Loss: -1.4083362817764282\n",
      "Batch 267, Loss: 0.4023159146308899\n",
      "Batch 268, Loss: 0.412714421749115\n",
      "Batch 269, Loss: -1.3968353271484375\n",
      "Batch 270, Loss: 0.39951497316360474\n",
      "Batch 271, Loss: 0.40834593772888184\n",
      "Batch 272, Loss: 0.3983234763145447\n",
      "Batch 273, Loss: 0.3968675434589386\n",
      "Batch 274, Loss: -1.4066609144210815\n",
      "Batch 275, Loss: -1.3964641094207764\n",
      "Batch 276, Loss: -1.399296522140503\n",
      "Batch 277, Loss: -1.413453221321106\n",
      "Batch 278, Loss: -1.3967161178588867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 279, Loss: -1.4226489067077637\n",
      "Batch 280, Loss: 0.4036082923412323\n",
      "Batch 281, Loss: -1.4024360179901123\n",
      "Batch 282, Loss: -1.3967269659042358\n",
      "Batch 283, Loss: -1.4144506454467773\n",
      "Batch 284, Loss: 0.40541085600852966\n",
      "Batch 285, Loss: 0.40259432792663574\n",
      "Batch 286, Loss: 0.39531847834587097\n",
      "Batch 287, Loss: -1.4041094779968262\n",
      "Batch 288, Loss: -1.4108893871307373\n",
      "Batch 289, Loss: -1.4113961458206177\n",
      "Batch 290, Loss: -1.4204492568969727\n",
      "Batch 291, Loss: -1.4085791110992432\n",
      "Batch 292, Loss: -1.4027037620544434\n",
      "Batch 293, Loss: -1.430510401725769\n",
      "Batch 294, Loss: 0.4189351499080658\n",
      "Batch 295, Loss: 0.4146530032157898\n",
      "Batch 296, Loss: 0.42374399304389954\n",
      "Batch 297, Loss: -1.4271438121795654\n",
      "Batch 298, Loss: -1.4149737358093262\n",
      "Batch 299, Loss: 0.41904541850090027\n",
      "Batch 300, Loss: -1.4099440574645996\n",
      "Batch 301, Loss: 0.4391289949417114\n",
      "Batch 302, Loss: 0.4157813787460327\n",
      "Batch 303, Loss: 0.41487687826156616\n",
      "Batch 304, Loss: 0.41952747106552124\n",
      "Batch 305, Loss: -1.4182634353637695\n",
      "Batch 306, Loss: -1.4323410987854004\n",
      "Batch 307, Loss: -1.4199353456497192\n",
      "Batch 308, Loss: 0.405246376991272\n",
      "Batch 309, Loss: 0.4211425185203552\n",
      "Batch 310, Loss: -1.4303326606750488\n",
      "Batch 311, Loss: -1.4157031774520874\n",
      "Batch 312, Loss: 0.4382765293121338\n",
      "Batch 313, Loss: 0.4142551124095917\n",
      "Batch 314, Loss: -1.4357566833496094\n",
      "Batch 315, Loss: -1.4175071716308594\n",
      "Batch 316, Loss: -1.4424339532852173\n",
      "Batch 317, Loss: -1.417536735534668\n",
      "Batch 318, Loss: -1.412267804145813\n",
      "Batch 319, Loss: -1.422287940979004\n",
      "Batch 320, Loss: 0.4374641180038452\n",
      "Batch 321, Loss: 0.41316473484039307\n",
      "Batch 322, Loss: -1.4199247360229492\n",
      "Batch 323, Loss: 0.42648717761039734\n",
      "Batch 324, Loss: 0.43436354398727417\n",
      "Batch 325, Loss: -1.4144186973571777\n",
      "Batch 326, Loss: 0.41848093271255493\n",
      "Batch 327, Loss: -1.411108374595642\n",
      "Batch 328, Loss: 0.42475801706314087\n",
      "Batch 329, Loss: -1.4096834659576416\n",
      "Batch 330, Loss: 0.40582275390625\n",
      "Batch 331, Loss: 0.42063450813293457\n",
      "Batch 332, Loss: 0.42601048946380615\n",
      "Batch 333, Loss: 0.422951340675354\n",
      "Batch 334, Loss: 0.42009198665618896\n",
      "Batch 335, Loss: -1.4121822118759155\n",
      "Batch 336, Loss: -1.4182095527648926\n",
      "Batch 337, Loss: 0.39862293004989624\n",
      "Batch 338, Loss: -1.4158402681350708\n",
      "Batch 339, Loss: -1.393940806388855\n",
      "Batch 340, Loss: 0.403710275888443\n",
      "Batch 341, Loss: -1.418116569519043\n",
      "Batch 342, Loss: 0.393243670463562\n",
      "Batch 343, Loss: -1.4396686553955078\n",
      "Batch 344, Loss: 0.40630626678466797\n",
      "Batch 345, Loss: 0.3976789712905884\n",
      "Batch 346, Loss: 0.40296751260757446\n",
      "Batch 347, Loss: -1.3861298561096191\n",
      "Batch 348, Loss: 0.40421923995018005\n",
      "Batch 349, Loss: 0.35406360030174255\n",
      "Batch 350, Loss: 0.29807934165000916\n",
      "Batch 351, Loss: 0.4162500500679016\n",
      "Batch 352, Loss: -1.3901073932647705\n",
      "Batch 353, Loss: 0.4119347035884857\n",
      "Batch 354, Loss: -1.2993930578231812\n",
      "Batch 355, Loss: -1.3670921325683594\n",
      "Batch 356, Loss: 0.335077166557312\n",
      "Batch 357, Loss: 0.13415710628032684\n",
      "Batch 358, Loss: -1.2507615089416504\n",
      "Batch 359, Loss: -1.2588125467300415\n",
      "Batch 360, Loss: -1.2147513628005981\n",
      "Batch 361, Loss: 0.36854127049446106\n",
      "Batch 362, Loss: -1.2669135332107544\n",
      "Batch 363, Loss: 0.32604071497917175\n",
      "Batch 364, Loss: 0.4179002642631531\n",
      "Batch 365, Loss: 0.19778607785701752\n",
      "Batch 366, Loss: -1.194471001625061\n",
      "Batch 367, Loss: -1.3582228422164917\n",
      "Batch 368, Loss: -1.1898584365844727\n",
      "Batch 369, Loss: -1.3197345733642578\n",
      "Batch 370, Loss: 0.3675922155380249\n",
      "Batch 371, Loss: 0.3716554641723633\n",
      "Batch 372, Loss: 0.29826122522354126\n",
      "Batch 373, Loss: 0.38024866580963135\n",
      "Batch 374, Loss: -1.3674335479736328\n",
      "Batch 375, Loss: -1.3779182434082031\n",
      "Batch 376, Loss: -1.371598720550537\n",
      "Batch 377, Loss: -1.3697586059570312\n",
      "Batch 378, Loss: 0.4106101393699646\n",
      "Batch 379, Loss: -1.3862056732177734\n",
      "Batch 380, Loss: 0.31734317541122437\n",
      "Batch 381, Loss: -1.372416377067566\n",
      "Batch 382, Loss: 0.4137854278087616\n",
      "Batch 383, Loss: 0.3862685263156891\n",
      "Batch 384, Loss: 0.4303128123283386\n",
      "Batch 385, Loss: -1.300990343093872\n",
      "Batch 386, Loss: -1.4294387102127075\n",
      "Batch 387, Loss: 0.32090649008750916\n",
      "Batch 388, Loss: 0.4119158983230591\n",
      "Batch 389, Loss: -1.3898365497589111\n",
      "Batch 390, Loss: -1.3859541416168213\n",
      "Batch 391, Loss: -1.399045705795288\n",
      "Batch 392, Loss: 0.34169256687164307\n",
      "Batch 393, Loss: -1.3947066068649292\n",
      "Batch 394, Loss: 0.3932034969329834\n",
      "Batch 395, Loss: -1.3983807563781738\n",
      "Batch 396, Loss: 0.31686750054359436\n",
      "Batch 397, Loss: 0.3992484211921692\n",
      "Batch 398, Loss: 0.39447787404060364\n",
      "Batch 399, Loss: 0.37102481722831726\n",
      "Training [20%]\tLoss: -0.4987\n",
      "Batch 0, Loss: -1.4407856464385986\n",
      "Batch 1, Loss: -1.410635232925415\n",
      "Batch 2, Loss: -1.4102665185928345\n",
      "Batch 3, Loss: -1.3804267644882202\n",
      "Batch 4, Loss: 0.35470977425575256\n",
      "Batch 5, Loss: -1.270383358001709\n",
      "Batch 6, Loss: -1.2557528018951416\n",
      "Batch 7, Loss: -1.4265544414520264\n",
      "Batch 8, Loss: 0.4459065794944763\n",
      "Batch 9, Loss: 0.38984549045562744\n",
      "Batch 10, Loss: 0.3113335072994232\n",
      "Batch 11, Loss: -1.3716530799865723\n",
      "Batch 12, Loss: -1.4301854372024536\n",
      "Batch 13, Loss: 0.4188401401042938\n",
      "Batch 14, Loss: -1.4159053564071655\n",
      "Batch 15, Loss: -1.3948930501937866\n",
      "Batch 16, Loss: -1.4042599201202393\n",
      "Batch 17, Loss: 0.39503568410873413\n",
      "Batch 18, Loss: 0.38710692524909973\n",
      "Batch 19, Loss: -1.3211114406585693\n",
      "Batch 20, Loss: -1.2874106168746948\n",
      "Batch 21, Loss: 0.3025219738483429\n",
      "Batch 22, Loss: -1.4089330434799194\n",
      "Batch 23, Loss: 0.3462669551372528\n",
      "Batch 24, Loss: 0.443298876285553\n",
      "Batch 25, Loss: 0.4230799973011017\n",
      "Batch 26, Loss: -1.3892922401428223\n",
      "Batch 27, Loss: 0.3896452784538269\n",
      "Batch 28, Loss: -1.3746367692947388\n",
      "Batch 29, Loss: -1.3761264085769653\n",
      "Batch 30, Loss: 0.39502671360969543\n",
      "Batch 31, Loss: -1.331704020500183\n",
      "Batch 32, Loss: 0.4498196244239807\n",
      "Batch 33, Loss: 0.40088194608688354\n",
      "Batch 34, Loss: 0.452467143535614\n",
      "Batch 35, Loss: 0.39439401030540466\n",
      "Batch 36, Loss: -1.439968466758728\n",
      "Batch 37, Loss: 0.37553519010543823\n",
      "Batch 38, Loss: -1.4230633974075317\n",
      "Batch 39, Loss: 0.439033567905426\n",
      "Batch 40, Loss: -1.4158601760864258\n",
      "Batch 41, Loss: 0.39538466930389404\n",
      "Batch 42, Loss: -1.4147511720657349\n",
      "Batch 43, Loss: 0.348519504070282\n",
      "Batch 44, Loss: -1.422176718711853\n",
      "Batch 45, Loss: 0.40867578983306885\n",
      "Batch 46, Loss: -1.3914718627929688\n",
      "Batch 47, Loss: -1.4224064350128174\n",
      "Batch 48, Loss: 0.4383115768432617\n",
      "Batch 49, Loss: 0.41936010122299194\n",
      "Batch 50, Loss: 0.40905073285102844\n",
      "Batch 51, Loss: -1.3581644296646118\n",
      "Batch 52, Loss: -1.352229118347168\n",
      "Batch 53, Loss: -1.33451247215271\n",
      "Batch 54, Loss: -1.422829508781433\n",
      "Batch 55, Loss: 0.39800700545310974\n",
      "Batch 56, Loss: -1.38254976272583\n",
      "Batch 57, Loss: 0.4263198971748352\n",
      "Batch 58, Loss: -1.3974196910858154\n",
      "Batch 59, Loss: 0.4176209568977356\n",
      "Batch 60, Loss: 0.39177781343460083\n",
      "Batch 61, Loss: -1.4027197360992432\n",
      "Batch 62, Loss: 0.3886352777481079\n",
      "Batch 63, Loss: -1.4077973365783691\n",
      "Batch 64, Loss: -1.4175845384597778\n",
      "Batch 65, Loss: -1.4390708208084106\n",
      "Batch 66, Loss: 0.3976817727088928\n",
      "Batch 67, Loss: 0.4137548506259918\n",
      "Batch 68, Loss: -1.4231857061386108\n",
      "Batch 69, Loss: -1.3938119411468506\n",
      "Batch 70, Loss: -1.4257478713989258\n",
      "Batch 71, Loss: -1.407386302947998\n",
      "Batch 72, Loss: 0.3981282114982605\n",
      "Batch 73, Loss: -1.4300965070724487\n",
      "Batch 74, Loss: -1.4546782970428467\n",
      "Batch 75, Loss: 0.3977905809879303\n",
      "Batch 76, Loss: 0.43467193841934204\n",
      "Batch 77, Loss: -1.4158756732940674\n",
      "Batch 78, Loss: -1.458552598953247\n",
      "Batch 79, Loss: -1.433585286140442\n",
      "Batch 80, Loss: -1.4199697971343994\n",
      "Batch 81, Loss: 0.4365916848182678\n",
      "Batch 82, Loss: 0.4374220669269562\n",
      "Batch 83, Loss: 0.45034486055374146\n",
      "Batch 84, Loss: 0.43785524368286133\n",
      "Batch 85, Loss: -1.4072208404541016\n",
      "Batch 86, Loss: 0.40758568048477173\n",
      "Batch 87, Loss: -1.447160005569458\n",
      "Batch 88, Loss: 0.4267760217189789\n",
      "Batch 89, Loss: 0.43425142765045166\n",
      "Batch 90, Loss: 0.4025419056415558\n",
      "Batch 91, Loss: 0.43999671936035156\n",
      "Batch 92, Loss: -1.3980035781860352\n",
      "Batch 93, Loss: 0.4276314973831177\n",
      "Batch 94, Loss: 0.41705521941185\n",
      "Batch 95, Loss: 0.41461923718452454\n",
      "Batch 96, Loss: -1.41659677028656\n",
      "Batch 97, Loss: 0.413722425699234\n",
      "Batch 98, Loss: -1.4190014600753784\n",
      "Batch 99, Loss: 0.39844903349876404\n",
      "Batch 100, Loss: -1.415332317352295\n",
      "Batch 101, Loss: 0.4095449149608612\n",
      "Batch 102, Loss: 0.44146913290023804\n",
      "Batch 103, Loss: 0.3900711238384247\n",
      "Batch 104, Loss: 0.4110689163208008\n",
      "Batch 105, Loss: 0.4004223942756653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 106, Loss: -1.3964295387268066\n",
      "Batch 107, Loss: -1.3991618156433105\n",
      "Batch 108, Loss: -1.407484769821167\n",
      "Batch 109, Loss: -1.4014487266540527\n",
      "Batch 110, Loss: -1.3972623348236084\n",
      "Batch 111, Loss: 0.37490057945251465\n",
      "Batch 112, Loss: -1.4017854928970337\n",
      "Batch 113, Loss: -1.369051456451416\n",
      "Batch 114, Loss: 0.3707854151725769\n",
      "Batch 115, Loss: -1.365692377090454\n",
      "Batch 116, Loss: 0.4092620611190796\n",
      "Batch 117, Loss: 0.3820072412490845\n",
      "Batch 118, Loss: 0.4327285587787628\n",
      "Batch 119, Loss: -1.3591110706329346\n",
      "Batch 120, Loss: 0.403566837310791\n",
      "Batch 121, Loss: -1.367992877960205\n",
      "Batch 122, Loss: -1.4341068267822266\n",
      "Batch 123, Loss: 0.3954926133155823\n",
      "Batch 124, Loss: 0.40061402320861816\n",
      "Batch 125, Loss: -1.402172327041626\n",
      "Batch 126, Loss: 0.37561333179473877\n",
      "Batch 127, Loss: -1.4023786783218384\n",
      "Batch 128, Loss: 0.41862785816192627\n",
      "Batch 129, Loss: 0.3968414068222046\n",
      "Batch 130, Loss: 0.3772578537464142\n",
      "Batch 131, Loss: 0.4019443094730377\n",
      "Batch 132, Loss: 0.4281763434410095\n",
      "Batch 133, Loss: -1.3996049165725708\n",
      "Batch 134, Loss: -1.3692572116851807\n",
      "Batch 135, Loss: 0.418293833732605\n",
      "Batch 136, Loss: 0.393568754196167\n",
      "Batch 137, Loss: -1.3878523111343384\n",
      "Batch 138, Loss: 0.3959748148918152\n",
      "Batch 139, Loss: 0.43154793977737427\n",
      "Batch 140, Loss: 0.3920392692089081\n",
      "Batch 141, Loss: -1.3571529388427734\n",
      "Batch 142, Loss: -1.3674263954162598\n",
      "Batch 143, Loss: 0.3855065107345581\n",
      "Batch 144, Loss: 0.3909527361392975\n",
      "Batch 145, Loss: 0.3852488398551941\n",
      "Batch 146, Loss: 0.40209317207336426\n",
      "Batch 147, Loss: 0.3860608637332916\n",
      "Batch 148, Loss: 0.38308069109916687\n",
      "Batch 149, Loss: 0.38483455777168274\n",
      "Batch 150, Loss: 0.39273548126220703\n",
      "Batch 151, Loss: 0.37682825326919556\n",
      "Batch 152, Loss: 0.38640356063842773\n",
      "Batch 153, Loss: 0.3976147472858429\n",
      "Batch 154, Loss: -1.3408359289169312\n",
      "Batch 155, Loss: -1.3813329935073853\n",
      "Batch 156, Loss: 0.3399413228034973\n",
      "Batch 157, Loss: -1.3779098987579346\n",
      "Batch 158, Loss: -1.372367262840271\n",
      "Batch 159, Loss: -1.3632746934890747\n",
      "Batch 160, Loss: -1.3947643041610718\n",
      "Batch 161, Loss: 0.380045086145401\n",
      "Batch 162, Loss: -1.3872780799865723\n",
      "Batch 163, Loss: 0.4000261127948761\n",
      "Batch 164, Loss: -1.3368489742279053\n",
      "Batch 165, Loss: -1.3751709461212158\n",
      "Batch 166, Loss: -1.3656657934188843\n",
      "Batch 167, Loss: 0.36434006690979004\n",
      "Batch 168, Loss: -1.3566429615020752\n",
      "Batch 169, Loss: 0.3730839490890503\n",
      "Batch 170, Loss: 0.3524671494960785\n",
      "Batch 171, Loss: -1.391014575958252\n",
      "Batch 172, Loss: 0.3728569447994232\n",
      "Batch 173, Loss: -1.3541536331176758\n",
      "Batch 174, Loss: -1.3581035137176514\n",
      "Batch 175, Loss: 0.36580392718315125\n",
      "Batch 176, Loss: -1.37689208984375\n",
      "Batch 177, Loss: 0.3691181242465973\n",
      "Batch 178, Loss: -1.390221357345581\n",
      "Batch 179, Loss: -1.3871570825576782\n",
      "Batch 180, Loss: 0.37465059757232666\n",
      "Batch 181, Loss: 0.38352152705192566\n",
      "Batch 182, Loss: 0.3789524435997009\n",
      "Batch 183, Loss: -1.3839960098266602\n",
      "Batch 184, Loss: -1.3743420839309692\n",
      "Batch 185, Loss: -1.3806040287017822\n",
      "Batch 186, Loss: -1.3971267938613892\n",
      "Batch 187, Loss: 0.3928995132446289\n",
      "Batch 188, Loss: -1.369922161102295\n",
      "Batch 189, Loss: 0.38795873522758484\n",
      "Batch 190, Loss: 0.3870548605918884\n",
      "Batch 191, Loss: 0.369439959526062\n",
      "Batch 192, Loss: 0.39364615082740784\n",
      "Batch 193, Loss: -1.3753036260604858\n",
      "Batch 194, Loss: -1.383063554763794\n",
      "Batch 195, Loss: -1.3919481039047241\n",
      "Batch 196, Loss: -1.3863961696624756\n",
      "Batch 197, Loss: -1.3842391967773438\n",
      "Batch 198, Loss: 0.38857364654541016\n",
      "Batch 199, Loss: 0.3876030445098877\n",
      "Batch 200, Loss: 0.3841785490512848\n",
      "Batch 201, Loss: 0.3913525640964508\n",
      "Batch 202, Loss: 0.3970411419868469\n",
      "Batch 203, Loss: -1.3949024677276611\n",
      "Batch 204, Loss: -1.416640043258667\n",
      "Batch 205, Loss: -1.3867920637130737\n",
      "Batch 206, Loss: -1.3849973678588867\n",
      "Batch 207, Loss: -1.3903062343597412\n",
      "Batch 208, Loss: -1.3756186962127686\n",
      "Batch 209, Loss: -1.4022364616394043\n",
      "Batch 210, Loss: -1.3885796070098877\n",
      "Batch 211, Loss: 0.37975284457206726\n",
      "Batch 212, Loss: -1.4200072288513184\n",
      "Batch 213, Loss: 0.38133618235588074\n",
      "Batch 214, Loss: 0.3966801166534424\n",
      "Batch 215, Loss: 0.37855443358421326\n",
      "Batch 216, Loss: -1.3842893838882446\n",
      "Batch 217, Loss: -1.410233497619629\n",
      "Batch 218, Loss: -1.3927490711212158\n",
      "Batch 219, Loss: -1.4112370014190674\n",
      "Batch 220, Loss: -1.384687066078186\n",
      "Batch 221, Loss: -1.403281807899475\n",
      "Batch 222, Loss: 0.4223804771900177\n",
      "Batch 223, Loss: 0.3973202705383301\n",
      "Batch 224, Loss: 0.3993094861507416\n",
      "Batch 225, Loss: -1.4004957675933838\n",
      "Batch 226, Loss: -1.405271291732788\n",
      "Batch 227, Loss: 0.40376943349838257\n",
      "Batch 228, Loss: -1.4059028625488281\n",
      "Batch 229, Loss: -1.3899716138839722\n",
      "Batch 230, Loss: -1.3906984329223633\n",
      "Batch 231, Loss: 0.40616047382354736\n",
      "Batch 232, Loss: 0.40888604521751404\n",
      "Batch 233, Loss: -1.4248995780944824\n",
      "Batch 234, Loss: 0.417295902967453\n",
      "Batch 235, Loss: -1.4072424173355103\n",
      "Batch 236, Loss: -1.4054687023162842\n",
      "Batch 237, Loss: 0.41257143020629883\n",
      "Batch 238, Loss: 0.4396403431892395\n",
      "Batch 239, Loss: 0.4101721942424774\n",
      "Batch 240, Loss: -1.4127984046936035\n",
      "Batch 241, Loss: 0.394842267036438\n",
      "Batch 242, Loss: -1.4145210981369019\n",
      "Batch 243, Loss: 0.39430344104766846\n",
      "Batch 244, Loss: -1.3920387029647827\n",
      "Batch 245, Loss: 0.39461395144462585\n",
      "Batch 246, Loss: 0.4079316556453705\n",
      "Batch 247, Loss: -1.39341139793396\n",
      "Batch 248, Loss: -1.4036204814910889\n",
      "Batch 249, Loss: 0.4140550494194031\n",
      "Batch 250, Loss: 0.40230607986450195\n",
      "Batch 251, Loss: -1.4012370109558105\n",
      "Batch 252, Loss: -1.4054509401321411\n",
      "Batch 253, Loss: -1.4364778995513916\n",
      "Batch 254, Loss: -1.408478856086731\n",
      "Batch 255, Loss: 0.40638864040374756\n",
      "Batch 256, Loss: -1.4119139909744263\n",
      "Batch 257, Loss: -1.4068900346755981\n",
      "Batch 258, Loss: -1.4191808700561523\n",
      "Batch 259, Loss: 0.4053283929824829\n",
      "Batch 260, Loss: -1.4149378538131714\n",
      "Batch 261, Loss: 0.4327249825000763\n",
      "Batch 262, Loss: 0.3979341685771942\n",
      "Batch 263, Loss: -1.4012250900268555\n",
      "Batch 264, Loss: 0.4239444136619568\n",
      "Batch 265, Loss: 0.39611709117889404\n",
      "Batch 266, Loss: -1.4116742610931396\n",
      "Batch 267, Loss: 0.41223573684692383\n",
      "Batch 268, Loss: 0.4009384512901306\n",
      "Batch 269, Loss: 0.4143217206001282\n",
      "Batch 270, Loss: -1.4230238199234009\n",
      "Batch 271, Loss: -1.3960859775543213\n",
      "Batch 272, Loss: 0.420917809009552\n",
      "Batch 273, Loss: -1.3978534936904907\n",
      "Batch 274, Loss: -1.4106680154800415\n",
      "Batch 275, Loss: -1.4018235206604004\n",
      "Batch 276, Loss: -1.4308648109436035\n",
      "Batch 277, Loss: 0.4026091396808624\n",
      "Batch 278, Loss: 0.403083860874176\n",
      "Batch 279, Loss: 0.40742307901382446\n",
      "Batch 280, Loss: 0.4125085473060608\n",
      "Batch 281, Loss: -1.4162747859954834\n",
      "Batch 282, Loss: 0.41135871410369873\n",
      "Batch 283, Loss: 0.44534093141555786\n",
      "Batch 284, Loss: 0.4429183602333069\n",
      "Batch 285, Loss: 0.42932239174842834\n",
      "Batch 286, Loss: 0.438538521528244\n",
      "Batch 287, Loss: -1.4098231792449951\n",
      "Batch 288, Loss: -1.4019659757614136\n",
      "Batch 289, Loss: -1.4331419467926025\n",
      "Batch 290, Loss: -1.4241199493408203\n",
      "Batch 291, Loss: -1.3993215560913086\n",
      "Batch 292, Loss: -1.3998217582702637\n",
      "Batch 293, Loss: -1.4006357192993164\n",
      "Batch 294, Loss: -1.4059094190597534\n",
      "Batch 295, Loss: 0.40426331758499146\n",
      "Batch 296, Loss: 0.41625791788101196\n",
      "Batch 297, Loss: 0.41031134128570557\n",
      "Batch 298, Loss: -1.4168434143066406\n",
      "Batch 299, Loss: -1.4116196632385254\n",
      "Batch 300, Loss: -1.4099440574645996\n",
      "Batch 301, Loss: -1.4386059045791626\n",
      "Batch 302, Loss: -1.4203970432281494\n",
      "Batch 303, Loss: -1.425624132156372\n",
      "Batch 304, Loss: 0.421159565448761\n",
      "Batch 305, Loss: -1.4222491979599\n",
      "Batch 306, Loss: -1.4173732995986938\n",
      "Batch 307, Loss: -1.4135682582855225\n",
      "Batch 308, Loss: 0.43950462341308594\n",
      "Batch 309, Loss: 0.4151078462600708\n",
      "Batch 310, Loss: 0.4301828444004059\n",
      "Batch 311, Loss: 0.4293365478515625\n",
      "Batch 312, Loss: 0.44298675656318665\n",
      "Batch 313, Loss: 0.4363877773284912\n",
      "Batch 314, Loss: -1.428152084350586\n",
      "Batch 315, Loss: -1.427603006362915\n",
      "Batch 316, Loss: -1.4223695993423462\n",
      "Batch 317, Loss: -1.4259834289550781\n",
      "Batch 318, Loss: 0.4283950626850128\n",
      "Batch 319, Loss: -1.409816026687622\n",
      "Batch 320, Loss: 0.4215930998325348\n",
      "Batch 321, Loss: 0.4245966672897339\n",
      "Batch 322, Loss: 0.44253775477409363\n",
      "Batch 323, Loss: -1.4321386814117432\n",
      "Batch 324, Loss: 0.40974050760269165\n",
      "Batch 325, Loss: -1.4329888820648193\n",
      "Batch 326, Loss: -1.4094980955123901\n",
      "Batch 327, Loss: -1.4116276502609253\n",
      "Batch 328, Loss: -1.4252814054489136\n",
      "Batch 329, Loss: 0.4395727813243866\n",
      "Batch 330, Loss: 0.4243587255477905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 331, Loss: -1.4100089073181152\n",
      "Batch 332, Loss: 0.41488805413246155\n",
      "Batch 333, Loss: -1.4297624826431274\n",
      "Batch 334, Loss: 0.4540886878967285\n",
      "Batch 335, Loss: 0.44275593757629395\n",
      "Batch 336, Loss: 0.4103836417198181\n",
      "Batch 337, Loss: -1.433623194694519\n",
      "Batch 338, Loss: -1.4226312637329102\n",
      "Batch 339, Loss: 0.4518476128578186\n",
      "Batch 340, Loss: 0.4227011799812317\n",
      "Batch 341, Loss: 0.41220057010650635\n",
      "Batch 342, Loss: 0.4213421046733856\n",
      "Batch 343, Loss: 0.40742865204811096\n",
      "Batch 344, Loss: 0.4080190658569336\n",
      "Batch 345, Loss: 0.4165215492248535\n",
      "Batch 346, Loss: 0.4204193353652954\n",
      "Batch 347, Loss: 0.40455129742622375\n",
      "Batch 348, Loss: 0.41531991958618164\n",
      "Batch 349, Loss: 0.41237136721611023\n",
      "Batch 350, Loss: -1.4052330255508423\n",
      "Batch 351, Loss: -1.396315097808838\n",
      "Batch 352, Loss: 0.4191153645515442\n",
      "Batch 353, Loss: -1.405561089515686\n",
      "Batch 354, Loss: 0.393801748752594\n",
      "Batch 355, Loss: 0.39288365840911865\n",
      "Batch 356, Loss: 0.3948737382888794\n",
      "Batch 357, Loss: -1.3898874521255493\n",
      "Batch 358, Loss: -1.39862060546875\n",
      "Batch 359, Loss: -1.4114434719085693\n",
      "Batch 360, Loss: -1.3965609073638916\n",
      "Batch 361, Loss: -1.3884861469268799\n",
      "Batch 362, Loss: -1.3937957286834717\n",
      "Batch 363, Loss: 0.3890581429004669\n",
      "Batch 364, Loss: -1.4192661046981812\n",
      "Batch 365, Loss: 0.3893898129463196\n",
      "Batch 366, Loss: -1.3894314765930176\n",
      "Batch 367, Loss: -1.3968870639801025\n",
      "Batch 368, Loss: -1.411792516708374\n",
      "Batch 369, Loss: 0.39954113960266113\n",
      "Batch 370, Loss: -1.3934890031814575\n",
      "Batch 371, Loss: -1.427544355392456\n",
      "Batch 372, Loss: 0.410468190908432\n",
      "Batch 373, Loss: -1.4022643566131592\n",
      "Batch 374, Loss: 0.39460891485214233\n",
      "Batch 375, Loss: 0.4100561738014221\n",
      "Batch 376, Loss: -1.4075002670288086\n",
      "Batch 377, Loss: 0.40291786193847656\n",
      "Batch 378, Loss: -1.393425464630127\n",
      "Batch 379, Loss: -1.40784752368927\n",
      "Batch 380, Loss: 0.39366787672042847\n",
      "Batch 381, Loss: 0.4177587032318115\n",
      "Batch 382, Loss: -1.4186862707138062\n",
      "Batch 383, Loss: -1.411217451095581\n",
      "Batch 384, Loss: -1.4084036350250244\n",
      "Batch 385, Loss: -1.399033546447754\n",
      "Batch 386, Loss: -1.396390438079834\n",
      "Batch 387, Loss: 0.41571709513664246\n",
      "Batch 388, Loss: 0.40419742465019226\n",
      "Batch 389, Loss: 0.42195624113082886\n",
      "Batch 390, Loss: 0.4135286211967468\n",
      "Batch 391, Loss: -1.4188859462738037\n",
      "Batch 392, Loss: 0.4178224503993988\n",
      "Batch 393, Loss: 0.4073958694934845\n",
      "Batch 394, Loss: -1.4103907346725464\n",
      "Batch 395, Loss: 0.40572935342788696\n",
      "Batch 396, Loss: 0.39829587936401367\n",
      "Batch 397, Loss: -1.4052495956420898\n",
      "Batch 398, Loss: 0.3972748816013336\n",
      "Batch 399, Loss: 0.4002952575683594\n",
      "Training [30%]\tLoss: -0.4972\n",
      "Batch 0, Loss: 0.4085400700569153\n",
      "Batch 1, Loss: -1.407273292541504\n",
      "Batch 2, Loss: -1.4140715599060059\n",
      "Batch 3, Loss: -1.4188342094421387\n",
      "Batch 4, Loss: 0.39458733797073364\n",
      "Batch 5, Loss: 0.4031408429145813\n",
      "Batch 6, Loss: 0.4056764245033264\n",
      "Batch 7, Loss: 0.40719354152679443\n",
      "Batch 8, Loss: 0.41397589445114136\n",
      "Batch 9, Loss: 0.40487486124038696\n",
      "Batch 10, Loss: 0.4052489995956421\n",
      "Batch 11, Loss: 0.3981863260269165\n",
      "Batch 12, Loss: 0.40977030992507935\n",
      "Batch 13, Loss: 0.39553433656692505\n",
      "Batch 14, Loss: -1.3814022541046143\n",
      "Batch 15, Loss: -1.423590064048767\n",
      "Batch 16, Loss: -1.3895450830459595\n",
      "Batch 17, Loss: 0.38123100996017456\n",
      "Batch 18, Loss: -1.3882932662963867\n",
      "Batch 19, Loss: -1.3901251554489136\n",
      "Batch 20, Loss: -1.3877816200256348\n",
      "Batch 21, Loss: 0.3879290819168091\n",
      "Batch 22, Loss: -1.4008861780166626\n",
      "Batch 23, Loss: -1.4140321016311646\n",
      "Batch 24, Loss: 0.39002394676208496\n",
      "Batch 25, Loss: -1.3911269903182983\n",
      "Batch 26, Loss: -1.3890655040740967\n",
      "Batch 27, Loss: -1.3977138996124268\n",
      "Batch 28, Loss: 0.3797329068183899\n",
      "Batch 29, Loss: 0.3796716034412384\n",
      "Batch 30, Loss: 0.3960358798503876\n",
      "Batch 31, Loss: 0.40725967288017273\n",
      "Batch 32, Loss: 0.39100682735443115\n",
      "Batch 33, Loss: 0.38771843910217285\n",
      "Batch 34, Loss: 0.38453519344329834\n",
      "Batch 35, Loss: 0.3728943467140198\n",
      "Batch 36, Loss: 0.39117732644081116\n",
      "Batch 37, Loss: 0.389242559671402\n",
      "Batch 38, Loss: 0.386769562959671\n",
      "Batch 39, Loss: -1.3976069688796997\n",
      "Batch 40, Loss: -1.3650211095809937\n",
      "Batch 41, Loss: -1.3721166849136353\n",
      "Batch 42, Loss: 0.3598024249076843\n",
      "Batch 43, Loss: 0.40806418657302856\n",
      "Batch 44, Loss: 0.3317986726760864\n",
      "Batch 45, Loss: -1.3242329359054565\n",
      "Batch 46, Loss: -1.3282015323638916\n",
      "Batch 47, Loss: 0.37473058700561523\n",
      "Batch 48, Loss: -1.374258041381836\n",
      "Batch 49, Loss: -1.393449306488037\n",
      "Batch 50, Loss: 0.37905803322792053\n",
      "Batch 51, Loss: 0.3749896287918091\n",
      "Batch 52, Loss: -1.377471923828125\n",
      "Batch 53, Loss: 0.37641578912734985\n",
      "Batch 54, Loss: 0.37404385209083557\n",
      "Batch 55, Loss: -1.3781254291534424\n",
      "Batch 56, Loss: 0.33838310837745667\n",
      "Batch 57, Loss: -1.357526421546936\n",
      "Batch 58, Loss: -1.344903826713562\n",
      "Batch 59, Loss: 0.3781068027019501\n",
      "Batch 60, Loss: 0.38064345717430115\n",
      "Batch 61, Loss: -1.3801121711730957\n",
      "Batch 62, Loss: -1.3758610486984253\n",
      "Batch 63, Loss: -1.3530828952789307\n",
      "Batch 64, Loss: -1.3748347759246826\n",
      "Batch 65, Loss: -1.3682093620300293\n",
      "Batch 66, Loss: -1.3639285564422607\n",
      "Batch 67, Loss: 0.39257240295410156\n",
      "Batch 68, Loss: 0.36088302731513977\n",
      "Batch 69, Loss: 0.3867051601409912\n",
      "Batch 70, Loss: 0.3695605397224426\n",
      "Batch 71, Loss: 0.3786514103412628\n",
      "Batch 72, Loss: -1.380863904953003\n",
      "Batch 73, Loss: -1.3748064041137695\n",
      "Batch 74, Loss: -1.3996105194091797\n",
      "Batch 75, Loss: 0.3693799674510956\n",
      "Batch 76, Loss: 0.3953154683113098\n",
      "Batch 77, Loss: 0.38253507018089294\n",
      "Batch 78, Loss: -1.3838430643081665\n",
      "Batch 79, Loss: 0.3679022789001465\n",
      "Batch 80, Loss: -1.3863985538482666\n",
      "Batch 81, Loss: 0.3727313280105591\n",
      "Batch 82, Loss: 0.3847959339618683\n",
      "Batch 83, Loss: -1.3949947357177734\n",
      "Batch 84, Loss: 0.38107389211654663\n",
      "Batch 85, Loss: 0.35715025663375854\n",
      "Batch 86, Loss: -1.3758436441421509\n",
      "Batch 87, Loss: -1.380143642425537\n",
      "Batch 88, Loss: -1.3905084133148193\n",
      "Batch 89, Loss: 0.3831556737422943\n",
      "Batch 90, Loss: 0.3740915060043335\n",
      "Batch 91, Loss: -1.37875497341156\n",
      "Batch 92, Loss: -1.3806616067886353\n",
      "Batch 93, Loss: 0.3937026560306549\n",
      "Batch 94, Loss: -1.3808835744857788\n",
      "Batch 95, Loss: -1.4022626876831055\n",
      "Batch 96, Loss: 0.385456919670105\n",
      "Batch 97, Loss: 0.3798908591270447\n",
      "Batch 98, Loss: 0.39730796217918396\n",
      "Batch 99, Loss: 0.37620407342910767\n",
      "Batch 100, Loss: 0.33953484892845154\n",
      "Batch 101, Loss: -1.388037085533142\n",
      "Batch 102, Loss: 0.3648865818977356\n",
      "Batch 103, Loss: -1.3992818593978882\n",
      "Batch 104, Loss: -1.3486346006393433\n",
      "Batch 105, Loss: -1.3196827173233032\n",
      "Batch 106, Loss: -1.3398396968841553\n",
      "Batch 107, Loss: 0.3752761483192444\n",
      "Batch 108, Loss: 0.37838321924209595\n",
      "Batch 109, Loss: 0.3773662745952606\n",
      "Batch 110, Loss: -1.3844177722930908\n",
      "Batch 111, Loss: -1.3915271759033203\n",
      "Batch 112, Loss: 0.36037325859069824\n",
      "Batch 113, Loss: 0.3667493462562561\n",
      "Batch 114, Loss: 0.37531694769859314\n",
      "Batch 115, Loss: -1.3866667747497559\n",
      "Batch 116, Loss: -1.4078187942504883\n",
      "Batch 117, Loss: -1.3776540756225586\n",
      "Batch 118, Loss: -1.3564386367797852\n",
      "Batch 119, Loss: -1.374316930770874\n",
      "Batch 120, Loss: 0.3763599693775177\n",
      "Batch 121, Loss: -1.3790215253829956\n",
      "Batch 122, Loss: -1.3435640335083008\n",
      "Batch 123, Loss: -1.3851535320281982\n",
      "Batch 124, Loss: -1.3990784883499146\n",
      "Batch 125, Loss: -1.3809001445770264\n",
      "Batch 126, Loss: 0.38098955154418945\n",
      "Batch 127, Loss: -1.3678778409957886\n",
      "Batch 128, Loss: -1.371307134628296\n",
      "Batch 129, Loss: 0.3893493413925171\n",
      "Batch 130, Loss: -1.369321584701538\n",
      "Batch 131, Loss: 0.37748271226882935\n",
      "Batch 132, Loss: 0.3813086748123169\n",
      "Batch 133, Loss: 0.39004793763160706\n",
      "Batch 134, Loss: -1.3907177448272705\n",
      "Batch 135, Loss: 0.37824004888534546\n",
      "Batch 136, Loss: -1.394482135772705\n",
      "Batch 137, Loss: 0.3938046097755432\n",
      "Batch 138, Loss: 0.38327696919441223\n",
      "Batch 139, Loss: 0.3844703137874603\n",
      "Batch 140, Loss: 0.39872124791145325\n",
      "Batch 141, Loss: -1.4001774787902832\n",
      "Batch 142, Loss: -1.3930237293243408\n",
      "Batch 143, Loss: 0.40902256965637207\n",
      "Batch 144, Loss: -1.3890771865844727\n",
      "Batch 145, Loss: -1.4048775434494019\n",
      "Batch 146, Loss: -1.3946815729141235\n",
      "Batch 147, Loss: -1.380793809890747\n",
      "Batch 148, Loss: -1.3937731981277466\n",
      "Batch 149, Loss: -1.4001058340072632\n",
      "Batch 150, Loss: -1.4128926992416382\n",
      "Batch 151, Loss: 0.403675377368927\n",
      "Batch 152, Loss: 0.41596436500549316\n",
      "Batch 153, Loss: -1.386622667312622\n",
      "Batch 154, Loss: 0.4005815088748932\n",
      "Batch 155, Loss: -1.4027702808380127\n",
      "Batch 156, Loss: -1.3851008415222168\n",
      "Batch 157, Loss: -1.3890278339385986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 158, Loss: -1.3884871006011963\n",
      "Batch 159, Loss: 0.40922975540161133\n",
      "Batch 160, Loss: 0.4063093364238739\n",
      "Batch 161, Loss: -1.405576467514038\n",
      "Batch 162, Loss: -1.4139715433120728\n",
      "Batch 163, Loss: -1.4113729000091553\n",
      "Batch 164, Loss: -1.4127120971679688\n",
      "Batch 165, Loss: 0.39464715123176575\n",
      "Batch 166, Loss: 0.4235801696777344\n",
      "Batch 167, Loss: 0.39816826581954956\n",
      "Batch 168, Loss: 0.4152221083641052\n",
      "Batch 169, Loss: -1.4137187004089355\n",
      "Batch 170, Loss: 0.39887696504592896\n",
      "Batch 171, Loss: -1.394395351409912\n",
      "Batch 172, Loss: 0.4356296956539154\n",
      "Batch 173, Loss: 0.41748568415641785\n",
      "Batch 174, Loss: 0.3981224298477173\n",
      "Batch 175, Loss: 0.4119739830493927\n",
      "Batch 176, Loss: 0.4106672704219818\n",
      "Batch 177, Loss: 0.4012886881828308\n",
      "Batch 178, Loss: 0.40212947130203247\n",
      "Batch 179, Loss: -1.3919894695281982\n",
      "Batch 180, Loss: 0.41397249698638916\n",
      "Batch 181, Loss: 0.4009995460510254\n",
      "Batch 182, Loss: 0.38631635904312134\n",
      "Batch 183, Loss: 0.4001326858997345\n",
      "Batch 184, Loss: -1.3979876041412354\n",
      "Batch 185, Loss: -1.3998616933822632\n",
      "Batch 186, Loss: 0.39668434858322144\n",
      "Batch 187, Loss: 0.4074269235134125\n",
      "Batch 188, Loss: 0.4129865765571594\n",
      "Batch 189, Loss: -1.383926272392273\n",
      "Batch 190, Loss: -1.39473557472229\n",
      "Batch 191, Loss: 0.3896907567977905\n",
      "Batch 192, Loss: -1.3769017457962036\n",
      "Batch 193, Loss: 0.3860979974269867\n",
      "Batch 194, Loss: -1.3771312236785889\n",
      "Batch 195, Loss: 0.38683149218559265\n",
      "Batch 196, Loss: 0.3947976231575012\n",
      "Batch 197, Loss: 0.39342135190963745\n",
      "Batch 198, Loss: -1.3983632326126099\n",
      "Batch 199, Loss: -1.3912018537521362\n",
      "Batch 200, Loss: -1.37715744972229\n",
      "Batch 201, Loss: 0.39708489179611206\n",
      "Batch 202, Loss: -1.4012951850891113\n",
      "Batch 203, Loss: 0.38312920928001404\n",
      "Batch 204, Loss: 0.3769809901714325\n",
      "Batch 205, Loss: 0.37647339701652527\n",
      "Batch 206, Loss: 0.38072410225868225\n",
      "Batch 207, Loss: 0.3780933618545532\n",
      "Batch 208, Loss: 0.38480305671691895\n",
      "Batch 209, Loss: -1.391544222831726\n",
      "Batch 210, Loss: 0.3788086771965027\n",
      "Batch 211, Loss: 0.37319594621658325\n",
      "Batch 212, Loss: -1.3690516948699951\n",
      "Batch 213, Loss: 0.3938593864440918\n",
      "Batch 214, Loss: 0.3722231686115265\n",
      "Batch 215, Loss: -1.402347207069397\n",
      "Batch 216, Loss: -1.3869762420654297\n",
      "Batch 217, Loss: 0.3650670349597931\n",
      "Batch 218, Loss: -1.3672540187835693\n",
      "Batch 219, Loss: 0.37700995802879333\n",
      "Batch 220, Loss: -1.37614905834198\n",
      "Batch 221, Loss: -1.36232328414917\n",
      "Batch 222, Loss: -1.3639531135559082\n",
      "Batch 223, Loss: -1.3628294467926025\n",
      "Batch 224, Loss: 0.3664504885673523\n",
      "Batch 225, Loss: 0.3765202462673187\n",
      "Batch 226, Loss: -1.3699052333831787\n",
      "Batch 227, Loss: 0.36880967020988464\n",
      "Batch 228, Loss: -1.3684961795806885\n",
      "Batch 229, Loss: -1.3815340995788574\n",
      "Batch 230, Loss: 0.40401020646095276\n",
      "Batch 231, Loss: -1.3688408136367798\n",
      "Batch 232, Loss: 0.36649060249328613\n",
      "Batch 233, Loss: -1.3736932277679443\n",
      "Batch 234, Loss: -1.3713058233261108\n",
      "Batch 235, Loss: 0.4049295485019684\n",
      "Batch 236, Loss: 0.381868839263916\n",
      "Batch 237, Loss: 0.370890736579895\n",
      "Batch 238, Loss: -1.377990961074829\n",
      "Batch 239, Loss: 0.38423460721969604\n",
      "Batch 240, Loss: -1.3725297451019287\n",
      "Batch 241, Loss: 0.37129178643226624\n",
      "Batch 242, Loss: -1.3653934001922607\n",
      "Batch 243, Loss: -1.3638782501220703\n",
      "Batch 244, Loss: 0.37017929553985596\n",
      "Batch 245, Loss: -1.3704020977020264\n",
      "Batch 246, Loss: 0.37070968747138977\n",
      "Batch 247, Loss: -1.3716235160827637\n",
      "Batch 248, Loss: -1.378934621810913\n",
      "Batch 249, Loss: 0.3712379038333893\n",
      "Batch 250, Loss: -1.3701258897781372\n",
      "Batch 251, Loss: -1.3708739280700684\n",
      "Batch 252, Loss: -1.4027140140533447\n",
      "Batch 253, Loss: -1.3908512592315674\n",
      "Batch 254, Loss: 0.39205652475357056\n",
      "Batch 255, Loss: -1.4050558805465698\n",
      "Batch 256, Loss: -1.3793931007385254\n",
      "Batch 257, Loss: -1.3775513172149658\n",
      "Batch 258, Loss: -1.3753080368041992\n",
      "Batch 259, Loss: 0.37164127826690674\n",
      "Batch 260, Loss: 0.37227362394332886\n",
      "Batch 261, Loss: 0.39872512221336365\n",
      "Batch 262, Loss: -1.3899855613708496\n",
      "Batch 263, Loss: 0.37778985500335693\n",
      "Batch 264, Loss: -1.3871920108795166\n",
      "Batch 265, Loss: -1.3778085708618164\n",
      "Batch 266, Loss: -1.3823647499084473\n",
      "Batch 267, Loss: 0.40304526686668396\n",
      "Batch 268, Loss: 0.3809226453304291\n",
      "Batch 269, Loss: 0.387736976146698\n",
      "Batch 270, Loss: 0.38157951831817627\n",
      "Batch 271, Loss: 0.37935200333595276\n",
      "Batch 272, Loss: -1.3769733905792236\n",
      "Batch 273, Loss: -1.3903632164001465\n",
      "Batch 274, Loss: 0.38593214750289917\n",
      "Batch 275, Loss: -1.3920327425003052\n",
      "Batch 276, Loss: 0.388510525226593\n",
      "Batch 277, Loss: -1.385796308517456\n",
      "Batch 278, Loss: 0.3689076602458954\n",
      "Batch 279, Loss: -1.385366439819336\n",
      "Batch 280, Loss: 0.390239953994751\n",
      "Batch 281, Loss: -1.3868035078048706\n",
      "Batch 282, Loss: 0.38735640048980713\n",
      "Batch 283, Loss: 0.38255852460861206\n",
      "Batch 284, Loss: 0.38015735149383545\n",
      "Batch 285, Loss: 0.3874923586845398\n",
      "Batch 286, Loss: 0.38547825813293457\n",
      "Batch 287, Loss: -1.3786985874176025\n",
      "Batch 288, Loss: -1.3672107458114624\n",
      "Batch 289, Loss: -1.3691189289093018\n",
      "Batch 290, Loss: -1.3966178894042969\n",
      "Batch 291, Loss: -1.369861125946045\n",
      "Batch 292, Loss: -1.3832664489746094\n",
      "Batch 293, Loss: 0.3726019561290741\n",
      "Batch 294, Loss: -1.3852148056030273\n",
      "Batch 295, Loss: -1.4117106199264526\n",
      "Batch 296, Loss: 0.3727247714996338\n",
      "Batch 297, Loss: 0.37804487347602844\n",
      "Batch 298, Loss: 0.38575848937034607\n",
      "Batch 299, Loss: -1.3714021444320679\n",
      "Batch 300, Loss: -1.3879663944244385\n",
      "Batch 301, Loss: 0.3959803581237793\n",
      "Batch 302, Loss: -1.4016320705413818\n",
      "Batch 303, Loss: -1.3764210939407349\n",
      "Batch 304, Loss: 0.3900606036186218\n",
      "Batch 305, Loss: 0.379750519990921\n",
      "Batch 306, Loss: 0.379344642162323\n",
      "Batch 307, Loss: -1.3837627172470093\n",
      "Batch 308, Loss: 0.3731483519077301\n",
      "Batch 309, Loss: -1.3767037391662598\n",
      "Batch 310, Loss: 0.3817499279975891\n",
      "Batch 311, Loss: -1.3774199485778809\n",
      "Batch 312, Loss: 0.38970476388931274\n",
      "Batch 313, Loss: -1.3821656703948975\n",
      "Batch 314, Loss: 0.37742042541503906\n",
      "Batch 315, Loss: 0.3786350190639496\n",
      "Batch 316, Loss: -1.3849940299987793\n",
      "Batch 317, Loss: 0.3828958570957184\n",
      "Batch 318, Loss: -1.3918477296829224\n",
      "Batch 319, Loss: 0.3777751624584198\n",
      "Batch 320, Loss: 0.37877434492111206\n",
      "Batch 321, Loss: -1.374934434890747\n",
      "Batch 322, Loss: -1.3819689750671387\n",
      "Batch 323, Loss: -1.4003956317901611\n",
      "Batch 324, Loss: -1.3856180906295776\n",
      "Batch 325, Loss: -1.3817397356033325\n",
      "Batch 326, Loss: 0.3868955373764038\n",
      "Batch 327, Loss: -1.386979103088379\n",
      "Batch 328, Loss: -1.38326895236969\n",
      "Batch 329, Loss: 0.3953312039375305\n",
      "Batch 330, Loss: -1.3756284713745117\n",
      "Batch 331, Loss: 0.380214661359787\n",
      "Batch 332, Loss: 0.39856359362602234\n",
      "Batch 333, Loss: -1.3832550048828125\n",
      "Batch 334, Loss: -1.387920618057251\n",
      "Batch 335, Loss: 0.3917243480682373\n",
      "Batch 336, Loss: 0.39079251885414124\n",
      "Batch 337, Loss: 0.39341628551483154\n",
      "Batch 338, Loss: -1.3908582925796509\n",
      "Batch 339, Loss: 0.3743993937969208\n",
      "Batch 340, Loss: 0.38989490270614624\n",
      "Batch 341, Loss: 0.3739137053489685\n",
      "Batch 342, Loss: 0.3962239921092987\n",
      "Batch 343, Loss: -1.3721473217010498\n",
      "Batch 344, Loss: -1.3769843578338623\n",
      "Batch 345, Loss: -1.377772569656372\n",
      "Batch 346, Loss: -1.372405767440796\n",
      "Batch 347, Loss: -1.3826756477355957\n",
      "Batch 348, Loss: -1.3880059719085693\n",
      "Batch 349, Loss: 0.39221012592315674\n",
      "Batch 350, Loss: -1.3743385076522827\n",
      "Batch 351, Loss: -1.3803225755691528\n",
      "Batch 352, Loss: -1.396023154258728\n",
      "Batch 353, Loss: -1.3818943500518799\n",
      "Batch 354, Loss: -1.3724844455718994\n",
      "Batch 355, Loss: 0.39477765560150146\n",
      "Batch 356, Loss: 0.3852898180484772\n",
      "Batch 357, Loss: -1.3866915702819824\n",
      "Batch 358, Loss: -1.3882957696914673\n",
      "Batch 359, Loss: -1.3867835998535156\n",
      "Batch 360, Loss: 0.3906582295894623\n",
      "Batch 361, Loss: -1.3862874507904053\n",
      "Batch 362, Loss: -1.388444423675537\n",
      "Batch 363, Loss: 0.39105117321014404\n",
      "Batch 364, Loss: 0.40197286009788513\n",
      "Batch 365, Loss: 0.3862990736961365\n",
      "Batch 366, Loss: -1.3983217477798462\n",
      "Batch 367, Loss: -1.4038338661193848\n",
      "Batch 368, Loss: -1.394851803779602\n",
      "Batch 369, Loss: 0.3929824233055115\n",
      "Batch 370, Loss: -1.3931682109832764\n",
      "Batch 371, Loss: -1.399320125579834\n",
      "Batch 372, Loss: 0.3937414884567261\n",
      "Batch 373, Loss: -1.3999388217926025\n",
      "Batch 374, Loss: -1.4042681455612183\n",
      "Batch 375, Loss: -1.3912937641143799\n",
      "Batch 376, Loss: -1.4234815835952759\n",
      "Batch 377, Loss: -1.4052860736846924\n",
      "Batch 378, Loss: -1.4068924188613892\n",
      "Batch 379, Loss: 0.39699244499206543\n",
      "Batch 380, Loss: 0.41275668144226074\n",
      "Batch 381, Loss: -1.4159764051437378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 382, Loss: -1.4156506061553955\n",
      "Batch 383, Loss: 0.42742884159088135\n",
      "Batch 384, Loss: -1.413530707359314\n",
      "Batch 385, Loss: -1.4352105855941772\n",
      "Batch 386, Loss: 0.4061305522918701\n",
      "Batch 387, Loss: -1.4263596534729004\n",
      "Batch 388, Loss: 0.42766278982162476\n",
      "Batch 389, Loss: 0.4418582022190094\n",
      "Batch 390, Loss: -1.4198012351989746\n",
      "Batch 391, Loss: 0.43131452798843384\n",
      "Batch 392, Loss: 0.4153537452220917\n",
      "Batch 393, Loss: 0.40999147295951843\n",
      "Batch 394, Loss: 0.4110954999923706\n",
      "Batch 395, Loss: 0.4069860875606537\n",
      "Batch 396, Loss: 0.40840113162994385\n",
      "Batch 397, Loss: -1.4086302518844604\n",
      "Batch 398, Loss: 0.4171266257762909\n",
      "Batch 399, Loss: 0.41648179292678833\n",
      "Training [40%]\tLoss: -0.4985\n",
      "Batch 0, Loss: -1.4338555335998535\n",
      "Batch 1, Loss: -1.4173243045806885\n",
      "Batch 2, Loss: 0.4335828423500061\n",
      "Batch 3, Loss: 0.4113594591617584\n",
      "Batch 4, Loss: -1.4131876230239868\n",
      "Batch 5, Loss: 0.410252183675766\n",
      "Batch 6, Loss: 0.3985874354839325\n",
      "Batch 7, Loss: 0.3941558599472046\n",
      "Batch 8, Loss: 0.39284083247184753\n",
      "Batch 9, Loss: -1.4188216924667358\n",
      "Batch 10, Loss: -1.4080016613006592\n",
      "Batch 11, Loss: 0.40081197023391724\n",
      "Batch 12, Loss: -1.393810749053955\n",
      "Batch 13, Loss: 0.4027905762195587\n",
      "Batch 14, Loss: -1.4024933576583862\n",
      "Batch 15, Loss: -1.392346739768982\n",
      "Batch 16, Loss: -1.3943805694580078\n",
      "Batch 17, Loss: 0.40843796730041504\n",
      "Batch 18, Loss: 0.3956942856311798\n",
      "Batch 19, Loss: -1.396965503692627\n",
      "Batch 20, Loss: 0.4030970335006714\n",
      "Batch 21, Loss: 0.40084129571914673\n",
      "Batch 22, Loss: 0.40599650144577026\n",
      "Batch 23, Loss: 0.4152710437774658\n",
      "Batch 24, Loss: 0.40989407896995544\n",
      "Batch 25, Loss: -1.387920618057251\n",
      "Batch 26, Loss: 0.39287394285202026\n",
      "Batch 27, Loss: 0.38505735993385315\n",
      "Batch 28, Loss: -1.389566421508789\n",
      "Batch 29, Loss: -1.3978426456451416\n",
      "Batch 30, Loss: -1.400780200958252\n",
      "Batch 31, Loss: 0.37826651334762573\n",
      "Batch 32, Loss: 0.39265260100364685\n",
      "Batch 33, Loss: -1.4064611196517944\n",
      "Batch 34, Loss: 0.3962221145629883\n",
      "Batch 35, Loss: -1.3962160348892212\n",
      "Batch 36, Loss: 0.3984651267528534\n",
      "Batch 37, Loss: 0.39538365602493286\n",
      "Batch 38, Loss: 0.38689255714416504\n",
      "Batch 39, Loss: -1.3964407444000244\n",
      "Batch 40, Loss: -1.3794721364974976\n",
      "Batch 41, Loss: 0.3840879797935486\n",
      "Batch 42, Loss: -1.3777316808700562\n",
      "Batch 43, Loss: 0.3920346796512604\n",
      "Batch 44, Loss: 0.38984161615371704\n",
      "Batch 45, Loss: -1.3932240009307861\n",
      "Batch 46, Loss: -1.3869569301605225\n",
      "Batch 47, Loss: -1.3860821723937988\n",
      "Batch 48, Loss: 0.3690347373485565\n",
      "Batch 49, Loss: 0.39751046895980835\n",
      "Batch 50, Loss: 0.3673781752586365\n",
      "Batch 51, Loss: -1.4017648696899414\n",
      "Batch 52, Loss: -1.3920947313308716\n",
      "Batch 53, Loss: 0.3924899995326996\n",
      "Batch 54, Loss: -1.3925813436508179\n",
      "Batch 55, Loss: 0.38268494606018066\n",
      "Batch 56, Loss: 0.38238388299942017\n",
      "Batch 57, Loss: -1.3608795404434204\n",
      "Batch 58, Loss: -1.3864452838897705\n",
      "Batch 59, Loss: -1.376731038093567\n",
      "Batch 60, Loss: -1.3848973512649536\n",
      "Batch 61, Loss: -1.3851567506790161\n",
      "Batch 62, Loss: -1.3969788551330566\n",
      "Batch 63, Loss: 0.38670462369918823\n",
      "Batch 64, Loss: 0.37782981991767883\n",
      "Batch 65, Loss: 0.38835856318473816\n",
      "Batch 66, Loss: -1.3921709060668945\n",
      "Batch 67, Loss: -1.3774198293685913\n",
      "Batch 68, Loss: 0.3962112069129944\n",
      "Batch 69, Loss: 0.40720516443252563\n",
      "Batch 70, Loss: 0.39359691739082336\n",
      "Batch 71, Loss: 0.3786284327507019\n",
      "Batch 72, Loss: 0.42024824023246765\n",
      "Batch 73, Loss: 0.3845587968826294\n",
      "Batch 74, Loss: 0.36576783657073975\n",
      "Batch 75, Loss: -1.3772804737091064\n",
      "Batch 76, Loss: -1.3847482204437256\n",
      "Batch 77, Loss: -1.3739511966705322\n",
      "Batch 78, Loss: -1.3941819667816162\n",
      "Batch 79, Loss: -1.360844373703003\n",
      "Batch 80, Loss: 0.3946772813796997\n",
      "Batch 81, Loss: 0.38473987579345703\n",
      "Batch 82, Loss: 0.3808971643447876\n",
      "Batch 83, Loss: -1.3783596754074097\n",
      "Batch 84, Loss: -1.3749843835830688\n",
      "Batch 85, Loss: -1.3837673664093018\n",
      "Batch 86, Loss: -1.3691962957382202\n",
      "Batch 87, Loss: -1.3706947565078735\n",
      "Batch 88, Loss: 0.41618478298187256\n",
      "Batch 89, Loss: -1.4028376340866089\n",
      "Batch 90, Loss: 0.3766193687915802\n",
      "Batch 91, Loss: 0.3885038197040558\n",
      "Batch 92, Loss: 0.3859812319278717\n",
      "Batch 93, Loss: 0.39751768112182617\n",
      "Batch 94, Loss: -1.3908716440200806\n",
      "Batch 95, Loss: 0.39358586072921753\n",
      "Batch 96, Loss: 0.38113757967948914\n",
      "Batch 97, Loss: 0.38293561339378357\n",
      "Batch 98, Loss: 0.38449981808662415\n",
      "Batch 99, Loss: -1.37638258934021\n",
      "Batch 100, Loss: -1.3941669464111328\n",
      "Batch 101, Loss: -1.3827553987503052\n",
      "Batch 102, Loss: -1.4010231494903564\n",
      "Batch 103, Loss: 0.387412428855896\n",
      "Batch 104, Loss: -1.3767825365066528\n",
      "Batch 105, Loss: 0.3811473250389099\n",
      "Batch 106, Loss: -1.389638066291809\n",
      "Batch 107, Loss: 0.37730228900909424\n",
      "Batch 108, Loss: 0.3769965171813965\n",
      "Batch 109, Loss: -1.3869044780731201\n",
      "Batch 110, Loss: -1.3838216066360474\n",
      "Batch 111, Loss: -1.3843324184417725\n",
      "Batch 112, Loss: 0.3785891532897949\n",
      "Batch 113, Loss: 0.3801681697368622\n",
      "Batch 114, Loss: 0.3849325180053711\n",
      "Batch 115, Loss: -1.3800137042999268\n",
      "Batch 116, Loss: -1.3817654848098755\n",
      "Batch 117, Loss: -1.3976655006408691\n",
      "Batch 118, Loss: 0.3933224678039551\n",
      "Batch 119, Loss: -1.3927032947540283\n",
      "Batch 120, Loss: 0.40495091676712036\n",
      "Batch 121, Loss: 0.4049479365348816\n",
      "Batch 122, Loss: 0.3986934423446655\n",
      "Batch 123, Loss: 0.3883186876773834\n",
      "Batch 124, Loss: 0.3861154019832611\n",
      "Batch 125, Loss: -1.38533353805542\n",
      "Batch 126, Loss: 0.39596027135849\n",
      "Batch 127, Loss: -1.3854608535766602\n",
      "Batch 128, Loss: 0.3786604404449463\n",
      "Batch 129, Loss: 0.3880942761898041\n",
      "Batch 130, Loss: -1.3744639158248901\n",
      "Batch 131, Loss: -1.3821660280227661\n",
      "Batch 132, Loss: -1.39121675491333\n",
      "Batch 133, Loss: -1.3771634101867676\n",
      "Batch 134, Loss: 0.38073864579200745\n",
      "Batch 135, Loss: 0.3773912191390991\n",
      "Batch 136, Loss: -1.377352237701416\n",
      "Batch 137, Loss: -1.3915109634399414\n",
      "Batch 138, Loss: -1.379900574684143\n",
      "Batch 139, Loss: 0.3850458264350891\n",
      "Batch 140, Loss: -1.379701018333435\n",
      "Batch 141, Loss: 0.3757323622703552\n",
      "Batch 142, Loss: 0.3785526752471924\n",
      "Batch 143, Loss: -1.3808413743972778\n",
      "Batch 144, Loss: -1.3985188007354736\n",
      "Batch 145, Loss: -1.3799904584884644\n",
      "Batch 146, Loss: -1.3795864582061768\n",
      "Batch 147, Loss: -1.3872672319412231\n",
      "Batch 148, Loss: -1.3832075595855713\n",
      "Batch 149, Loss: -1.3963428735733032\n",
      "Batch 150, Loss: 0.3993538022041321\n",
      "Batch 151, Loss: -1.3946070671081543\n",
      "Batch 152, Loss: -1.4048247337341309\n",
      "Batch 153, Loss: -1.390336275100708\n",
      "Batch 154, Loss: 0.3977169394493103\n",
      "Batch 155, Loss: 0.38312745094299316\n",
      "Batch 156, Loss: -1.3910949230194092\n",
      "Batch 157, Loss: -1.409820795059204\n",
      "Batch 158, Loss: -1.392004370689392\n",
      "Batch 159, Loss: 0.42374467849731445\n",
      "Batch 160, Loss: 0.3975794315338135\n",
      "Batch 161, Loss: -1.3872907161712646\n",
      "Batch 162, Loss: -1.4037163257598877\n",
      "Batch 163, Loss: 0.41660380363464355\n",
      "Batch 164, Loss: 0.4104515612125397\n",
      "Batch 165, Loss: 0.4154214859008789\n",
      "Batch 166, Loss: 0.4053107500076294\n",
      "Batch 167, Loss: 0.3876836597919464\n",
      "Batch 168, Loss: -1.39646577835083\n",
      "Batch 169, Loss: 0.4084160327911377\n",
      "Batch 170, Loss: 0.3951501250267029\n",
      "Batch 171, Loss: -1.4022222757339478\n",
      "Batch 172, Loss: 0.4203966557979584\n",
      "Batch 173, Loss: 0.396195650100708\n",
      "Batch 174, Loss: 0.39160972833633423\n",
      "Batch 175, Loss: -1.394025444984436\n",
      "Batch 176, Loss: 0.40855276584625244\n",
      "Batch 177, Loss: 0.3920862078666687\n",
      "Batch 178, Loss: -1.3868056535720825\n",
      "Batch 179, Loss: -1.3959800004959106\n",
      "Batch 180, Loss: 0.3919508457183838\n",
      "Batch 181, Loss: -1.3943785429000854\n",
      "Batch 182, Loss: 0.38721132278442383\n",
      "Batch 183, Loss: 0.41129785776138306\n",
      "Batch 184, Loss: 0.37852656841278076\n",
      "Batch 185, Loss: 0.3812350630760193\n",
      "Batch 186, Loss: 0.38693690299987793\n",
      "Batch 187, Loss: 0.3796958327293396\n",
      "Batch 188, Loss: 0.3838329613208771\n",
      "Batch 189, Loss: -1.378906488418579\n",
      "Batch 190, Loss: -1.3909472227096558\n",
      "Batch 191, Loss: 0.3876210153102875\n",
      "Batch 192, Loss: 0.3884633481502533\n",
      "Batch 193, Loss: -1.3765790462493896\n",
      "Batch 194, Loss: 0.3837891221046448\n",
      "Batch 195, Loss: -1.3775155544281006\n",
      "Batch 196, Loss: -1.3766067028045654\n",
      "Batch 197, Loss: -1.3928937911987305\n",
      "Batch 198, Loss: 0.37557369470596313\n",
      "Batch 199, Loss: -1.38604736328125\n",
      "Batch 200, Loss: -1.3767379522323608\n",
      "Batch 201, Loss: 0.3635251224040985\n",
      "Batch 202, Loss: -1.3793072700500488\n",
      "Batch 203, Loss: 0.39918744564056396\n",
      "Batch 204, Loss: -1.3873577117919922\n",
      "Batch 205, Loss: -1.3614990711212158\n",
      "Batch 206, Loss: -1.3878594636917114\n",
      "Batch 207, Loss: -1.3887953758239746\n",
      "Batch 208, Loss: 0.3823064863681793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 209, Loss: 0.38432204723358154\n",
      "Batch 210, Loss: -1.3910155296325684\n",
      "Batch 211, Loss: 0.38491734862327576\n",
      "Batch 212, Loss: 0.38033056259155273\n",
      "Batch 213, Loss: -1.3897693157196045\n",
      "Batch 214, Loss: 0.3841508626937866\n",
      "Batch 215, Loss: 0.38532301783561707\n",
      "Batch 216, Loss: -1.3902630805969238\n",
      "Batch 217, Loss: -1.376871943473816\n",
      "Batch 218, Loss: -1.383559226989746\n",
      "Batch 219, Loss: -1.3787097930908203\n",
      "Batch 220, Loss: 0.39261072874069214\n",
      "Batch 221, Loss: -1.3788857460021973\n",
      "Batch 222, Loss: -1.396632432937622\n",
      "Batch 223, Loss: 0.37916916608810425\n",
      "Batch 224, Loss: 0.378387987613678\n",
      "Batch 225, Loss: 0.38550302386283875\n",
      "Batch 226, Loss: 0.3782309293746948\n",
      "Batch 227, Loss: -1.3938398361206055\n",
      "Batch 228, Loss: 0.3874056339263916\n",
      "Batch 229, Loss: -1.3850831985473633\n",
      "Batch 230, Loss: 0.38090601563453674\n",
      "Batch 231, Loss: -1.379278540611267\n",
      "Batch 232, Loss: 0.3789743185043335\n",
      "Batch 233, Loss: -1.3828163146972656\n",
      "Batch 234, Loss: 0.38887616991996765\n",
      "Batch 235, Loss: -1.4009475708007812\n",
      "Batch 236, Loss: 0.37901997566223145\n",
      "Batch 237, Loss: -1.3976144790649414\n",
      "Batch 238, Loss: -1.3791037797927856\n",
      "Batch 239, Loss: 0.3821720480918884\n",
      "Batch 240, Loss: 0.3873291313648224\n",
      "Batch 241, Loss: 0.3757469654083252\n",
      "Batch 242, Loss: 0.37879472970962524\n",
      "Batch 243, Loss: -1.3816585540771484\n",
      "Batch 244, Loss: 0.3729039132595062\n",
      "Batch 245, Loss: -1.3801395893096924\n",
      "Batch 246, Loss: 0.37546032667160034\n",
      "Batch 247, Loss: 0.37640881538391113\n",
      "Batch 248, Loss: -1.3816728591918945\n",
      "Batch 249, Loss: 0.39185959100723267\n",
      "Batch 250, Loss: -1.3820505142211914\n",
      "Batch 251, Loss: -1.3844568729400635\n",
      "Batch 252, Loss: 0.3905187249183655\n",
      "Batch 253, Loss: -1.394721508026123\n",
      "Batch 254, Loss: -1.3785579204559326\n",
      "Batch 255, Loss: -1.375797986984253\n",
      "Batch 256, Loss: -1.3791415691375732\n",
      "Batch 257, Loss: -1.3964684009552002\n",
      "Batch 258, Loss: -1.3758149147033691\n",
      "Batch 259, Loss: 0.38055747747421265\n",
      "Batch 260, Loss: 0.39896219968795776\n",
      "Batch 261, Loss: 0.38162726163864136\n",
      "Batch 262, Loss: -1.3853802680969238\n",
      "Batch 263, Loss: 0.39488735795021057\n",
      "Batch 264, Loss: -1.3997690677642822\n",
      "Batch 265, Loss: 0.4042513966560364\n",
      "Batch 266, Loss: -1.3795182704925537\n",
      "Batch 267, Loss: 0.3997608721256256\n",
      "Batch 268, Loss: 0.38370978832244873\n",
      "Batch 269, Loss: -1.3696589469909668\n",
      "Batch 270, Loss: 0.39730826020240784\n",
      "Batch 271, Loss: 0.3809657096862793\n",
      "Batch 272, Loss: 0.3854822516441345\n",
      "Batch 273, Loss: -1.3705074787139893\n",
      "Batch 274, Loss: -1.4035347700119019\n",
      "Batch 275, Loss: 0.3794759213924408\n",
      "Batch 276, Loss: 0.3838624358177185\n",
      "Batch 277, Loss: -1.3757076263427734\n",
      "Batch 278, Loss: 0.3847970962524414\n",
      "Batch 279, Loss: 0.3960556387901306\n",
      "Batch 280, Loss: 0.40223291516304016\n",
      "Batch 281, Loss: -1.3915808200836182\n",
      "Batch 282, Loss: 0.39382076263427734\n",
      "Batch 283, Loss: -1.3922778367996216\n",
      "Batch 284, Loss: 0.38804271817207336\n",
      "Batch 285, Loss: 0.3941653370857239\n",
      "Batch 286, Loss: -1.365750789642334\n",
      "Batch 287, Loss: -1.3747352361679077\n",
      "Batch 288, Loss: -1.3748053312301636\n",
      "Batch 289, Loss: -1.3808255195617676\n",
      "Batch 290, Loss: 0.39067769050598145\n",
      "Batch 291, Loss: 0.38137999176979065\n",
      "Batch 292, Loss: -1.3757636547088623\n",
      "Batch 293, Loss: 0.38192474842071533\n",
      "Batch 294, Loss: -1.379425287246704\n",
      "Batch 295, Loss: 0.3867602050304413\n",
      "Batch 296, Loss: 0.3691091537475586\n",
      "Batch 297, Loss: 0.38594916462898254\n",
      "Batch 298, Loss: -1.3737144470214844\n",
      "Batch 299, Loss: 0.38786861300468445\n",
      "Batch 300, Loss: -1.3663434982299805\n",
      "Batch 301, Loss: 0.3838633894920349\n",
      "Batch 302, Loss: -1.3832777738571167\n",
      "Batch 303, Loss: -1.386178731918335\n",
      "Batch 304, Loss: 0.3763439655303955\n",
      "Batch 305, Loss: -1.379457950592041\n",
      "Batch 306, Loss: 0.37647151947021484\n",
      "Batch 307, Loss: 0.39266037940979004\n",
      "Batch 308, Loss: -1.3750324249267578\n",
      "Batch 309, Loss: 0.37566179037094116\n",
      "Batch 310, Loss: 0.36667177081108093\n",
      "Batch 311, Loss: -1.391228437423706\n",
      "Batch 312, Loss: 0.3755526542663574\n",
      "Batch 313, Loss: 0.37554195523262024\n",
      "Batch 314, Loss: 0.3670753240585327\n",
      "Batch 315, Loss: -1.3641945123672485\n",
      "Batch 316, Loss: 0.3666856586933136\n",
      "Batch 317, Loss: 0.3758070170879364\n",
      "Batch 318, Loss: 0.36654388904571533\n",
      "Batch 319, Loss: -1.366856575012207\n",
      "Batch 320, Loss: -1.3794344663619995\n",
      "Batch 321, Loss: -1.3626031875610352\n",
      "Batch 322, Loss: -1.3661868572235107\n",
      "Batch 323, Loss: -1.3806309700012207\n",
      "Batch 324, Loss: 0.36289864778518677\n",
      "Batch 325, Loss: 0.3635161221027374\n",
      "Batch 326, Loss: 0.3635927140712738\n",
      "Batch 327, Loss: -1.3713524341583252\n",
      "Batch 328, Loss: -1.3726255893707275\n",
      "Batch 329, Loss: 0.3681443929672241\n",
      "Batch 330, Loss: -1.3689758777618408\n",
      "Batch 331, Loss: 0.3719172179698944\n",
      "Batch 332, Loss: 0.3753490447998047\n",
      "Batch 333, Loss: 0.36957889795303345\n",
      "Batch 334, Loss: -1.366253137588501\n",
      "Batch 335, Loss: -1.366226077079773\n",
      "Batch 336, Loss: 0.3606189489364624\n",
      "Batch 337, Loss: -1.3661463260650635\n",
      "Batch 338, Loss: 0.3584827184677124\n",
      "Batch 339, Loss: -1.366842269897461\n",
      "Batch 340, Loss: -1.3645983934402466\n",
      "Batch 341, Loss: -1.3662207126617432\n",
      "Batch 342, Loss: -1.3745224475860596\n",
      "Batch 343, Loss: 0.3675716519355774\n",
      "Batch 344, Loss: -1.3679111003875732\n",
      "Batch 345, Loss: -1.367340087890625\n",
      "Batch 346, Loss: 0.37274467945098877\n",
      "Batch 347, Loss: -1.3676851987838745\n",
      "Batch 348, Loss: 0.36935627460479736\n",
      "Batch 349, Loss: -1.3709149360656738\n",
      "Batch 350, Loss: 0.3725951313972473\n",
      "Batch 351, Loss: -1.3715331554412842\n",
      "Batch 352, Loss: 0.37430816888809204\n",
      "Batch 353, Loss: -1.382986068725586\n",
      "Batch 354, Loss: 0.37084487080574036\n",
      "Batch 355, Loss: -1.3613131046295166\n",
      "Batch 356, Loss: -1.3605990409851074\n",
      "Batch 357, Loss: 0.3745710551738739\n",
      "Batch 358, Loss: -1.3728151321411133\n",
      "Batch 359, Loss: 0.37140628695487976\n",
      "Batch 360, Loss: -1.3745081424713135\n",
      "Batch 361, Loss: -1.3741583824157715\n",
      "Batch 362, Loss: 0.370838463306427\n",
      "Batch 363, Loss: -1.3870207071304321\n",
      "Batch 364, Loss: 0.37605345249176025\n",
      "Batch 365, Loss: -1.3830647468566895\n",
      "Batch 366, Loss: -1.3722224235534668\n",
      "Batch 367, Loss: 0.3820762634277344\n",
      "Batch 368, Loss: -1.3845487833023071\n",
      "Batch 369, Loss: -1.3823847770690918\n",
      "Batch 370, Loss: 0.3859252333641052\n",
      "Batch 371, Loss: 0.39120784401893616\n",
      "Batch 372, Loss: -1.379849910736084\n",
      "Batch 373, Loss: -1.3870331048965454\n",
      "Batch 374, Loss: -1.3734582662582397\n",
      "Batch 375, Loss: 0.38054296374320984\n",
      "Batch 376, Loss: -1.3796188831329346\n",
      "Batch 377, Loss: -1.3824279308319092\n",
      "Batch 378, Loss: -1.375370740890503\n",
      "Batch 379, Loss: -1.3841753005981445\n",
      "Batch 380, Loss: -1.3776750564575195\n",
      "Batch 381, Loss: 0.3866051733493805\n",
      "Batch 382, Loss: -1.387294054031372\n",
      "Batch 383, Loss: 0.386752188205719\n",
      "Batch 384, Loss: -1.3897747993469238\n",
      "Batch 385, Loss: -1.3913288116455078\n",
      "Batch 386, Loss: -1.397396206855774\n",
      "Batch 387, Loss: 0.39658066630363464\n",
      "Batch 388, Loss: -1.4086675643920898\n",
      "Batch 389, Loss: 0.3943909704685211\n",
      "Batch 390, Loss: 0.39126336574554443\n",
      "Batch 391, Loss: 0.39321401715278625\n",
      "Batch 392, Loss: -1.3855541944503784\n",
      "Batch 393, Loss: -1.39569890499115\n",
      "Batch 394, Loss: 0.390624463558197\n",
      "Batch 395, Loss: 0.3955295979976654\n",
      "Batch 396, Loss: -1.3904142379760742\n",
      "Batch 397, Loss: -1.4060873985290527\n",
      "Batch 398, Loss: -1.4076344966888428\n",
      "Batch 399, Loss: 0.41299235820770264\n",
      "Training [50%]\tLoss: -0.4986\n",
      "Batch 0, Loss: -1.400617003440857\n",
      "Batch 1, Loss: -1.3968781232833862\n",
      "Batch 2, Loss: -1.3873167037963867\n",
      "Batch 3, Loss: 0.38760319352149963\n",
      "Batch 4, Loss: 0.3993332087993622\n",
      "Batch 5, Loss: -1.4042431116104126\n",
      "Batch 6, Loss: -1.391176700592041\n",
      "Batch 7, Loss: 0.39185136556625366\n",
      "Batch 8, Loss: -1.3946716785430908\n",
      "Batch 9, Loss: 0.39942410588264465\n",
      "Batch 10, Loss: -1.3996427059173584\n",
      "Batch 11, Loss: -1.4081149101257324\n",
      "Batch 12, Loss: 0.4026857614517212\n",
      "Batch 13, Loss: -1.4053738117218018\n",
      "Batch 14, Loss: -1.4115785360336304\n",
      "Batch 15, Loss: 0.4126024842262268\n",
      "Batch 16, Loss: 0.392566055059433\n",
      "Batch 17, Loss: 0.4007222056388855\n",
      "Batch 18, Loss: 0.40301841497421265\n",
      "Batch 19, Loss: 0.4104604125022888\n",
      "Batch 20, Loss: 0.4019812047481537\n",
      "Batch 21, Loss: 0.40648287534713745\n",
      "Batch 22, Loss: 0.4016883671283722\n",
      "Batch 23, Loss: -1.3964039087295532\n",
      "Batch 24, Loss: 0.40796858072280884\n",
      "Batch 25, Loss: -1.4087566137313843\n",
      "Batch 26, Loss: 0.41347306966781616\n",
      "Batch 27, Loss: 0.4052177369594574\n",
      "Batch 28, Loss: 0.40632155537605286\n",
      "Batch 29, Loss: 0.38914796710014343\n",
      "Batch 30, Loss: -1.387921690940857\n",
      "Batch 31, Loss: 0.4010918140411377\n",
      "Batch 32, Loss: 0.399104505777359\n",
      "Batch 33, Loss: 0.4091455340385437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34, Loss: -1.3905956745147705\n",
      "Batch 35, Loss: -1.3825339078903198\n",
      "Batch 36, Loss: -1.3844237327575684\n",
      "Batch 37, Loss: -1.3888534307479858\n",
      "Batch 38, Loss: -1.3971424102783203\n",
      "Batch 39, Loss: 0.3882763087749481\n",
      "Batch 40, Loss: -1.381411075592041\n",
      "Batch 41, Loss: 0.38673192262649536\n",
      "Batch 42, Loss: 0.3827071189880371\n",
      "Batch 43, Loss: 0.39695441722869873\n",
      "Batch 44, Loss: -1.3839337825775146\n",
      "Batch 45, Loss: -1.3851499557495117\n",
      "Batch 46, Loss: 0.38625866174697876\n",
      "Batch 47, Loss: 0.3872092366218567\n",
      "Batch 48, Loss: 0.39248719811439514\n",
      "Batch 49, Loss: -1.391654372215271\n",
      "Batch 50, Loss: 0.3826829195022583\n",
      "Batch 51, Loss: -1.3810235261917114\n",
      "Batch 52, Loss: -1.3933624029159546\n",
      "Batch 53, Loss: 0.3896852731704712\n",
      "Batch 54, Loss: -1.3892905712127686\n",
      "Batch 55, Loss: -1.3812813758850098\n",
      "Batch 56, Loss: 0.38240277767181396\n",
      "Batch 57, Loss: -1.3841586112976074\n",
      "Batch 58, Loss: -1.3824894428253174\n",
      "Batch 59, Loss: 0.38441595435142517\n",
      "Batch 60, Loss: -1.385427474975586\n",
      "Batch 61, Loss: -1.3833646774291992\n",
      "Batch 62, Loss: -1.3884165287017822\n",
      "Batch 63, Loss: 0.37998610734939575\n",
      "Batch 64, Loss: -1.3829267024993896\n",
      "Batch 65, Loss: -1.3934786319732666\n",
      "Batch 66, Loss: -1.3916493654251099\n",
      "Batch 67, Loss: -1.3848857879638672\n",
      "Batch 68, Loss: -1.394811749458313\n",
      "Batch 69, Loss: 0.3883862793445587\n",
      "Batch 70, Loss: 0.38573262095451355\n",
      "Batch 71, Loss: -1.399152398109436\n",
      "Batch 72, Loss: -1.3888344764709473\n",
      "Batch 73, Loss: -1.390432596206665\n",
      "Batch 74, Loss: 0.40159741044044495\n",
      "Batch 75, Loss: -1.40117347240448\n",
      "Batch 76, Loss: -1.398895263671875\n",
      "Batch 77, Loss: 0.39626798033714294\n",
      "Batch 78, Loss: -1.3890798091888428\n",
      "Batch 79, Loss: 0.4000418782234192\n",
      "Batch 80, Loss: 0.3982493579387665\n",
      "Batch 81, Loss: -1.4072234630584717\n",
      "Batch 82, Loss: -1.3972773551940918\n",
      "Batch 83, Loss: 0.4084586501121521\n",
      "Batch 84, Loss: -1.4110496044158936\n",
      "Batch 85, Loss: -1.3951183557510376\n",
      "Batch 86, Loss: -1.4008774757385254\n",
      "Batch 87, Loss: 0.40070152282714844\n",
      "Batch 88, Loss: -1.3922741413116455\n",
      "Batch 89, Loss: 0.4098321497440338\n",
      "Batch 90, Loss: 0.41707250475883484\n",
      "Batch 91, Loss: 0.41238290071487427\n",
      "Batch 92, Loss: 0.4145846664905548\n",
      "Batch 93, Loss: 0.40239864587783813\n",
      "Batch 94, Loss: 0.40877193212509155\n",
      "Batch 95, Loss: -1.4008897542953491\n",
      "Batch 96, Loss: -1.402043104171753\n",
      "Batch 97, Loss: 0.4000501334667206\n",
      "Batch 98, Loss: 0.4028494358062744\n",
      "Batch 99, Loss: 0.4016203284263611\n",
      "Batch 100, Loss: -1.4094029664993286\n",
      "Batch 101, Loss: 0.40979811549186707\n",
      "Batch 102, Loss: 0.3995465636253357\n",
      "Batch 103, Loss: -1.39975905418396\n",
      "Batch 104, Loss: -1.393983244895935\n",
      "Batch 105, Loss: -1.392359733581543\n",
      "Batch 106, Loss: 0.4016745686531067\n",
      "Batch 107, Loss: -1.3974417448043823\n",
      "Batch 108, Loss: -1.3959124088287354\n",
      "Batch 109, Loss: -1.398332118988037\n",
      "Batch 110, Loss: 0.3983295261859894\n",
      "Batch 111, Loss: 0.3981214463710785\n",
      "Batch 112, Loss: -1.3967667818069458\n",
      "Batch 113, Loss: -1.3904633522033691\n",
      "Batch 114, Loss: -1.3972753286361694\n",
      "Batch 115, Loss: 0.39102810621261597\n",
      "Batch 116, Loss: 0.399748295545578\n",
      "Batch 117, Loss: -1.4026726484298706\n",
      "Batch 118, Loss: -1.41391921043396\n",
      "Batch 119, Loss: -1.4022088050842285\n",
      "Batch 120, Loss: 0.40124836564064026\n",
      "Batch 121, Loss: -1.400159478187561\n",
      "Batch 122, Loss: -1.3980238437652588\n",
      "Batch 123, Loss: -1.402794599533081\n",
      "Batch 124, Loss: 0.40944114327430725\n",
      "Batch 125, Loss: 0.40320950746536255\n",
      "Batch 126, Loss: 0.40336722135543823\n",
      "Batch 127, Loss: -1.3986458778381348\n",
      "Batch 128, Loss: 0.41880929470062256\n",
      "Batch 129, Loss: -1.4031420946121216\n",
      "Batch 130, Loss: -1.4032938480377197\n",
      "Batch 131, Loss: -1.4157001972198486\n",
      "Batch 132, Loss: -1.404728889465332\n",
      "Batch 133, Loss: -1.4171984195709229\n",
      "Batch 134, Loss: 0.39773690700531006\n",
      "Batch 135, Loss: 0.41878458857536316\n",
      "Batch 136, Loss: -1.410752296447754\n",
      "Batch 137, Loss: -1.400425910949707\n",
      "Batch 138, Loss: 0.40471598505973816\n",
      "Batch 139, Loss: -1.3999465703964233\n",
      "Batch 140, Loss: 0.41926032304763794\n",
      "Batch 141, Loss: 0.41381198167800903\n",
      "Batch 142, Loss: 0.41770732402801514\n",
      "Batch 143, Loss: -1.4101332426071167\n",
      "Batch 144, Loss: 0.4042947292327881\n",
      "Batch 145, Loss: -1.4002488851547241\n",
      "Batch 146, Loss: 0.41163134574890137\n",
      "Batch 147, Loss: 0.40705424547195435\n",
      "Batch 148, Loss: -1.400566816329956\n",
      "Batch 149, Loss: 0.40433958172798157\n",
      "Batch 150, Loss: 0.4051734209060669\n",
      "Batch 151, Loss: 0.4031692147254944\n",
      "Batch 152, Loss: -1.4159212112426758\n",
      "Batch 153, Loss: 0.40913084149360657\n",
      "Batch 154, Loss: 0.4008026719093323\n",
      "Batch 155, Loss: 0.4050223231315613\n",
      "Batch 156, Loss: 0.41085588932037354\n",
      "Batch 157, Loss: -1.4005589485168457\n",
      "Batch 158, Loss: -1.3998174667358398\n",
      "Batch 159, Loss: 0.4010574519634247\n",
      "Batch 160, Loss: 0.4128400385379791\n",
      "Batch 161, Loss: -1.398010492324829\n",
      "Batch 162, Loss: -1.393890619277954\n",
      "Batch 163, Loss: 0.41452252864837646\n",
      "Batch 164, Loss: -1.3997471332550049\n",
      "Batch 165, Loss: -1.3961715698242188\n",
      "Batch 166, Loss: -1.3938517570495605\n",
      "Batch 167, Loss: -1.4072377681732178\n",
      "Batch 168, Loss: -1.3907707929611206\n",
      "Batch 169, Loss: -1.3944437503814697\n",
      "Batch 170, Loss: -1.393673300743103\n",
      "Batch 171, Loss: -1.412383794784546\n",
      "Batch 172, Loss: -1.3957974910736084\n",
      "Batch 173, Loss: -1.4063339233398438\n",
      "Batch 174, Loss: 0.40008020401000977\n",
      "Batch 175, Loss: 0.4069386124610901\n",
      "Batch 176, Loss: 0.40451502799987793\n",
      "Batch 177, Loss: 0.40913259983062744\n",
      "Batch 178, Loss: 0.40110141038894653\n",
      "Batch 179, Loss: -1.4094979763031006\n",
      "Batch 180, Loss: 0.40872061252593994\n",
      "Batch 181, Loss: 0.4006783962249756\n",
      "Batch 182, Loss: 0.40350213646888733\n",
      "Batch 183, Loss: 0.40305233001708984\n",
      "Batch 184, Loss: 0.4002777636051178\n",
      "Batch 185, Loss: 0.4119316637516022\n",
      "Batch 186, Loss: 0.4000355899333954\n",
      "Batch 187, Loss: 0.3950921595096588\n",
      "Batch 188, Loss: -1.3989351987838745\n",
      "Batch 189, Loss: -1.3943439722061157\n",
      "Batch 190, Loss: 0.4019019603729248\n",
      "Batch 191, Loss: -1.3969037532806396\n",
      "Batch 192, Loss: 0.39213722944259644\n",
      "Batch 193, Loss: -1.3916399478912354\n",
      "Batch 194, Loss: 0.4039185345172882\n",
      "Batch 195, Loss: -1.3892064094543457\n",
      "Batch 196, Loss: 0.39510107040405273\n",
      "Batch 197, Loss: -1.394622564315796\n",
      "Batch 198, Loss: -1.3998051881790161\n",
      "Batch 199, Loss: -1.3945493698120117\n",
      "Batch 200, Loss: -1.3917533159255981\n",
      "Batch 201, Loss: 0.39755767583847046\n",
      "Batch 202, Loss: 0.3966979384422302\n",
      "Batch 203, Loss: 0.3990733325481415\n",
      "Batch 204, Loss: -1.3961751461029053\n",
      "Batch 205, Loss: 0.39475217461586\n",
      "Batch 206, Loss: -1.3900721073150635\n",
      "Batch 207, Loss: -1.3967889547348022\n",
      "Batch 208, Loss: 0.3927415907382965\n",
      "Batch 209, Loss: 0.39239394664764404\n",
      "Batch 210, Loss: 0.3957989513874054\n",
      "Batch 211, Loss: 0.390724241733551\n",
      "Batch 212, Loss: -1.3932840824127197\n",
      "Batch 213, Loss: -1.4023405313491821\n",
      "Batch 214, Loss: 0.39628326892852783\n",
      "Batch 215, Loss: -1.3926584720611572\n",
      "Batch 216, Loss: 0.3926542103290558\n",
      "Batch 217, Loss: 0.40758216381073\n",
      "Batch 218, Loss: 0.3944348394870758\n",
      "Batch 219, Loss: -1.3888992071151733\n",
      "Batch 220, Loss: 0.3986018896102905\n",
      "Batch 221, Loss: -1.385342001914978\n",
      "Batch 222, Loss: 0.3930549621582031\n",
      "Batch 223, Loss: 0.39213913679122925\n",
      "Batch 224, Loss: -1.392401099205017\n",
      "Batch 225, Loss: -1.388890266418457\n",
      "Batch 226, Loss: 0.3909720182418823\n",
      "Batch 227, Loss: 0.38461947441101074\n",
      "Batch 228, Loss: -1.385585904121399\n",
      "Batch 229, Loss: 0.3842608630657196\n",
      "Batch 230, Loss: 0.3898420035839081\n",
      "Batch 231, Loss: 0.38280797004699707\n",
      "Batch 232, Loss: -1.3858001232147217\n",
      "Batch 233, Loss: -1.3836883306503296\n",
      "Batch 234, Loss: 0.3864177167415619\n",
      "Batch 235, Loss: 0.3886266350746155\n",
      "Batch 236, Loss: -1.390649676322937\n",
      "Batch 237, Loss: 0.3849039673805237\n",
      "Batch 238, Loss: -1.3867292404174805\n",
      "Batch 239, Loss: 0.3791636824607849\n",
      "Batch 240, Loss: 0.3843648135662079\n",
      "Batch 241, Loss: 0.3892676532268524\n",
      "Batch 242, Loss: 0.38803765177726746\n",
      "Batch 243, Loss: -1.3727102279663086\n",
      "Batch 244, Loss: -1.3820126056671143\n",
      "Batch 245, Loss: -1.372190237045288\n",
      "Batch 246, Loss: 0.38373470306396484\n",
      "Batch 247, Loss: 0.3952925503253937\n",
      "Batch 248, Loss: -1.3885302543640137\n",
      "Batch 249, Loss: -1.3948132991790771\n",
      "Batch 250, Loss: -1.375091791152954\n",
      "Batch 251, Loss: -1.3893746137619019\n",
      "Batch 252, Loss: 0.3839356303215027\n",
      "Batch 253, Loss: -1.3806722164154053\n",
      "Batch 254, Loss: -1.3814315795898438\n",
      "Batch 255, Loss: -1.3837519884109497\n",
      "Batch 256, Loss: -1.3868298530578613\n",
      "Batch 257, Loss: 0.3896991014480591\n",
      "Batch 258, Loss: -1.3799123764038086\n",
      "Batch 259, Loss: 0.3862590193748474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 260, Loss: 0.384304404258728\n",
      "Batch 261, Loss: -1.3837753534317017\n",
      "Batch 262, Loss: 0.3853951096534729\n",
      "Batch 263, Loss: -1.3823487758636475\n",
      "Batch 264, Loss: -1.3840715885162354\n",
      "Batch 265, Loss: -1.3844534158706665\n",
      "Batch 266, Loss: -1.3882932662963867\n",
      "Batch 267, Loss: 0.3842512369155884\n",
      "Batch 268, Loss: 0.38987094163894653\n",
      "Batch 269, Loss: 0.39001062512397766\n",
      "Batch 270, Loss: -1.3896586894989014\n",
      "Batch 271, Loss: -1.3938610553741455\n",
      "Batch 272, Loss: 0.3867490887641907\n",
      "Batch 273, Loss: -1.3844670057296753\n",
      "Batch 274, Loss: -1.3890058994293213\n",
      "Batch 275, Loss: 0.38985559344291687\n",
      "Batch 276, Loss: 0.3954257369041443\n",
      "Batch 277, Loss: -1.389626383781433\n",
      "Batch 278, Loss: -1.3960611820220947\n",
      "Batch 279, Loss: 0.3870100677013397\n",
      "Batch 280, Loss: 0.38618528842926025\n",
      "Batch 281, Loss: -1.398482084274292\n",
      "Batch 282, Loss: -1.3857029676437378\n",
      "Batch 283, Loss: 0.39504915475845337\n",
      "Batch 284, Loss: -1.397178053855896\n",
      "Batch 285, Loss: -1.3918873071670532\n",
      "Batch 286, Loss: 0.3935544788837433\n",
      "Batch 287, Loss: 0.38980361819267273\n",
      "Batch 288, Loss: 0.39222002029418945\n",
      "Batch 289, Loss: -1.3876028060913086\n",
      "Batch 290, Loss: -1.4050675630569458\n",
      "Batch 291, Loss: 0.38907381892204285\n",
      "Batch 292, Loss: -1.3875360488891602\n",
      "Batch 293, Loss: 0.38753271102905273\n",
      "Batch 294, Loss: 0.393415242433548\n",
      "Batch 295, Loss: -1.389020323753357\n",
      "Batch 296, Loss: 0.3899070918560028\n",
      "Batch 297, Loss: -1.3947978019714355\n",
      "Batch 298, Loss: -1.393235683441162\n",
      "Batch 299, Loss: -1.3902479410171509\n",
      "Batch 300, Loss: -1.3922007083892822\n",
      "Batch 301, Loss: 0.3939337134361267\n",
      "Batch 302, Loss: -1.4072445631027222\n",
      "Batch 303, Loss: -1.3925496339797974\n",
      "Batch 304, Loss: 0.38937151432037354\n",
      "Batch 305, Loss: -1.3978798389434814\n",
      "Batch 306, Loss: -1.3989710807800293\n",
      "Batch 307, Loss: 0.3964877128601074\n",
      "Batch 308, Loss: -1.4062227010726929\n",
      "Batch 309, Loss: -1.395022988319397\n",
      "Batch 310, Loss: 0.39737290143966675\n",
      "Batch 311, Loss: 0.39230233430862427\n",
      "Batch 312, Loss: -1.392354130744934\n",
      "Batch 313, Loss: 0.39459913969039917\n",
      "Batch 314, Loss: 0.39677363634109497\n",
      "Batch 315, Loss: 0.4007067084312439\n",
      "Batch 316, Loss: 0.39755353331565857\n",
      "Batch 317, Loss: -1.393553376197815\n",
      "Batch 318, Loss: -1.40135657787323\n",
      "Batch 319, Loss: -1.3913599252700806\n",
      "Batch 320, Loss: 0.3926640748977661\n",
      "Batch 321, Loss: 0.3996608257293701\n",
      "Batch 322, Loss: 0.3953971862792969\n",
      "Batch 323, Loss: -1.4129711389541626\n",
      "Batch 324, Loss: 0.39155828952789307\n",
      "Batch 325, Loss: -1.3908251523971558\n",
      "Batch 326, Loss: -1.3916902542114258\n",
      "Batch 327, Loss: -1.3958663940429688\n",
      "Batch 328, Loss: 0.4034654200077057\n",
      "Batch 329, Loss: -1.3974530696868896\n",
      "Batch 330, Loss: -1.3916759490966797\n",
      "Batch 331, Loss: 0.39682537317276\n",
      "Batch 332, Loss: -1.3925551176071167\n",
      "Batch 333, Loss: -1.39601469039917\n",
      "Batch 334, Loss: 0.3933185935020447\n",
      "Batch 335, Loss: -1.3980423212051392\n",
      "Batch 336, Loss: 0.3990837633609772\n",
      "Batch 337, Loss: -1.4004347324371338\n",
      "Batch 338, Loss: -1.3983886241912842\n",
      "Batch 339, Loss: 0.3947819471359253\n",
      "Batch 340, Loss: -1.4031860828399658\n",
      "Batch 341, Loss: -1.406080961227417\n",
      "Batch 342, Loss: -1.3971774578094482\n",
      "Batch 343, Loss: 0.3978860676288605\n",
      "Batch 344, Loss: -1.399847149848938\n",
      "Batch 345, Loss: 0.40037059783935547\n",
      "Batch 346, Loss: -1.4080777168273926\n",
      "Batch 347, Loss: 0.4035080075263977\n",
      "Batch 348, Loss: 0.4014713168144226\n",
      "Batch 349, Loss: -1.4076993465423584\n",
      "Batch 350, Loss: -1.4003307819366455\n",
      "Batch 351, Loss: -1.4108645915985107\n",
      "Batch 352, Loss: 0.4032060205936432\n",
      "Batch 353, Loss: 0.40886190533638\n",
      "Batch 354, Loss: -1.4015440940856934\n",
      "Batch 355, Loss: -1.423384666442871\n",
      "Batch 356, Loss: -1.4066734313964844\n",
      "Batch 357, Loss: -1.4057343006134033\n",
      "Batch 358, Loss: -1.408236026763916\n",
      "Batch 359, Loss: 0.4050910174846649\n",
      "Batch 360, Loss: 0.4047510623931885\n",
      "Batch 361, Loss: 0.41121184825897217\n",
      "Batch 362, Loss: 0.41846349835395813\n",
      "Batch 363, Loss: 0.42057743668556213\n",
      "Batch 364, Loss: 0.40970686078071594\n",
      "Batch 365, Loss: 0.40465885400772095\n",
      "Batch 366, Loss: 0.4120105504989624\n",
      "Batch 367, Loss: 0.41835302114486694\n",
      "Batch 368, Loss: -1.4049667119979858\n",
      "Batch 369, Loss: -1.4024041891098022\n",
      "Batch 370, Loss: 0.4089798927307129\n",
      "Batch 371, Loss: 0.4008615016937256\n",
      "Batch 372, Loss: 0.4113103151321411\n",
      "Batch 373, Loss: 0.40087229013442993\n",
      "Batch 374, Loss: 0.4105466604232788\n",
      "Batch 375, Loss: -1.405846118927002\n",
      "Batch 376, Loss: -1.4009082317352295\n",
      "Batch 377, Loss: 0.404599666595459\n",
      "Batch 378, Loss: -1.3995857238769531\n",
      "Batch 379, Loss: 0.40562260150909424\n",
      "Batch 380, Loss: -1.4101735353469849\n",
      "Batch 381, Loss: -1.401090383529663\n",
      "Batch 382, Loss: 0.4084469974040985\n",
      "Batch 383, Loss: 0.40351033210754395\n",
      "Batch 384, Loss: 0.39484113454818726\n",
      "Batch 385, Loss: 0.3984084129333496\n",
      "Batch 386, Loss: 0.3984546363353729\n",
      "Batch 387, Loss: -1.3920350074768066\n",
      "Batch 388, Loss: 0.39554429054260254\n",
      "Batch 389, Loss: -1.3907442092895508\n",
      "Batch 390, Loss: 0.4037190079689026\n",
      "Batch 391, Loss: -1.3934201002120972\n",
      "Batch 392, Loss: 0.4000251889228821\n",
      "Batch 393, Loss: 0.3894530236721039\n",
      "Batch 394, Loss: 0.3932650685310364\n",
      "Batch 395, Loss: 0.3950605094432831\n",
      "Batch 396, Loss: 0.3875226378440857\n",
      "Batch 397, Loss: -1.3906368017196655\n",
      "Batch 398, Loss: -1.3930937051773071\n",
      "Batch 399, Loss: -1.389685869216919\n",
      "Training [60%]\tLoss: -0.4984\n",
      "Batch 0, Loss: -1.3849307298660278\n",
      "Batch 1, Loss: -1.3945775032043457\n",
      "Batch 2, Loss: 0.39477428793907166\n",
      "Batch 3, Loss: -1.393662929534912\n",
      "Batch 4, Loss: 0.3890461027622223\n",
      "Batch 5, Loss: 0.3857264816761017\n",
      "Batch 6, Loss: -1.388655424118042\n",
      "Batch 7, Loss: 0.38816967606544495\n",
      "Batch 8, Loss: 0.3849838674068451\n",
      "Batch 9, Loss: -1.3907642364501953\n",
      "Batch 10, Loss: -1.393193244934082\n",
      "Batch 11, Loss: -1.3977406024932861\n",
      "Batch 12, Loss: -1.3915908336639404\n",
      "Batch 13, Loss: -1.3845175504684448\n",
      "Batch 14, Loss: -1.3955504894256592\n",
      "Batch 15, Loss: -1.3927210569381714\n",
      "Batch 16, Loss: 0.399731308221817\n",
      "Batch 17, Loss: -1.3916254043579102\n",
      "Batch 18, Loss: -1.391828179359436\n",
      "Batch 19, Loss: -1.4057648181915283\n",
      "Batch 20, Loss: -1.3997119665145874\n",
      "Batch 21, Loss: 0.39513927698135376\n",
      "Batch 22, Loss: 0.41343769431114197\n",
      "Batch 23, Loss: -1.400503396987915\n",
      "Batch 24, Loss: -1.3969664573669434\n",
      "Batch 25, Loss: -1.4020237922668457\n",
      "Batch 26, Loss: 0.3969869315624237\n",
      "Batch 27, Loss: 0.3946898877620697\n",
      "Batch 28, Loss: -1.3954684734344482\n",
      "Batch 29, Loss: 0.40690505504608154\n",
      "Batch 30, Loss: 0.4046589136123657\n",
      "Batch 31, Loss: 0.40716552734375\n",
      "Batch 32, Loss: -1.411388635635376\n",
      "Batch 33, Loss: -1.4044231176376343\n",
      "Batch 34, Loss: -1.3957092761993408\n",
      "Batch 35, Loss: -1.406262755393982\n",
      "Batch 36, Loss: 0.4047725796699524\n",
      "Batch 37, Loss: -1.409939169883728\n",
      "Batch 38, Loss: -1.3969404697418213\n",
      "Batch 39, Loss: 0.4019593894481659\n",
      "Batch 40, Loss: 0.4048534631729126\n",
      "Batch 41, Loss: 0.4016370475292206\n",
      "Batch 42, Loss: 0.40657901763916016\n",
      "Batch 43, Loss: -1.4012541770935059\n",
      "Batch 44, Loss: 0.42056822776794434\n",
      "Batch 45, Loss: 0.4084916114807129\n",
      "Batch 46, Loss: 0.40046072006225586\n",
      "Batch 47, Loss: 0.40001556277275085\n",
      "Batch 48, Loss: 0.40750986337661743\n",
      "Batch 49, Loss: 0.4027976393699646\n",
      "Batch 50, Loss: -1.4159345626831055\n",
      "Batch 51, Loss: 0.39845186471939087\n",
      "Batch 52, Loss: 0.403207004070282\n",
      "Batch 53, Loss: -1.3979270458221436\n",
      "Batch 54, Loss: -1.4088643789291382\n",
      "Batch 55, Loss: 0.40769433975219727\n",
      "Batch 56, Loss: -1.3963441848754883\n",
      "Batch 57, Loss: 0.39622437953948975\n",
      "Batch 58, Loss: 0.3968276381492615\n",
      "Batch 59, Loss: -1.4050769805908203\n",
      "Batch 60, Loss: -1.399856686592102\n",
      "Batch 61, Loss: 0.3921990692615509\n",
      "Batch 62, Loss: 0.3978482186794281\n",
      "Batch 63, Loss: -1.3977904319763184\n",
      "Batch 64, Loss: -1.4044569730758667\n",
      "Batch 65, Loss: 0.39074236154556274\n",
      "Batch 66, Loss: -1.393546223640442\n",
      "Batch 67, Loss: -1.4046847820281982\n",
      "Batch 68, Loss: 0.39495497941970825\n",
      "Batch 69, Loss: -1.3908571004867554\n",
      "Batch 70, Loss: -1.39106023311615\n",
      "Batch 71, Loss: 0.4022246301174164\n",
      "Batch 72, Loss: -1.4101393222808838\n",
      "Batch 73, Loss: -1.4011855125427246\n",
      "Batch 74, Loss: -1.4018607139587402\n",
      "Batch 75, Loss: 0.3911552429199219\n",
      "Batch 76, Loss: 0.39342695474624634\n",
      "Batch 77, Loss: 0.409321665763855\n",
      "Batch 78, Loss: 0.3962480425834656\n",
      "Batch 79, Loss: 0.39332395792007446\n",
      "Batch 80, Loss: -1.4040558338165283\n",
      "Batch 81, Loss: 0.3918007016181946\n",
      "Batch 82, Loss: 0.3950903117656708\n",
      "Batch 83, Loss: -1.3943216800689697\n",
      "Batch 84, Loss: 0.4084656536579132\n",
      "Batch 85, Loss: 0.3984692692756653\n",
      "Batch 86, Loss: -1.3937225341796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 87, Loss: -1.3888170719146729\n",
      "Batch 88, Loss: -1.393251895904541\n",
      "Batch 89, Loss: 0.3988754153251648\n",
      "Batch 90, Loss: -1.3871304988861084\n",
      "Batch 91, Loss: -1.4064126014709473\n",
      "Batch 92, Loss: -1.3940359354019165\n",
      "Batch 93, Loss: -1.3991100788116455\n",
      "Batch 94, Loss: -1.3974583148956299\n",
      "Batch 95, Loss: -1.3915185928344727\n",
      "Batch 96, Loss: -1.4011728763580322\n",
      "Batch 97, Loss: -1.3974963426589966\n",
      "Batch 98, Loss: -1.3979014158248901\n",
      "Batch 99, Loss: 0.39683112502098083\n",
      "Batch 100, Loss: 0.4145124554634094\n",
      "Batch 101, Loss: -1.3966853618621826\n",
      "Batch 102, Loss: 0.41100165247917175\n",
      "Batch 103, Loss: 0.39817506074905396\n",
      "Batch 104, Loss: 0.4161093235015869\n",
      "Batch 105, Loss: 0.3995484709739685\n",
      "Batch 106, Loss: 0.39899688959121704\n",
      "Batch 107, Loss: -1.3986124992370605\n",
      "Batch 108, Loss: 0.3956276774406433\n",
      "Batch 109, Loss: -1.40257728099823\n",
      "Batch 110, Loss: 0.4097701609134674\n",
      "Batch 111, Loss: -1.4021453857421875\n",
      "Batch 112, Loss: 0.40904027223587036\n",
      "Batch 113, Loss: -1.4041197299957275\n",
      "Batch 114, Loss: -1.394728183746338\n",
      "Batch 115, Loss: 0.39633873105049133\n",
      "Batch 116, Loss: 0.4057168960571289\n",
      "Batch 117, Loss: 0.39701539278030396\n",
      "Batch 118, Loss: -1.3968794345855713\n",
      "Batch 119, Loss: 0.39651140570640564\n",
      "Batch 120, Loss: -1.4112263917922974\n",
      "Batch 121, Loss: -1.4040143489837646\n",
      "Batch 122, Loss: 0.3959179222583771\n",
      "Batch 123, Loss: -1.3956583738327026\n",
      "Batch 124, Loss: 0.3957568407058716\n",
      "Batch 125, Loss: 0.41097986698150635\n",
      "Batch 126, Loss: 0.41059884428977966\n",
      "Batch 127, Loss: 0.4051920771598816\n",
      "Batch 128, Loss: -1.3983314037322998\n",
      "Batch 129, Loss: 0.39200782775878906\n",
      "Batch 130, Loss: 0.4079062342643738\n",
      "Batch 131, Loss: -1.3932918310165405\n",
      "Batch 132, Loss: -1.4078481197357178\n",
      "Batch 133, Loss: -1.391687035560608\n",
      "Batch 134, Loss: -1.3955073356628418\n",
      "Batch 135, Loss: -1.396966576576233\n",
      "Batch 136, Loss: 0.392066091299057\n",
      "Batch 137, Loss: -1.3976126909255981\n",
      "Batch 138, Loss: 0.39802491664886475\n",
      "Batch 139, Loss: -1.4019126892089844\n",
      "Batch 140, Loss: 0.40313470363616943\n",
      "Batch 141, Loss: -1.3915704488754272\n",
      "Batch 142, Loss: -1.3939311504364014\n",
      "Batch 143, Loss: -1.3940913677215576\n",
      "Batch 144, Loss: 0.3947080969810486\n",
      "Batch 145, Loss: -1.402403473854065\n",
      "Batch 146, Loss: -1.3933372497558594\n",
      "Batch 147, Loss: -1.4052278995513916\n",
      "Batch 148, Loss: -1.397550344467163\n",
      "Batch 149, Loss: 0.405465304851532\n",
      "Batch 150, Loss: 0.3963485658168793\n",
      "Batch 151, Loss: -1.4150099754333496\n",
      "Batch 152, Loss: 0.39901381731033325\n",
      "Batch 153, Loss: -1.4075121879577637\n",
      "Batch 154, Loss: 0.41815340518951416\n",
      "Batch 155, Loss: 0.4062618911266327\n",
      "Batch 156, Loss: -1.4003784656524658\n",
      "Batch 157, Loss: -1.3978567123413086\n",
      "Batch 158, Loss: -1.4077951908111572\n",
      "Batch 159, Loss: -1.4111967086791992\n",
      "Batch 160, Loss: -1.4059181213378906\n",
      "Batch 161, Loss: 0.4002816677093506\n",
      "Batch 162, Loss: -1.4122505187988281\n",
      "Batch 163, Loss: -1.4011359214782715\n",
      "Batch 164, Loss: 0.4090110659599304\n",
      "Batch 165, Loss: -1.4114274978637695\n",
      "Batch 166, Loss: -1.4060214757919312\n",
      "Batch 167, Loss: 0.40708479285240173\n",
      "Batch 168, Loss: -1.4046757221221924\n",
      "Batch 169, Loss: -1.4091482162475586\n",
      "Batch 170, Loss: 0.4269987940788269\n",
      "Batch 171, Loss: 0.4063694179058075\n",
      "Batch 172, Loss: -1.409388780593872\n",
      "Batch 173, Loss: -1.4117002487182617\n",
      "Batch 174, Loss: -1.4282591342926025\n",
      "Batch 175, Loss: -1.4156807661056519\n",
      "Batch 176, Loss: 0.4202103614807129\n",
      "Batch 177, Loss: -1.4277572631835938\n",
      "Batch 178, Loss: 0.42452141642570496\n",
      "Batch 179, Loss: -1.4146251678466797\n",
      "Batch 180, Loss: -1.4118012189865112\n",
      "Batch 181, Loss: 0.4205777943134308\n",
      "Batch 182, Loss: -1.4210785627365112\n",
      "Batch 183, Loss: -1.4181911945343018\n",
      "Batch 184, Loss: -1.4147800207138062\n",
      "Batch 185, Loss: 0.4213939309120178\n",
      "Batch 186, Loss: 0.4197584390640259\n",
      "Batch 187, Loss: 0.42003005743026733\n",
      "Batch 188, Loss: -1.4249558448791504\n",
      "Batch 189, Loss: 0.4220675826072693\n",
      "Batch 190, Loss: 0.42033836245536804\n",
      "Batch 191, Loss: 0.42571350932121277\n",
      "Batch 192, Loss: -1.4347128868103027\n",
      "Batch 193, Loss: 0.42464083433151245\n",
      "Batch 194, Loss: 0.4329878091812134\n",
      "Batch 195, Loss: 0.4193936586380005\n",
      "Batch 196, Loss: 0.4157584309577942\n",
      "Batch 197, Loss: -1.4151272773742676\n",
      "Batch 198, Loss: 0.42621108889579773\n",
      "Batch 199, Loss: 0.4187930226325989\n",
      "Batch 200, Loss: -1.4124616384506226\n",
      "Batch 201, Loss: -1.4165147542953491\n",
      "Batch 202, Loss: 0.4164186418056488\n",
      "Batch 203, Loss: -1.4214075803756714\n",
      "Batch 204, Loss: 0.4177587628364563\n",
      "Batch 205, Loss: -1.4207258224487305\n",
      "Batch 206, Loss: 0.43064209818840027\n",
      "Batch 207, Loss: -1.4277000427246094\n",
      "Batch 208, Loss: 0.4184635877609253\n",
      "Batch 209, Loss: -1.4078049659729004\n",
      "Batch 210, Loss: 0.42411983013153076\n",
      "Batch 211, Loss: 0.4131816625595093\n",
      "Batch 212, Loss: -1.41879141330719\n",
      "Batch 213, Loss: -1.4149818420410156\n",
      "Batch 214, Loss: -1.4109336137771606\n",
      "Batch 215, Loss: 0.4088599681854248\n",
      "Batch 216, Loss: 0.4191949665546417\n",
      "Batch 217, Loss: 0.4075310230255127\n",
      "Batch 218, Loss: -1.4219471216201782\n",
      "Batch 219, Loss: 0.42184925079345703\n",
      "Batch 220, Loss: -1.4109954833984375\n",
      "Batch 221, Loss: 0.4132465720176697\n",
      "Batch 222, Loss: 0.4129957854747772\n",
      "Batch 223, Loss: 0.41104432940483093\n",
      "Batch 224, Loss: 0.42281049489974976\n",
      "Batch 225, Loss: -1.4091757535934448\n",
      "Batch 226, Loss: 0.40376442670822144\n",
      "Batch 227, Loss: 0.41419410705566406\n",
      "Batch 228, Loss: 0.4093245267868042\n",
      "Batch 229, Loss: -1.4082882404327393\n",
      "Batch 230, Loss: 0.4112677574157715\n",
      "Batch 231, Loss: -1.4042608737945557\n",
      "Batch 232, Loss: -1.4069387912750244\n",
      "Batch 233, Loss: -1.4067621231079102\n",
      "Batch 234, Loss: 0.3988216519355774\n",
      "Batch 235, Loss: -1.4080504179000854\n",
      "Batch 236, Loss: -1.4056891202926636\n",
      "Batch 237, Loss: 0.402820885181427\n",
      "Batch 238, Loss: 0.39644482731819153\n",
      "Batch 239, Loss: -1.4104132652282715\n",
      "Batch 240, Loss: 0.40403318405151367\n",
      "Batch 241, Loss: 0.4047352373600006\n",
      "Batch 242, Loss: 0.39991018176078796\n",
      "Batch 243, Loss: -1.3960034847259521\n",
      "Batch 244, Loss: -1.4076964855194092\n",
      "Batch 245, Loss: -1.3903495073318481\n",
      "Batch 246, Loss: -1.3915770053863525\n",
      "Batch 247, Loss: -1.408642053604126\n",
      "Batch 248, Loss: 0.41780611872673035\n",
      "Batch 249, Loss: -1.4174494743347168\n",
      "Batch 250, Loss: 0.4075637459754944\n",
      "Batch 251, Loss: -1.4167745113372803\n",
      "Batch 252, Loss: -1.4062254428863525\n",
      "Batch 253, Loss: -1.4048131704330444\n",
      "Batch 254, Loss: 0.4079562723636627\n",
      "Batch 255, Loss: 0.40091830492019653\n",
      "Batch 256, Loss: -1.4115321636199951\n",
      "Batch 257, Loss: 0.40761542320251465\n",
      "Batch 258, Loss: -1.4077627658843994\n",
      "Batch 259, Loss: 0.4098880887031555\n",
      "Batch 260, Loss: -1.4150053262710571\n",
      "Batch 261, Loss: -1.416135311126709\n",
      "Batch 262, Loss: 0.41215914487838745\n",
      "Batch 263, Loss: 0.4083751440048218\n",
      "Batch 264, Loss: -1.4107862710952759\n",
      "Batch 265, Loss: 0.41632965207099915\n",
      "Batch 266, Loss: 0.4130406975746155\n",
      "Batch 267, Loss: -1.404296875\n",
      "Batch 268, Loss: 0.4075523018836975\n",
      "Batch 269, Loss: -1.4157617092132568\n",
      "Batch 270, Loss: 0.4173715114593506\n",
      "Batch 271, Loss: -1.4046449661254883\n",
      "Batch 272, Loss: -1.4229134321212769\n",
      "Batch 273, Loss: 0.40555062890052795\n",
      "Batch 274, Loss: 0.4109897017478943\n",
      "Batch 275, Loss: -1.4159324169158936\n",
      "Batch 276, Loss: -1.416288137435913\n",
      "Batch 277, Loss: 0.41038841009140015\n",
      "Batch 278, Loss: -1.4103273153305054\n",
      "Batch 279, Loss: -1.4183170795440674\n",
      "Batch 280, Loss: 0.4179898202419281\n",
      "Batch 281, Loss: -1.4129396677017212\n",
      "Batch 282, Loss: 0.4201451539993286\n",
      "Batch 283, Loss: 0.41917502880096436\n",
      "Batch 284, Loss: 0.41894614696502686\n",
      "Batch 285, Loss: -1.4180115461349487\n",
      "Batch 286, Loss: 0.40841132402420044\n",
      "Batch 287, Loss: -1.409700632095337\n",
      "Batch 288, Loss: 0.4183666706085205\n",
      "Batch 289, Loss: 0.411176472902298\n",
      "Batch 290, Loss: 0.41705963015556335\n",
      "Batch 291, Loss: -1.409508228302002\n",
      "Batch 292, Loss: -1.4043346643447876\n",
      "Batch 293, Loss: 0.40600255131721497\n",
      "Batch 294, Loss: -1.408288836479187\n",
      "Batch 295, Loss: -1.4046344757080078\n",
      "Batch 296, Loss: -1.4055581092834473\n",
      "Batch 297, Loss: -1.417994737625122\n",
      "Batch 298, Loss: 0.41605061292648315\n",
      "Batch 299, Loss: 0.4109860360622406\n",
      "Batch 300, Loss: 0.40967804193496704\n",
      "Batch 301, Loss: 0.40625715255737305\n",
      "Batch 302, Loss: 0.423528790473938\n",
      "Batch 303, Loss: 0.4045943021774292\n",
      "Batch 304, Loss: -1.4031436443328857\n",
      "Batch 305, Loss: 0.4019036889076233\n",
      "Batch 306, Loss: -1.4026710987091064\n",
      "Batch 307, Loss: 0.40510451793670654\n",
      "Batch 308, Loss: 0.40775957703590393\n",
      "Batch 309, Loss: 0.40319281816482544\n",
      "Batch 310, Loss: -1.409923791885376\n",
      "Batch 311, Loss: 0.40200790762901306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 312, Loss: 0.40652674436569214\n",
      "Batch 313, Loss: -1.3995437622070312\n",
      "Batch 314, Loss: 0.39718884229660034\n",
      "Batch 315, Loss: 0.40658265352249146\n",
      "Batch 316, Loss: -1.414757490158081\n",
      "Batch 317, Loss: -1.4064295291900635\n",
      "Batch 318, Loss: -1.397348165512085\n",
      "Batch 319, Loss: 0.39968106150627136\n",
      "Batch 320, Loss: 0.41213932633399963\n",
      "Batch 321, Loss: -1.4012255668640137\n",
      "Batch 322, Loss: 0.4117877185344696\n",
      "Batch 323, Loss: -1.3967735767364502\n",
      "Batch 324, Loss: -1.394869327545166\n",
      "Batch 325, Loss: -1.3933768272399902\n",
      "Batch 326, Loss: 0.39594390988349915\n",
      "Batch 327, Loss: 0.40480107069015503\n",
      "Batch 328, Loss: 0.40515953302383423\n",
      "Batch 329, Loss: -1.4101327657699585\n",
      "Batch 330, Loss: -1.3973329067230225\n",
      "Batch 331, Loss: -1.3934651613235474\n",
      "Batch 332, Loss: -1.3955926895141602\n",
      "Batch 333, Loss: -1.3984441757202148\n",
      "Batch 334, Loss: 0.405317485332489\n",
      "Batch 335, Loss: -1.406380534172058\n",
      "Batch 336, Loss: -1.396101474761963\n",
      "Batch 337, Loss: 0.4142283499240875\n",
      "Batch 338, Loss: 0.4010593295097351\n",
      "Batch 339, Loss: 0.39770904183387756\n",
      "Batch 340, Loss: 0.404649943113327\n",
      "Batch 341, Loss: 0.41315871477127075\n",
      "Batch 342, Loss: -1.397234559059143\n",
      "Batch 343, Loss: 0.4019703269004822\n",
      "Batch 344, Loss: -1.396824598312378\n",
      "Batch 345, Loss: -1.3988059759140015\n",
      "Batch 346, Loss: -1.4129403829574585\n",
      "Batch 347, Loss: -1.4122127294540405\n",
      "Batch 348, Loss: -1.3977937698364258\n",
      "Batch 349, Loss: -1.4037097692489624\n",
      "Batch 350, Loss: -1.4071476459503174\n",
      "Batch 351, Loss: -1.3995351791381836\n",
      "Batch 352, Loss: 0.4064595699310303\n",
      "Batch 353, Loss: 0.4102821350097656\n",
      "Batch 354, Loss: -1.4104390144348145\n",
      "Batch 355, Loss: 0.4063172936439514\n",
      "Batch 356, Loss: 0.4052971601486206\n",
      "Batch 357, Loss: 0.4103665351867676\n",
      "Batch 358, Loss: -1.41766357421875\n",
      "Batch 359, Loss: -1.4089339971542358\n",
      "Batch 360, Loss: 0.40932005643844604\n",
      "Batch 361, Loss: 0.41285884380340576\n",
      "Batch 362, Loss: -1.404197096824646\n",
      "Batch 363, Loss: 0.4119197130203247\n",
      "Batch 364, Loss: -1.418309211730957\n",
      "Batch 365, Loss: 0.4038500189781189\n",
      "Batch 366, Loss: 0.401195764541626\n",
      "Batch 367, Loss: 0.4044710099697113\n",
      "Batch 368, Loss: -1.4180370569229126\n",
      "Batch 369, Loss: 0.4103619456291199\n",
      "Batch 370, Loss: 0.40888017416000366\n",
      "Batch 371, Loss: 0.41691869497299194\n",
      "Batch 372, Loss: -1.3999232053756714\n",
      "Batch 373, Loss: -1.4158704280853271\n",
      "Batch 374, Loss: 0.39909350872039795\n",
      "Batch 375, Loss: 0.3974572420120239\n",
      "Batch 376, Loss: 0.39931023120880127\n",
      "Batch 377, Loss: -1.3958489894866943\n",
      "Batch 378, Loss: 0.3998035192489624\n",
      "Batch 379, Loss: 0.40261349081993103\n",
      "Batch 380, Loss: 0.3973967730998993\n",
      "Batch 381, Loss: -1.394749641418457\n",
      "Batch 382, Loss: 0.39709484577178955\n",
      "Batch 383, Loss: 0.3929569721221924\n",
      "Batch 384, Loss: 0.3951040506362915\n",
      "Batch 385, Loss: 0.394047349691391\n",
      "Batch 386, Loss: -1.3929023742675781\n",
      "Batch 387, Loss: 0.4070262312889099\n",
      "Batch 388, Loss: -1.3973087072372437\n",
      "Batch 389, Loss: 0.3872527480125427\n",
      "Batch 390, Loss: 0.4026586413383484\n",
      "Batch 391, Loss: 0.3886434733867645\n",
      "Batch 392, Loss: -1.3879268169403076\n",
      "Batch 393, Loss: -1.3938733339309692\n",
      "Batch 394, Loss: 0.3844403028488159\n",
      "Batch 395, Loss: -1.3915135860443115\n",
      "Batch 396, Loss: 0.3809066414833069\n",
      "Batch 397, Loss: -1.3864610195159912\n",
      "Batch 398, Loss: -1.3805471658706665\n",
      "Batch 399, Loss: 0.3904974162578583\n",
      "Training [70%]\tLoss: -0.4989\n",
      "Batch 0, Loss: -1.4003140926361084\n",
      "Batch 1, Loss: -1.3834902048110962\n",
      "Batch 2, Loss: 0.3869627118110657\n",
      "Batch 3, Loss: -1.3907006978988647\n",
      "Batch 4, Loss: -1.3876636028289795\n",
      "Batch 5, Loss: -1.3900507688522339\n",
      "Batch 6, Loss: 0.38690245151519775\n",
      "Batch 7, Loss: 0.39318352937698364\n",
      "Batch 8, Loss: 0.4018315076828003\n",
      "Batch 9, Loss: 0.3892741799354553\n",
      "Batch 10, Loss: -1.4019004106521606\n",
      "Batch 11, Loss: -1.3808969259262085\n",
      "Batch 12, Loss: -1.3832165002822876\n",
      "Batch 13, Loss: -1.3933031558990479\n",
      "Batch 14, Loss: 0.38731127977371216\n",
      "Batch 15, Loss: -1.3893235921859741\n",
      "Batch 16, Loss: 0.39496275782585144\n",
      "Batch 17, Loss: -1.3880491256713867\n",
      "Batch 18, Loss: 0.4031434655189514\n",
      "Batch 19, Loss: 0.38862109184265137\n",
      "Batch 20, Loss: 0.3954508900642395\n",
      "Batch 21, Loss: 0.39268413186073303\n",
      "Batch 22, Loss: -1.389593482017517\n",
      "Batch 23, Loss: -1.3860313892364502\n",
      "Batch 24, Loss: 0.3883659839630127\n",
      "Batch 25, Loss: -1.3871104717254639\n",
      "Batch 26, Loss: -1.3918261528015137\n",
      "Batch 27, Loss: 0.3884478807449341\n",
      "Batch 28, Loss: 0.40248844027519226\n",
      "Batch 29, Loss: 0.3892515301704407\n",
      "Batch 30, Loss: 0.38807108998298645\n",
      "Batch 31, Loss: -1.3877687454223633\n",
      "Batch 32, Loss: -1.3886711597442627\n",
      "Batch 33, Loss: 0.3842359781265259\n",
      "Batch 34, Loss: 0.4000323414802551\n",
      "Batch 35, Loss: 0.392217218875885\n",
      "Batch 36, Loss: 0.39728960394859314\n",
      "Batch 37, Loss: 0.3907838761806488\n",
      "Batch 38, Loss: -1.3834729194641113\n",
      "Batch 39, Loss: -1.3816901445388794\n",
      "Batch 40, Loss: -1.38320791721344\n",
      "Batch 41, Loss: -1.3853728771209717\n",
      "Batch 42, Loss: 0.3817388415336609\n",
      "Batch 43, Loss: 0.39747151732444763\n",
      "Batch 44, Loss: -1.3841723203659058\n",
      "Batch 45, Loss: -1.3886598348617554\n",
      "Batch 46, Loss: -1.387041449546814\n",
      "Batch 47, Loss: 0.3881777226924896\n",
      "Batch 48, Loss: 0.38741356134414673\n",
      "Batch 49, Loss: -1.3833876848220825\n",
      "Batch 50, Loss: -1.3881356716156006\n",
      "Batch 51, Loss: -1.3843448162078857\n",
      "Batch 52, Loss: 0.3862905502319336\n",
      "Batch 53, Loss: -1.3895270824432373\n",
      "Batch 54, Loss: 0.3847355842590332\n",
      "Batch 55, Loss: 0.3850969672203064\n",
      "Batch 56, Loss: 0.3866509795188904\n",
      "Batch 57, Loss: -1.389225959777832\n",
      "Batch 58, Loss: -1.3851516246795654\n",
      "Batch 59, Loss: 0.38957902789115906\n",
      "Batch 60, Loss: -1.3851139545440674\n",
      "Batch 61, Loss: -1.3853894472122192\n",
      "Batch 62, Loss: 0.3970485329627991\n",
      "Batch 63, Loss: 0.38881829380989075\n",
      "Batch 64, Loss: 0.3894312381744385\n",
      "Batch 65, Loss: -1.3847609758377075\n",
      "Batch 66, Loss: -1.388251543045044\n",
      "Batch 67, Loss: 0.3841473460197449\n",
      "Batch 68, Loss: -1.387704610824585\n",
      "Batch 69, Loss: -1.3896846771240234\n",
      "Batch 70, Loss: 0.38645362854003906\n",
      "Batch 71, Loss: -1.390244722366333\n",
      "Batch 72, Loss: 0.38557904958724976\n",
      "Batch 73, Loss: -1.3857122659683228\n",
      "Batch 74, Loss: 0.38913533091545105\n",
      "Batch 75, Loss: 0.3916235566139221\n",
      "Batch 76, Loss: 0.3875780403614044\n",
      "Batch 77, Loss: -1.3890173435211182\n",
      "Batch 78, Loss: 0.3878285586833954\n",
      "Batch 79, Loss: 0.3856513500213623\n",
      "Batch 80, Loss: 0.38493603467941284\n",
      "Batch 81, Loss: -1.3853751420974731\n",
      "Batch 82, Loss: 0.3847045302391052\n",
      "Batch 83, Loss: -1.3842878341674805\n",
      "Batch 84, Loss: 0.3882943093776703\n",
      "Batch 85, Loss: 0.3832228183746338\n",
      "Batch 86, Loss: -1.3811789751052856\n",
      "Batch 87, Loss: -1.382110357284546\n",
      "Batch 88, Loss: 0.38707053661346436\n",
      "Batch 89, Loss: -1.3825979232788086\n",
      "Batch 90, Loss: -1.3853023052215576\n",
      "Batch 91, Loss: -1.3822253942489624\n",
      "Batch 92, Loss: -1.384429931640625\n",
      "Batch 93, Loss: -1.3825069665908813\n",
      "Batch 94, Loss: 0.3844286799430847\n",
      "Batch 95, Loss: 0.39563092589378357\n",
      "Batch 96, Loss: 0.38389042019844055\n",
      "Batch 97, Loss: -1.3830795288085938\n",
      "Batch 98, Loss: -1.3871454000473022\n",
      "Batch 99, Loss: -1.3849222660064697\n",
      "Batch 100, Loss: -1.3847599029541016\n",
      "Batch 101, Loss: -1.3854427337646484\n",
      "Batch 102, Loss: -1.3895372152328491\n",
      "Batch 103, Loss: 0.39571934938430786\n",
      "Batch 104, Loss: -1.3869984149932861\n",
      "Batch 105, Loss: 0.3878124952316284\n",
      "Batch 106, Loss: -1.397850513458252\n",
      "Batch 107, Loss: -1.3881207704544067\n",
      "Batch 108, Loss: -1.389817714691162\n",
      "Batch 109, Loss: 0.38975363969802856\n",
      "Batch 110, Loss: 0.39033105969429016\n",
      "Batch 111, Loss: 0.3926316201686859\n",
      "Batch 112, Loss: 0.3903253972530365\n",
      "Batch 113, Loss: -1.3916082382202148\n",
      "Batch 114, Loss: 0.39115995168685913\n",
      "Batch 115, Loss: 0.3908304274082184\n",
      "Batch 116, Loss: -1.3945343494415283\n",
      "Batch 117, Loss: 0.38845232129096985\n",
      "Batch 118, Loss: 0.40047815442085266\n",
      "Batch 119, Loss: 0.39473599195480347\n",
      "Batch 120, Loss: -1.3898507356643677\n",
      "Batch 121, Loss: 0.392461895942688\n",
      "Batch 122, Loss: 0.38819825649261475\n",
      "Batch 123, Loss: -1.3919650316238403\n",
      "Batch 124, Loss: 0.38668233156204224\n",
      "Batch 125, Loss: -1.3872802257537842\n",
      "Batch 126, Loss: 0.3864782154560089\n",
      "Batch 127, Loss: 0.3898944854736328\n",
      "Batch 128, Loss: 0.390048086643219\n",
      "Batch 129, Loss: -1.3856877088546753\n",
      "Batch 130, Loss: -1.3950390815734863\n",
      "Batch 131, Loss: 0.3829941749572754\n",
      "Batch 132, Loss: -1.3880505561828613\n",
      "Batch 133, Loss: -1.382153868675232\n",
      "Batch 134, Loss: -1.3946969509124756\n",
      "Batch 135, Loss: 0.3958800435066223\n",
      "Batch 136, Loss: 0.38837122917175293\n",
      "Batch 137, Loss: 0.3890388607978821\n",
      "Batch 138, Loss: 0.3834162652492523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 139, Loss: -1.3940913677215576\n",
      "Batch 140, Loss: 0.3837241530418396\n",
      "Batch 141, Loss: 0.3853647708892822\n",
      "Batch 142, Loss: -1.388136625289917\n",
      "Batch 143, Loss: 0.3929867744445801\n",
      "Batch 144, Loss: -1.3796756267547607\n",
      "Batch 145, Loss: -1.3924312591552734\n",
      "Batch 146, Loss: 0.38150233030319214\n",
      "Batch 147, Loss: -1.3841440677642822\n",
      "Batch 148, Loss: 0.3831999599933624\n",
      "Batch 149, Loss: -1.3835240602493286\n",
      "Batch 150, Loss: -1.39236581325531\n",
      "Batch 151, Loss: 0.384304404258728\n",
      "Batch 152, Loss: 0.38325631618499756\n",
      "Batch 153, Loss: 0.3811227083206177\n",
      "Batch 154, Loss: -1.3850483894348145\n",
      "Batch 155, Loss: 0.3863182067871094\n",
      "Batch 156, Loss: -1.3827693462371826\n",
      "Batch 157, Loss: -1.3802499771118164\n",
      "Batch 158, Loss: 0.38232341408729553\n",
      "Batch 159, Loss: -1.384346604347229\n",
      "Batch 160, Loss: -1.3784711360931396\n",
      "Batch 161, Loss: 0.38101452589035034\n",
      "Batch 162, Loss: -1.3870954513549805\n",
      "Batch 163, Loss: -1.3807833194732666\n",
      "Batch 164, Loss: -1.393635869026184\n",
      "Batch 165, Loss: -1.3819377422332764\n",
      "Batch 166, Loss: 0.3881881833076477\n",
      "Batch 167, Loss: -1.3881341218948364\n",
      "Batch 168, Loss: -1.3929252624511719\n",
      "Batch 169, Loss: -1.3897438049316406\n",
      "Batch 170, Loss: -1.3841049671173096\n",
      "Batch 171, Loss: -1.391402006149292\n",
      "Batch 172, Loss: 0.386701375246048\n",
      "Batch 173, Loss: 0.39179107546806335\n",
      "Batch 174, Loss: -1.393385410308838\n",
      "Batch 175, Loss: 0.3926720917224884\n",
      "Batch 176, Loss: 0.3944673538208008\n",
      "Batch 177, Loss: -1.3929564952850342\n",
      "Batch 178, Loss: 0.395332396030426\n",
      "Batch 179, Loss: -1.4027767181396484\n",
      "Batch 180, Loss: 0.39507901668548584\n",
      "Batch 181, Loss: 0.4022970199584961\n",
      "Batch 182, Loss: -1.3913261890411377\n",
      "Batch 183, Loss: -1.3869619369506836\n",
      "Batch 184, Loss: 0.39597439765930176\n",
      "Batch 185, Loss: -1.3881146907806396\n",
      "Batch 186, Loss: -1.395750880241394\n",
      "Batch 187, Loss: -1.3935225009918213\n",
      "Batch 188, Loss: -1.3940163850784302\n",
      "Batch 189, Loss: 0.3893219530582428\n",
      "Batch 190, Loss: -1.398221731185913\n",
      "Batch 191, Loss: 0.3928580582141876\n",
      "Batch 192, Loss: 0.3914576768875122\n",
      "Batch 193, Loss: 0.3928202986717224\n",
      "Batch 194, Loss: 0.3936798870563507\n",
      "Batch 195, Loss: 0.3967733085155487\n",
      "Batch 196, Loss: -1.3900750875473022\n",
      "Batch 197, Loss: 0.39331984519958496\n",
      "Batch 198, Loss: -1.395346760749817\n",
      "Batch 199, Loss: 0.4038313329219818\n",
      "Batch 200, Loss: 0.3913251757621765\n",
      "Batch 201, Loss: -1.3921247720718384\n",
      "Batch 202, Loss: 0.3970385789871216\n",
      "Batch 203, Loss: 0.3876343369483948\n",
      "Batch 204, Loss: -1.39087975025177\n",
      "Batch 205, Loss: -1.3909618854522705\n",
      "Batch 206, Loss: 0.39310094714164734\n",
      "Batch 207, Loss: 0.38917893171310425\n",
      "Batch 208, Loss: 0.40060222148895264\n",
      "Batch 209, Loss: 0.39420831203460693\n",
      "Batch 210, Loss: 0.3888123631477356\n",
      "Batch 211, Loss: 0.39944392442703247\n",
      "Batch 212, Loss: 0.3863162398338318\n",
      "Batch 213, Loss: -1.3902604579925537\n",
      "Batch 214, Loss: -1.3894810676574707\n",
      "Batch 215, Loss: -1.3838245868682861\n",
      "Batch 216, Loss: -1.388757586479187\n",
      "Batch 217, Loss: 0.3884924054145813\n",
      "Batch 218, Loss: 0.38301852345466614\n",
      "Batch 219, Loss: 0.38336774706840515\n",
      "Batch 220, Loss: -1.3852415084838867\n",
      "Batch 221, Loss: -1.3795685768127441\n",
      "Batch 222, Loss: 0.37961697578430176\n",
      "Batch 223, Loss: -1.3869707584381104\n",
      "Batch 224, Loss: -1.3871614933013916\n",
      "Batch 225, Loss: 0.39361411333084106\n",
      "Batch 226, Loss: 0.38571229577064514\n",
      "Batch 227, Loss: -1.3858861923217773\n",
      "Batch 228, Loss: -1.390143871307373\n",
      "Batch 229, Loss: 0.38063883781433105\n",
      "Batch 230, Loss: 0.381520539522171\n",
      "Batch 231, Loss: 0.3833310008049011\n",
      "Batch 232, Loss: -1.3814337253570557\n",
      "Batch 233, Loss: 0.38684284687042236\n",
      "Batch 234, Loss: 0.3839740455150604\n",
      "Batch 235, Loss: -1.3903725147247314\n",
      "Batch 236, Loss: -1.3817145824432373\n",
      "Batch 237, Loss: -1.3803138732910156\n",
      "Batch 238, Loss: -1.3837616443634033\n",
      "Batch 239, Loss: -1.3810083866119385\n",
      "Batch 240, Loss: -1.3797986507415771\n",
      "Batch 241, Loss: -1.3836959600448608\n",
      "Batch 242, Loss: -1.3940025568008423\n",
      "Batch 243, Loss: 0.3844318389892578\n",
      "Batch 244, Loss: -1.3890706300735474\n",
      "Batch 245, Loss: 0.3818056583404541\n",
      "Batch 246, Loss: -1.389451265335083\n",
      "Batch 247, Loss: 0.38414180278778076\n",
      "Batch 248, Loss: -1.3886051177978516\n",
      "Batch 249, Loss: -1.3890089988708496\n",
      "Batch 250, Loss: 0.38682347536087036\n",
      "Batch 251, Loss: 0.3872276544570923\n",
      "Batch 252, Loss: 0.3919563293457031\n",
      "Batch 253, Loss: -1.3873075246810913\n",
      "Batch 254, Loss: 0.38829320669174194\n",
      "Batch 255, Loss: -1.391950249671936\n",
      "Batch 256, Loss: 0.391827791929245\n",
      "Batch 257, Loss: 0.3889692723751068\n",
      "Batch 258, Loss: 0.3933474123477936\n",
      "Batch 259, Loss: -1.3854107856750488\n",
      "Batch 260, Loss: -1.392984390258789\n",
      "Batch 261, Loss: -1.3876516819000244\n",
      "Batch 262, Loss: -1.3877285718917847\n",
      "Batch 263, Loss: 0.389248788356781\n",
      "Batch 264, Loss: -1.3914995193481445\n",
      "Batch 265, Loss: -1.3861900568008423\n",
      "Batch 266, Loss: -1.391230821609497\n",
      "Batch 267, Loss: -1.3908283710479736\n",
      "Batch 268, Loss: -1.4008350372314453\n",
      "Batch 269, Loss: 0.3876990079879761\n",
      "Batch 270, Loss: 0.3929253816604614\n",
      "Batch 271, Loss: 0.3980278968811035\n",
      "Batch 272, Loss: -1.3900184631347656\n",
      "Batch 273, Loss: -1.405726671218872\n",
      "Batch 274, Loss: 0.39398837089538574\n",
      "Batch 275, Loss: -1.406618595123291\n",
      "Batch 276, Loss: 0.39119112491607666\n",
      "Batch 277, Loss: 0.39208489656448364\n",
      "Batch 278, Loss: -1.394788146018982\n",
      "Batch 279, Loss: -1.3964643478393555\n",
      "Batch 280, Loss: 0.40063637495040894\n",
      "Batch 281, Loss: -1.3906939029693604\n",
      "Batch 282, Loss: -1.3971502780914307\n",
      "Batch 283, Loss: -1.3929424285888672\n",
      "Batch 284, Loss: -1.408095359802246\n",
      "Batch 285, Loss: 0.39368534088134766\n",
      "Batch 286, Loss: -1.393992304801941\n",
      "Batch 287, Loss: 0.4099075198173523\n",
      "Batch 288, Loss: -1.3959182500839233\n",
      "Batch 289, Loss: -1.3955836296081543\n",
      "Batch 290, Loss: 0.41051143407821655\n",
      "Batch 291, Loss: -1.4001387357711792\n",
      "Batch 292, Loss: 0.3975973427295685\n",
      "Batch 293, Loss: 0.4052657186985016\n",
      "Batch 294, Loss: -1.4031894207000732\n",
      "Batch 295, Loss: -1.3994046449661255\n",
      "Batch 296, Loss: 0.3978976011276245\n",
      "Batch 297, Loss: 0.4037185311317444\n",
      "Batch 298, Loss: 0.406625896692276\n",
      "Batch 299, Loss: 0.4009391963481903\n",
      "Batch 300, Loss: 0.3957436680793762\n",
      "Batch 301, Loss: 0.4030628800392151\n",
      "Batch 302, Loss: 0.3968972861766815\n",
      "Batch 303, Loss: -1.3972731828689575\n",
      "Batch 304, Loss: 0.39416661858558655\n",
      "Batch 305, Loss: 0.4018827974796295\n",
      "Batch 306, Loss: 0.401081919670105\n",
      "Batch 307, Loss: 0.39762115478515625\n",
      "Batch 308, Loss: 0.3953792452812195\n",
      "Batch 309, Loss: 0.3964957594871521\n",
      "Batch 310, Loss: -1.3961284160614014\n",
      "Batch 311, Loss: -1.396068811416626\n",
      "Batch 312, Loss: -1.3884155750274658\n",
      "Batch 313, Loss: 0.38961827754974365\n",
      "Batch 314, Loss: -1.3884083032608032\n",
      "Batch 315, Loss: -1.400249719619751\n",
      "Batch 316, Loss: 0.39437466859817505\n",
      "Batch 317, Loss: -1.3887020349502563\n",
      "Batch 318, Loss: 0.3924976587295532\n",
      "Batch 319, Loss: 0.3922188878059387\n",
      "Batch 320, Loss: -1.3864866495132446\n",
      "Batch 321, Loss: 0.38588836789131165\n",
      "Batch 322, Loss: 0.3877272605895996\n",
      "Batch 323, Loss: 0.3869086802005768\n",
      "Batch 324, Loss: 0.40059080719947815\n",
      "Batch 325, Loss: 0.3884524703025818\n",
      "Batch 326, Loss: -1.3849937915802002\n",
      "Batch 327, Loss: -1.382838249206543\n",
      "Batch 328, Loss: -1.3880460262298584\n",
      "Batch 329, Loss: -1.387993335723877\n",
      "Batch 330, Loss: 0.3887714743614197\n",
      "Batch 331, Loss: 0.38384950160980225\n",
      "Batch 332, Loss: -1.386967420578003\n",
      "Batch 333, Loss: 0.38816091418266296\n",
      "Batch 334, Loss: 0.3863741457462311\n",
      "Batch 335, Loss: -1.3824189901351929\n",
      "Batch 336, Loss: -1.3827199935913086\n",
      "Batch 337, Loss: -1.3812880516052246\n",
      "Batch 338, Loss: 0.38670510053634644\n",
      "Batch 339, Loss: -1.3811731338500977\n",
      "Batch 340, Loss: 0.38526028394699097\n",
      "Batch 341, Loss: 0.38583797216415405\n",
      "Batch 342, Loss: 0.38141152262687683\n",
      "Batch 343, Loss: 0.3810717463493347\n",
      "Batch 344, Loss: -1.3914270401000977\n",
      "Batch 345, Loss: 0.38639581203460693\n",
      "Batch 346, Loss: 0.38108325004577637\n",
      "Batch 347, Loss: 0.3806787133216858\n",
      "Batch 348, Loss: -1.3838279247283936\n",
      "Batch 349, Loss: -1.3795464038848877\n",
      "Batch 350, Loss: -1.383236289024353\n",
      "Batch 351, Loss: -1.3823072910308838\n",
      "Batch 352, Loss: 0.3778264820575714\n",
      "Batch 353, Loss: 0.38104844093322754\n",
      "Batch 354, Loss: -1.3793609142303467\n",
      "Batch 355, Loss: 0.38152390718460083\n",
      "Batch 356, Loss: -1.3831474781036377\n",
      "Batch 357, Loss: 0.39019352197647095\n",
      "Batch 358, Loss: 0.37828776240348816\n",
      "Batch 359, Loss: -1.3883192539215088\n",
      "Batch 360, Loss: 0.379743754863739\n",
      "Batch 361, Loss: -1.3891345262527466\n",
      "Batch 362, Loss: -1.389095425605774\n",
      "Batch 363, Loss: 0.38180065155029297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 364, Loss: -1.378922700881958\n",
      "Batch 365, Loss: 0.3787764012813568\n",
      "Batch 366, Loss: -1.3826557397842407\n",
      "Batch 367, Loss: 0.3804002106189728\n",
      "Batch 368, Loss: 0.3799780309200287\n",
      "Batch 369, Loss: 0.369737833738327\n",
      "Batch 370, Loss: 0.3782289922237396\n",
      "Batch 371, Loss: -1.379993200302124\n",
      "Batch 372, Loss: -1.3811780214309692\n",
      "Batch 373, Loss: 0.3771747052669525\n",
      "Batch 374, Loss: -1.3738899230957031\n",
      "Batch 375, Loss: 0.3884487450122833\n",
      "Batch 376, Loss: 0.36249130964279175\n",
      "Batch 377, Loss: 0.37191852927207947\n",
      "Batch 378, Loss: -1.3645834922790527\n",
      "Batch 379, Loss: -1.3736388683319092\n",
      "Batch 380, Loss: 0.37212035059928894\n",
      "Batch 381, Loss: -1.3770110607147217\n",
      "Batch 382, Loss: -1.3602197170257568\n",
      "Batch 383, Loss: -1.360329508781433\n",
      "Batch 384, Loss: 0.38648563623428345\n",
      "Batch 385, Loss: 0.37561312317848206\n",
      "Batch 386, Loss: 0.3844495415687561\n",
      "Batch 387, Loss: -1.3773763179779053\n",
      "Batch 388, Loss: -1.3790373802185059\n",
      "Batch 389, Loss: -1.3770843744277954\n",
      "Batch 390, Loss: -1.3731448650360107\n",
      "Batch 391, Loss: -1.376094102859497\n",
      "Batch 392, Loss: -1.3790459632873535\n",
      "Batch 393, Loss: -1.3770368099212646\n",
      "Batch 394, Loss: -1.371321201324463\n",
      "Batch 395, Loss: 0.377678781747818\n",
      "Batch 396, Loss: -1.3790674209594727\n",
      "Batch 397, Loss: 0.37818804383277893\n",
      "Batch 398, Loss: 0.3803488314151764\n",
      "Batch 399, Loss: -1.3803017139434814\n",
      "Training [80%]\tLoss: -0.4990\n",
      "Batch 0, Loss: 0.38078105449676514\n",
      "Batch 1, Loss: -1.3857775926589966\n",
      "Batch 2, Loss: -1.3812177181243896\n",
      "Batch 3, Loss: -1.3862285614013672\n",
      "Batch 4, Loss: -1.3863530158996582\n",
      "Batch 5, Loss: -1.3961288928985596\n",
      "Batch 6, Loss: -1.3834712505340576\n",
      "Batch 7, Loss: 0.3888179063796997\n",
      "Batch 8, Loss: -1.3880870342254639\n",
      "Batch 9, Loss: -1.3855193853378296\n",
      "Batch 10, Loss: -1.3864245414733887\n",
      "Batch 11, Loss: -1.3950508832931519\n",
      "Batch 12, Loss: 0.39422017335891724\n",
      "Batch 13, Loss: -1.3965368270874023\n",
      "Batch 14, Loss: -1.3905845880508423\n",
      "Batch 15, Loss: 0.39593857526779175\n",
      "Batch 16, Loss: -1.3953804969787598\n",
      "Batch 17, Loss: 0.3919163942337036\n",
      "Batch 18, Loss: 0.39122334122657776\n",
      "Batch 19, Loss: -1.3914158344268799\n",
      "Batch 20, Loss: -1.3946318626403809\n",
      "Batch 21, Loss: 0.3935621976852417\n",
      "Batch 22, Loss: 0.3969714045524597\n",
      "Batch 23, Loss: -1.3928990364074707\n",
      "Batch 24, Loss: -1.401902437210083\n",
      "Batch 25, Loss: -1.3996021747589111\n",
      "Batch 26, Loss: 0.3941158056259155\n",
      "Batch 27, Loss: -1.397794485092163\n",
      "Batch 28, Loss: -1.4135977029800415\n",
      "Batch 29, Loss: -1.40822434425354\n",
      "Batch 30, Loss: -1.4019018411636353\n",
      "Batch 31, Loss: -1.4036409854888916\n",
      "Batch 32, Loss: -1.415752649307251\n",
      "Batch 33, Loss: -1.4020767211914062\n",
      "Batch 34, Loss: -1.4141521453857422\n",
      "Batch 35, Loss: 0.40888938307762146\n",
      "Batch 36, Loss: -1.4242682456970215\n",
      "Batch 37, Loss: 0.4180036783218384\n",
      "Batch 38, Loss: 0.40979790687561035\n",
      "Batch 39, Loss: -1.4059505462646484\n",
      "Batch 40, Loss: 0.41525137424468994\n",
      "Batch 41, Loss: 0.4087337255477905\n",
      "Batch 42, Loss: -1.4205005168914795\n",
      "Batch 43, Loss: -1.4162325859069824\n",
      "Batch 44, Loss: -1.4066156148910522\n",
      "Batch 45, Loss: -1.4315240383148193\n",
      "Batch 46, Loss: -1.4134527444839478\n",
      "Batch 47, Loss: -1.424254059791565\n",
      "Batch 48, Loss: 0.4168514609336853\n",
      "Batch 49, Loss: 0.422762393951416\n",
      "Batch 50, Loss: 0.4168132543563843\n",
      "Batch 51, Loss: 0.42394888401031494\n",
      "Batch 52, Loss: -1.426098346710205\n",
      "Batch 53, Loss: 0.42664992809295654\n",
      "Batch 54, Loss: 0.4350680112838745\n",
      "Batch 55, Loss: 0.43524640798568726\n",
      "Batch 56, Loss: 0.43325600028038025\n",
      "Batch 57, Loss: -1.4168829917907715\n",
      "Batch 58, Loss: -1.4080389738082886\n",
      "Batch 59, Loss: 0.4163402318954468\n",
      "Batch 60, Loss: 0.4289816915988922\n",
      "Batch 61, Loss: -1.4281724691390991\n",
      "Batch 62, Loss: -1.4169855117797852\n",
      "Batch 63, Loss: -1.4233672618865967\n",
      "Batch 64, Loss: -1.422402024269104\n",
      "Batch 65, Loss: 0.4269590675830841\n",
      "Batch 66, Loss: 0.40912604331970215\n",
      "Batch 67, Loss: 0.40907907485961914\n",
      "Batch 68, Loss: 0.4075917899608612\n",
      "Batch 69, Loss: -1.407199501991272\n",
      "Batch 70, Loss: 0.41556301712989807\n",
      "Batch 71, Loss: 0.4222349524497986\n",
      "Batch 72, Loss: -1.409212350845337\n",
      "Batch 73, Loss: -1.409416675567627\n",
      "Batch 74, Loss: 0.4189261496067047\n",
      "Batch 75, Loss: -1.4061634540557861\n",
      "Batch 76, Loss: 0.40570735931396484\n",
      "Batch 77, Loss: -1.4102115631103516\n",
      "Batch 78, Loss: 0.4246886968612671\n",
      "Batch 79, Loss: 0.41779857873916626\n",
      "Batch 80, Loss: 0.4054758548736572\n",
      "Batch 81, Loss: 0.40500330924987793\n",
      "Batch 82, Loss: -1.4096342325210571\n",
      "Batch 83, Loss: -1.423231601715088\n",
      "Batch 84, Loss: 0.4065890610218048\n",
      "Batch 85, Loss: 0.40408703684806824\n",
      "Batch 86, Loss: 0.4057615399360657\n",
      "Batch 87, Loss: -1.4076933860778809\n",
      "Batch 88, Loss: 0.40579918026924133\n",
      "Batch 89, Loss: -1.4158339500427246\n",
      "Batch 90, Loss: 0.40403294563293457\n",
      "Batch 91, Loss: -1.416546106338501\n",
      "Batch 92, Loss: -1.4051117897033691\n",
      "Batch 93, Loss: 0.422493040561676\n",
      "Batch 94, Loss: -1.4209424257278442\n",
      "Batch 95, Loss: -1.4013346433639526\n",
      "Batch 96, Loss: -1.4110727310180664\n",
      "Batch 97, Loss: 0.4009477496147156\n",
      "Batch 98, Loss: -1.4093163013458252\n",
      "Batch 99, Loss: -1.4180412292480469\n",
      "Batch 100, Loss: -1.4039415121078491\n",
      "Batch 101, Loss: -1.4164214134216309\n",
      "Batch 102, Loss: 0.4183083772659302\n",
      "Batch 103, Loss: -1.4047932624816895\n",
      "Batch 104, Loss: -1.4068076610565186\n",
      "Batch 105, Loss: 0.40774255990982056\n",
      "Batch 106, Loss: 0.4159215986728668\n",
      "Batch 107, Loss: -1.4246190786361694\n",
      "Batch 108, Loss: 0.41215211153030396\n",
      "Batch 109, Loss: -1.4104220867156982\n",
      "Batch 110, Loss: -1.4094479084014893\n",
      "Batch 111, Loss: -1.4107385873794556\n",
      "Batch 112, Loss: 0.4089827239513397\n",
      "Batch 113, Loss: 0.4078868329524994\n",
      "Batch 114, Loss: -1.4298748970031738\n",
      "Batch 115, Loss: -1.4098457098007202\n",
      "Batch 116, Loss: -1.4125359058380127\n",
      "Batch 117, Loss: 0.42957985401153564\n",
      "Batch 118, Loss: 0.41401904821395874\n",
      "Batch 119, Loss: 0.41149261593818665\n",
      "Batch 120, Loss: -1.411657691001892\n",
      "Batch 121, Loss: 0.41461727023124695\n",
      "Batch 122, Loss: -1.4367449283599854\n",
      "Batch 123, Loss: -1.4294768571853638\n",
      "Batch 124, Loss: -1.4248669147491455\n",
      "Batch 125, Loss: 0.41092392802238464\n",
      "Batch 126, Loss: -1.417823314666748\n",
      "Batch 127, Loss: 0.4397430419921875\n",
      "Batch 128, Loss: 0.435445636510849\n",
      "Batch 129, Loss: 0.42251715064048767\n",
      "Batch 130, Loss: -1.4226421117782593\n",
      "Batch 131, Loss: -1.4285904169082642\n",
      "Batch 132, Loss: 0.44043123722076416\n",
      "Batch 133, Loss: -1.4145680665969849\n",
      "Batch 134, Loss: 0.41483408212661743\n",
      "Batch 135, Loss: 0.4448770582675934\n",
      "Batch 136, Loss: -1.4227274656295776\n",
      "Batch 137, Loss: 0.4227568209171295\n",
      "Batch 138, Loss: 0.41140201687812805\n",
      "Batch 139, Loss: 0.41286778450012207\n",
      "Batch 140, Loss: 0.42842477560043335\n",
      "Batch 141, Loss: 0.43311911821365356\n",
      "Batch 142, Loss: -1.4224090576171875\n",
      "Batch 143, Loss: -1.415331482887268\n",
      "Batch 144, Loss: -1.4155991077423096\n",
      "Batch 145, Loss: -1.4099977016448975\n",
      "Batch 146, Loss: -1.432525634765625\n",
      "Batch 147, Loss: -1.4190607070922852\n",
      "Batch 148, Loss: 0.4229271113872528\n",
      "Batch 149, Loss: -1.427123785018921\n",
      "Batch 150, Loss: -1.431050419807434\n",
      "Batch 151, Loss: -1.4169881343841553\n",
      "Batch 152, Loss: -1.4192583560943604\n",
      "Batch 153, Loss: 0.4271448850631714\n",
      "Batch 154, Loss: -1.432882308959961\n",
      "Batch 155, Loss: 0.43320798873901367\n",
      "Batch 156, Loss: 0.4367595613002777\n",
      "Batch 157, Loss: 0.4154791235923767\n",
      "Batch 158, Loss: 0.4217056334018707\n",
      "Batch 159, Loss: 0.42945313453674316\n",
      "Batch 160, Loss: -1.4144465923309326\n",
      "Batch 161, Loss: 0.43481001257896423\n",
      "Batch 162, Loss: 0.4237831234931946\n",
      "Batch 163, Loss: -1.424067735671997\n",
      "Batch 164, Loss: 0.423748642206192\n",
      "Batch 165, Loss: 0.4176541566848755\n",
      "Batch 166, Loss: 0.4185965061187744\n",
      "Batch 167, Loss: 0.4133038818836212\n",
      "Batch 168, Loss: 0.41096484661102295\n",
      "Batch 169, Loss: -1.4183940887451172\n",
      "Batch 170, Loss: 0.4075016975402832\n",
      "Batch 171, Loss: -1.4117969274520874\n",
      "Batch 172, Loss: -1.4279799461364746\n",
      "Batch 173, Loss: -1.4215043783187866\n",
      "Batch 174, Loss: -1.4110479354858398\n",
      "Batch 175, Loss: 0.41131162643432617\n",
      "Batch 176, Loss: 0.4169619381427765\n",
      "Batch 177, Loss: -1.4129977226257324\n",
      "Batch 178, Loss: 0.42042776942253113\n",
      "Batch 179, Loss: 0.4116714894771576\n",
      "Batch 180, Loss: 0.42653602361679077\n",
      "Batch 181, Loss: 0.4190203845500946\n",
      "Batch 182, Loss: 0.41343507170677185\n",
      "Batch 183, Loss: -1.404428482055664\n",
      "Batch 184, Loss: 0.40554049611091614\n",
      "Batch 185, Loss: -1.4089168310165405\n",
      "Batch 186, Loss: 0.4042733907699585\n",
      "Batch 187, Loss: 0.4090336561203003\n",
      "Batch 188, Loss: 0.40286803245544434\n",
      "Batch 189, Loss: -1.4197365045547485\n",
      "Batch 190, Loss: 0.4042532444000244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 191, Loss: -1.4151989221572876\n",
      "Batch 192, Loss: -1.402187705039978\n",
      "Batch 193, Loss: 0.4174061715602875\n",
      "Batch 194, Loss: -1.4114735126495361\n",
      "Batch 195, Loss: 0.4149188995361328\n",
      "Batch 196, Loss: 0.3996856212615967\n",
      "Batch 197, Loss: 0.41591060161590576\n",
      "Batch 198, Loss: -1.4026286602020264\n",
      "Batch 199, Loss: 0.3998698890209198\n",
      "Batch 200, Loss: -1.3993539810180664\n",
      "Batch 201, Loss: 0.4035751223564148\n",
      "Batch 202, Loss: -1.397498607635498\n",
      "Batch 203, Loss: 0.3952929377555847\n",
      "Batch 204, Loss: -1.3949638605117798\n",
      "Batch 205, Loss: -1.3966193199157715\n",
      "Batch 206, Loss: 0.406404972076416\n",
      "Batch 207, Loss: 0.41020268201828003\n",
      "Batch 208, Loss: 0.40969908237457275\n",
      "Batch 209, Loss: -1.3939006328582764\n",
      "Batch 210, Loss: 0.393694669008255\n",
      "Batch 211, Loss: -1.4049370288848877\n",
      "Batch 212, Loss: -1.4046993255615234\n",
      "Batch 213, Loss: -1.393998146057129\n",
      "Batch 214, Loss: 0.40392863750457764\n",
      "Batch 215, Loss: -1.3950906991958618\n",
      "Batch 216, Loss: -1.3962302207946777\n",
      "Batch 217, Loss: -1.4037195444107056\n",
      "Batch 218, Loss: 0.4018072783946991\n",
      "Batch 219, Loss: -1.4052488803863525\n",
      "Batch 220, Loss: -1.4082603454589844\n",
      "Batch 221, Loss: 0.39914482831954956\n",
      "Batch 222, Loss: 0.40677112340927124\n",
      "Batch 223, Loss: -1.411275863647461\n",
      "Batch 224, Loss: 0.40571409463882446\n",
      "Batch 225, Loss: 0.4104578495025635\n",
      "Batch 226, Loss: -1.3991435766220093\n",
      "Batch 227, Loss: -1.3980543613433838\n",
      "Batch 228, Loss: 0.4005171060562134\n",
      "Batch 229, Loss: 0.39829206466674805\n",
      "Batch 230, Loss: -1.4016612768173218\n",
      "Batch 231, Loss: -1.4003816843032837\n",
      "Batch 232, Loss: -1.3983234167099\n",
      "Batch 233, Loss: 0.40107226371765137\n",
      "Batch 234, Loss: -1.3992722034454346\n",
      "Batch 235, Loss: -1.4035344123840332\n",
      "Batch 236, Loss: -1.3983778953552246\n",
      "Batch 237, Loss: -1.4111860990524292\n",
      "Batch 238, Loss: 0.4023922085762024\n",
      "Batch 239, Loss: -1.4137133359909058\n",
      "Batch 240, Loss: 0.4034349024295807\n",
      "Batch 241, Loss: -1.4111188650131226\n",
      "Batch 242, Loss: -1.409846544265747\n",
      "Batch 243, Loss: -1.4107494354248047\n",
      "Batch 244, Loss: -1.405806303024292\n",
      "Batch 245, Loss: 0.4131453037261963\n",
      "Batch 246, Loss: 0.41875961422920227\n",
      "Batch 247, Loss: -1.4154936075210571\n",
      "Batch 248, Loss: 0.4210910499095917\n",
      "Batch 249, Loss: 0.4053114056587219\n",
      "Batch 250, Loss: 0.4074251055717468\n",
      "Batch 251, Loss: 0.42277875542640686\n",
      "Batch 252, Loss: -1.4083003997802734\n",
      "Batch 253, Loss: 0.4041587710380554\n",
      "Batch 254, Loss: 0.4066285789012909\n",
      "Batch 255, Loss: -1.409053087234497\n",
      "Batch 256, Loss: -1.4047342538833618\n",
      "Batch 257, Loss: 0.418209046125412\n",
      "Batch 258, Loss: -1.4178011417388916\n",
      "Batch 259, Loss: 0.40382224321365356\n",
      "Batch 260, Loss: 0.4190376400947571\n",
      "Batch 261, Loss: -1.4069139957427979\n",
      "Batch 262, Loss: -1.4137966632843018\n",
      "Batch 263, Loss: -1.4104044437408447\n",
      "Batch 264, Loss: 0.4146076440811157\n",
      "Batch 265, Loss: -1.4035519361495972\n",
      "Batch 266, Loss: -1.417014241218567\n",
      "Batch 267, Loss: 0.41017937660217285\n",
      "Batch 268, Loss: 0.4159573018550873\n",
      "Batch 269, Loss: -1.4062472581863403\n",
      "Batch 270, Loss: -1.422733187675476\n",
      "Batch 271, Loss: 0.406360924243927\n",
      "Batch 272, Loss: 0.40684521198272705\n",
      "Batch 273, Loss: 0.40584853291511536\n",
      "Batch 274, Loss: 0.4063759744167328\n",
      "Batch 275, Loss: 0.4050297439098358\n",
      "Batch 276, Loss: 0.41793519258499146\n",
      "Batch 277, Loss: 0.41818001866340637\n",
      "Batch 278, Loss: -1.4160289764404297\n",
      "Batch 279, Loss: 0.406488835811615\n",
      "Batch 280, Loss: -1.4022397994995117\n",
      "Batch 281, Loss: -1.4050161838531494\n",
      "Batch 282, Loss: 0.4004746079444885\n",
      "Batch 283, Loss: 0.41440701484680176\n",
      "Batch 284, Loss: -1.4019991159439087\n",
      "Batch 285, Loss: 0.40001538395881653\n",
      "Batch 286, Loss: 0.3981305658817291\n",
      "Batch 287, Loss: -1.4024335145950317\n",
      "Batch 288, Loss: 0.404799222946167\n",
      "Batch 289, Loss: 0.4019941985607147\n",
      "Batch 290, Loss: -1.4011937379837036\n",
      "Batch 291, Loss: -1.3981950283050537\n",
      "Batch 292, Loss: 0.40252554416656494\n",
      "Batch 293, Loss: 0.4005427360534668\n",
      "Batch 294, Loss: 0.40666213631629944\n",
      "Batch 295, Loss: 0.3953903913497925\n",
      "Batch 296, Loss: 0.39741161465644836\n",
      "Batch 297, Loss: -1.4071084260940552\n",
      "Batch 298, Loss: -1.3916540145874023\n",
      "Batch 299, Loss: 0.4063106179237366\n",
      "Batch 300, Loss: -1.3975989818572998\n",
      "Batch 301, Loss: 0.39624884724617004\n",
      "Batch 302, Loss: -1.4044857025146484\n",
      "Batch 303, Loss: 0.4018954634666443\n",
      "Batch 304, Loss: 0.4046582281589508\n",
      "Batch 305, Loss: 0.3946104645729065\n",
      "Batch 306, Loss: -1.3905467987060547\n",
      "Batch 307, Loss: 0.3921118378639221\n",
      "Batch 308, Loss: 0.3993828296661377\n",
      "Batch 309, Loss: 0.39033687114715576\n",
      "Batch 310, Loss: 0.4041295051574707\n",
      "Batch 311, Loss: -1.398781418800354\n",
      "Batch 312, Loss: -1.395804762840271\n",
      "Batch 313, Loss: 0.38698118925094604\n",
      "Batch 314, Loss: 0.3975028693675995\n",
      "Batch 315, Loss: 0.3891300559043884\n",
      "Batch 316, Loss: 0.38954904675483704\n",
      "Batch 317, Loss: 0.3927694261074066\n",
      "Batch 318, Loss: -1.3917765617370605\n",
      "Batch 319, Loss: 0.38358187675476074\n",
      "Batch 320, Loss: -1.3925131559371948\n",
      "Batch 321, Loss: -1.395462155342102\n",
      "Batch 322, Loss: -1.3919291496276855\n",
      "Batch 323, Loss: -1.381866693496704\n",
      "Batch 324, Loss: -1.3876301050186157\n",
      "Batch 325, Loss: -1.3845205307006836\n",
      "Batch 326, Loss: 0.393060564994812\n",
      "Batch 327, Loss: 0.3887871503829956\n",
      "Batch 328, Loss: 0.3886876404285431\n",
      "Batch 329, Loss: -1.3850334882736206\n",
      "Batch 330, Loss: -1.3904082775115967\n",
      "Batch 331, Loss: 0.38270705938339233\n",
      "Batch 332, Loss: 0.3881695568561554\n",
      "Batch 333, Loss: -1.386038899421692\n",
      "Batch 334, Loss: -1.3821043968200684\n",
      "Batch 335, Loss: -1.3856955766677856\n",
      "Batch 336, Loss: 0.3831137418746948\n",
      "Batch 337, Loss: -1.3861383199691772\n",
      "Batch 338, Loss: 0.38346657156944275\n",
      "Batch 339, Loss: 0.3856930732727051\n",
      "Batch 340, Loss: 0.38517504930496216\n",
      "Batch 341, Loss: 0.38285914063453674\n",
      "Batch 342, Loss: -1.39068603515625\n",
      "Batch 343, Loss: 0.3857613801956177\n",
      "Batch 344, Loss: -1.3806049823760986\n",
      "Batch 345, Loss: 0.38162195682525635\n",
      "Batch 346, Loss: -1.3804906606674194\n",
      "Batch 347, Loss: 0.38633599877357483\n",
      "Batch 348, Loss: -1.3919835090637207\n",
      "Batch 349, Loss: -1.3832454681396484\n",
      "Batch 350, Loss: -1.3872407674789429\n",
      "Batch 351, Loss: 0.3826943635940552\n",
      "Batch 352, Loss: 0.38310936093330383\n",
      "Batch 353, Loss: -1.3881862163543701\n",
      "Batch 354, Loss: 0.3788914382457733\n",
      "Batch 355, Loss: -1.3970751762390137\n",
      "Batch 356, Loss: -1.380009412765503\n",
      "Batch 357, Loss: 0.3882376551628113\n",
      "Batch 358, Loss: 0.38028159737586975\n",
      "Batch 359, Loss: 0.3821422755718231\n",
      "Batch 360, Loss: -1.3936007022857666\n",
      "Batch 361, Loss: 0.3979845345020294\n",
      "Batch 362, Loss: -1.3766534328460693\n",
      "Batch 363, Loss: 0.37676554918289185\n",
      "Batch 364, Loss: -1.3820408582687378\n",
      "Batch 365, Loss: 0.38268786668777466\n",
      "Batch 366, Loss: 0.3834680914878845\n",
      "Batch 367, Loss: -1.3752226829528809\n",
      "Batch 368, Loss: -1.3920766115188599\n",
      "Batch 369, Loss: -1.3826451301574707\n",
      "Batch 370, Loss: -1.3799216747283936\n",
      "Batch 371, Loss: -1.3774223327636719\n",
      "Batch 372, Loss: 0.3841385245323181\n",
      "Batch 373, Loss: 0.3844956159591675\n",
      "Batch 374, Loss: 0.37931525707244873\n",
      "Batch 375, Loss: -1.3828357458114624\n",
      "Batch 376, Loss: 0.3989626467227936\n",
      "Batch 377, Loss: -1.3787484169006348\n",
      "Batch 378, Loss: 0.3986203968524933\n",
      "Batch 379, Loss: 0.3910420835018158\n",
      "Batch 380, Loss: -1.3820573091506958\n",
      "Batch 381, Loss: 0.3812233507633209\n",
      "Batch 382, Loss: 0.3811168670654297\n",
      "Batch 383, Loss: -1.380298137664795\n",
      "Batch 384, Loss: -1.3836157321929932\n",
      "Batch 385, Loss: 0.3952125608921051\n",
      "Batch 386, Loss: -1.3813831806182861\n",
      "Batch 387, Loss: 0.3824259638786316\n",
      "Batch 388, Loss: -1.3823362588882446\n",
      "Batch 389, Loss: 0.3824402987957001\n",
      "Batch 390, Loss: -1.3873732089996338\n",
      "Batch 391, Loss: -1.3804107904434204\n",
      "Batch 392, Loss: -1.3822276592254639\n",
      "Batch 393, Loss: 0.38325539231300354\n",
      "Batch 394, Loss: -1.3824436664581299\n",
      "Batch 395, Loss: -1.3919188976287842\n",
      "Batch 396, Loss: 0.38336437940597534\n",
      "Batch 397, Loss: -1.3823060989379883\n",
      "Batch 398, Loss: 0.3846237361431122\n",
      "Batch 399, Loss: 0.3821231722831726\n",
      "Training [90%]\tLoss: -0.4989\n",
      "Batch 0, Loss: 0.3831794261932373\n",
      "Batch 1, Loss: -1.3829591274261475\n",
      "Batch 2, Loss: -1.3823111057281494\n",
      "Batch 3, Loss: 0.38330745697021484\n",
      "Batch 4, Loss: 0.393829345703125\n",
      "Batch 5, Loss: -1.382688283920288\n",
      "Batch 6, Loss: -1.3828787803649902\n",
      "Batch 7, Loss: 0.39607566595077515\n",
      "Batch 8, Loss: 0.38257819414138794\n",
      "Batch 9, Loss: -1.3816653490066528\n",
      "Batch 10, Loss: 0.381930947303772\n",
      "Batch 11, Loss: 0.38659584522247314\n",
      "Batch 12, Loss: -1.382552146911621\n",
      "Batch 13, Loss: -1.3848109245300293\n",
      "Batch 14, Loss: 0.3839852809906006\n",
      "Batch 15, Loss: 0.3831114172935486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16, Loss: -1.3805866241455078\n",
      "Batch 17, Loss: -1.393511176109314\n",
      "Batch 18, Loss: -1.3836305141448975\n",
      "Batch 19, Loss: -1.3912155628204346\n",
      "Batch 20, Loss: 0.38427361845970154\n",
      "Batch 21, Loss: 0.3824498951435089\n",
      "Batch 22, Loss: 0.39177805185317993\n",
      "Batch 23, Loss: 0.3824065029621124\n",
      "Batch 24, Loss: 0.3839761018753052\n",
      "Batch 25, Loss: 0.3817044496536255\n",
      "Batch 26, Loss: 0.3837891221046448\n",
      "Batch 27, Loss: -1.390413522720337\n",
      "Batch 28, Loss: -1.3812246322631836\n",
      "Batch 29, Loss: 0.38186970353126526\n",
      "Batch 30, Loss: -1.3809359073638916\n",
      "Batch 31, Loss: 0.38230758905410767\n",
      "Batch 32, Loss: -1.3828706741333008\n",
      "Batch 33, Loss: 0.37935590744018555\n",
      "Batch 34, Loss: -1.3803858757019043\n",
      "Batch 35, Loss: -1.3784846067428589\n",
      "Batch 36, Loss: -1.3780521154403687\n",
      "Batch 37, Loss: 0.3814544677734375\n",
      "Batch 38, Loss: 0.38107332587242126\n",
      "Batch 39, Loss: -1.380014181137085\n",
      "Batch 40, Loss: 0.3877480626106262\n",
      "Batch 41, Loss: 0.38251879811286926\n",
      "Batch 42, Loss: -1.3808767795562744\n",
      "Batch 43, Loss: -1.3794941902160645\n",
      "Batch 44, Loss: 0.3857610821723938\n",
      "Batch 45, Loss: -1.3885631561279297\n",
      "Batch 46, Loss: 0.37936338782310486\n",
      "Batch 47, Loss: 0.3793959319591522\n",
      "Batch 48, Loss: -1.390572428703308\n",
      "Batch 49, Loss: 0.3900100588798523\n",
      "Batch 50, Loss: 0.38827094435691833\n",
      "Batch 51, Loss: -1.3843293190002441\n",
      "Batch 52, Loss: 0.37844014167785645\n",
      "Batch 53, Loss: 0.3798440992832184\n",
      "Batch 54, Loss: -1.384775161743164\n",
      "Batch 55, Loss: 0.3761448562145233\n",
      "Batch 56, Loss: -1.3842380046844482\n",
      "Batch 57, Loss: 0.3886505961418152\n",
      "Batch 58, Loss: 0.3788498640060425\n",
      "Batch 59, Loss: -1.3856486082077026\n",
      "Batch 60, Loss: -1.3759435415267944\n",
      "Batch 61, Loss: -1.3745940923690796\n",
      "Batch 62, Loss: -1.3772227764129639\n",
      "Batch 63, Loss: -1.3749815225601196\n",
      "Batch 64, Loss: 0.3752962350845337\n",
      "Batch 65, Loss: -1.3806401491165161\n",
      "Batch 66, Loss: -1.3810091018676758\n",
      "Batch 67, Loss: 0.3863246738910675\n",
      "Batch 68, Loss: 0.37913087010383606\n",
      "Batch 69, Loss: 0.37930572032928467\n",
      "Batch 70, Loss: 0.37904709577560425\n",
      "Batch 71, Loss: -1.3757917881011963\n",
      "Batch 72, Loss: 0.3770725429058075\n",
      "Batch 73, Loss: 0.3784981966018677\n",
      "Batch 74, Loss: 0.3783811032772064\n",
      "Batch 75, Loss: 0.37713849544525146\n",
      "Batch 76, Loss: -1.3769081830978394\n",
      "Batch 77, Loss: 0.37978971004486084\n",
      "Batch 78, Loss: -1.374630331993103\n",
      "Batch 79, Loss: 0.37489551305770874\n",
      "Batch 80, Loss: 0.3859456479549408\n",
      "Batch 81, Loss: -1.3833409547805786\n",
      "Batch 82, Loss: -1.3830268383026123\n",
      "Batch 83, Loss: 0.3744087219238281\n",
      "Batch 84, Loss: -1.3743735551834106\n",
      "Batch 85, Loss: 0.38028228282928467\n",
      "Batch 86, Loss: 0.3746311366558075\n",
      "Batch 87, Loss: -1.3725502490997314\n",
      "Batch 88, Loss: -1.3727152347564697\n",
      "Batch 89, Loss: 0.37352895736694336\n",
      "Batch 90, Loss: 0.3716728091239929\n",
      "Batch 91, Loss: -1.3701804876327515\n",
      "Batch 92, Loss: -1.373654842376709\n",
      "Batch 93, Loss: 0.37192606925964355\n",
      "Batch 94, Loss: 0.3700801432132721\n",
      "Batch 95, Loss: -1.3708745241165161\n",
      "Batch 96, Loss: -1.3788409233093262\n",
      "Batch 97, Loss: -1.3789756298065186\n",
      "Batch 98, Loss: 0.3728857636451721\n",
      "Batch 99, Loss: -1.3735425472259521\n",
      "Batch 100, Loss: -1.3702141046524048\n",
      "Batch 101, Loss: -1.3707151412963867\n",
      "Batch 102, Loss: -1.36991286277771\n",
      "Batch 103, Loss: -1.3834209442138672\n",
      "Batch 104, Loss: 0.37386682629585266\n",
      "Batch 105, Loss: 0.3748093247413635\n",
      "Batch 106, Loss: -1.3739736080169678\n",
      "Batch 107, Loss: 0.38655954599380493\n",
      "Batch 108, Loss: -1.3807284832000732\n",
      "Batch 109, Loss: 0.37614333629608154\n",
      "Batch 110, Loss: -1.3751164674758911\n",
      "Batch 111, Loss: -1.3752111196517944\n",
      "Batch 112, Loss: 0.37809890508651733\n",
      "Batch 113, Loss: -1.3876819610595703\n",
      "Batch 114, Loss: -1.3792130947113037\n",
      "Batch 115, Loss: 0.37576475739479065\n",
      "Batch 116, Loss: 0.37541624903678894\n",
      "Batch 117, Loss: -1.3818261623382568\n",
      "Batch 118, Loss: 0.3758660852909088\n",
      "Batch 119, Loss: 0.3766028881072998\n",
      "Batch 120, Loss: -1.39046311378479\n",
      "Batch 121, Loss: -1.3871480226516724\n",
      "Batch 122, Loss: 0.3805314302444458\n",
      "Batch 123, Loss: 0.37815868854522705\n",
      "Batch 124, Loss: 0.3907660245895386\n",
      "Batch 125, Loss: -1.3800323009490967\n",
      "Batch 126, Loss: -1.373982310295105\n",
      "Batch 127, Loss: 0.3721422553062439\n",
      "Batch 128, Loss: -1.386839747428894\n",
      "Batch 129, Loss: -1.3687958717346191\n",
      "Batch 130, Loss: 0.37454667687416077\n",
      "Batch 131, Loss: -1.3931931257247925\n",
      "Batch 132, Loss: 0.36831381916999817\n",
      "Batch 133, Loss: 0.3642720580101013\n",
      "Batch 134, Loss: -1.3920375108718872\n",
      "Batch 135, Loss: -1.3792558908462524\n",
      "Batch 136, Loss: 0.38448119163513184\n",
      "Batch 137, Loss: -1.3696750402450562\n",
      "Batch 138, Loss: 0.37101760506629944\n",
      "Batch 139, Loss: -1.3895145654678345\n",
      "Batch 140, Loss: 0.3729791045188904\n",
      "Batch 141, Loss: -1.3847484588623047\n",
      "Batch 142, Loss: 0.359249085187912\n",
      "Batch 143, Loss: -1.3543384075164795\n",
      "Batch 144, Loss: 0.384257972240448\n",
      "Batch 145, Loss: 0.39256513118743896\n",
      "Batch 146, Loss: -1.3629801273345947\n",
      "Batch 147, Loss: -1.355171799659729\n",
      "Batch 148, Loss: 0.39317643642425537\n",
      "Batch 149, Loss: -1.3724722862243652\n",
      "Batch 150, Loss: 0.3842597007751465\n",
      "Batch 151, Loss: -1.371389627456665\n",
      "Batch 152, Loss: -1.3540284633636475\n",
      "Batch 153, Loss: 0.37574389576911926\n",
      "Batch 154, Loss: 0.3764958381652832\n",
      "Batch 155, Loss: -1.362783670425415\n",
      "Batch 156, Loss: 0.37300050258636475\n",
      "Batch 157, Loss: -1.3844751119613647\n",
      "Batch 158, Loss: 0.3675794005393982\n",
      "Batch 159, Loss: 0.3896946310997009\n",
      "Batch 160, Loss: -1.3645451068878174\n",
      "Batch 161, Loss: -1.3909144401550293\n",
      "Batch 162, Loss: 0.3818776309490204\n",
      "Batch 163, Loss: 0.3654782474040985\n",
      "Batch 164, Loss: 0.37215715646743774\n",
      "Batch 165, Loss: 0.380501389503479\n",
      "Batch 166, Loss: 0.389578253030777\n",
      "Batch 167, Loss: -1.3919552564620972\n",
      "Batch 168, Loss: 0.3720434606075287\n",
      "Batch 169, Loss: 0.37166720628738403\n",
      "Batch 170, Loss: -1.3637025356292725\n",
      "Batch 171, Loss: -1.379164695739746\n",
      "Batch 172, Loss: 0.37526586651802063\n",
      "Batch 173, Loss: -1.3746427297592163\n",
      "Batch 174, Loss: 0.37927907705307007\n",
      "Batch 175, Loss: -1.359788179397583\n",
      "Batch 176, Loss: 0.3727746307849884\n",
      "Batch 177, Loss: 0.35393357276916504\n",
      "Batch 178, Loss: -1.3644356727600098\n",
      "Batch 179, Loss: -1.3450217247009277\n",
      "Batch 180, Loss: -1.386831283569336\n",
      "Batch 181, Loss: -1.3712886571884155\n",
      "Batch 182, Loss: 0.3614378571510315\n",
      "Batch 183, Loss: -1.3665043115615845\n",
      "Batch 184, Loss: 0.35120731592178345\n",
      "Batch 185, Loss: -1.3682353496551514\n",
      "Batch 186, Loss: -1.3732292652130127\n",
      "Batch 187, Loss: 0.35634467005729675\n",
      "Batch 188, Loss: 0.3636971414089203\n",
      "Batch 189, Loss: -1.3782975673675537\n",
      "Batch 190, Loss: -1.3688994646072388\n",
      "Batch 191, Loss: 0.35468485951423645\n",
      "Batch 192, Loss: -1.3907935619354248\n",
      "Batch 193, Loss: 0.34892982244491577\n",
      "Batch 194, Loss: -1.3914415836334229\n",
      "Batch 195, Loss: 0.37437084317207336\n",
      "Batch 196, Loss: 0.3916935622692108\n",
      "Batch 197, Loss: -1.3756296634674072\n",
      "Batch 198, Loss: -1.3818577527999878\n",
      "Batch 199, Loss: -1.3678524494171143\n",
      "Batch 200, Loss: 0.3881435692310333\n",
      "Batch 201, Loss: 0.36146193742752075\n",
      "Batch 202, Loss: 0.3488571345806122\n",
      "Batch 203, Loss: 0.346636027097702\n",
      "Batch 204, Loss: -1.3724638223648071\n",
      "Batch 205, Loss: -1.3863346576690674\n",
      "Batch 206, Loss: -1.3871251344680786\n",
      "Batch 207, Loss: -1.3717656135559082\n",
      "Batch 208, Loss: -1.3724405765533447\n",
      "Batch 209, Loss: -1.3886924982070923\n",
      "Batch 210, Loss: 0.37415310740470886\n",
      "Batch 211, Loss: 0.38418447971343994\n",
      "Batch 212, Loss: -1.3945175409317017\n",
      "Batch 213, Loss: 0.3444218933582306\n",
      "Batch 214, Loss: -1.3844906091690063\n",
      "Batch 215, Loss: 0.3973870277404785\n",
      "Batch 216, Loss: -1.3816618919372559\n",
      "Batch 217, Loss: -1.3560398817062378\n",
      "Batch 218, Loss: -1.3784226179122925\n",
      "Batch 219, Loss: -1.3469514846801758\n",
      "Batch 220, Loss: -1.3983300924301147\n",
      "Batch 221, Loss: -1.3800866603851318\n",
      "Batch 222, Loss: -1.3703099489212036\n",
      "Batch 223, Loss: 0.403227299451828\n",
      "Batch 224, Loss: 0.36855459213256836\n",
      "Batch 225, Loss: -1.3693733215332031\n",
      "Batch 226, Loss: 0.3900243043899536\n",
      "Batch 227, Loss: -1.3935980796813965\n",
      "Batch 228, Loss: 0.3719632029533386\n",
      "Batch 229, Loss: 0.39925307035446167\n",
      "Batch 230, Loss: -1.373390793800354\n",
      "Batch 231, Loss: 0.3777509331703186\n",
      "Batch 232, Loss: -1.3935755491256714\n",
      "Batch 233, Loss: -1.3660614490509033\n",
      "Batch 234, Loss: -1.3704352378845215\n",
      "Batch 235, Loss: 0.380541056394577\n",
      "Batch 236, Loss: -1.3964173793792725\n",
      "Batch 237, Loss: -1.3971467018127441\n",
      "Batch 238, Loss: 0.3731757700443268\n",
      "Batch 239, Loss: 0.3967948853969574\n",
      "Batch 240, Loss: -1.4036606550216675\n",
      "Batch 241, Loss: 0.39755287766456604\n",
      "Batch 242, Loss: -1.402204155921936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 243, Loss: 0.40794169902801514\n",
      "Batch 244, Loss: -1.3692831993103027\n",
      "Batch 245, Loss: 0.3831225335597992\n",
      "Batch 246, Loss: 0.38304078578948975\n",
      "Batch 247, Loss: -1.376150369644165\n",
      "Batch 248, Loss: 0.38643136620521545\n",
      "Batch 249, Loss: -1.409327507019043\n",
      "Batch 250, Loss: 0.405036062002182\n",
      "Batch 251, Loss: 0.4043567180633545\n",
      "Batch 252, Loss: 0.3716154098510742\n",
      "Batch 253, Loss: 0.3660808503627777\n",
      "Batch 254, Loss: 0.39786067605018616\n",
      "Batch 255, Loss: -1.3696162700653076\n",
      "Batch 256, Loss: -1.3987557888031006\n",
      "Batch 257, Loss: -1.4063246250152588\n",
      "Batch 258, Loss: 0.36113208532333374\n",
      "Batch 259, Loss: 0.4064100980758667\n",
      "Batch 260, Loss: -1.3979010581970215\n",
      "Batch 261, Loss: 0.37165892124176025\n",
      "Batch 262, Loss: -1.3992154598236084\n",
      "Batch 263, Loss: 0.3609795570373535\n",
      "Batch 264, Loss: -1.3913886547088623\n",
      "Batch 265, Loss: -1.3559821844100952\n",
      "Batch 266, Loss: 0.3640144467353821\n",
      "Batch 267, Loss: -1.406317949295044\n",
      "Batch 268, Loss: -1.3674492835998535\n",
      "Batch 269, Loss: 0.40738019347190857\n",
      "Batch 270, Loss: -1.3682031631469727\n",
      "Batch 271, Loss: -1.3712654113769531\n",
      "Batch 272, Loss: -1.4086965322494507\n",
      "Batch 273, Loss: 0.4098833203315735\n",
      "Batch 274, Loss: -1.3757327795028687\n",
      "Batch 275, Loss: -1.3677194118499756\n",
      "Batch 276, Loss: 0.3721926212310791\n",
      "Batch 277, Loss: 0.38638222217559814\n",
      "Batch 278, Loss: 0.37313705682754517\n",
      "Batch 279, Loss: -1.3744667768478394\n",
      "Batch 280, Loss: 0.3731299042701721\n",
      "Batch 281, Loss: 0.3708100914955139\n",
      "Batch 282, Loss: 0.36722642183303833\n",
      "Batch 283, Loss: -1.353530764579773\n",
      "Batch 284, Loss: -1.4086825847625732\n",
      "Batch 285, Loss: 0.40240901708602905\n",
      "Batch 286, Loss: -1.3670634031295776\n",
      "Batch 287, Loss: -1.408290982246399\n",
      "Batch 288, Loss: -1.3721243143081665\n",
      "Batch 289, Loss: -1.4099376201629639\n",
      "Batch 290, Loss: -1.4086833000183105\n",
      "Batch 291, Loss: 0.3991284668445587\n",
      "Batch 292, Loss: -1.3748949766159058\n",
      "Batch 293, Loss: -1.3842617273330688\n",
      "Batch 294, Loss: -1.3778347969055176\n",
      "Batch 295, Loss: 0.40179237723350525\n",
      "Batch 296, Loss: 0.38584479689598083\n",
      "Batch 297, Loss: -1.3963638544082642\n",
      "Batch 298, Loss: 0.4010458290576935\n",
      "Batch 299, Loss: -1.3922195434570312\n",
      "Batch 300, Loss: 0.39000391960144043\n",
      "Batch 301, Loss: 0.39015305042266846\n",
      "Batch 302, Loss: -1.4094955921173096\n",
      "Batch 303, Loss: 0.3907007575035095\n",
      "Batch 304, Loss: 0.3916153907775879\n",
      "Batch 305, Loss: -1.3970904350280762\n",
      "Batch 306, Loss: -1.3980474472045898\n",
      "Batch 307, Loss: -1.4067199230194092\n",
      "Batch 308, Loss: 0.4035966098308563\n",
      "Batch 309, Loss: -1.3945167064666748\n",
      "Batch 310, Loss: -1.408090353012085\n",
      "Batch 311, Loss: 0.39135390520095825\n",
      "Batch 312, Loss: -1.4150705337524414\n",
      "Batch 313, Loss: 0.3924418091773987\n",
      "Batch 314, Loss: -1.410584807395935\n",
      "Batch 315, Loss: -1.41349458694458\n",
      "Batch 316, Loss: 0.3960031270980835\n",
      "Batch 317, Loss: 0.3925476372241974\n",
      "Batch 318, Loss: -1.3934671878814697\n",
      "Batch 319, Loss: 0.3974117934703827\n",
      "Batch 320, Loss: 0.4101434648036957\n",
      "Batch 321, Loss: 0.39575809240341187\n",
      "Batch 322, Loss: 0.3902795612812042\n",
      "Batch 323, Loss: -1.394372582435608\n",
      "Batch 324, Loss: 0.4198872745037079\n",
      "Batch 325, Loss: 0.4043194651603699\n",
      "Batch 326, Loss: 0.40160098671913147\n",
      "Batch 327, Loss: -1.3900611400604248\n",
      "Batch 328, Loss: -1.3896173238754272\n",
      "Batch 329, Loss: 0.38276973366737366\n",
      "Batch 330, Loss: 0.3989255130290985\n",
      "Batch 331, Loss: 0.3914145231246948\n",
      "Batch 332, Loss: -1.4069433212280273\n",
      "Batch 333, Loss: -1.3794926404953003\n",
      "Batch 334, Loss: -1.3790106773376465\n",
      "Batch 335, Loss: -1.3882575035095215\n",
      "Batch 336, Loss: -1.3898873329162598\n",
      "Batch 337, Loss: 0.3828120827674866\n",
      "Batch 338, Loss: 0.38918328285217285\n",
      "Batch 339, Loss: 0.3831833004951477\n",
      "Batch 340, Loss: 0.4048606753349304\n",
      "Batch 341, Loss: 0.4176725447177887\n",
      "Batch 342, Loss: -1.385893702507019\n",
      "Batch 343, Loss: -1.3941872119903564\n",
      "Batch 344, Loss: 0.3872203230857849\n",
      "Batch 345, Loss: 0.41431498527526855\n",
      "Batch 346, Loss: -1.3859283924102783\n",
      "Batch 347, Loss: 0.39698266983032227\n",
      "Batch 348, Loss: 0.38478922843933105\n",
      "Batch 349, Loss: -1.3810819387435913\n",
      "Batch 350, Loss: 0.39187705516815186\n",
      "Batch 351, Loss: -1.3935465812683105\n",
      "Batch 352, Loss: 0.40923216938972473\n",
      "Batch 353, Loss: -1.4058548212051392\n",
      "Batch 354, Loss: 0.38554301857948303\n",
      "Batch 355, Loss: -1.4029600620269775\n",
      "Batch 356, Loss: -1.3833069801330566\n",
      "Batch 357, Loss: 0.3838746249675751\n",
      "Batch 358, Loss: -1.3791429996490479\n",
      "Batch 359, Loss: 0.3918018341064453\n",
      "Batch 360, Loss: -1.3898897171020508\n",
      "Batch 361, Loss: -1.3952076435089111\n",
      "Batch 362, Loss: -1.3845775127410889\n",
      "Batch 363, Loss: 0.38385969400405884\n",
      "Batch 364, Loss: 0.38226717710494995\n",
      "Batch 365, Loss: 0.40748611092567444\n",
      "Batch 366, Loss: 0.3870975971221924\n",
      "Batch 367, Loss: -1.4056802988052368\n",
      "Batch 368, Loss: 0.38079002499580383\n",
      "Batch 369, Loss: -1.3797938823699951\n",
      "Batch 370, Loss: 0.40391209721565247\n",
      "Batch 371, Loss: 0.38081544637680054\n",
      "Batch 372, Loss: 0.3914880156517029\n",
      "Batch 373, Loss: 0.3845004439353943\n",
      "Batch 374, Loss: 0.3882346749305725\n",
      "Batch 375, Loss: 0.38261327147483826\n",
      "Batch 376, Loss: -1.3923535346984863\n",
      "Batch 377, Loss: 0.3993679881095886\n",
      "Batch 378, Loss: -1.3784222602844238\n",
      "Batch 379, Loss: -1.3867915868759155\n",
      "Batch 380, Loss: -1.3786604404449463\n",
      "Batch 381, Loss: 0.40192627906799316\n",
      "Batch 382, Loss: -1.3774675130844116\n",
      "Batch 383, Loss: -1.4005320072174072\n",
      "Batch 384, Loss: -1.3912911415100098\n",
      "Batch 385, Loss: -1.3779557943344116\n",
      "Batch 386, Loss: 0.37938880920410156\n",
      "Batch 387, Loss: 0.4020991027355194\n",
      "Batch 388, Loss: -1.388343095779419\n",
      "Batch 389, Loss: 0.4019394516944885\n",
      "Batch 390, Loss: 0.40150290727615356\n",
      "Batch 391, Loss: -1.3904685974121094\n",
      "Batch 392, Loss: -1.373136043548584\n",
      "Batch 393, Loss: 0.38904938101768494\n",
      "Batch 394, Loss: 0.38905709981918335\n",
      "Batch 395, Loss: 0.4010767340660095\n",
      "Batch 396, Loss: 0.38276803493499756\n",
      "Batch 397, Loss: -1.3758563995361328\n",
      "Batch 398, Loss: -1.3891798257827759\n",
      "Batch 399, Loss: -1.3801946640014648\n",
      "Training [100%]\tLoss: -0.4996\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: -0.00036855798680335283 s\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: -0.4997\n",
      "\tAccuracy: 50.0%\n",
      "Test time: 0.009940285002812743 s\n"
     ]
    }
   ],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
