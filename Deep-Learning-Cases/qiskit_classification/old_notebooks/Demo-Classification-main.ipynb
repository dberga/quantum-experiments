{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "from qiskit import IBMQ\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796b049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network args\n",
    "n_classes = 3\n",
    "n_qubits = 4\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"hybridqnn_shallow\" # will run that name from qnetwork classes\n",
    "\n",
    "# declare quantum instance\n",
    "qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\")) # Hybrid QNN\n",
    "#qi = QuantumInstance(BasicAer.get_backend('qasm_simulator'), shots=1024,seed_simulator=algorithm_globals.random_seed, seed_transpiler=algorithm_globals.random_seed) # QSVM                                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog', 'bird']\n"
     ]
    }
   ],
   "source": [
    "# train args\n",
    "batch_size = 1\n",
    "epochs = 5\n",
    "LR = 0.0001\n",
    "n_samples_train = 200 #128\n",
    "n_samples_test = 50 #64\n",
    "# plot args\n",
    "n_samples_show = 16\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"CIFAR10\" # MNIST / CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = [\"cat\",\"dog\",'bird'] # ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "if network == 'hybridqnn_shallow' or network == 'QSVM':\n",
    "    # Set preprocessing transforms\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "else:\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    \n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: 0.00016461685299873352 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABxMAAADGCAYAAAAZmK7TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxnklEQVR4nO3df5RcZ33f8c93ZnZ2pdVqpZVkyV5JtuW15V8EMGtCCm1oQyJCAz5NSMApDTQkblrT0pM0LYTkxIHQJM05aZuaFhwgSkiKAdOCUrDASTDkgI2QMQZLtiPZkvXDsn7/3tXuzsy3f+zs/TG6Wmut9cx+pffrnDnnmbnfufPMfHzvszqP73PN3QUAAAAAAAAAAAAArUqd7gAAAAAAAAAAAACAuYnJRAAAAAAAAAAAAACFmEwEAAAAAAAAAAAAUIjJRAAAAAAAAAAAAACFmEwEAAAAAAAAAAAAUIjJRAAAAAAAAAAAAACFmEyco8zsk2Z2wMweP8d2M7M/NrPtZvZ9M7ul3X3EuZFffGQYHxnGR4bxkWFs5BcfGcZHhvGRYWzkFx8ZxkeG8ZFhfGQIicnEuWy9pDdOs/0nJV3bfNwh6X+1oU84f+tFftGtFxlGt15kGN16kWF060WGka0X+UW3XmQY3XqRYXTrRYaRrRf5RbdeZBjdepFhdOtFhtGtFxle8phMnKPc/RuSjkxTcpukP/dJD0taZGaXt6d3eCHkFx8ZxkeG8ZFhfGQYG/nFR4bxkWF8ZBgb+cVHhvGRYXxkGB8ZQmIyMbJBSbszz/c0X0MM5BcfGcZHhvGRYXxkGBv5xUeG8ZFhfGQYG/nFR4bxkWF8ZBgfGV4CKp3uAF5aZnaHJi8tVm9v76uuv/76Dvfo0nHzzTdr+/btMrOD7r7sxe6HDDuHDOMjw/jIML7ZyJD8OodjMD4yjI8M42MsjI1jMD4yjI8M4yPD+Ph75uLwyCOPHHqx+TGZGNdeSasyz1c2X8tx93sk3SNJw8PDvnnz5vb0Dtq5c6d+6qd+Slu2bHm2YPN55SeRYSeRYXxkGB8ZxjcbGZJf53AMxkeG8ZFhfIyFsXEMxkeG8ZFhfGQYH3/PXBzMrCi/88Iyp3FtkPQLNuk1ko67+75OdwrnjfziI8P4yDA+MoyPDGMjv/jIMD4yjI8MYyO/+MgwPjKMjwzjI8NLAFcmzlFm9mlJr5e01Mz2SPptSV2S5O4flfRlSW+StF3SiKR/2Zmeosjtt9+uBx98UIcOHZKkHzKzd4v8QiHD+MgwPjKMjwxjI7/4yDA+MoyPDGMjv/jIMD4yjI8M4yNDSJK5e6f7gDbh0uHOMLNH3H14NvZFhp1BhvGRYXxkGN9sZUh+ncExGB8ZxkeG8TEWxsYxGB8ZxkeG8ZFhfPw9E9uF5McypwAAAAAAAAAAAAAKMZkIAAAAAAAAAAAAoBCTiQAAAAAAAAAAAAAKMZkIAAAAAAAAAAAAoBCTiQAAAAAAAAAAAAAKMZkIAAAAAAAAAAAAoBCTiQAAAAAAAAAAAAAKMZkIAAAAAAAAAAAAoBCTiQAAAAAAAAAAAAAKMZkIAAAAAAAAAAAAoBCTiQAAAAAAAAAAAAAKMZkIAAAAAAAAAAAAoBCTiQAAAAAAAAAAAAAKMZkIAAAAAAAAAAAAoNCcmEw0swfN7Jfm8nvN7C4z+4tptm8xs9fP4LOn3R8AAAAAAAAAAADQabM6mWhmO83sDbO5zyjc/SZ3f7DT/QAAAAAAAAAAAABmy5y4MvFiZ2aVTvcBAAAAAAAAAAAAmKm2TCaa2WIz+39mdtDMjjbbK1vKrjGzTWZ2wsy+aGYDmfe/xsy+ZWbHzOyx6ZYTNbNfNLMnmp/zFTO7MrPtx83sSTM7bmZ3S7IZfpUeM/uMmZ00s++a2csz+06uymwuYXqfmf2FmZ2Q9C4zu9rMvt587wOSls7wswEAAAAAAAAAAIC2ateViSVJfyrpSkmrJY1Kurul5hck/aKkyyXVJP2xJJnZoKQvSfpdSQOS/oOkz5vZstYPMbPbJP2GpJ+WtEzS30n6dHPbUkn/R9JvanIi72lJr828d3VzsnL1NN/jNkmfa/bjf0v6gpl1TVN7n6RFkv6yWf9I87M/JOmd03wOAAAAAAAAAAAA0HFtmUx098Pu/nl3H3H3k5I+LOlHW8o+5e6Pu/tpSb8l6efMrCzpHZK+7O5fdveGuz8gabOkNxV81K9I+j13f8Lda5L+s6RXNK9OfJOkLe5+n7tPSPpvkp7P9HGXuy9y913TfJVHMu//I0k9kl5zjtqH3P0L7t7Q5MTmrZJ+y93H3P0bkv5qms8BAAAAAAAAAAAAOq5dy5zON7OPmdmzzWU/vyFpUXOycMruTPtZSV2avIrvSkk/27xq8JiZHZP0Ok1ewdjqSkn/PVN3RJNLmQ5KuiL7Ge7uLZ95PrLvb0ja09zvtLXNmqPNidIpz87wswEAAAAAAAAAAIC2qrTpc35N0lpJP+zuz5vZKyQ9qvw9C1dl2qslTUg6pMlJuU+5+y+fx+fslvRhd//L1g1mdm32M8zMWj7zfGTfX5K0UtJz56j1THufpMVm1puZUFzdUgMAAAAAAAAAAADMKS/FlYldZtaTeVQk9WnyPonHzGxA0m8XvO8dZnajmc2X9EFJ97l7XdJfSHqzma0zs3Jzn683s5UF+/iopPeb2U2SZGb9ZvazzW1fknSTmf10s0//TtKKGX63V2Xe/+8ljUl6+IXe5O7PanJp1t8xs6qZvU7Sm2f42QAAAAAAAAAAAEBbvRSTiV/W5MTh1OMuTd6fcJ4mrzR8WNLGgvd9StJ6Td7HsEeTk31y992SbpP0G5IOavLqw18v6ru7/19JfyDp3uZyqo9L+snmtkOSflbS70s6LOlaSd+ceq+ZrTazU2a2eprv9kVJb5N0VNK/kPTTzfsnno+fl/TDmlx69bcl/fl5vg8AAAAAAAAAAADoiFmdTHT3q9zdWh6/6e7Pufvr3X2Bu1/n7h9rbqs13/d6d3+/u7/a3Re6+5ubk39T+/22u/+ouw+4+zJ3/6fuvivz3o9naj/l7i9r7meVu/9iZtvG5uf3u/t7mvv8eHPbrmb/dp3ju93l7m9197e5e5+7v9Ldv9vy3f86U/uOlvc/4+7/sPkZP978/He0fs4UM3ujmT1lZtvN7H0F21eb2dfM7FEz+76ZvemFE0I7bdy4UWvXrpWkm8kwJjKMjwxjm8pvaGhIKlhNgPzmPjKMj/NofGQYHxnGxlgYH8dgfGQYHxnGxlgYHxlCemmuTMQFMrOypI9o8qrKGyXdbmY3tpT9pqTPuvsrJb1d0v9sby8xnXq9rjvvvFP333+/JG0RGYZDhvGRYWzZ/LZu3SpJA+QXCxnGx3k0PjKMjwxjYyyMj2MwPjKMjwxjYyyMjwwxhcnEuenVkrY3r2Ycl3SvJpd6zXJJC5vtfknPtbF/eAGbNm3S0NCQ1qxZI01mRYbBkGF8ZBhbNr9qtSpNLhNOfoGQYXycR+Mjw/jIMDbGwvg4BuMjw/jIMDbGwvjIEFMqne4ACg1q8t6QU/Zo8n6LWXdJ+qqZ/VtJvZLeULQjM7tD0h2StHr1dLeDxGzau3evVq1alX2JDIMhw/jIMLaC/MY1OT5m3aXzyE8iw06YzQzJrzM4j8ZHhvGRYWyMhfFxDMZHhvGRYWz82z4+/p7BFK5MjOt2SevdfaWkN0n6lJmdlae73+Puw+4+vGzZsrZ3EtMiw/jIMD4yjO288pPIcA7jGIyPDOMjw/jIMDbyi48M4yPD+MgwNv5tHx/H4CVgRlcmLl261K+66qqXqCvnz92TdqPRSNr1ej1XV6mkX69UuvB5U2944etWsgved9batWu1b98+DQ8Pv7v50mlJH24pe7ekN0qSuz9kZj2Slko6MKudwYsyODio3buzF5dqpaS9LWVkOIeRYXxkGFtBflWRXyhkGB/n0fjIMD4yjI2xMD6OwfjIMD4yjI2xMD4yxJQZTSZeddVV2rx5c/HGzASfbBYm1zK7y04eSlKtXkvaI6dPJe0TJ07k6hYtHkjavb29affO/VHTGh8bL9xHV3c1V2eZ728v4reo1Wq67rrr9LnPfU6Dg4Pq7u4uSdrQUrZL0o9JWm9mN0jqkXRwxh+Gl8Stt96qbdu2aceOHdLkfy5vl/TzLWVkOIeRYXxkGFs2v8HBQUkaEGNhKGQYH+fR+MgwPjKMjbEwPo7B+MgwPjKMjbEwPjLEFJY5nYMqlYruvvturVu3TjfccIMkHXH3LWb2QTN7S7Ps1yT9spk9JunTkt7lrbOu6JhshpJukvRZMoyFDOMjw9gYC+Mjw/g4j8ZHhvGRYWyMhfFxDMZHhvGRYWyMhfGRIabYTDIdHh72qSsTz3pfZgnQ3LKfs3CVYuvypadOnkzaRw+nE9w7n34mV9e/eEnSvvHmm5P2RK2Wqzt1Kr26MXsl4cnM65K069ldSXvxov6kfdPNN+XquqrplYov5srEVmb2iLsPX+h+svmhfWYrP4kMO4UM4yPD+MgwPv6eiY1jMD4yjI8M42MsjI1jMD4yjI8M4yPD+Ph7JrYLyY8rEwEAAAAAAAAAAAAUYjIRAAAAAAAAAAAAQCEmEwEAAAAAAAAAAAAUqsz0DVP3Shw5PZJ7/cTRw0m7t7c3bff35+rK5XLm2bnvJ5i9J+P4xERu2+nMvQxHM/0YO3MmV7fpoYeS9tFj6X0Wn9t/OFe3ffuOtH+V9Cc5evR4ru7IkaNJ+7prr0raVsrPyV5/4/VJu7vands2G/dQBAAAAAAAAAAAANqBKxMBAAAAAAAAAAAAFGIyEQAAAAAAAAAAAEChGS9z2mg0JEn7d+/Mvb7pG19P2ouXLknar/wHr8vVLV1+edIulc5vmdN6rZbbVq+ly54eO3okaXd3V3N1R46ly5R++t4vJu3nj5zO1Z04PZq0G5bOr2a6IEkyT/vx3P70cw8fzi+bevvt/yxp3/LKV+T3kVvmFQAAAAAAAAAAAJi7uDIRAAAAAAAAAAAAQCEmEwEAAAAAAAAAAAAUmtEyp+4NNSbGJUlHntuT27Zv+5NJe8eT9aTd278wV9e/eCBpV7t7kra1rHhq2Re8kds2OnIqaZ88kS5lumfXc7m6Z59Llx/duSdtj9XGcnX1zJxqw9KfpHUZ1nIl3TY6ni61+t3vPZWrW9D3QNJeOTiY23bZ8suSdvY7WusPAAAAAAAAAAAAAHQYVyYCAAAAAAAAAAAAKMRkIgAAAAAAAAAAAIBCTCYCAAAAAAAAAAAAKDSjeyZKJjXvI7h8Vf5egK963WuT9re++a2k/dTWH+TqrrvppqS9bMXKc36SuyftRr2e23b40KGkfehQei/Ex554Jlf35DP7knatlu5PnitTuXte0u6Z1520q5X8XKt72o+J0eyGfN1T23cm7Y1f/nJu2+v/yT9O2qtWr077UC4LAAAAAAAAAAAAmEu4MhEAAAAAAAAAAABAISYTAQAAAAAAAAAAABSa0TKnZqZypSpJuvzqody2htJlOr/7+JNJ+5lnns3VPfzNv0vayy67PH2/N/IdK6dd6+7qzm17Ztu2pP39H6Sf9fSz+5RnSaurkvmq5fzX7h1YkmkvS9rzuvJ7K2eWOa2qlrRXXbkqV3dg/4Gk/egj389tu25oTdK+4vIVSbtU6jlHzwEAAAAAAAAAAIDO4MpEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQaEbLnLq7GrUJSdLY2Fhu266dTyft/fueS9qHjhzO1X35S19NP7w6P2l3tSxl2t+XblvcPz+37Zmde5L284dHk/a8pVfm6sqeLr16Yu8zSXv+/N5c3bLV6dKjS5YsTtqL+vJLj5Yza48u6El/uiuWLc3VDSxcmLQffzT//b/90DfTJ7UzSXP1Ndfk6voWLhIAAAAAAAAAAADQSVyZCAAAAAAAAAAAAKAQk4kAAAAAAAAAAAAACjGZCAAAAAAAAAAAAKDQjO6ZODF2Rs/vfFKSdPr06dy2p77/aNI+cWR/0vaJ8VzdaK2RtLvq6euNej1Xd7Q+kbT379+f27bvuYNpnzzzFUYmcnXjo+k9CUuNdFvZ+nJ1Pd3pPgYWpPdJLFlXrm7izMm07739Sbuvd16ubuDaobQPZ0Zz2/bvfDxpf+uvH0jaTz/5ZK7u6b379Sfr/0yNRkOSVqiAmf2cpLskuaTH3P3ni+rQGRs3btR73/teSbrZzN7n7r/fWkOGcxsZxkeGsU3lV5/8G4GxMCAyjI/zaHxkGBvn0fjIMD7Oo/GRYXxkGBtjYXxkCGmGk4loj0ajoY998pP6nQ98QEuWLNFb//k7BszsRnffOlVjZtdKer+k17r7UTO7rHM9Rqt6va4777xTDzzwgK655potkm43sw1kGAcZxkeGsWXzW7lypbq7uxkLgyHD+DiPxkeGsXEejY8M4+M8Gh8ZxkeGsTEWxkeGmMIyp3PQ0zt3acXyFVqxfLm6KhVJOiLptpayX5b0EXc/KknufqDN3cQ0Nm3apKGhIa1Zs0aa/L8x7hUZhkKG8ZFhbNn8qtWqxFgYDhnGx3k0PjKMjfNofGQYH+fR+MgwPjKMjbEwPjLElBldmThy6rg2/d1XJUleyy9Lenz/nqQ9r5zOUXotv1SoLN1m9XQJ1MZEfl7zjKXLjY6Ol8/Zpx4bS9r10VO5bY3TI0l7bCLt78nx/BKtdvVV6Xs8nTQ/vP/5XN3+XdvS/l13c9IevGxZrq63N+37ylWrc9smxtI+nt77VNLe8mS67ye3P63aREMPfuPbUy+NSxpU3nWSZGbflFSWdJe7bxTmhL1792rVqlXZl/ZI+uGWMjKcw8gwPjKMrSA/xsJgyDA+zqPxkWFsnEfjI8P4OI/GR4bxkWFsjIXxkSGmsMxpXBVJ10p6vaSVkr5hZi9z92PZIjO7Q9IdkrR69WphTiHD+MgwPjKM7bzyk8hwDuMYjI8M4yPD2BgL4+MYjI8M4yPD+MgwNv6eiY9j8BLAMqdzUF9vr06eOpl9qSppb0vZHkkb3H3C3XdI+ntNHrA57n6Puw+7+/CyZctaN+MlMjg4qN27d2dfWikyDIUM4yPD2Arye9FjoUSGnTCbGZJfZ3AejY8MY2MsjI+xMD7Oo/GRYXxkGBt/z8TH3zOYwmTiHHT58st09PhRHT9xXPV6XZIGJG1oKfuCJmf6ZWZLNXkp8TNt7Camceutt2rbtm3asWOHJJmkt4sMQyHD+Mgwtmx+4+PjEmNhOGQYH+fR+MgwNs6j8ZFhfJxH4yPD+MgwNsbC+MgQU2a0zOn4xISe29uchZ6YyG2rn0nvBdhdtqRd68rf79Ab6fxlrdKdtnsuy9U1yvOSdsnz90LsG/CkXbFa0p44dSJX199In5fmpe/ZNTqaqxs5sj9tr7gyaZ8azd9b8cBzzybt0/X5SXvl4OX5z62m33Fed09uW2/fkqR9vNKb7u94vk8/8urX6r4vfl7uLklH3H2LmX1Q0mZ33yDpK5J+wsy2SqpL+nV3PyzMCZVKRXfffbfWrVsnSTdJ+hAZxkKG8ZFhbNn8mv9jDWNhMGQYH+fR+MgwNs6j8ZFhfJxH4yPD+MgwNsbC+MgQU6w5WXVeVg8u9//4r942+aRlMnHkWDpxd/zkmaR9eiK//7FGOn9Zq6YTct6zPFeXnUxsjOcnE+sjx5P2dJOJ5ePp1bYlZScT8xdkrrr51Un7ihvT9uHDh3J1Ox57KGn3Lr8haf/EG16bq7tpcHHSPj1Wz217dlc6Ifnc9seS9tGWzzpTTydkP/6nH33E3Yd1gYaHh33z5s0XuhvMkJnNSn4SGXYKGcZHhvGRYXyzlSH5dQbHYHxkGB8ZxsdYGBvHYHxkGB8ZxkeG8fH3TGwXkh/LnAIAAAAAAAAAAAAoNKNlTk2mcrkqSWrU81fclUvpvGS1klnKtGW+0i1d9nO8J10edKKyqKVj6RWHleq83DYrZZZOrY+n7fFGrq6nN11GtEtpfxd2z8/VjWaunjw1ki43WipZrm7kZHpF5FglvQry8PH8cqj1K/rSfeR3IWX6Xi91Je1KtTtX1s08LwAAAAAAAAAAADqMGSsAAAAAAAAAAAAAhZhMBAAAAAAAAAAAAFBoRsucykzeXObUKp7bVK6Ope0z6VKhXZZf53O8Z2nSritdhtTHjufqavV0f7Ku3DaV06VCu7rS5UFtXl+u7OCZy9In9TNJs7RwWa5u/uXXpdu60mVYu8aO5ur6+xel/e1Nl0r12qlc3ehYuuypl/JLtPb1pd+5b+HCpD1x5kSurjaWX0YWAAAAAAAAAAAAaDeuTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQaEb3TLRyl3r6V0iSGhPjLRvTXVVr6f0Uj9UW5Mqe3XMofUt3eh/Drp5qrq7cld5b0Cr5bSVL7yfYUPpZ1d5Fubqap/dWHB9N7+NY7p6fq7PMPQ/7q+k9GOu9+Z9n2cqVSbt3ftqn+Z6/32PDlyftedVybtuVK1ck7dFj+5P2kf278/sYGxEAAAAAAAAAAADQSVyZCAAAAAAAAAAAAKAQk4kAAAAAAAAAAAAACs1omdNypaL+pcskSfWJidy2U/MWJu3DXVcm7UN7T+TqJuZ3Je0uq6UdqXTl6hbMt6RtjfySnyOn0mVJT4+cTje45eq6Th9J2n0T6XsOH8nPoc6bly6VWl4xL2kPXLEyVzdwRfq9Rg7uTNrjtXz/ap5+r+5K/rPGMn3vLqdLoFbyXVdJ+d8XAAAAAAAAAAAAaDeuTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIVmtsypSX1ddUnShHlu254D6bKc+w6kS5surJzJ1Q0srift3p709cWLe3J18+any41Wq9XctrpflrRPnBpP2k9u3ZarO7k/049FS5J2z0S+Twvnp8uN9vamn7t88YJcXXdXOvd6cGJ+prP9ubr5venzrsz+JKlSTZdzXXA6rbNKOVfn+Z8XAAAAAAAAAAAAaDuuTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQaEb3TOzurmrN1VdLks6MjeW2uZ5L2qv6LX2P5e8ZOHY8vRmgVdP7Dnb19uX3V0m7Nj7eyG3bvf9U0n7q6eeT9sGjJ3J1Xb0L021nMv3tMuWlffJ6LWlPjBzLVdUa6f0ey9W0f70LF+XqJmrp/SO9ke97dv+VzFxuV6UrV1fq6hYAAAAAAAAAAADQSVyZCAAAAAAAAAAAAKAQk4kAAAAAAAAAAAAACs1omdNqz3xddcMtkiRv1HLb1lyfLiNam0jb42Pjubra2Jmk7enqopoYzy+bun3H7qT90Hcez2174oltSXt0JF3ydFF3OVeXXWH0zERmKVPL9/3o83uSdknpUqYnjwzk6vqXLE3avQOXpRtK+Z9xNPOdG/X8Z3nmS1s5ncvt6a7mP6t/gQAAAAAAAAAAAIBO4spEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQaEaTiWYlVarzJh89C3KPnr4lyaNv4PLkMXD56txj+ZXXJY9lq65JHvtPl3KPrz30ePLYumVr7tEYPZo8qj6WPGqNWu7RmDeQPLqWDCaPbF97+pZoXv+K5DEx4cnj0P4DucfouCePRnlB8jgzodyjVvfkMTFRyz3GG548StWu5NEzvzv3OLB/tz72yT/VRz/+CUlace5M7GfMzM1s+EL/Y8Ds2rhxo9auXStJN5vZ+85VR4ZzFxnGR4axTeU3NDQkMRaGRIbxcR6NjwzjI8PYGAvj4xiMjwzjI8PYGAvjI0NIXJk4JzUaDX3+r76qO975c/pP771DkgbM7MbWOjPrk/ReSd9udx8xvXq9rjvvvFP333+/JG2RdDsZxkKG8ZFhbNn8tm7dKjEWhkOG8XEejY8M4yPD2BgL4+MYjI8M4yPD2BgL4yNDTGEycQ7atXuPlg4s1tKBxapUypJ0RNJtBaUfkvQHks60s394YZs2bdLQ0JDWrFkjSS7pXpFhKGQYHxnGls2vWq1KjIXhkGF8nEfjI8P4yDA2xsL4OAbjI8P4yDA2xsL4yBBTmEycg44dP65F/QuzL41LGsy+YGa3SFrl7l9qZ99wfvbu3atVq1ZlX9ojMgyFDOMjw9gK8mMsDIYM4+M8Gh8ZxkeGsTEWxscxGB8ZxkeGsTEWxkeGmFJ5sW+0s17wtO3nrmtkXnn40a1J+4uf+WyubteT301318jvo2vZ1Wl7yZqkXe7qztX1zpuX9qOcdqpazs+hdvekz0cP7U43jI/k6uoTE2m7nr7esHJ+f12Zb+2e29ZTSX/yiXJaZ5m3mEnlktTdVTzXa2YlSX8k6V2FBfnaOyTdIUmrV69+oXK0CRnGR4bxkWFsM8mvWU+GcwzHYHxkGB8ZxkeGsZFffGQYHxnGR4ax8W/7+DgGLx1cmTgH9S/s19HjJ7MvVSXtzTzvk3SzpAfNbKek10jaUHRjU3e/x92H3X142bJlL2GvkTU4OKjdu3dnX1opMgyFDOMjw9gK8nvRY6FEhp0wmxmSX2dwHo2PDOMjw9gYC+PjGIyPDOMjw9j4t318/D2DKUwmzkGrVw3qwKEjOnTkmGq1uiQNSNowtd3dj7v7Une/yt2vkvSwpLe4++bO9Bitbr31Vm3btk07duyQJi/QfbvIMBQyjI8MY8vmNz4+LjEWhkOG8XEejY8M4yPD2BgL4+MYjI8M4yPD2BgL4yNDTJnxMqeeLNuZX77z7HVPJ50aGcs9//qD30raX/rC55L28ed35OoWLFqStKvLb8jvdH46a10rL0jrentyZaXGeNJetmxF0vbakVzd+IlDSbu7N71XYaNl2dRSJd1/pasrfb1lTtYbmXVZW5Y5bdQyS6VOpL+NN9K6kpX0M29ep//xJ59WY3KN1yPuvsXMPihps7tvEOa0SqWiu+++W+vWrZOkmyR9iAxjIcP4yDC2bH71ybXFGQuDIcP4OI/GR4bxkWFsjIXxcQzGR4bxkWFsjIXxkSGmmLdMdk1neHjYv/Od7zSfTTOZmNl0auRMrux8JxPnLbzwycTKi5hMrI2l/W2M5ydC+/rTicalq69N2j3Vrlzdwmo6mbikrzffp8zvdOJE2o+nfvBorq7USO/X+N73f/gRdy+8tHsmhoeHffNm/oeAdjOzWclPIsNOIcP4yDA+MoxvtjIkv87gGIyPDOMjw/gYC2PjGIyPDOMjw/jIMD7+nontQvJjmVMAAAAAAAAAAAAAhWa0zKm7q9ZcprNeG89tGzuTXsV37OjRpP23f/Ngru6Rb309aS9Y0Je0l77sllxdvZ5ewnemnr+68fj+rWmd0qsCy/0DubqFyy5L+zdyIH3PyQO5ukols2RpV/qTlCrlXF2pe37SrtVrSXt8rJGrq5fSOdrxzLKmkmSZfY6cPpn2b3QkV9ddqgkAAAAAAAAAAADoJK5MBAAAAAAAAAAAAFCIyUQAAAAAAAAAAAAAhZhMBAAAAAAAAAAAAFBoRvdMHBkZ0fe++11J0tHMfREl6dDR40l73770noQHn9+bq1sx9ENJu9qb3jPR1HKPwIn0HoK1lnsc9oycSNrHTqR1Z47tzNUdObUyac9fnN4/0cxzdeVqd9IuZW5/WKl25eoss81q6ZPKgp6W/WXu9ziev7fk6Ej6/MyZ0aTd3ZWf1+1puV8jAAAAAAAAAAAA0G5cmQgAAAAAAAAAAACgEJOJAAAAAAAAAAAAAArNaJnTk6dO62vffESSND6WX76z1kiX/SxnluhctPyKXF0ps8RouZ4uV1pWfn+VUvq81FfNbevrSZcs7Tt1Omkf3H8oV3fqxMGkPTqeLinataAvVzfRPS/dlmnP612cq+vq7U3a4xPpsqzVeiNXV7L0Z22MT+S2qZG+r1KuZNr5ed1KuS4AAAAAAAAAAACgk7gyEQAAAAAAAAAAAEAhJhMBAAAAAAAAAAAAFGIyEQAAAAAAAAAAAEChGd0z0d01PnFGklTyM7lt8yy9x2HZ0jnKcsNzdSVL7wVYydw/sXTWtGbmPoSe30el2pW0+xctSto9PfNydUcOH0/aBw6fStpWWpSry97v0Wtp/2r1/OeqlN4L0stp2yzfeW+k+6jX8veC7Mn0fXw83TYy2vJ7LuwSAAAAAAAAAAAA0ElcmQgAAAAAAAAAAACgEJOJAAAAAAAAAAAAAArNaJnTkuqaX5pcLtRbljkt1cfSnXq623JmaVBJklnazMxluufrJmrptpLXc9u6Kpl9ZJZKrVbzS4MuvWxRuv/MUqSHR8dydY15aX9rEyNJe/TUyVxd/5Kl6edmll7tav2Kme/VcMttG6+nS6pmlzkttfxOVw9dLwAAAAAAAAAAAKCTuDIRAAAAAAAAAAAAQCEmEwEAAAAAAAAAAAAUmtEyp2ZSd2Vymc6G57fVx2uZwnTJzobyy3xmt1kp/XhTfplPU7q/suU/rFzKLo+a1lk5PzfaVZmftJcvTfc/djC/zOmJerp/K6d9KlXyy6aaZbZl+qeWZVitPC/dVMrvozaRLg/bqKfv6+9fnKu77mWvFgAAAAAAAAAAANBJXJkIAAAAAAAAAAAAoBCTiQAAAAAAAAAAAAAKMZkIAAAAAAAAAAAAoNCM7pkoSapP3gPRPf9Wz9xPsO7pfRJbbncoKXN/wkYjaVer+Xsm9lQyz/3c90wsVapJu2H5+zN65j6MvV1pe/6J8VzdidG03Sh1J+3xkZFc3ejJo0m7b8VlSTv7fSVpNHP/yFMj+fszluvpZ9fTr6/Blatydf1LlwsAAAAAAAAAAADoJK5MBAAAAAAAAAAAAFCIycQ56oEH/lq33PIqvfzlr5CkFa3bzexXzWyrmX3fzP7GzK5seycxrY0bN2rt2rWSdLOZva91OxnOfWQYHxnGNpXf0NCQxFgYEhnGx3k0PjKMjfNofGQYH+fR+MgwPjKMjbEwPjKENMNlTnfv2XfoPb/+O8++VJ1Bzs2S/l7ShKSXm9mN7r41s/1RScPuPmJm/1rSf5H0tg70EwXq9bruvPNOPfDAA7rmmmu2SLrdzDaQYRxkGB8ZxpbNb+XKleru7h5gLIyFDOPjPBofGcbGeTQ+MoyP82h8ZBgfGcbGWBgfGWLKjCYT3X3ZS9URpMzsRyTd5e7rms/fL+k2SckB6u5fy7zlYUnvaGsnMa1NmzZpaGhIa9askSZvFHqvyDAUMoyPDGNryU+Sjoj8QiHD+DiPxkeGsXEejY8M4+M8Gh8ZxkeGsTEWxkeGmMIyp3PToKTdmed7mq+dy7sl3V+0wczuMLPNZrb54MGDs9hFTGfv3r1atWpV9iUyDIYM4yPD2AryG9eLzE8iw06YzQzJrzM4j8ZHhrExFsbHWBgf59H4yDA+MoyNv2fi4+8ZTGEyMTgze4ekYUl/WLTd3e9x92F3H162jAtL5yIyjI8M4yPD2F4oP4kM5zqOwfjIMD4yjI2xMD6OwfjIMD4yjI8MY+Pvmfg4Bi9uM1rmFG2zV1J2un9l87UcM3uDpA9I+lF3H2tT33AeBgcHtXt39uJSMoyGDOMjw9gK8quK/EIhw/g4j8ZHhrFxHo2PDOPjPBofGcZHhrExFsZHhpjClYlz03ckXWtmV5tZVdLbJW3IFpjZKyV9TNJb3P1AB/qIadx6663atm2bduzYIUkmMgyHDOMjw9iy+Y2Pj0vSgMgvFDKMj/NofGQYG+fR+MgwPs6j8ZFhfGQYG2NhfGSIKUwmzkHuXpP0HklfkfSEpM+6+xYz+6CZvaVZ9oeSFkj6nJl9z8w2nGN36IBKpaK7775b69atk6SbRIbhkGF8ZBhbNr8bbrhBko6QXyxkGB/n0fjIMDbOo/GRYXycR+Mjw/jIMDbGwvjIEFPM3TvdB7TJ8PCwb968udPduOSY2SPuPjwb+yLDziDD+MgwPjKMb7YyJL/O4BiMjwzjI8P4GAtj4xiMjwzjI8P4yDA+/p6J7ULy48pEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTAQAAAAAAAAAAABQiMlEAAAAAAAAAAAAAIWYTJyjzOyNZvaUmW03s/cVbO82s880t3/bzK7qQDcxjY0bN2rt2rWSdDMZxkSG8ZFhbFP5DQ0NSdKK1u3kN/eRYXycR+Mjw/jIMDbGwvg4BuMjw/jIMDbGwvjIEBKTiXOSmZUlfUTST0q6UdLtZnZjS9m7JR119yFJ/1XSH7S3l5hOvV7XnXfeqfvvv1+StogMwyHD+Mgwtmx+W7dulaQB8ouFDOPjPBofGcZHhrExFsbHMRgfGcZHhrExFsZHhpjCZOLc9GpJ2939GXcfl3SvpNtaam6T9GfN9n2SfszMrI19xDQ2bdqkoaEhrVmzRpJcZBgOGcZHhrFl86tWq5J0ROQXChnGx3k0PjKMjwxjYyyMj2MwPjKMjwxjYyyMjwwxhcnEuWlQ0u7M8z3N1wpr3L0m6bikJW3pHV7Q3r17tWrVquxLZBgMGcZHhrEV5Dcu8guFDOPjPBofGcZHhrExFsbHMRgfGcZHhrExFsZHhphS6XQH8NIyszsk3dF8OmZmj3eyPxdgqaRDne7EDCyWtPATn/jEs5LWXsiOyLBjyPBsZDiJDNsjm58k3XQhOyPDjpi1DMmvYziPno0MJ5Fh+5Dh2SJlyFh4tkj5SRyDRchwEhm2DxmeLVKG/Nu+2CWZIfnNCS/6PMpk4ty0V1J2un9l87Wimj1mVpHUL+lw647c/R5J90iSmW129+GXpMcvsWh9N7MfkXSXu68zs80iw3B9J8OzRes7GZ4tUt+z+TWf79GLzE8iw06YzQzJrzM4j54tWt/J8GzR+k6GZ4vUd8bCs0XrO8fg2aL1nQzPFq3vZHi2SH3n3/bFIvWdv2fOFr3vL/a9LHM6N31H0rVmdrWZVSW9XdKGlpoNkt7ZbL9V0t+6u7exj5hekqEkExlGRIbxkWFsrWPhgMgvGjKMj/NofGQYHxnGxlgYH8dgfGQYHxnGxlgYHxlCEpOJc1JzXeH3SPqKpCckfdbdt5jZB83sLc2yT0haYmbbJf2qpPd1prco0pLhTSLDcMgwPjKMrWAsPEJ+sZBhfJxH4yPD+MgwNsbC+DgG4yPD+MgwNsbC+MgQU4wJ4kuHmd3RvJQ4HPo++/tqN/o++/tqN/o++/tqN/o++/tqN/rOb9ApHIOT6Pvs76vd6Pvs76vd6Du/QadwDE6i77O/r3aj77O/r3aj77O/r3aj7/wGnXIhfWcyEQAAAAAAAAAAAEAhljkFAAAAAAAAAAAAUIjJxIuQmb3RzJ4ys+1mdtb6xGbWbWafaW7/tpld1YFuFjqPvr/LzA6a2feaj1/qRD+LmNknzeyAmT1+ju1mZn/c/G7fN7NbptkXGbbZbObXrCfDNiPDSVHzkziPTiHDpJYMO4AMJ0XNkLEwRYbk1ylkOIkMk1oybDPGwhQZJvUhM4yan8R5dAoZJrVk2GazfR5NuDuPi+ghqSzpaUlrJFUlPSbpxpaafyPpo8322yV9ptP9nkHf3yXp7k739Rz9/0eSbpH0+Dm2v0nS/ZJM0mskfZsM585jtvIjQzIkv85nGDU/MiTDufAgw9gZzlZ+ZBg/Q/IjQzIkw0s1w9nKjwzJkPw6n2HU/MiQDDv9mM3zaPbBlYkXn1dL2u7uz7j7uKR7Jd3WUnObpD9rtu+T9GNmZm3s47mcT9/nLHf/hqQj05TcJunPfdLDkhaZ2eUFdWTYAbOYn0SGHUGGkgLnJ3EebSLDSWTYIWQoKXCGjIUJMiS/jiFDSWQ4hQw7gLEwQYaTomYYNj+J82gTGU4iww6Y5fNogsnEi8+gpN2Z53uarxXWuHtN0nFJS9rSu+mdT98l6Weal9/eZ2ar2tO1WXG+348M56bz/W7nW0uG7XcpZHgx5ydxHs0iQzLsFDJMRczwUhgLJTI83zry6wwyTJEhGXYCY2EeGc7NDC/m/CTOo1lkSIadMJPzaILJRETzV5KucvcfkvSA0v9rAXGQYXxkGBv5xUeG8ZFhfGQYHxnGRn7xkWF8ZBgfGcZGfvGRYXyXVIZMJl589krKzoCvbL5WWGNmFUn9kg63pXfTe8G+u/thdx9rPv24pFe1qW+z4XyyOd86Mmy/883vfGvJsP0uhQwv5vwkzqOSyLCohgzbigwVOsNLYSyUyPB868ivM8hQZFhUQ4Ztw1jYRIZn18yhDC/m/CTOo5LIsKiGDNtmJufRBJOJF5/vSLrWzK42s6omb1q6oaVmg6R3NttvlfS37pN33uywF+x7y9q9b5H0RBv7d6E2SPoFm/QaScfdfV9BHRnOTeebn0SGc9WlkOHFnJ/EeVQSGbbsiwzbjwwVOsNLYSyUyFAiv7mMDEWGLfsiw/ZiLGwiw9z+5lqGF3N+EudRSWTYsi8ybK+ZnEdT7s7jIntIepOkv5f0tKQPNF/7oKS3NNs9kj4nabukTZLWdLrPM+j770naIukxSV+TdH2n+5zp+6cl7ZM0ocl1ht8t6Vck/Upzu0n6SPO7/UDSMBnOnQxnMz8yJEPy63yGUfMjQzLs9IMMY2c4m/mRYfwMyY8MyZAML8UMZzM/MiRD8ut8hlHzI0MyvFjyyz6s+WYAAAAAAAAAAAAAyGGZUwAAAAAAAAAAAACFmEwEAAAAAAAAAAAAUIjJRAAAAAAAAAAAAACFmEwEAAAAAAAAAAAAUIjJRAAAAAAAAAAAAACFmEwEAAAAAAAAAAAAUIjJRAAAAAAAAAAAAACFmEwEAAAAAAAAAAAAUOj/A9GPoQyCqBOAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABxMAAADGCAYAAAAZmK7TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAyw0lEQVR4nO3de5BcZ3nn8d/T3dNz10gjyRdGkm15bBnbkAAjLklIXBs2AifgWpKwOEUBCcTJrsluJdlsQWArLCSbsKlsdimTTbyBiLAJhpBkERcLCJewQECWN4FYsrFky9bFsu63ufbt3T+655zzto5kjTXunkf6fqhT9U6/T59+e34+54zq5bzHQggCAAAAAAAAAAAAgHaFbg8AAAAAAAAAAAAAwNLEZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmLlFm9hEzO2xmD52j38zsg2a228y+Z2Yv7vQYcW7k5x8Z+keG/pGhf2ToG/n5R4b+kaF/ZOgb+flHhv6RoX9k6B8ZQmIycSnbLOnV5+l/jaQbWttdkv5nB8aEC7dZ5OfdZpGhd5tFht5tFhl6t1lk6NlmkZ93m0WG3m0WGXq3WWTo2WaRn3ebRYbebRYZerdZZOjdZpHhZY/JxCUqhPB1ScfPU3KHpD8PTd+WtNzMru7M6PBMyM8/MvSPDP0jQ//I0Dfy848M/SND/8jQN/Lzjwz9I0P/yNA/MoTEZKJnY5L2ZX7e33oNPpCff2ToHxn6R4b+kaFv5OcfGfpHhv6RoW/k5x8Z+keG/pGhf2R4GSh1ewB4bpnZXWreWqzBwcGX3HTTTV0e0eXj1ltv1e7du2VmR0IIq5/tfsiwe8jQPzL0jwz9W4wMya97OAb9I0P/yNA/roW+cQz6R4b+kaF/ZOgff89cGh588MGjzzY/JhP9OiBpbebnNa3XIiGEeyXdK0kTExNh+/btnRkd9MQTT+infuqntGPHjidzui8oP4kMu4kM/SND/8jQv8XIkPy6h2PQPzL0jwz941roG8egf2ToHxn6R4b+8ffMpcHM8vK7ICxz6tcWSW+2ppdLOhVCONjtQeGCkZ9/ZOgfGfpHhv6RoW/k5x8Z+keG/pGhb+TnHxn6R4b+kaF/ZHgZ4M7EJcrMPi7pNkmrzGy/pN+S1CNJIYQ/lvR5SbdL2i1pWtLPd2ekyHPnnXfqa1/7mo4ePSpJLzSzt4n8XCFD/8jQPzL0jwx9Iz//yNA/MvSPDH0jP//I0D8y9I8M/SNDSJKFELo9BnQItw53h5k9GEKYWIx9kWF3kKF/ZOgfGfq3WBmSX3dwDPpHhv6RoX9cC33jGPSPDP0jQ//I0D/+nvHtYvJjmVMAAAAAAAAAAAAAuZhMBAAAAAAAAAAAAJCLyUQAAAAAAAAAAAAAuZhMBAAAAAAAAAAAAJCLyUQAAAAAAAAAAAAAuZhMBAAAAAAAAAAAAJCLyUQAAAAAAAAAAAAAuZhMBAAAAAAAAAAAAJCLyUQAAAAAAAAAAAAAuZhMBAAAAAAAAAAAAJCLyUQAAAAAAAAAAAAAuZhMBAAAAAAAAAAAAJCLyUQAAAAAAAAAAAAAuZhMBAAAAAAAAAAAAJBryUwmmtnXzOztzt57m5ntfzbvBQAAAAAAAAAAAJa6RZ9MNLMnzOxVi71fAAAAAAAAAAAAAJ21ZO5MBAAAAAAAAAAAALC0dGwy0cxWmNlnzeyImZ1otde0lV1vZtvM7LSZfdrMRjPvf7mZfcvMTprZd83stvN81i+Y2cOtz/mCmV2T6fuXZvaImZ0ys3sk2QK+Q7+ZbW7td6ekjW39z28tm3rSzHaY2esyfSvN7DOt7/aAmf22mX3jQj8bAAAAAAAAAAAA6LRO3plYkPRnkq6RtE7SjKR72mreLOkXJF0tqSbpg5JkZmOSPifptyWNSvoPkv7azFa3f4iZ3SHpNyW9XtJqSf9X0sdbfask/Y2k90haJekxST+cee+61kTgunN8h9+SdH1r2yTpLZn39kj6jKQvSrpC0q9I+gsz29Aq+ZCkKUlXtd73FgEAAAAAAAAAAABLWMcmE0MIx0IIfx1CmA4hnJH0O5J+rK3sYyGEh0IIU5L+k6Q3mFlR0pskfT6E8PkQQiOE8CVJ2yXdnvNRvyzpd0MID4cQapL+i6QfbN2deLukHSGET4UQqpL+u6SnM2PcG0JYHkLYe46v8QZJvxNCOB5C2KfWZGfLyyUNSfq9EEIlhPAVSZ+VdGfrO/y0pN9qff+dkj56gb86AAAAAAAAAAAAoCs6uczpgJn9iZk9aWanJX1d0vLWRNu8fZn2k5J61LyD8BpJP9u6a/CkmZ2U9CNq3sHY7hpJ/yNTd1zNpUzHJD0v+xkhhND2mc8ken9rjFFfCKHR1j+m5h2Spbb3LuRzAQAAAAAAAAAAgI7r5DKnvy5pg6SXhRCWSfrR1uvZZxauzbTXSapKOqrmxNvHWncNzm+DIYTfy/mcfZJ+qa22P4TwLUkHs59hZtb2mc8ken9rjPOekrTWzApt/QckHVFz2dbsMyIX8rkAAAAAAAAAAABAxz1Xk4k9ZtaX2UqShtV8TuJJMxtV8/mD7d5kZjeb2YCk90n6VAihLul/S3qtmW0ys2Jrn7eZ2ZqcffyxpHeZ2S2SZGYjZvazrb7PSbrFzF7fGtO/U/MZhhfqk619r2h99q9k+r4jaVrSfzSzHjO7TdJrJd3X+g5/I+m9rTs0b1Lz+ZAAAAAAAAAAAADAkvVcTSZ+Xs2Jw/ntvWo+n7BfzTsNvy1pa877PiZps5rPMexTc7JPrecT3iHpN9W8y2+fpN/IG38I4W8lfUDSfa3lVB+S9JpW31FJPyvp9yQdk3SDpG/Ov9fM1pnZpJmta99vy39Wc+nSPZK+2Brv/OdW1Jw8fE3rO/6RpDeHEB5plbxD0kjru31M0sclzZ3jcwAAAAAAAAAAAICuW/TJxBDCtSEEa9veE0J4KoRwWwhhKIRwYwjhT1p9tdb7bgshvCuE8NIQwrIQwmtbk3/z+/1OCOHHQgijIYTVIYSfDCHszbz3TzO1HwshvKC1n7UhhF/I9G1tff5ICOEdrX3+aatvb2t8e8/x3aZDCG9uLZ16cwjh90MIazL9O1r7G2n1/22m70hrzMtCCBtbL+8/1+/RzF5tZt83s91m9s6c/nVm9lUz+0cz+56Z3X4B8aCDtm7dqg0bNkjSrWToExn6R4a+zec3Pj4u5awkQH5LHxn6x3nUPzL0jwx941roH8egf2ToHxn6xrXQPzKE1NlnJl7WzOwmM3uhNb1U0tsk/e05aouSPqTmXY43S7rTzG5uK3uPpE+GEF4k6Y1q3gmJJaJer+vuu+/W/fffL0k7RIbukKF/ZOhbNr+dO3dK0ij5+UKG/nEe9Y8M/SND37gW+scx6B8Z+keGvnEt9I8MMY/JxM4ZVvO5iVOSPiHpDyR9+hy1L5W0O4TweGv51PvUXOY1K0ha1mqPSHpq0UeMZ23btm0aHx/X+vXrpWZWZOgMGfpHhr5l8yuXy5J0XOTnChn6x3nUPzL0jwx941roH8egf2ToHxn6xrXQPzLEvFK3B3C5CCE8IGn8AsvH1Hwu5Lz9kl7WVvNeSV80s1+RNCjpVXk7MrO7JN0lSevWnetRkFhsBw4c0Nq1a7MvkaEzZOgfGfqWk19Fzetj1nt1AflJZNgNi5kh+XUH51H/yNA/MvSNa6F/HIP+kaF/ZOgb/7b3j79nMI87E/26U9Lm1jMbb5f0MTM7K88Qwr0hhIkQwsTq1as7PkicFxn6R4b+kaFvF5SfRIZLGMegf2ToHxn6R4a+kZ9/ZOgfGfpHhr7xb3v/OAYvAwu6M3FwaCCMrlwuSWo0GnGnWV5TzTtcU7V6+r5qrZbpOff+zieEdP+Nev08Yypk2nbOukYjs4/QXpbZRyHTbhtTPbOPsz8qfV/I/A6zYxpeMaSZqVmtfN7o21ovTUn6nbaPeZukV0tSCOEfzKxP0ipJh4WuGxsb07592ZtLtUbSgbYyMlzCyNA/MvQtJ7+yyM8VMvSP86h/ZOgfGfrGtdA/jkH/yNA/MvSNa6F/ZIh5C5pMHF25XL/67l+UJM1Mz0R92UmyUk9mcq5tkvD4qcmk/fSxI0m7XqhEdYVSZuK6fUYuM4FYqaTvm548E++jkH69Uu9A0u4p98T7KxaT5tTUdPoxtXjsPeXBzP760tcLcd307Om0rif+FRcL6WdXZ9Oxl0vpGBqNhu7/yN/pla9/hfqX9eu+3/3rgqQt8aC1V9KPS9psZs+X1CfpiLAkbNy4Ubt27dKePXuk5nzzGyX9XFsZGS5hZOgfGfqWzW9sbEySRsW10BUy9I/zqH9k6B8Z+sa10D+OQf/I0D8y9I1roX9kiHk8M3EJKhQKmnj1i/WVj39doREk6XgIYYeZvU/S9hDCFkm/Lul/mdmvqnkP5VtD9jZNdFWpVNI999yjTZs2SdItkt5Phr6QoX9k6Fs2v3pz5QGuhc6QoX+cR/0jQ//I0Deuhf5xDPpHhv6RoW9cC/0jQ8yzhWQ6ds3zwt3vfrskaXJyMuoLIbt86VzSfvrwoahucmo2aff0pnfp9Q3G85q1kC6BWmtbvrSQuYtvZGgoac/MTEd1k2fSz1LmLsXBkXJUN1dN77K0em/SvmbVTVHdjdffnLRL5fTOydpc/LmHM3dc7j+4L+47ejBpnzp+Ih1Tf3y35OBoeiflxz/wmQdDCBO6SBMTE2H79u0XuxsskJktSn4SGXYLGfpHhv6RoX+LlSH5dQfHoH9k6B8Z+se10DeOQf/I0D8y9I8M/ePvGd8uJr/cB5kCAAAAAAAAAAAAAJOJAAAAAAAAAAAAAHIxmQgAAAAAAAAAAAAgV+mZS1K1ek1HTjWf8zczPX1W3zzL7LXa9gn9y9JnHFar1aQ9NT0b1dWr6TMYlw2PRH0rRlYk7StWr07afT3xh50+kz4L8diJU0l7unoiqpuspH1rr1qXtP/Fy14V1d1y3QuS9mB/Zh62ET938uipdH8P7IjX/f3KP3w5/dyp9NmSKsX7mJutCAAAAAAAAAAAAOgm7kwEAAAAAAAAAAAAkIvJRAAAAAAAAAAAAAC5FrTMqcxkraVEi709UVfILEtaL6RLdpb7B+O6zOqd9Vr6nv6+ZVHdddfdkLRfsfGHor41V40l7eHM/kvFeG50ZjZdOnVyNl3ydP/T+6K6R/bsSNqNevr62MqrororV4xmPsuSdgiNqK5U7E3aL9zwg1Hfk/sPJO0jR44l7crMTFRXmakLAAAAAAAAAAAA6CbuTAQAAAAAAAAAAACQi8lEAAAAAAAAAAAAALkWtMxpQw1N1aabb2x7Z3aJ0VBNlwC97urxqG7N6JqkPTKULlG6fPlwVDd21bqkvfbqNVFff6kvaReteM7xhpAuFRrSIWnDtRuiupfc/JKkffLkyaR95fJ4mdNSIbOUa72WdhTjMfSU01/OlaMro74bM8u3PrJrZ9KemmtbNlZBAAAAAAAAAAAAQDdxZyIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXAt6ZmKoN1SdnGy2zaK+yuxM0i6Xlifta1atj+pefutLk/YVIyuSthVrUV2pPx1aKcSf1ainz0JUpqtQiOuyCpnCvkL8fMKrRlYl7dXDmWccWvuvJ/O5oZqWtdeF9HmHhbZHH64aWZ60B/r6k/bJqRNxYencz4IEAAAAAAAAAAAAOoE7EwEAAAAAAAAAAADkYjIRAAAAAAAAAAAAQK4FLXNqhaLKA8skSY25StQ3W5lO2gU10o7Qtnyppet+1ufSpVHN4rqCetNd9MZzno1SumRpyHQ1FC9zGn25zHKjjWr8WfVG+nM9u4tSX7y/zNKjhZ60r33J11IhrRvq74/6br3x+Un7tZtuT9pf/saXo7pHn3xcAAAAAAAAAAAAQDdxZyIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMi1oGVOZUGNQnO90GqlGnVVJtNlT6dnDyXtQ1fuj+qmrj2ZtHv660m7v783qqvPpEuPTk2fisdRTJcVLfb0pPsrxsuNVhrpcquWWXm1VqtHdaGRroFaKpfT95QbUV2j1JNbVyzE+7PMFG2pEP+KhwuDSXvjLRNJe/+++Pf0/UcfEQAAAAAAAAAAANBN3JkIAAAAAAAAAAAAIBeTiQAAAAAAAAAAAAByMZkIAAAAAAAAAAAAINeCJhPr1ZpOHD6sE4cP68zp09FWq9eTzRSSbXhoINrqtUayzU3PJtv0qZPRdvipJ5Nt/57H423Xo8l2bM/3k+3Ek/F26PFHku3o/j3JdurwwWg7fijdTh45lGxzZ05G29SpE8k2mdlmZqajrREayRba/letVpKtXOhJthe98EXR1lcc0Lc//w/61ue+JUlX5eVhZm8ws51mtsPM/nIx/oPA4tm6das2bNggSbea2TvzashwaSND/8jQt/n8xsfHJa6FLpGhf5xH/SND3ziP+keG/nEe9Y8M/SND37gW+keGkKRStweAs9XrdX3l03+n17/tDRoeGdYH3/MHo2Z2cwhh53yNmd0g6V2SfjiEcMLMrujeiNGuXq/r7rvv1pe+9CVdf/31OyTdaWZbyNAPMvSPDH3L5rdmzRr19vZyLXSGDP3jPOofGfrGedQ/MvSP86h/ZOgfGfrGtdA/MsQ8ljldgnY+tEPLVy7X8pXLVSwVJem4pDvayn5R0odCCCckKYRwuMPDxHls27ZN4+PjWr9+vSQFSfeJDF0hQ//I0LdsfuVyWeJa6A4Z+sd51D8y9I3zqH9k6B/nUf/I0D8y9I1roX9kiHkLujOxUCxq2ciIJGn65GTUV5+tJu3hwaGkPTDQF9VV65WkPVebStrHD+yO6vbtfSxpn5q0qG/laDqxfdWK3qRd0mxUd3oy3X9D5aR9xdXXRHW9/cvS71EZSNpFC1Gd9aWfFeppu7evrHNpNBrRz4VC5rvMzSXNVStXJe3Z6VldvW6trhvfMP9SRdJY265vlCQz+6akoqT3hhC2nnMg6KgDBw5o7dq12Zf2S3pZWxkZLmFk6B8Z+paTH9dCZ8jQP86j/pGhb5xH/SND/ziP+keG/pGhb1wL/SNDzGOZU79Kkm6QdJukNZK+bmYvCCGczBaZ2V2S7pKkdevWdXiIeAZk6B8Z+keGvl1QfhIZLmEcg/6RoX9k6BvXQv84Bv0jQ//I0D8y9I2/Z/zjGLwMsMzpEnTl1Vfp5NHj2ZfKkg60le2XtCWEUA0h7JH0qJoHbCSEcG8IYSKEMLF69ernbMyIjY2Nad++fdmX1ogMXSFD/8jQt5z8nvW1UCLDbljMDMmvOziP+keGvnEt9I9roX+cR/0jQ//I0Df+nvGPv2cwj8nEJeiFP/BCHTl4SMcOHVGtWpOkUUlb2sr+j5oz/TKzVWreSvx4B4eJ89i4caN27dqlPXv2SJJJeqPI0BUy9I8MfcvmV6lUJK6F7pChf5xH/SND3ziP+keG/nEe9Y8M/SND37gW+keGmLfAZU6DavXmsxH7S/GzEBuN9Pl/1cmZpP3Y7kejuvpsLWlfvSx91uDcoYeiuqf3pe+bbYxGfVeuSmet+8rpV6ieORnV7f3+95L27icOJe3xDeNR3Y033Zq0V6xan35uMSpTqdGftMtDy9PXi/GvsVZNnx9Zq9WivlCvZ/rS50c2aunrBZn+1c//nP7o/X84/8zF4yGEHWb2PknbQwhbJH1B0k+Y2U5JdUm/EUI4JiwJpVJJ99xzjzZt2iRJt0h6Pxn6Qob+kaFv2fzqzWsn10JnyNA/zqP+kaFvnEf9I0P/OI/6R4b+kaFvXAv9I0PMsxDCBRevGlsZfvKXXiNJKs5Y1Dd19FTSLjTS16+7Jp64u3ZNOll3vsnE/U+cezLxpg0vSdprVg8l7eqZ/VHdjocubjJxYCS+1bY0kD+ZuPyK50V1jczv9HyTidVKOgF7sjoX1X3223+ftH/tjW9/MIQwoYs0MTERtm/ffrG7wQKZ2aLkJ5Fht5Chf2ToHxn6t1gZkl93cAz6R4b+kaF/XAt94xj0jwz9I0P/yNA//p7x7WLyY5lTAAAAAAAAAAAAALkWtMxpvVrTmUPNu1N76/Eyp9Uz6Z11c9V0+c5d9Ueiuj37nkzaP/j8m5P2i9deF9WtGB1L2uXBq6K+4eErknZfZjrUeoeiuquvnk7aR49OpmM99VhUd2xf5o7bSvqeWvXGqK53+YqkHTLLvM7NxncVFi29a7PedsdhdW42/ah6etfibGUmqjvyVHyXJQAAAAAAAAAAANBp3JkIAAAAAAAAAAAAIBeTiQAAAAAAAAAAAAByLWiZ09CQGq1VOoPFfYVi+sLwYLrcaKNQj+pOTB5P2qfnqkl7aPlYVNcbQvqe6UbU98BDj6Y/ZHZ/49rnRXXX/8Ark/aa9RuSdu30rqhuauqJpF2dSZcXPXWsHNUNFtK518HMeGtz8VKmtZCOtzIXL186N5Muo9rIfMdqLd7HmcnTAgAAAAAAAAAAALqJOxMBAAAAAAAAAAAA5GIyEQAAAAAAAAAAAEAuJhMBAAAAAAAAAAAA5FrQMxMLVlRfT/N5iNWZ2aiv3NebtAeWDaSvDxSjuuHR9CGHy1eOJu1KPUR1J04cSNpPHjwR9W394reS9ulTU0n7jtf8aFT3Qy99QdLu70s/a6hwc1RX7El/DcdOp59rxUpUNziwLH1PMf1etbn4d1Gvp8+CnJmejPqqs+kzFAul9JmMs9X4mYnTM/GzFgEAAAAAAAAAAIBO485EAAAAAAAAAAAAALmYTAQAAAAAAAAAAACQa0HLnNbrNZ0+0VxytNzTF/WdySz1OTWXLg/asHgJ0IFlQ0l71eorknZ5cFlUt//A95L28ZN7o76R4XT/Vwz3J+3RwUZUN3ni6aQ9az1J2+pnorq5mfTnRjEd39DIaFRX7BtO2jOVdFnSYr0e1YV6Ld33bLzMaW0u7StaulTq6dn492TFsgAAAAAAAAAAAIBu4s5EAAAAAAAAAAAAALmYTAQAAAAAAAAAAACQa0HLnBaLRQ2vaC5HOjk1E/f1p8ty1qvVpF2rWFQ3O5kuCVqbSetU6o/qhpddnbRXjZ6IP+um5Um7bOn+6rXHorpjhw6ldT3p8qWFQjymvv7BpN0/lH5u38C6+HNL6dKuhcyvrjJXjeqmpo4l7cnJg/E+lP6eeu3KpH3m9FRUV5mK9wkAAAAAAAAAAAB0GncmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMi1oGcmWrGgnuEBSdLAUDnqG1q+LGnPZp+nWKlHdZVMX70yl74+PRsPrCd9huLIitXxQBppbVG1pN3fE8+NNkLaroZG0u7rGYjqir3p2Aul4aQdrC+qq2W+imWe1TgzFz/v8ODB3Ul7anJv1DeyIn0m41ShmLQf/v7DUd30qSMCAAAAAAAAAAAAuok7EwEAAAAAAAAAAADkYjIRAAAAAAAAAAAAQK4FLXMqSVZozj8OD8RLhZpVk3ZPb/p6JbvWqKRKqCTt05Mnk3YjxMuhNoKlbcVLqvYPrkrbpfQr9JXiudFQT5dRze6/3D8c1TWsJ2nXMuOt1CtRXaGeLq9qpfRL7jt6KKp7Yu/jSbu3N14CtXd0LGmfOH06ae89djCqKw33CAAAAAAAAAAAAOgm7kwEAAAAAAAAAAAAkIvJRAAAAAAAAAAAAAC5FjaZGKRQqStU6jr99LFoO7bnqWSbfPpYshXq9Wirq5Fsj+59PNmOTZ6MttlaLdmmZyrRVij2J1vfwPJkK5aGo81KA8mmYk+y1Roh2qpByVYJc8k2WzkTb3On0q0ylWyP7NkTbfuOnky2U3OlaPvnRw8k2ze/9UCyHX7q6Wjb/9hefe6jW/TZP/u0JF11rkjM7KfNLJjZxEX+t4BFtnXrVm3YsEGSbjWzd56rjgyXLjL0jwx9m89vfHxc4lroEhn6x3nUPzL0jwx941roH8egf2ToHxn6xrXQPzKExJ2JS1JoBP3T1/9RP/yTP6KfuHOTJI2a2c3tdWY2LOnfS/pOp8eI86vX67r77rt1//33S9IOSXeSoS9k6B8Z+pbNb+fOnRLXQnfI0D/Oo/6RoX9k6BvXQv84Bv0jQ//I0Deuhf6RIeYxmbgEnTh2QoMjQxoaGVKhWJCk45LuyCl9v6QPSJrt5PjwzLZt26bx8XGtX79ekoKk+0SGrpChf2ToWza/crkscS10hwz94zzqHxn6R4a+cS30j2PQPzL0jwx941roHxliHpOJS9Ds9KwGhvqzL1UkjWVfMLMXS1obQvhcJ8eGC3PgwAGtXbs2+9J+kaErZOgfGfqWkx/XQmfI0D/Oo/6RoX9k6BvXQv84Bv0jQ//I0Deuhf6RIeaVFlJcb9Q1eWZKkjQ9ORX1DfX2Ju1QC0l7JkxHddXM+548tjtpP7zy6qjuhrXp0rtzczNRX0+xnP5QTOdDrRiPt1BIv16tkhlDvRbVhUa6/0bB0v3NWVRXLKb7m5lN9/HP330oqnvq6T1Je3h4IOqbm0vfV6nU07H2pJ9VmZ5WdbaiySMnlMfMCpL+m6S35hbEtXdJukuS1q1b90zl6BAy9I8M/SND3xaSX6ueDJcYjkH/yNA/MvSPDH0jP//I0D8y9I8MfePf9v5xDF4+uDNxCert69XsdHQ3cFnSgczPw5JulfQ1M3tC0sslbcl7sGkI4d4QwkQIYWL16tXP4aiRNTY2pn379mVfWiMydIUM/SND33Lye9bXQokMu2ExMyS/7uA86h8Z+keGvnEt9I9j0D8y9I8MfePf9v7x9wzmMZm4BC1bPqzpyWlNT82o0WhI0qikLfP9IYRTIYRVIYRrQwjXSvq2pNeFELZ3Z8Rot3HjRu3atUt79uyRJJP0RpGhK2ToHxn6ls2vUqlIXAvdIUP/OI/6R4b+kaFvXAv94xj0jwz9I0PfuBb6R4aYt6BlThWkRr25NOfqK66Iuo4fPpK0C709SbsxMxfVTT11NGkXM6uNnjx2NKobvPG6pD032BP1VeZOJe2ZzAqo/UPDUd3pqcmkXZ1Nl1vtLcRzqCFdlVWDhUY6vniVU9Uay5L2nr0Hk/buXY9EdUeOHU7afX3lqG/VypXp/pV+QKGQ1hUk3XDzem3/+/83P7bjIYQdZvY+SdtDCMnBiqWpVCrpnnvu0aZNmyTpFknvJ0NfyNA/MvQtm1+9+bcH10JnyNA/zqP+kaF/ZOgb10L/OAb9I0P/yNA3roX+kSHmWcjOpD2D5auWhx997SslScPD8cTdOScTQzWqO7UvvQM2O5n40le8Mqq77RUb030f2xP1VeZOJ+2RoZGkfdZk4unjSfuCJxOH0gnD3vJQVNc/lD7X8dG96eTnR/7iL6O6ZzOZWOqN62qWDuorn/3GgyGE3Fu7F2JiYiJs387/IaDTzGxR8pPIsFvI0D8y9I8M/VusDMmvOzgG/SND/8jQP66FvnEM+keG/pGhf2ToH3/P+HYx+bHMKQAAAAAAAAAAAIBcC1rmtNzTo3VXj0mS+oYH23rTu+wqs+nSpmcmT0dVA8PpnX+F1pKpktTXEw9leGAgadcrvVHficps0j5+PF3n9Ni+Q1Hdk3vTuyBXDKb7f97q/nhMfcWkXe4JmXa8vOrJM+mdjl//5teT9uTpU1FdObNkaantV2yNdBnVueYaw5KkWtu07tDoiAAAAAAAAAAAAIBu4s5EAAAAAAAAAAAAALmYTAQAAAAAAAAAAACQi8lEAAAAAAAAAAAAALkW9MzEer2u0ydOSpKOnzoZ9VkhnZecOpU+Q7BUsKhu2TXPy/Slr1+97sqorlqbTtqFUIv6BvrSZxIePXYiaX/ngT1R3aOP7U/at24YSz/rqvGoLjN09ZTS5ySW+waiuqPHJ9P2ySPp+0vxd+wplTPttl9xI9POfHCwYlQWmOcFAAAAAAAAAABAlzFjBQAAAAAAAAAAACAXk4kAAAAAAAAAAAAAci1omdNGvaGpyeZSn4VyT9RX7utN2qFRT9qnJyejup7lad3gyLKkPV2bieoqjbm0rn9Z1Dc7lfYdPLwvaR946nhUV8+sjlrO7MNK8fKlA4N9mfZo0i6WR6K6ydmp9IfMqqTlwd6oTrXssqfxEqjVatqenssMcPZ0VNcT4vcBAAAAAAAAAAAAncadiQAAAAAAAAAAAAByMZkIAAAAAAAAAAAAIBeTiQAAAAAAAAAAAAByLeiZiWamUrksSaplH0goaer0maTdqDWSdqiFqC7Mps9TPDpzJGk/8PSxqG5Fb/qMw9XLh6O+Jw+lz1fctedw0p7OPEtRknp70mchnj6Vjm92Jh577xVDSbtu5aRdrcZjP3j0ZNKemk33USrFv8ZCKX2gYr0edamemb8NlrYtNKK6UolnJgIAAAAAAAAAAKC7uDMRAAAAAAAAAAAAQC4mEwEAAAAAAAAAAADkWtAyp4ViQQPDg5KkMydPRX0hs7RpX2+6vGh1rhLVnT58ImnXLV0D9ETbeqBf/ca3k/ZAX3/Ud+x4uiTq8cnZtG75QFTXm3nfTDUdX8OGorr+oauTdk/vYNKerhejusnZ9LPm6un3alTjZVMHBtLvXx4oR30K6fxtTyntm5udisqqFu8TAAAAAAAAAAAA6DTuTAQAAAAAAAAAAACQi8lEAAAAAAAAAAAAALkWtMxpo9HQ5GRzOc7pqemoL7vMaamcLt/ZaFuts1pJlwodHF2WtAt9PVHd43v3Ju3ent6ob8XK0aR9xbo1SXt2ejKqyy4jevRIujTq9x7eE9WNjFyZ7u+KdInSg4efjuqePnQoaZfK6a+u3ojKFCxtFy2ery2X0/1XGjPpe0IcxczcnAAAAAAAAAAAAIBu4s5EAAAAAAAAAAAAALmYTAQAAAAAAAAAAACQi8lEAAAAAAAAAAAAALkW9MzESqWqg089JUnqLcTPOBweHkraZ6bTZwHKLKobXbkqrauldSNDy6O6Vc9Ln3c4dyp+FmKtVk3aszNpe+b0maiur5w+a7ERQtJ+6ulDUd3Wr/x90i6Wikn71IkTUd1cSD9rcEX6vMfSYDGqa1TThyhOnpyK+mZC+qxJK6Z1vX3lqK6v7TmRAAAAAAAAAAAAQKdxZyIAAAAAAAAAAACAXEwmLlFHD5/Qt778oL75d9sl6ar2fjP7NTPbaWbfM7Mvm9k1nR8lzmfr1q3asGGDJN1qZu9s7yfDpY8M/SND3+bzGx8fl7gWukSG/nEe9Y8MfeM86h8Z+sd51D8y9I8MfeNa6B8ZQlrgMqdTp6eOfvP+7zz5XA0GkVslPSqpKukHzOzmEMLOTP8/SpoIIUyb2b+R9F8l/esujBM56vW67r77bn3pS1/S9ddfv0PSnWa2hQz9IEP/yNC3bH5r1qxRb2/vKNdCX8jQP86j/pGhb5xH/SND/ziP+keG/pGhb1wL/SNDzFvQZGIIYfVzNRCkzOwVkt4bQtjU+vldku6QlBygIYSvZt7ybUlv6uggcV7btm3T+Pi41q9fL0lB0n0iQ1fI0D8y9K0tP0k6LvJzhQz94zzqHxn6xnnUPzL0j/Oof2ToHxn6xrXQPzLEPJY5XZrGJO3L/Ly/9dq5vE3S/XkdZnaXmW03s+1HjhxZxCHifA4cOKC1a9dmXyJDZ8jQPzL0LSe/ip5lfhIZdsNiZkh+3cF51D8y9I1roX9cC/3jPOofGfpHhr7x94x//D2DeUwmOmdmb5I0Ien38/pDCPeGECZCCBOrV3Nj6VJEhv6RoX9k6Nsz5SeR4VLHMegfGfpHhr5xLfSPY9A/MvSPDP0jQ9/4e8Y/jsFL24KWOUXHHJCUne5f03otYmavkvRuST8WQpjr0NhwAcbGxrRvX/bmUjL0hgz9I0PfcvIri/xcIUP/OI/6R4a+cR71jwz94zzqHxn6R4a+cS30jwwxjzsTl6YHJN1gZteZWVnSGyVtyRaY2Ysk/Ymk14UQDndhjDiPjRs3ateuXdqzZ48kmcjQHTL0jwx9y+ZXqVQkaVTk5woZ+sd51D8y9I3zqH9k6B/nUf/I0D8y9I1roX9kiHlMJi5BIYSapHdI+oKkhyV9MoSww8zeZ2ava5X9vqQhSX9lZv9kZlvOsTt0QalU0j333KNNmzZJ0i0iQ3fI0D8y9C2b3/Of/3xJOk5+vpChf5xH/SND3ziP+keG/nEe9Y8M/SND37gW+keGmGchhG6PAR0yMTERtm/f3u1hXHbM7MEQwsRi7IsMu4MM/SND/8jQv8XKkPy6g2PQPzL0jwz941roG8egf2ToHxn6R4b+8feMbxeTH3cmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZOISZWavNrPvm9luM3tnTn+vmX2i1f8dM7u2C8PEeWzdulUbNmyQpFvJ0Ccy9I8MfZvPb3x8XJKuau8nv6WPDP3jPOofGfpHhr5xLfSPY9A/MvSPDH3jWugfGUJiMnFJMrOipA9Jeo2kmyXdaWY3t5W9TdKJEMK4pD+U9IHOjhLnU6/Xdffdd+v++++XpB0iQ3fI0D8y9C2b386dOyVplPx8IUP/OI/6R4b+kaFvXAv94xj0jwz9I0PfuBb6R4aYx2Ti0vRSSbtDCI+HECqS7pN0R1vNHZI+2mp/StKPm5l1cIw4j23btml8fFzr16+XpCAydIcM/SND37L5lctlSTou8nOFDP3jPOofGfpHhr5xLfSPY9A/MvSPDH3jWugfGWIek4lL05ikfZmf97dey60JIdQknZK0siOjwzM6cOCA1q5dm32JDJ0hQ//I0Lec/CoiP1fI0D/Oo/6RoX9k6BvXQv84Bv0jQ//I0Deuhf6RIeaVuj0APLfM7C5Jd7V+nDOzh7o5nouwStLRbg9iAVZIWvbhD3/4SUkbLmZHZNg1ZHg2Mmwiw87I5idJt1zMzsiwKxYtQ/LrGs6jZyPDJjLsHDI8m6cMuRaezVN+EsdgHjJsIsPOIcOzecqQf9vnuywzJL8l4VmfR5lMXJoOSMpO969pvZZXs9/MSpJGJB1r31EI4V5J90qSmW0PIUw8JyN+jnkbu5m9QtJ7QwibzGy7yNDd2MnwbN7GToZn8zT2bH6tn/frWeYnkWE3LGaG5NcdnEfP5m3sZHg2b2Mnw7N5GjvXwrN5GzvH4Nm8jZ0Mz+Zt7GR4Nk9j59/2+TyNnb9nzuZ97M/2vSxzujQ9IOkGM7vOzMqS3ihpS1vNFklvabV/RtJXQgihg2PE+SUZSjKRoUdk6B8Z+tZ+LRwV+XlDhv5xHvWPDP0jQ9+4FvrHMegfGfpHhr5xLfSPDCGJycQlqbWu8DskfUHSw5I+GULYYWbvM7PXtco+LGmlme2W9GuS3tmd0SJPW4a3iAzdIUP/yNC3nGvhcfLzhQz94zzqHxn6R4a+cS30j2PQPzL0jwx941roHxlinjFBfPkws7tatxK7w9gXf1+dxtgXf1+dxtgXf1+dxtgXf1+dxtj5HXQLx2ATY1/8fXUaY1/8fXUaY+d30C0cg02MffH31WmMffH31WmMffH31WmMnd9Bt1zM2JlMBAAAAAAAAAAAAJCLZU4BAAAAAAAAAAAA5GIy8RJkZq82s++b2W4zO2t9YjPrNbNPtPq/Y2bXdmGYuS5g7G81syNm9k+t7e3dGGceM/uImR02s4fO0W9m9sHWd/uemb34PPsiww5bzPxa9WTYYWTY5DU/ifPoPDJMasmwC8iwyWuGXAtTZEh+3UKGTWSY1JJhh3EtTJFhUu8yQ6/5SZxH55FhUkuGHbbY59FECIHtEtokFSU9Jmm9pLKk70q6ua3m30r641b7jZI+0e1xL2Dsb5V0T7fHeo7x/6ikF0t66Bz9t0u6X5JJermk75Dh0tkWKz8yJEPy636GXvMjQzJcChsZ+s5wsfIjQ/8Zkh8ZkiEZXq4ZLlZ+ZEiG5Nf9DL3mR4Zk2O1tMc+j2Y07Ey89L5W0O4TweAihIuk+SXe01dwh6aOt9qck/biZWQfHeC4XMvYlK4TwdUnHz1Nyh6Q/D03flrTczK7OqSPDLljE/CQy7AoylOQ4P4nzaAsZNpFhl5ChJMcZci1MkCH5dQ0ZSiLDeWTYBVwLE2TY5DVDt/lJnEdbyLCJDLtgkc+jCSYTLz1jkvZlft7fei23JoRQk3RK0sqOjO78LmTskvTTrdtvP2VmazsztEVxod+PDJemC/1uF1pLhp13OWR4KecncR7NIkMy7BYyTHnM8HK4FkpkeKF15NcdZJgiQzLsBq6FMTJcmhleyvlJnEezyJAMu2Eh59EEk4nw5jOSrg0hvFDSl5T+vxbgBxn6R4a+kZ9/ZOgfGfpHhv6RoW/k5x8Z+keG/pGhb+TnHxn6d1llyGTipeeApOwM+JrWa7k1ZlaSNCLpWEdGd37POPYQwrEQwlzrxz+V9JIOjW0xXEg2F1pHhp13ofldaC0Zdt7lkOGlnJ/EeVQSGebVkGFHkaFcZ3g5XAslMrzQOvLrDjIUGebVkGHHcC1sIcOza5ZQhpdyfhLnUUlkmFdDhh2zkPNogsnES88Dkm4ws+vMrKzmQ0u3tNVskfSWVvtnJH0lhOaTN7vsGcfetnbv6yQ93MHxXawtkt5sTS+XdCqEcDCnjgyXpgvNTyLDpepyyPBSzk/iPCqJDNv2RYadR4ZyneHlcC2UyFAiv6WMDEWGbfsiw87iWthChtH+llqGl3J+EudRSWTYti8y7KyFnEdTIQS2S2yTdLukRyU9JundrdfeJ+l1rXafpL+StFvSNknruz3mBYz9dyXtkPRdSV+VdFO3x5wZ+8clHZRUVXOd4bdJ+mVJv9zqN0kfan23f5Y0QYZLJ8PFzI8MyZD8up+h1/zIkAy7vZGh7wwXMz8y9J8h+ZEhGZLh5ZjhYuZHhmRIft3P0Gt+ZEiGl0p+2c1abwYAAAAAAAAAAACACMucAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXEwmAgAAAAAAAAAAAMjFZCIAAAAAAAAAAACAXP8f0/T4Qpaqp8oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABxMAAADGCAYAAAAZmK7TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxnElEQVR4nO3dfZBc11nn8d/T09Mzo3eNLVuyRpItjy1bVl4Z5wW2Eu8SUJIFe4sAG1OpJEvAy67DpgqWrQTYwhuWJSy1b5SzCy7CCrK7cUIgIEisYEhCAnmR5SJvkhNbtmxLsh1JlvU6793P/tE9997TuiNr7HF3P9L3U9VVZ/qcvvf0/HzvGdXxPcfcXQAAAAAAAAAAAADQrtLtDgAAAAAAAAAAAADoTUwmAgAAAAAAAAAAACjFZCIAAAAAAAAAAACAUkwmAgAAAAAAAAAAACjFZCIAAAAAAAAAAACAUkwmAgAAAAAAAAAAACjFZGKPMrM/MLMjZvbteerNzH7HzPab2TfN7NWd7iPmR37xkWF8ZBgfGcZHhrGRX3xkGB8ZxkeGsZFffGQYHxnGR4bxkSEkJhN72Q5Jbz5P/VskXdd63SHpf3WgT7hwO0R+0e0QGUa3Q2QY3Q6RYXQ7RIaR7RD5RbdDZBjdDpFhdDtEhpHtEPlFt0NkGN0OkWF0O0SG0e0QGV7ymEzsUe7+RUnHz9PkNkl/5E1flbTKzNZ1pnd4PuQXHxnGR4bxkWF8ZBgb+cVHhvGRYXxkGBv5xUeG8ZFhfGQYHxlCYjIxsvWSDhZ+PtR6DzGQX3xkGB8ZxkeG8ZFhbOQXHxnGR4bxkWFs5BcfGcZHhvGRYXxkeAmodrsDeGmZ2R1qPlqspUuXft8NN9zQ5R5dOrZt26b9+/fLzI66+5oXehwy7B4yjI8M4yPD+BYjQ/LrHq7B+MgwPjKMj7EwNq7B+MgwPjKMjwzj4++Zi8ODDz547IXmx2RiXIclbSj8PNJ6L+Hu90i6R5LGxsZ8z549nekd9Pjjj+tHfuRHtHfv3idKqi8oP4kMu4kM4yPD+MgwvsXIkPy6h2swPjKMjwzjYyyMjWswPjKMjwzjI8P4+Hvm4mBmZfldEJY5jWunpHda0+sknXT3p7vdKVww8ouPDOMjw/jIMD4yjI384iPD+MgwPjKMjfziI8P4yDA+MoyPDC8BPJnYo8zsY5JukXS5mR2S9GuS+iXJ3X9X0mckvVXSfknjkv5Fd3qKMrfffru+8IUv6NixY5L0cjN7j8gvFDKMjwzjI8P4yDA28ouPDOMjw/jIMDbyi48M4yPD+MgwPjKEJJm7d7sP6BAeHe4OM3vQ3ccW41hk2B1kGB8ZxkeG8S1WhuTXHVyD8ZFhfGQYH2NhbFyD8ZFhfGQYHxnGx98zsb2Y/FjmFAAAAAAAAAAAAEApJhMBAAAAAAAAAAAAlGIyEQAAAAAAAAAAAEApJhMBAAAAAAAAAAAAlGIyEQAAAAAAAAAAAEApJhMBAAAAAAAAAAAAlGIyEQAAAAAAAAAAAEApJhMBAAAAAAAAAAAAlGIyEQAAAAAAAAAAAEApJhMBAAAAAAAAAAAAlGIyEQAAAAAAAAAAAEApJhMBAAAAAAAAAAAAlGIyEQAAAAAAAAAAAEApJhMBAAAAAAAAAAAAlOqZyUQz+4KZ/Uywz95iZodeyGcBAAAAAAAAAACAXrfok4lm9riZvWmxjwsAAAAAAAAAAACgs3rmyUQAAAAAAAAAAAAAvaVjk4lmttrM/tLMjprZc63ySFuza81st5mdMrM/N7PhwudfZ2ZfNrMTZvYNM7vlPOf6aTN7qHWez5rZpkLdD5nZd8zspJndLckW8B2GzGxH67j7JN3cVn9ja9nUE2a218xuLdRdZmZ/0fpuD5jZfzSzv7vQcwMAAAAAAAAAAACd1sknEyuS/rekTZI2SpqQdHdbm3dK+mlJ6yTNSvodSTKz9ZI+Lek/ShqW9G8l/YmZrWk/iZndJumXJf2YpDWSviTpY626yyX9qaRflXS5pEcl/UDhsxtbE4Eb5/kOvybp2tZru6R3FT7bL+kvJP2VpCsk/byk/2tmW1pNPizprKS1rc+9SwAAAAAAAAAAAEAP69hkors/6+5/4u7j7n5a0m9IemNbs4+6+7fd/aykfy/pJ82sT9I7JH3G3T/j7g13v1/SHklvLTnVz0n6TXd/yN1nJf0nSa9sPZ34Vkl73f2T7j4j6b9LeqbQxyfdfZW7PznP1/hJSb/h7sfd/aBak50tr5O0TNKH3H3a3T8n6S8l3d76Dm+T9Gut779P0h9e4K8OAAAAAAAAAAAA6IpOLnO6xMx+z8yeMLNTkr4oaVVrom3OwUL5CUn9aj5BuEnST7SeGjxhZick/SM1n2Bst0nS/yi0O67mUqbrJV1VPIe7e9s5n0/y+VYfkzp3b7TVr1fzCclq22cXcl4AAAAAAAAAAACg4zq5zOkvStoi6bXuvkLSG1rvF/cs3FAob5Q0I+mYmhNvH209NTj3WuruHyo5z0FJ/7Kt7ZC7f1nS08VzmJm1nfP5JJ9v9XHOU5I2mFmlrf6wpKNqLtta3CNyIecFAAAAAAAAAAAAOu6lmkzsN7PBwqsqabma+ySeMLNhNfcfbPcOM9tqZkskfVDSJ929Lun/SPpRM9tuZn2tY95iZiMlx/hdSR8ws5skycxWmtlPtOo+LekmM/uxVp/+jZp7GF6oT7SOvbp17p8v1H1N0rikf2dm/WZ2i6QflXRv6zv8qaS7Wk9o3qDm/pAAAAAAAAAAAABAz3qpJhM/o+bE4dzrLjX3JxxS80nDr0raVfK5j0raoeY+hoNqTvaptT/hbZJ+Wc2n/A5K+qWy/rv7pyT9lqR7W8upflvSW1p1xyT9hKQPSXpW0nWS/n7us2a20czOmNnG9uO2/Ac1ly49IOmvWv2dO++0mpOHb2l9x/8p6Z3u/p1Wk/dKWtn6bh+V9DFJU/OcBwAAAAAAAAAAAOi6RZ9MdPer3d3aXr/q7k+5+y3uvszdr3f332vVzbY+d4u7f8DdX+PuK9z9R1uTf3PH/Zq7v9Hdh919jbv/U3d/svDZ3y+0/ai7v6x1nA3u/tOFul2t86909/e2jvn7rbonW/17cp7vNu7u72wtnbrV3X/b3UcK9Xtbx1vZqv9Uoe5oq88r3P3m1tuH5vs9mtmbzey7ZrbfzN5fUr/RzD5vZv9gZt80s7deQDzooF27dmnLli2StI0MYyLD+Mgwtrn8RkdHpZKVBMiv95FhfNxH4yPD+MgwNsbC+LgG4yPD+MgwNsbC+MgQUmf3TLykmdkNZvZya3qNpPdI+tQ8bfskfVjNpxy3SrrdzLa2NftVSZ9w91dJeruaT0KiR9Trdd1555267777JGmvyDAcMoyPDGMr5rdv3z5JGia/WMgwPu6j8ZFhfGQYG2NhfFyD8ZFhfGQYG2NhfGSIOUwmds5yNfdNPCvp45L+i6Q/n6ftayTtd/fHWsun3qvmMq9FLmlFq7xS0lOL3mO8YLt379bo6Kg2b94sNbMiw2DIMD4yjK2YX61Wk6TjIr9QyDA+7qPxkWF8ZBgbY2F8XIPxkWF8ZBgbY2F8ZIg51W534FLh7g9IGr3A5uvV3BdyziFJr21rc5ekvzKzn5e0VNKbyg5kZndIukOSNm6cbytILLbDhw9rw4YNxbfIMBgyjI8MYyvJb1rN8bHoLl1AfhIZdsNiZkh+3cF9ND4yjI8MY2MsjI9rMD4yjI8MY+Pf9vHx9wzm8GRiXLdL2tHas/Gtkj5qZufk6e73uPuYu4+tWbOm453EeZFhfGQYHxnGdkH5SWTYw7gG4yPD+MgwPjKMjfziI8P4yDA+MoyNf9vHxzV4CVjQk4lm5mbW+snnbVeZp9w8RuGH+Q+RVHnyIcmScqFl+/HmO1fb8TTfMdqbvQAXeojiaasm1V0a6LP3tN46K+k32j7yHklvliR3/4qZDUq6XNKRF9NfLI7169fr4MHiw6UakXS4rRkZ9jAyjI8MYyvJrybyC4UM4+M+Gh8ZxkeGsTEWxsc1GB8ZxkeGsTEWxkeGmLPQyUT19/dLktwbSV2lMB02VJhBXNI2m1gtzK5ZYQat4ulM4GRhGm6m0pfUDRSOUfXZwvHSPqkw+V08vLcdzwqVxe9llvbpQh/jtELf2ycTi8fwQmW92D93HZx0XV6rqM+kgxONiqSdbYd6UtIPStphZjdKGpR09AK7iJfYzTffrEceeUQHDhyQmv8ZvF3ST7U1I8MeRobxkWFsxfzWr18vScNiLAyFDOPjPhofGcZHhrExFsbHNRgfGcZHhrExFsZHhpjDMqc9yMw0XKvoyFRDT082JOm4u+81sw+a2a2tZr8o6WfN7BuSPibp3e5+nmc90UnValV33323tm/fLkk3SfoEGcZChvGRYWzF/G688UaJsTAcMoyP+2h8ZBgfGcbGWBgf12B8ZBgfGcbGWBgfGWKOLSTTSqXiA7XWw4xtHys+xVcrlIfaHs3rS8rzn9sLj+1NtDVrVPI50CHlTxL2nfNkYuHpxuLTgm3HK/bDi5VtDW2eRUvbv0UleTIx/Uwl+TH/ZMPTdsVvcnC8/qC7j5WefAHGxsZ8z549L/YwWCAzW5T8JDLsFjKMjwzjI8P4FitD8usOrsH4yDA+MoyPsTA2rsH4yDA+MoyPDOPj75nYXkx+PJkIAAAAAAAAAAAAoBSTiQAAAAAAAAAAAABKMZkIAAAAAAAAAAAAoFR1IY3dXbP1ud382vcCzPf/mylUWds2g9VCu2rhGOfMahb2cqy2bUo4Wc93FJwsvD/YdrLix8zyM1QrbXsrFs5l7RsqljeTz7N/oiT5/FWqF7dkLByj/aznOQQAAAAAAAAAAADQETyZCAAAAAAAAAAAAKAUk4kAAAAAAAAAAAAASi1omVMzy5YLrVTShTiH+vLy4DmLduYahbVCZxvF5UXTdoMDtay8ojaQ1M0WGhePN9Q2NVo8vheWRm3UZ5N2MzNT+Q/nW6N0nsVH27+t+/lqi4djMVMAAAAAAAAAAAD0Lp5MBAAAAAAAAAAAAFCKyUQAAAAAAAAAAAAApRa2zKmk/mpz/nHpksGk7srCOqer+vKlPev1etJupp7XTU7nS4/Wli5J2m3dtjUrb968ua0f+TGKq61W+9K50ZMnThbKp7Nyo20p06effiYrP/bYY1n5zJkzSTu34pKlVlJamIbPvwQqC6ACAAAAAAAAAACg23gyEQAAAAAAAAAAAEApJhMBAAAAAAAAAAAAlGIyEQAAAAAAAAAAAECphe2ZWDENDPRLkpYPDiR1Kwt7Jq7oL+z45+meiV7Y79As/8yrXv/9SbvXv/GNWXlk09VJ3cDSpaXHmJmZStqdOXkiK1f6+rNyX3/a96cOHcrKf/2Zz2blz/3155N2J0+dLPxU2DPxvBscpvsinmebxPN8CgAAAAAAAAAAAOg8nkwEAAAAAAAAAAAAUIrJRAAAAAAAAAAAAAClFrTMaaPhmpyalSSd8PGkbnYqX270WDVf93PVUH/SbvVQfsqRkfVZeetN1yftNoyszY+xekVSt+qqa/If+mpZceLU8aTdksG8bnp6IivXG+kiotdenx9vydJ/Vjh0uhzq33z2/qz83HPPaT59ffnvotFoJHXujfbmAAAAAAAAAAAAQE/iyUQAAAAAAAAAAAAApZhMBAAAAAAAAAAAAFBqQcucurump2ckSTMzM0ndGcuXNi0UNdSXnuIV1+ZLm37/LW/Iytdv2ZK0azTygzTa5jxrg8uzcqWaL2XqbUuKTp85lvfvxKmsfPbM6aRd/9CyrHzF2iuz8q1vuzVpt3L5kqy87xvfyspLCu9L0orl+fEe/u6jSd139x/IyjMz03nf05VXJRMAAAAAAAAAAADQVTyZCAAAAAAAAAAAAKAUk4kAAAAAAAAAAAAASjGZCAAAAAAAAAAAAKDUgvZMrEgaaG2IaG3TkFbYKLGw3aHOtu2tWK8NZeWly5Zm5VPHjyTtjh87mZWv2faKpG71FSNZuX/JiqzcV+lLzzWdn3vizNms3L7foyvfQ7Gv0L9Vl69O2t3yljdl5Ve/9pVZ2RpTSbvaQL6H4tqvfT2pK+6TeOpkvo/jTD3t0/Ezk3rmTHbctSphZj8p6S5JLukb7v5TZe3QHbt27dL73vc+SdpmZu939w+1tyHD3kaG8ZFhbHP51et1ibEwJDKMj/tofGQYG/fR+MgwPu6j8ZFhfGQYG2NhfGQIaYGTiegMd9fTp6d09eohVSumh46eHTazre6+b66NmV0n6QOSfsDdnzOzK7rXY7Sr1+u68847df/99+vaa6/dK+l2M9tJhnGQYXxkGFsxv5GREQ0MDDAWBkOG8XEfjY8MY+M+Gh8Zxsd9ND4yjI8MY2MsjI8MMYdlTnvQ+HRdtWpFtb6KKs0nPo9Luq2t2c9K+rC7PydJ7n5E6Bm7d+/W6OioNm/eLDX/b4x7RYahkGF8ZBhbMb9arSYxFoZDhvFxH42PDGPjPhofGcbHfTQ+MoyPDGNjLIyPDDFnQU8mukn11hKm1lbXJ8/KVqjt70tbHjx0OCt/5UtfzsqvvunapN30+ERWPrV6KKk7snQgK69ctznvQzVd5vTgww9l5d1f2ZOVB5cvS9qNXr8p728tX6K0v68/abdseb6kaqX5SK8k6czJY0m7gYF8+dY1a9Onfq9cl0/KV/vz3417/vt75rkzWtmQrlp3uSRp75Ez05LWK3W9JJnZ30vqk3SXu+8SesLhw4e1YcOG4luHJL22rRkZ9jAyjI8MYyvJj7EwGDKMj/tofGQYG/fR+MgwPu6j8ZFhfGQYG2NhfGSIOSxzGldV0nWSbpE0IumLZvYydz9RbGRmd0i6Q5I2btzY4S7ieZBhfGQYHxnGdkH5SWTYw7gG4yPD+MgwNsbC+LgG4yPD+MgwPjKMjb9n4uMavASwzGkPGqxVNTU9W3yrJulwW7NDkna6+4y7H5D0sJoXbMLd73H3MXcfW7NmzUvWZ6TWr1+vgwcPFt8aERmGQobxkWFsJfm94LFQIsNuWMwMya87uI/GR4axMRbGx1gYH/fR+MgwPjKMjb9n4uPvGcxhMrEHrVgyoPHpWU1MzajRcEkalrSzrdmfqTnTLzO7XM1HiR/rYDdxHjfffLMeeeQRHThwQGquCvx2kWEoZBgfGcZWzG96elpiLAyHDOPjPhofGcbGfTQ+MoyP+2h8ZBgfGcbGWBgfGWLOgpY5NZmqfc35Ry/skXjOz4X9/xqN9Bhnxqfy8tkzWXk8fRJPw1delZUHlq1K6mYb+V6DM83/gJvHOHEqaffYN/dm5Qe+nO+Z2LdkedJu6bJ8j8Plq4azcqU2nbSrDuZ7Lc4W+jtzNm03U5nMyuvWr0vqNl6d78945nTe39mZmaTdtk1X6B8OHJn7rR53971m9kFJe9x9p6TPSvphM9snqS7pl9z9WaEnVKtV3X333dq+fbsk3STp18kwFjKMjwxjK+ZXb+5TzFgYDBnGx300PjKMjftofGQYH/fR+MgwPjKMjbEwPjLEHHP352/V0lep+NBAv6RzJxOVTCbmxfbJxKUDtaz8Y294WVYee+VNSbvhy/LHXK/alGzwqdVr8/V0V6zJ9/qcGU8nE//2kx/Pyvfd/6X8e7RNJv7QW96Qla/fen1WHlh+WdKuOJl4/Kn8Sd7Tx44k7ZYsX5mVpzz9BfzdF/42K+/71reycvtkouXzpfr0Aw8/6O5jepHGxsZ8z549z98Qi8rMFiU/iQy7hQzjI8P4yDC+xcqQ/LqDazA+MoyPDONjLIyNazA+MoyPDOMjw/j4eya2F5Mfy5wCAAAAAAAAAAAAKLWwZU5NqlWb84+zbU8m1ouPIBar2h5gvGzlkqx8aiJfKvSr33o8abflhsGsPHLT2qRu+RX504jW15eVx8+mTyauXJUvXzo29oqs3L8q3dxz840vz8orLluRlaem0qcFvZH3d3YmX6610vYlJ06dzspLLxtO6jZtHs3KDz/03aw8OT6ZtKvV+gQAAAAAAAAAAAB0E08mAgAAAAAAAAAAACjFZCIAAAAAAAAAAACAUgta5lQuVWabS3q2L8JZq+XLki5bviwrX7MuXebz+pHVWXnvI09l5amDzyXtTp3NlxS94eWvSuo2b92alc3y5VX7ly5L2q0bvTYrb9iaL3O68qprknbLVud9mprIl0qtH382aTcxfjb/wfJibemStF1h+dbpej2pu3LduqzcqA5l5QNPP5a0G+hnmVMAAAAAAAAAAAB0F08mAgAAAAAAAAAAACjFZCIAAAAAAAAAAACAUkwmAgAAAAAAAAAAACi1oD0TTVKltUVhpW0ast88K1+xbCArf/+NG5N2V1y+Kis//ky+B+GMTyTtNm3alH9m7dqkbsmKywudyvdMdE+aqT4znZVrtbxPS5YvTdo1ZvN201NTWblSSfcttMI+iYNL82P09a1I253Mv9dkoQ+StGz58rw8nO/V+OzkTNJu9kz6+wAAAAAAAAAAAAA6jScTAQAAAAAAAAAAAJRiMhEAAAAAAAAAAABAqQUvc9rfWvnT26YhvZ4v0zkzMZ6Vjzx1OGk3VM+XEX3dthvyimWrk3Zv+uF/kpWvue66pK5SWG+0rzaUlZdfviHtb18tK0+dOZmVG25Ju8mzJwvlfInSeiNdN7U+M1s4eF43uHRZ0m5iIv+Ozx75XlK3ZGoy/1xhFdWl6YqqGq+3rdkKAAAAAAAAAAAAdBhPJgIAAAAAAAAAAAAoxWQiAAAAAAAAAAAAgFILWuZUktRoLhHq9fRtKyzTeepUvlTo3v3pMqcTz53Jyisvn87Ko6+4LGlXmcqXSj15+NGkbvZUvnToQGGJ0dry9Bjmjaxcr+dLlM7UG0m7qdlCO8+XF214utTozEy+lOv0VP4dJydmknYPPPjNrPy9o0eTule9YltWrlQHsvJs29KrlQbLnAIAAAAAAAAAAKC7eDIRAAAAAAAAAAAAQCkmEwEAAAAAAAAAAACUYjIRAAAAAAAAAAAAQKkF75nolebefv2VdE+/WjXfd7Bm+YaKJ8fTvQD3Th7PyiufncrK0+OzSbtVM/l+ikeXDyR1Q0trWXnFVWuy8hXX35S0qy5flZWnJqcK5XSPwzNnTmXl2dm8ztvmWs+ezfd7nJ7M90w8/NSxpN0ff+rTWXnl8Iqkbv1V67LygYNPZeXxyfT71xrp7w0AAAAAAAAAAADoNJ5MBAAAAAAAAAAAAFCKyUQAAAAAAAAAAAAApRa2zGnFZEPN+ce+SiOpGugrlAsrdM420iVFz07mS6D+46u3ZeXv3/KKpN2yyrKs3Dh2Oqkbf/J7Wfn4Y09k5cmJiaTdqs1bsrJVB7Py6VOnknYnjj2dn6vQ9+pgukRpfTZfirTh+Rc+depM0m6msFTqwUPPJHW77v9CVn58/+NZeaCR/j77WOUUAAAAAAAAAAAAXcaTiQAAAAAAAAAAAABKMZkIAAAAAAAAAAAAoNTCJhNN6qs2X6pUkte0+rLXhFezl+qV5DVs1ey1be1V2WvrleuT1+r+wew1WK0lr/6+/NWw/HX6xInkdfypJ7LXsaefzF5Hn/hu8jp95FD2mp08m72souQ1sGRJ9lKllr2mJ6eT16rBoex18tjx5PXNr387e02cOJm9+s2T13SjrqPTMzoyNSNJa+eNxOxtZuZmNvbi/lPAYtu1a5e2bNkiSdvM7P3ztSPD3kWG8ZFhbHP5jY6OSoyFIZFhfNxH4yPD+MgwNsbC+LgG4yPD+MgwNsbC+MgQEk8m9iR314mZhi7r79OVtT5JGjazre3tzGy5pPdJ+lqn+4jzq9fruvPOO3XfffdJ0l5Jt5NhLGQYHxnGVsxv3759EmNhOGQYH/fR+MgwPjKMjbEwPq7B+MgwPjKMjbEwPjLEHCYTe9C0S1UzVc1kZpJ0XNJtJU1/XdJvSZrsZP/w/Hbv3q3R0VFt3rxZklzSvSLDUMgwPjKMrZhfrVaTGAvDIcP4uI/GR4bxkWFsjIXxcQ3GR4bxkWFsjIXxkSHmMJnYgxru6rPkrWlJ64tvmNmrJW1w9093sGu4QIcPH9aGDRuKbx0SGYZChvGRYWwl+TEWBkOG8XEfjY8M4yPD2BgL4+MajI8M4yPD2BgL4yNDzKkupLFJqjSflFNFnlY2+vKDej5HOdhIZ8VqhfnLR557Jm/3zONJu+9NTGTlcU0ldddtuTYrj1yb/3c7O3s6aXf25HNZeXr22fx4p04m7fqHBrPympFrsvLy4SuTdidPnsrKzx7Pz9Xw9Hdx5epVWfnpal9SN3E2/179lfxzVjhERd78XbcdN2trVpH0XyW9u7RB2vYOSXdI0saNG5+vOTqEDOMjw/jIMLaF5NdqT4Y9hmswPjKMjwzjI8PYyC8+MoyPDOMjw9j4t318XIOXDp5M7EFVSfV0IrEm6XDh5+WStkn6gpk9Lul1knaWbWzq7ve4+5i7j61Zs+al6zQS69ev18GDB4tvjYgMQyHD+MgwtpL8XvBYKJFhNyxmhuTXHdxH4yPD+MgwNsbC+LgG4yPD+MgwNv5tHx9/z2AOk4k9qGbSjEuz7vLmpOKwpJ1z9e5+0t0vd/er3f1qSV+VdKu77+lOj9Hu5ptv1iOPPKIDBw5IzYd63y4yDIUM4yPD2Ir5TU9PS4yF4ZBhfNxH4yPD+MgwNsbC+LgG4yPD+MgwNsbC+MgQcxa0zGlF0tK5D86m85D99Xw5z8HCMqdD/ekp3BtZ+dHnvpcfe+aKpN3waL7c6OimkaRuZGRtVh6o5uc6+sSZpF1jZiYrT0/kS6WuWpcs6avhq67OyqvX5Y/XTk6ly6vO1gvlwg8rVq1M2q1bny+Pevixx5K6ZyfzZU7Thw8Ly8Ga6bJqQ0dnst/VcXffa2YflLTH3XcKPa1areruu+/W9u3bJekmSb9OhrGQYXxkGFsxv3q9LjEWhkOG8XEfjY8M4yPD2BgL4+MajI8M4yPD2BgL4yNDzDGfZ1++MkP9fb75siWSpOpsuhfiC5lMHL7qsqx83StflrQbvjqfTBxpm0zcMO9k4qNJu8kT+WTleGEycdmatonLC5xMfO7ZfA/Gpw7lT/Ie/d73knYHHt6flfd86StJ3bNHj2Xl9Hef/j7rhT0pH59uPOjupY92L8TY2Jjv2cP/ENBpZrYo+Ulk2C1kGB8ZxkeG8S1WhuTXHVyD8ZFhfGQYH2NhbFyD8ZFhfGQYHxnGx98zsb2Y/FjmFAAAAAAAAAAAAECpBS1z2u8VrZ0dlCRVPJ2H7CscqdoolK0vaTdbaPjU0eNZefDgk0m7kQ35Bpz906eSuqf25U8F+vRkVp6enknaaSavW73p2qy87rr0KcjBJcvyYxSWL52YTJ9MPHv2bFYuPke4cng4abduw4b8e2w6nNSdOXEyK08V+tv+fKi1PakIAAAAAAAAAAAAdBpPJgIAAAAAAAAAAAAoxWQiAAAAAAAAAAAAgFJMJgIAAAAAAAAAAAAotaA9EytmGqoOSJL62upqlXyPv+JBT9Vnk3ZnZvKf6418c8Xnjh9P2j316MP5MY6k+w72VfLP9Vu+2+Dw2pGk3fJ1V+flK/K62pLlSbupmen8eLWBrDzQP5C0axT2U6wNDmbl6sBQ0m7psnyPxyVLlyR1/X35b25WhT0TLd01seLsmQgAAAAAAAAAAIDu4slEAAAAAAAAAAAAAKWYTAQAAAAAAAAAAABQamHLnFZMSwdqkqRavZHU1Wv9WXm8UNfXNl85UM+XFF1R+MwNV29O2m28/rqs3JidSeqGVuTLlA4tXZqVa/21pN0V1+TH6C8sRTo5Pp60K34T97y/E+Nnk3azheVQq9W876qk33FwSd6nFatXJ3VLhvKlU2cnJvLzKl3mtMEqpwAAAAAAAAAAAOgynkwEAAAAAAAAAAAAUIrJRAAAAAAAAAAAAAClmEwEAAAAAAAAAAAAUGpBeyZaw1WbnpUknamm85CNRr7zYG0g3xewr/0g0/m+g7P9+emXDQ8nzVZetiYrVyrpfoLV2mB+rsL+hMU+SNLZU2eyct3PFsptxxvI91r0Rj0rz8zW03b9+fcaKOzVWGnbM3FixWRWXjHctmdi4XPjJ07mfU+7JBebJgIAAAAAAAAAAKC7eDIRAAAAAAAAAAAAQCkmEwEAAAAAAAAAAACUWtgyp2aq9vdLkvr60wVMp6dmsvKAT+UnaFtStG9JvkTpTGEqs39wIGm3ZPmKrFypts95FpcALdRV0qVBx0+fzspnT49n5VOTU0m71VfmS6omp6qk33GgsKTqskL/Gm1rlNZnn8nKbmmfqoP5968U+ttIV1SVqW3dUwAAAAAAAAAAAKDDeDIRAAAAAAAAAAAAQCkmEwEAAAAAAAAAAACUWtAyp27SdGtlzhWzjaRu3ap82c/ZiXwZ0dOWLtfZaOSf6yusItrXly4pWh3IlwO1tuVLVVg6tD47XTh427kKS6zOFI6xfHh10m7JsmVZ2Qv9q1TSudbaQL4U6+xsvqxrve13US8cY7atT1Y4hqr9hQ+lS69WWOYUAAAAAAAAAAAAXcaTiQAAAAAAAAAAAABKMZkIAAAAAAAAAAAAoBSTiQAAAAAAAAAAAABKLWjPxIZL4/V684dKusfh5afGs3KtsP9hZaiWtPOpwt6AlrczpfsiVvrzrpm37TtYmAJt1PNj1Gcnk3ZnJvKfh5atzMrV4r6FkmqFc83MFPZgVLoXohr1/FyFvRAnJtLzFr9KvV5PquqFPST7BvN9IRvJedvPzP6JAAAAAAAAAAAA6DyeTAQAAAAAAAAAAABQisnEHnW67vrOZEPfmWxI0tr2ejP7BTPbZ2bfNLO/MbNNne8lzmfXrl3asmWLJG0zs/e315Nh7yPD+Mgwtrn8RkdHJcbCkMgwPu6j8ZFhbNxH4yPD+LiPxkeG8ZFhbIyF8ZEhpAUuc3piZvrYnx1+4omXoiP/75HvpG98+CMvxWki2SbpYUkzkl5hZlvdfV+h/h8kjbn7uJn9K0n/WdI/70I/UaJer+vOO+/U/fffr2uvvXavpNvNbCcZxkGG8ZFhbMX8RkZGNDAwMMxYGAsZxsd9ND4yjI37aHxkGB/30fjIMD4yjI2xMD4yxJwFTSa6+5qXqiPImdnrJd3l7ttbP39A0m2SsgvU3T9f+MhXJb2jo53Eee3evVujo6PavHmz1Nz08l6RYShkGB8ZxtaWnyQdF/mFQobxcR+Njwxj4z4aHxnGx300PjKMjwxjYyyMjwwxh2VOe9N6SQcLPx9qvTef90i6r6zCzO4wsz1mtufo0aOL2EWcz+HDh7Vhw4biW2QYDBnGR4axleQ3rReYn0SG3bCYGZJfd3AfjY8MY2MsjI+xMD7uo/GRYXxkGBt/z8TH3zOYw2RicGb2Dkljkn67rN7d73H3MXcfW7OGB0t7ERnGR4bxkWFsz5efRIa9jmswPjKMjwxjYyyMj2swPjKMjwzjI8PY+HsmPq7Bi9uCljlFxxyWVJzuH2m9lzCzN0n6FUlvdPepDvUNF2D9+vU6eLD4cCkZRkOG8ZFhbCX51UR+oZBhfNxH4yPD2LiPxkeG8XEfjY8M4yPD2BgL4yNDzOHJxN70gKTrzOwaM6tJerukncUGZvYqSb8n6VZ3P9KFPuI8br75Zj3yyCM6cOCAJJnIMBwyjI8MYyvmNz09LUnDIr9QyDA+7qPxkWFs3EfjI8P4uI/GR4bxkWFsjIXxkSHmMJnYg9x9VtJ7JX1W0kOSPuHue83sg2Z2a6vZb0taJumPzezrZrZznsOhC6rVqu6++25t375dkm4SGYZDhvGRYWzF/G688UZJOk5+sZBhfNxH4yPD2LiPxkeG8XEfjY8M4yPD2BgL4yNDzDF373Yf0CFjY2O+Z8+ebnfjkmNmD7r72GIciwy7gwzjI8P4yDC+xcqQ/LqDazA+MoyPDONjLIyNazA+MoyPDOMjw/j4eya2F5MfTyYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkIgAAAAAAAAAAAIBSTCYCAAAAAAAAAAAAKMVkYo8yszeb2XfNbL+Zvb+kfsDMPt6q/5qZXd2FbuI8du3apS1btkjSNjKMiQzjI8PY5vIbHR2VpLXt9eTX+8gwPu6j8ZFhfGQYG2NhfFyD8ZFhfGQYG2NhfGQIicnEnmRmfZI+LOktkrZKut3MtrY1e4+k59x9VNJ/k/Rbne0lzqder+vOO+/UfffdJ0l7RYbhkGF8ZBhbMb99+/ZJ0jD5xUKG8XEfjY8M4yPD2BgL4+MajI8M4yPD2BgL4yNDzGEysTe9RtJ+d3/M3acl3SvptrY2t0n6w1b5k5J+0Mysg33EeezevVujo6PavHmzJLnIMBwyjI8MYyvmV6vVJOm4yC8UMoyP+2h8ZBgfGcbGWBgf12B8ZBgfGcbGWBgfGWIOk4m9ab2kg4WfD7XeK23j7rOSTkq6rCO9w/M6fPiwNmzYUHyLDIMhw/jIMLaS/KZFfqGQYXzcR+Mjw/jIMDbGwvi4BuMjw/jIMDbGwvjIEHOq3e4AXlpmdoekO1o/TpnZt7vZnxfhcknHut2JBVgtacVHPvKRJyRteTEHIsOuIcNzkWETGXZGMT9JuunFHIwMu2LRMiS/ruE+ei4ybCLDziHDc0XKkLHwXJHyk7gGy5BhExl2DhmeK1KG/Nu+3CWZIfn1hBd8H2UysTcdllSc7h9pvVfW5pCZVSWtlPRs+4Hc/R5J90iSme1x97GXpMcvsWh9N7PXS7rL3beb2R6RYbi+k+G5ovWdDM8Vqe/F/Fo/H9ILzE8iw25YzAzJrzu4j54rWt/J8FzR+k6G54rUd8bCc0XrO9fguaL1nQzPFa3vZHiuSH3n3/blIvWdv2fOFb3vL/SzLHPamx6QdJ2ZXWNmNUlvl7Szrc1OSe9qlX9c0ufc3TvYR5xflqEkExlGRIbxkWFs7WPhsMgvGjKMj/tofGQYHxnGxlgYH9dgfGQYHxnGxlgYHxlCEpOJPam1rvB7JX1W0kOSPuHue83sg2Z2a6vZRyRdZmb7Jf2CpPd3p7co05bhTSLDcMgwPjKMrWQsPE5+sZBhfNxH4yPD+MgwNsbC+LgG4yPD+MgwNsbC+MgQc4wJ4kuHmd3RepQ4HPq++MfqNPq++MfqNPq++MfqNPq++MfqNPrO76BbuAab6PviH6vT6PviH6vT6Du/g27hGmyi74t/rE6j74t/rE6j74t/rE6j7/wOuuXF9J3JRAAAAAAAAAAAAAClWOYUAAAAAAAAAAAAQCkmEy9CZvZmM/uume03s3PWJzazATP7eKv+a2Z2dRe6WeoC+v5uMztqZl9vvX6mG/0sY2Z/YGZHzOzb89Sbmf1O67t908xefZ5jkWGHLWZ+rfZk2GFk2BQ1P4n76BwyzNqSYReQYVPUDBkLc2RIft1Chk1kmLUlww5jLMyRYdY+ZIZR85O4j84hw6wtGXbYYt9HM+7O6yJ6SeqT9KikzZJqkr4haWtbm38t6Xdb5bdL+ni3+72Avr9b0t3d7us8/X+DpFdL+vY89W+VdJ8kk/Q6SV8jw955LVZ+ZEiG5Nf9DKPmR4Zk2AsvMoyd4WLlR4bxMyQ/MiRDMrxUM1ys/MiQDMmv+xlGzY8MybDbr8W8jxZfPJl48XmNpP3u/pi7T0u6V9JtbW1uk/SHrfInJf2gmVkH+zifC+l7z3L3L0o6fp4mt0n6I2/6qqRVZraupB0ZdsEi5ieRYVeQoaTA+UncR1vIsIkMu4QMJQXOkLEwQ4bk1zVkKIkM55BhFzAWZsiwKWqGYfOTuI+2kGETGXbBIt9HM0wmXnzWSzpY+PlQ673SNu4+K+mkpMs60rvzu5C+S9LbWo/fftLMNnSma4viQr8fGfamC/1uF9qWDDvvUsjwYs5P4j5aRIZk2C1kmIuY4aUwFkpkeKHtyK87yDBHhmTYDYyFKTLszQwv5vwk7qNFZEiG3bCQ+2iGyURE8xeSrnb3l0u6X/n/tYA4yDA+MoyN/OIjw/jIMD4yjI8MYyO/+MgwPjKMjwxjI7/4yDC+SypDJhMvPoclFWfAR1rvlbYxs6qklZKe7Ujvzu95++7uz7r7VOvH35f0fR3q22K4kGwutB0Zdt6F5nehbcmw8y6FDC/m/CTuo5LIsKwNGXYUGSp0hpfCWCiR4YW2I7/uIEORYVkbMuwYxsIWMjy3TQ9leDHnJ3EflUSGZW3IsGMWch/NMJl48XlA0nVmdo2Z1dTctHRnW5udkt7VKv+4pM+5N3fe7LLn7Xvb2r23Snqog/17sXZKeqc1vU7SSXd/uqQdGfamC81PIsNedSlkeDHnJ3EflUSGbcciw84jQ4XO8FIYCyUylMivl5GhyLDtWGTYWYyFLWSYHK/XMryY85O4j0oiw7ZjkWFnLeQ+mnN3XhfZS9JbJT0s6VFJv9J674OSbm2VByX9saT9knZL2tztPi+g778paa+kb0j6vKQbut3nQt8/JulpSTNqrjP8Hkk/J+nnWvUm6cOt7/YtSWNk2DsZLmZ+ZEiG5Nf9DKPmR4Zk2O0XGcbOcDHzI8P4GZIfGZIhGV6KGS5mfmRIhuTX/Qyj5keGZHix5Fd8WevDAAAAAAAAAAAAAJBgmVMAAAAAAAAAAAAApZhMBAAAAAAAAAAAAFCKyUQAAAAAAAAAAAAApZhMBAAAAAAAAAAAAFCKyUQAAAAAAAAAAAAApZhMBAAAAAAAAAAAAFCKyUQAAAAAAAAAAAAApZhMBAAAAAAAAAAAAFDq/wP77V++TucgDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x216 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79126128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZZZ),\n",
      "  CircuitStateFn(\n",
      "       \n",
      "  q_0: 0                                   \n",
      "                                           \n",
      "  q_1: 1                                   \n",
      "         ZZFeatureMap(x[0],x[1],x[2],x[3]) \n",
      "  q_2: 2                                   \n",
      "                                           \n",
      "  q_3: 3                                   \n",
      "       \n",
      "       \n",
      "  q_0: 0                                                         \n",
      "                                                                 \n",
      "  q_1: 1                                                         \n",
      "         RealAmplitudes([0],[1],[2],[3],[4],[5],[6],[7]) \n",
      "  q_2: 2                                                         \n",
      "                                                                 \n",
      "  q_3: 3                                                         \n",
      "       \n",
      "  )\n",
      "])\n",
      "HybridQNN_Shallow(\n",
      "  (conv1): Conv2d(3, 2, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (qnn): TorchConnector()\n",
      "  (fc3): Linear(in_features=1, out_features=3, bias=True)\n",
      ")\n",
      "Network init elapsed time: -0.0002748612314462662 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        #n_filts_fc1 = int(((((input_resolution[0])/2)-4)/2)**2)*16 # +7\n",
    "        n_filts_fc1 = 256\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    \n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[ 0.2600,  0.5330, -1.1819]], grad_fn=<AddmmBackward0>)\n",
      "Batch 0, Loss: -0.533022940158844\n",
      "tensor([2])\n",
      "tensor([[ 0.2802,  0.5216, -1.1533]], grad_fn=<AddmmBackward0>)\n",
      "Batch 1, Loss: 1.153252363204956\n",
      "tensor([2])\n",
      "tensor([[ 0.2748,  0.5248, -1.1608]], grad_fn=<AddmmBackward0>)\n",
      "Batch 2, Loss: 1.160832405090332\n",
      "tensor([1])\n",
      "tensor([[ 0.2939,  0.5140, -1.1337]], grad_fn=<AddmmBackward0>)\n",
      "Batch 3, Loss: -0.5139763951301575\n",
      "tensor([1])\n",
      "tensor([[ 0.2650,  0.5305, -1.1745]], grad_fn=<AddmmBackward0>)\n",
      "Batch 4, Loss: -0.530538022518158\n",
      "tensor([0])\n",
      "tensor([[ 0.3149,  0.5022, -1.1039]], grad_fn=<AddmmBackward0>)\n",
      "Batch 5, Loss: -0.31487566232681274\n",
      "tensor([0])\n",
      "tensor([[ 0.3122,  0.5039, -1.1078]], grad_fn=<AddmmBackward0>)\n",
      "Batch 6, Loss: -0.3121533691883087\n",
      "tensor([0])\n",
      "tensor([[ 0.2887,  0.5174, -1.1410]], grad_fn=<AddmmBackward0>)\n",
      "Batch 7, Loss: -0.28869718313217163\n",
      "tensor([0])\n",
      "tensor([[ 0.3736,  0.4691, -1.0211]], grad_fn=<AddmmBackward0>)\n",
      "Batch 8, Loss: -0.3735761046409607\n",
      "tensor([0])\n",
      "tensor([[ 0.2614,  0.5332, -1.1797]], grad_fn=<AddmmBackward0>)\n",
      "Batch 9, Loss: -0.2614208459854126\n",
      "tensor([0])\n",
      "tensor([[ 0.2982,  0.5123, -1.1278]], grad_fn=<AddmmBackward0>)\n",
      "Batch 10, Loss: -0.2982312738895416\n",
      "tensor([0])\n",
      "tensor([[ 0.3606,  0.4768, -1.0397]], grad_fn=<AddmmBackward0>)\n",
      "Batch 11, Loss: -0.36060798168182373\n",
      "tensor([1])\n",
      "tensor([[ 0.2982,  0.5125, -1.1281]], grad_fn=<AddmmBackward0>)\n",
      "Batch 12, Loss: -0.5125463604927063\n",
      "tensor([2])\n",
      "tensor([[ 0.2796,  0.5233, -1.1546]], grad_fn=<AddmmBackward0>)\n",
      "Batch 13, Loss: 1.1545703411102295\n",
      "tensor([2])\n",
      "tensor([[ 0.3138,  0.5039, -1.1063]], grad_fn=<AddmmBackward0>)\n",
      "Batch 14, Loss: 1.1063235998153687\n",
      "tensor([1])\n",
      "tensor([[ 0.3075,  0.5075, -1.1152]], grad_fn=<AddmmBackward0>)\n",
      "Batch 15, Loss: -0.5075302720069885\n",
      "tensor([1])\n",
      "tensor([[ 0.3479,  0.4846, -1.0582]], grad_fn=<AddmmBackward0>)\n",
      "Batch 16, Loss: -0.4845888912677765\n",
      "tensor([1])\n",
      "tensor([[ 0.2703,  0.5291, -1.1679]], grad_fn=<AddmmBackward0>)\n",
      "Batch 17, Loss: -0.5290591716766357\n",
      "tensor([1])\n",
      "tensor([[ 0.3331,  0.4933, -1.0791]], grad_fn=<AddmmBackward0>)\n",
      "Batch 18, Loss: -0.4932776093482971\n",
      "tensor([1])\n",
      "tensor([[ 0.2606,  0.5349, -1.1817]], grad_fn=<AddmmBackward0>)\n",
      "Batch 19, Loss: -0.534885823726654\n",
      "tensor([1])\n",
      "tensor([[ 0.2929,  0.5165, -1.1361]], grad_fn=<AddmmBackward0>)\n",
      "Batch 20, Loss: -0.516542911529541\n",
      "tensor([2])\n",
      "tensor([[ 0.4212,  0.4432, -0.9546]], grad_fn=<AddmmBackward0>)\n",
      "Batch 21, Loss: 0.9545877575874329\n",
      "tensor([2])\n",
      "tensor([[ 0.3807,  0.4665, -1.0118]], grad_fn=<AddmmBackward0>)\n",
      "Batch 22, Loss: 1.0118407011032104\n",
      "tensor([0])\n",
      "tensor([[ 0.2925,  0.5172, -1.1366]], grad_fn=<AddmmBackward0>)\n",
      "Batch 23, Loss: -0.2925105690956116\n",
      "tensor([1])\n",
      "tensor([[ 0.3162,  0.5037, -1.1032]], grad_fn=<AddmmBackward0>)\n",
      "Batch 24, Loss: -0.5037132501602173\n",
      "tensor([0])\n",
      "tensor([[ 0.3574,  0.4802, -1.0448]], grad_fn=<AddmmBackward0>)\n",
      "Batch 25, Loss: -0.35739201307296753\n",
      "tensor([1])\n",
      "tensor([[ 0.3743,  0.4706, -1.0209]], grad_fn=<AddmmBackward0>)\n",
      "Batch 26, Loss: -0.47057992219924927\n",
      "tensor([0])\n",
      "tensor([[ 0.3626,  0.4774, -1.0375]], grad_fn=<AddmmBackward0>)\n",
      "Batch 27, Loss: -0.36261436343193054\n",
      "tensor([2])\n",
      "tensor([[ 0.3571,  0.4807, -1.0453]], grad_fn=<AddmmBackward0>)\n",
      "Batch 28, Loss: 1.0453399419784546\n",
      "tensor([2])\n",
      "tensor([[ 0.2715,  0.5300, -1.1666]], grad_fn=<AddmmBackward0>)\n",
      "Batch 29, Loss: 1.1665751934051514\n",
      "tensor([1])\n",
      "tensor([[ 0.2769,  0.5270, -1.1589]], grad_fn=<AddmmBackward0>)\n",
      "Batch 30, Loss: -0.5269618034362793\n",
      "tensor([0])\n",
      "tensor([[ 0.4354,  0.4360, -0.9346]], grad_fn=<AddmmBackward0>)\n",
      "Batch 31, Loss: -0.4353722035884857\n",
      "tensor([0])\n",
      "tensor([[ 0.3372,  0.4926, -1.0736]], grad_fn=<AddmmBackward0>)\n",
      "Batch 32, Loss: -0.3371948003768921\n",
      "tensor([2])\n",
      "tensor([[ 0.3612,  0.4789, -1.0397]], grad_fn=<AddmmBackward0>)\n",
      "Batch 33, Loss: 1.039711833000183\n",
      "tensor([2])\n",
      "tensor([[ 0.3445,  0.4886, -1.0634]], grad_fn=<AddmmBackward0>)\n",
      "Batch 34, Loss: 1.0634031295776367\n",
      "tensor([0])\n",
      "tensor([[ 0.3333,  0.4951, -1.0792]], grad_fn=<AddmmBackward0>)\n",
      "Batch 35, Loss: -0.333280473947525\n",
      "tensor([0])\n",
      "tensor([[ 0.3623,  0.4785, -1.0382]], grad_fn=<AddmmBackward0>)\n",
      "Batch 36, Loss: -0.3622645437717438\n",
      "tensor([2])\n",
      "tensor([[ 0.3331,  0.4954, -1.0795]], grad_fn=<AddmmBackward0>)\n",
      "Batch 37, Loss: 1.0795315504074097\n",
      "tensor([2])\n",
      "tensor([[ 0.4686,  0.4175, -0.8877]], grad_fn=<AddmmBackward0>)\n",
      "Batch 38, Loss: 0.8877273797988892\n",
      "tensor([0])\n",
      "tensor([[ 0.5059,  0.3961, -0.8350]], grad_fn=<AddmmBackward0>)\n",
      "Batch 39, Loss: -0.5058666467666626\n",
      "tensor([1])\n",
      "tensor([[ 0.3923,  0.4616, -0.9958]], grad_fn=<AddmmBackward0>)\n",
      "Batch 40, Loss: -0.4615614116191864\n",
      "tensor([2])\n",
      "tensor([[ 0.3902,  0.4628, -0.9988]], grad_fn=<AddmmBackward0>)\n",
      "Batch 41, Loss: 0.998771607875824\n",
      "tensor([0])\n",
      "tensor([[ 0.4720,  0.4158, -0.8830]], grad_fn=<AddmmBackward0>)\n",
      "Batch 42, Loss: -0.4719991385936737\n",
      "tensor([2])\n",
      "tensor([[ 0.3320,  0.4965, -1.0812]], grad_fn=<AddmmBackward0>)\n",
      "Batch 43, Loss: 1.0811767578125\n",
      "tensor([2])\n",
      "tensor([[ 0.4389,  0.4350, -0.9298]], grad_fn=<AddmmBackward0>)\n",
      "Batch 44, Loss: 0.9298493266105652\n",
      "tensor([1])\n",
      "tensor([[ 0.5130,  0.3924, -0.8250]], grad_fn=<AddmmBackward0>)\n",
      "Batch 45, Loss: -0.3924016058444977\n",
      "tensor([2])\n",
      "tensor([[ 0.3981,  0.4587, -0.9876]], grad_fn=<AddmmBackward0>)\n",
      "Batch 46, Loss: 0.9876418709754944\n",
      "tensor([1])\n",
      "tensor([[ 0.4830,  0.4098, -0.8675]], grad_fn=<AddmmBackward0>)\n",
      "Batch 47, Loss: -0.4098466634750366\n",
      "tensor([2])\n",
      "tensor([[ 0.5652,  0.3626, -0.7512]], grad_fn=<AddmmBackward0>)\n",
      "Batch 48, Loss: 0.7511579394340515\n",
      "tensor([0])\n",
      "tensor([[ 0.3855,  0.4661, -1.0054]], grad_fn=<AddmmBackward0>)\n",
      "Batch 49, Loss: -0.38549405336380005\n",
      "tensor([1])\n",
      "tensor([[ 0.4338,  0.4384, -0.9371]], grad_fn=<AddmmBackward0>)\n",
      "Batch 50, Loss: -0.43840768933296204\n",
      "tensor([0])\n",
      "tensor([[ 0.4864,  0.4082, -0.8626]], grad_fn=<AddmmBackward0>)\n",
      "Batch 51, Loss: -0.4863698184490204\n",
      "tensor([1])\n",
      "tensor([[ 0.5448,  0.3746, -0.7799]], grad_fn=<AddmmBackward0>)\n",
      "Batch 52, Loss: -0.3745855391025543\n",
      "tensor([1])\n",
      "tensor([[ 0.5110,  0.3942, -0.8278]], grad_fn=<AddmmBackward0>)\n",
      "Batch 53, Loss: -0.3941543400287628\n",
      "tensor([2])\n",
      "tensor([[ 0.4642,  0.4212, -0.8941]], grad_fn=<AddmmBackward0>)\n",
      "Batch 54, Loss: 0.8940554261207581\n",
      "tensor([1])\n",
      "tensor([[ 0.5039,  0.3984, -0.8378]], grad_fn=<AddmmBackward0>)\n",
      "Batch 55, Loss: -0.3984406888484955\n",
      "tensor([0])\n",
      "tensor([[ 0.5329,  0.3819, -0.7969]], grad_fn=<AddmmBackward0>)\n",
      "Batch 56, Loss: -0.532853901386261\n",
      "tensor([2])\n",
      "tensor([[ 0.5652,  0.3634, -0.7512]], grad_fn=<AddmmBackward0>)\n",
      "Batch 57, Loss: 0.7511886358261108\n",
      "tensor([1])\n",
      "tensor([[ 0.5471,  0.3739, -0.7768]], grad_fn=<AddmmBackward0>)\n",
      "Batch 58, Loss: -0.37388280034065247\n",
      "tensor([2])\n",
      "tensor([[ 0.4218,  0.4461, -0.9541]], grad_fn=<AddmmBackward0>)\n",
      "Batch 59, Loss: 0.9540778994560242\n",
      "tensor([1])\n",
      "tensor([[ 0.5493,  0.3728, -0.7737]], grad_fn=<AddmmBackward0>)\n",
      "Batch 60, Loss: -0.3728199303150177\n",
      "tensor([0])\n",
      "tensor([[ 0.4814,  0.4120, -0.8698]], grad_fn=<AddmmBackward0>)\n",
      "Batch 61, Loss: -0.4813555181026459\n",
      "tensor([1])\n",
      "tensor([[ 0.4396,  0.4362, -0.9289]], grad_fn=<AddmmBackward0>)\n",
      "Batch 62, Loss: -0.4361613988876343\n",
      "tensor([2])\n",
      "tensor([[ 0.4194,  0.4479, -0.9575]], grad_fn=<AddmmBackward0>)\n",
      "Batch 63, Loss: 0.9574592113494873\n",
      "tensor([2])\n",
      "tensor([[ 0.5767,  0.3575, -0.7348]], grad_fn=<AddmmBackward0>)\n",
      "Batch 64, Loss: 0.7348477840423584\n",
      "tensor([1])\n",
      "tensor([[ 0.4258,  0.4444, -0.9484]], grad_fn=<AddmmBackward0>)\n",
      "Batch 65, Loss: -0.44435518980026245\n",
      "tensor([0])\n",
      "tensor([[ 0.4691,  0.4195, -0.8870]], grad_fn=<AddmmBackward0>)\n",
      "Batch 66, Loss: -0.46913227438926697\n",
      "tensor([0])\n",
      "tensor([[ 0.5980,  0.3455, -0.7046]], grad_fn=<AddmmBackward0>)\n",
      "Batch 67, Loss: -0.5980135202407837\n",
      "tensor([2])\n",
      "tensor([[ 0.4606,  0.4246, -0.8991]], grad_fn=<AddmmBackward0>)\n",
      "Batch 68, Loss: 0.8991485834121704\n",
      "tensor([1])\n",
      "tensor([[ 0.3927,  0.4637, -0.9952]], grad_fn=<AddmmBackward0>)\n",
      "Batch 69, Loss: -0.46373629570007324\n",
      "tensor([0])\n",
      "tensor([[ 0.4636,  0.4231, -0.8950]], grad_fn=<AddmmBackward0>)\n",
      "Batch 70, Loss: -0.4635925590991974\n",
      "tensor([2])\n",
      "tensor([[ 0.4572,  0.4268, -0.9040]], grad_fn=<AddmmBackward0>)\n",
      "Batch 71, Loss: 0.9039666652679443\n",
      "tensor([1])\n",
      "tensor([[ 0.4787,  0.4146, -0.8736]], grad_fn=<AddmmBackward0>)\n",
      "Batch 72, Loss: -0.41455599665641785\n",
      "tensor([1])\n",
      "tensor([[ 0.5516,  0.3728, -0.7704]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 73, Loss: -0.37276196479797363\n",
      "tensor([0])\n",
      "tensor([[ 0.4778,  0.4153, -0.8749]], grad_fn=<AddmmBackward0>)\n",
      "Batch 74, Loss: -0.47783541679382324\n",
      "tensor([0])\n",
      "tensor([[ 0.4097,  0.4545, -0.9713]], grad_fn=<AddmmBackward0>)\n",
      "Batch 75, Loss: -0.4096969664096832\n",
      "tensor([2])\n",
      "tensor([[ 0.3585,  0.4840, -1.0438]], grad_fn=<AddmmBackward0>)\n",
      "Batch 76, Loss: 1.043819546699524\n",
      "tensor([2])\n",
      "tensor([[ 0.5406,  0.3795, -0.7861]], grad_fn=<AddmmBackward0>)\n",
      "Batch 77, Loss: 0.7861318588256836\n",
      "tensor([2])\n",
      "tensor([[ 0.5611,  0.3678, -0.7572]], grad_fn=<AddmmBackward0>)\n",
      "Batch 78, Loss: 0.7572135925292969\n",
      "tensor([0])\n",
      "tensor([[ 0.4440,  0.4352, -0.9228]], grad_fn=<AddmmBackward0>)\n",
      "Batch 79, Loss: -0.44402727484703064\n",
      "tensor([2])\n",
      "tensor([[ 0.5432,  0.3783, -0.7826]], grad_fn=<AddmmBackward0>)\n",
      "Batch 80, Loss: 0.7826038002967834\n",
      "tensor([0])\n",
      "tensor([[ 0.4572,  0.4277, -0.9042]], grad_fn=<AddmmBackward0>)\n",
      "Batch 81, Loss: -0.45720547437667847\n",
      "tensor([1])\n",
      "tensor([[ 0.5868,  0.3534, -0.7208]], grad_fn=<AddmmBackward0>)\n",
      "Batch 82, Loss: -0.3533875048160553\n",
      "tensor([2])\n",
      "tensor([[ 0.5293,  0.3865, -0.8022]], grad_fn=<AddmmBackward0>)\n",
      "Batch 83, Loss: 0.8022424578666687\n",
      "tensor([1])\n",
      "tensor([[ 0.5829,  0.3558, -0.7263]], grad_fn=<AddmmBackward0>)\n",
      "Batch 84, Loss: -0.35580331087112427\n",
      "tensor([2])\n",
      "tensor([[ 0.5985,  0.3470, -0.7043]], grad_fn=<AddmmBackward0>)\n",
      "Batch 85, Loss: 0.7043168544769287\n",
      "tensor([0])\n",
      "tensor([[ 0.4823,  0.4137, -0.8687]], grad_fn=<AddmmBackward0>)\n",
      "Batch 86, Loss: -0.4823014736175537\n",
      "tensor([0])\n",
      "tensor([[ 0.5068,  0.3998, -0.8341]], grad_fn=<AddmmBackward0>)\n",
      "Batch 87, Loss: -0.5068062543869019\n",
      "tensor([0])\n",
      "tensor([[ 0.5698,  0.3637, -0.7450]], grad_fn=<AddmmBackward0>)\n",
      "Batch 88, Loss: -0.5697706937789917\n",
      "tensor([2])\n",
      "tensor([[ 0.4422,  0.4370, -0.9255]], grad_fn=<AddmmBackward0>)\n",
      "Batch 89, Loss: 0.9255249500274658\n",
      "tensor([0])\n",
      "tensor([[ 0.4942,  0.4073, -0.8521]], grad_fn=<AddmmBackward0>)\n",
      "Batch 90, Loss: -0.49415212869644165\n",
      "tensor([1])\n",
      "tensor([[ 0.5249,  0.3897, -0.8087]], grad_fn=<AddmmBackward0>)\n",
      "Batch 91, Loss: -0.3897332549095154\n",
      "tensor([0])\n",
      "tensor([[ 0.5453,  0.3781, -0.7798]], grad_fn=<AddmmBackward0>)\n",
      "Batch 92, Loss: -0.5453486442565918\n",
      "tensor([2])\n",
      "tensor([[ 0.4057,  0.4582, -0.9774]], grad_fn=<AddmmBackward0>)\n",
      "Batch 93, Loss: 0.9773814082145691\n",
      "tensor([0])\n",
      "tensor([[ 0.5453,  0.3783, -0.7799]], grad_fn=<AddmmBackward0>)\n",
      "Batch 94, Loss: -0.5453070998191833\n",
      "tensor([0])\n",
      "tensor([[ 0.5401,  0.3814, -0.7874]], grad_fn=<AddmmBackward0>)\n",
      "Batch 95, Loss: -0.5400814414024353\n",
      "tensor([1])\n",
      "tensor([[ 0.4645,  0.4248, -0.8944]], grad_fn=<AddmmBackward0>)\n",
      "Batch 96, Loss: -0.4247807562351227\n",
      "tensor([2])\n",
      "tensor([[ 0.5420,  0.3805, -0.7849]], grad_fn=<AddmmBackward0>)\n",
      "Batch 97, Loss: 0.7849480509757996\n",
      "tensor([0])\n",
      "tensor([[ 0.5824,  0.3575, -0.7278]], grad_fn=<AddmmBackward0>)\n",
      "Batch 98, Loss: -0.5823774337768555\n",
      "tensor([2])\n",
      "tensor([[ 0.5919,  0.3522, -0.7145]], grad_fn=<AddmmBackward0>)\n",
      "Batch 99, Loss: 0.7145251035690308\n",
      "tensor([2])\n",
      "tensor([[ 0.4396,  0.4394, -0.9299]], grad_fn=<AddmmBackward0>)\n",
      "Batch 100, Loss: 0.9298797845840454\n",
      "tensor([2])\n",
      "tensor([[ 0.5878,  0.3547, -0.7203]], grad_fn=<AddmmBackward0>)\n",
      "Batch 101, Loss: 0.7203396558761597\n",
      "tensor([2])\n",
      "tensor([[ 0.3699,  0.4794, -1.0284]], grad_fn=<AddmmBackward0>)\n",
      "Batch 102, Loss: 1.0284346342086792\n",
      "tensor([2])\n",
      "tensor([[ 0.5768,  0.3611, -0.7359]], grad_fn=<AddmmBackward0>)\n",
      "Batch 103, Loss: 0.7359338998794556\n",
      "tensor([0])\n",
      "tensor([[ 0.5673,  0.3666, -0.7493]], grad_fn=<AddmmBackward0>)\n",
      "Batch 104, Loss: -0.5673084855079651\n",
      "tensor([0])\n",
      "tensor([[ 0.4460,  0.4360, -0.9208]], grad_fn=<AddmmBackward0>)\n",
      "Batch 105, Loss: -0.44597917795181274\n",
      "tensor([0])\n",
      "tensor([[ 0.5916,  0.3529, -0.7151]], grad_fn=<AddmmBackward0>)\n",
      "Batch 106, Loss: -0.5915508270263672\n",
      "tensor([2])\n",
      "tensor([[ 0.5431,  0.3806, -0.7836]], grad_fn=<AddmmBackward0>)\n",
      "Batch 107, Loss: 0.7835791707038879\n",
      "tensor([1])\n",
      "tensor([[ 0.4634,  0.4262, -0.8963]], grad_fn=<AddmmBackward0>)\n",
      "Batch 108, Loss: -0.4262314438819885\n",
      "tensor([1])\n",
      "tensor([[ 0.5781,  0.3608, -0.7342]], grad_fn=<AddmmBackward0>)\n",
      "Batch 109, Loss: -0.36078310012817383\n",
      "tensor([1])\n",
      "tensor([[ 0.5871,  0.3558, -0.7216]], grad_fn=<AddmmBackward0>)\n",
      "Batch 110, Loss: -0.3557848036289215\n",
      "tensor([1])\n",
      "tensor([[ 0.5871,  0.3559, -0.7215]], grad_fn=<AddmmBackward0>)\n",
      "Batch 111, Loss: -0.3558652997016907\n",
      "tensor([2])\n",
      "tensor([[ 0.5099,  0.4001, -0.8307]], grad_fn=<AddmmBackward0>)\n",
      "Batch 112, Loss: 0.8306980133056641\n",
      "tensor([1])\n",
      "tensor([[ 0.5956,  0.3512, -0.7095]], grad_fn=<AddmmBackward0>)\n",
      "Batch 113, Loss: -0.3512446880340576\n",
      "tensor([0])\n",
      "tensor([[ 0.5302,  0.3887, -0.8020]], grad_fn=<AddmmBackward0>)\n",
      "Batch 114, Loss: -0.5302202105522156\n",
      "tensor([0])\n",
      "tensor([[ 0.5299,  0.3890, -0.8025]], grad_fn=<AddmmBackward0>)\n",
      "Batch 115, Loss: -0.5298690795898438\n",
      "tensor([1])\n",
      "tensor([[ 0.5693,  0.3666, -0.7468]], grad_fn=<AddmmBackward0>)\n",
      "Batch 116, Loss: -0.3666289150714874\n",
      "tensor([2])\n",
      "tensor([[ 0.5969,  0.3511, -0.7079]], grad_fn=<AddmmBackward0>)\n",
      "Batch 117, Loss: 0.707888126373291\n",
      "tensor([1])\n",
      "tensor([[ 0.5707,  0.3661, -0.7449]], grad_fn=<AddmmBackward0>)\n",
      "Batch 118, Loss: -0.366072416305542\n",
      "tensor([2])\n",
      "tensor([[ 0.5664,  0.3687, -0.7511]], grad_fn=<AddmmBackward0>)\n",
      "Batch 119, Loss: 0.7510737180709839\n",
      "tensor([0])\n",
      "tensor([[ 0.5285,  0.3904, -0.8046]], grad_fn=<AddmmBackward0>)\n",
      "Batch 120, Loss: -0.5284793972969055\n",
      "tensor([1])\n",
      "tensor([[ 0.5964,  0.3519, -0.7087]], grad_fn=<AddmmBackward0>)\n",
      "Batch 121, Loss: -0.35185688734054565\n",
      "tensor([2])\n",
      "tensor([[ 0.4432,  0.4391, -0.9252]], grad_fn=<AddmmBackward0>)\n",
      "Batch 122, Loss: 0.9251607060432434\n",
      "tensor([1])\n",
      "tensor([[ 0.5800,  0.3614, -0.7319]], grad_fn=<AddmmBackward0>)\n",
      "Batch 123, Loss: -0.36143961548805237\n",
      "tensor([2])\n",
      "tensor([[ 0.5161,  0.3979, -0.8221]], grad_fn=<AddmmBackward0>)\n",
      "Batch 124, Loss: 0.8221395015716553\n",
      "tensor([1])\n",
      "tensor([[ 0.6049,  0.3475, -0.6967]], grad_fn=<AddmmBackward0>)\n",
      "Batch 125, Loss: -0.3475246727466583\n",
      "tensor([2])\n",
      "tensor([[ 0.5235,  0.3939, -0.8116]], grad_fn=<AddmmBackward0>)\n",
      "Batch 126, Loss: 0.8116377592086792\n",
      "tensor([1])\n",
      "tensor([[ 0.5354,  0.3872, -0.7948]], grad_fn=<AddmmBackward0>)\n",
      "Batch 127, Loss: -0.38723891973495483\n",
      "tensor([0])\n",
      "tensor([[ 0.5703,  0.3676, -0.7455]], grad_fn=<AddmmBackward0>)\n",
      "Batch 128, Loss: -0.5702773332595825\n",
      "tensor([2])\n",
      "tensor([[ 0.5717,  0.3668, -0.7434]], grad_fn=<AddmmBackward0>)\n",
      "Batch 129, Loss: 0.7434346675872803\n",
      "tensor([2])\n",
      "tensor([[ 0.5005,  0.4074, -0.8441]], grad_fn=<AddmmBackward0>)\n",
      "Batch 130, Loss: 0.8440876007080078\n",
      "tensor([2])\n",
      "tensor([[ 0.5534,  0.3774, -0.7693]], grad_fn=<AddmmBackward0>)\n",
      "Batch 131, Loss: 0.7692615985870361\n",
      "tensor([2])\n",
      "tensor([[ 0.5953,  0.3538, -0.7101]], grad_fn=<AddmmBackward0>)\n",
      "Batch 132, Loss: 0.7101174592971802\n",
      "tensor([1])\n",
      "tensor([[ 0.5785,  0.3634, -0.7337]], grad_fn=<AddmmBackward0>)\n",
      "Batch 133, Loss: -0.36339280009269714\n",
      "tensor([2])\n",
      "tensor([[ 0.5925,  0.3555, -0.7138]], grad_fn=<AddmmBackward0>)\n",
      "Batch 134, Loss: 0.7138252854347229\n",
      "tensor([0])\n",
      "tensor([[ 0.5473,  0.3812, -0.7776]], grad_fn=<AddmmBackward0>)\n",
      "Batch 135, Loss: -0.547341525554657\n",
      "tensor([2])\n",
      "tensor([[ 0.5763,  0.3650, -0.7367]], grad_fn=<AddmmBackward0>)\n",
      "Batch 136, Loss: 0.7367298007011414\n",
      "tensor([2])\n",
      "tensor([[ 0.5626,  0.3728, -0.7560]], grad_fn=<AddmmBackward0>)\n",
      "Batch 137, Loss: 0.7560453414916992\n",
      "tensor([2])\n",
      "tensor([[ 0.5516,  0.3791, -0.7714]], grad_fn=<AddmmBackward0>)\n",
      "Batch 138, Loss: 0.7714341878890991\n",
      "tensor([1])\n",
      "tensor([[ 0.5164,  0.3990, -0.8211]], grad_fn=<AddmmBackward0>)\n",
      "Batch 139, Loss: -0.39902809262275696\n",
      "tensor([2])\n",
      "tensor([[ 0.4941,  0.4117, -0.8526]], grad_fn=<AddmmBackward0>)\n",
      "Batch 140, Loss: 0.8525865077972412\n",
      "tensor([1])\n",
      "tensor([[ 0.5961,  0.3541, -0.7083]], grad_fn=<AddmmBackward0>)\n",
      "Batch 141, Loss: -0.3540864884853363\n",
      "tensor([0])\n",
      "tensor([[ 0.4856,  0.4167, -0.8645]], grad_fn=<AddmmBackward0>)\n",
      "Batch 142, Loss: -0.4855945408344269\n",
      "tensor([1])\n",
      "tensor([[ 0.5852,  0.3605, -0.7236]], grad_fn=<AddmmBackward0>)\n",
      "Batch 143, Loss: -0.36045217514038086\n",
      "tensor([2])\n",
      "tensor([[ 0.5441,  0.3838, -0.7817]], grad_fn=<AddmmBackward0>)\n",
      "Batch 144, Loss: 0.7817052006721497\n",
      "tensor([2])\n",
      "tensor([[ 0.5765,  0.3656, -0.7358]], grad_fn=<AddmmBackward0>)\n",
      "Batch 145, Loss: 0.7358351349830627\n",
      "tensor([1])\n",
      "tensor([[ 0.5946,  0.3555, -0.7102]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 146, Loss: -0.3554653525352478\n",
      "tensor([2])\n",
      "tensor([[ 0.5411,  0.3857, -0.7857]], grad_fn=<AddmmBackward0>)\n",
      "Batch 147, Loss: 0.7857030034065247\n",
      "tensor([2])\n",
      "tensor([[ 0.3922,  0.4699, -0.9963]], grad_fn=<AddmmBackward0>)\n",
      "Batch 148, Loss: 0.9962707161903381\n",
      "tensor([1])\n",
      "tensor([[ 0.5138,  0.4013, -0.8242]], grad_fn=<AddmmBackward0>)\n",
      "Batch 149, Loss: -0.40131786465644836\n",
      "tensor([1])\n",
      "tensor([[ 0.5903,  0.3582, -0.7159]], grad_fn=<AddmmBackward0>)\n",
      "Batch 150, Loss: -0.35823771357536316\n",
      "tensor([2])\n",
      "tensor([[ 0.4844,  0.4181, -0.8656]], grad_fn=<AddmmBackward0>)\n",
      "Batch 151, Loss: 0.8656400442123413\n",
      "tensor([1])\n",
      "tensor([[ 0.5956,  0.3555, -0.7083]], grad_fn=<AddmmBackward0>)\n",
      "Batch 152, Loss: -0.3554688096046448\n",
      "tensor([1])\n",
      "tensor([[ 0.4800,  0.4208, -0.8718]], grad_fn=<AddmmBackward0>)\n",
      "Batch 153, Loss: -0.42077457904815674\n",
      "tensor([1])\n",
      "tensor([[ 0.6070,  0.3493, -0.6921]], grad_fn=<AddmmBackward0>)\n",
      "Batch 154, Loss: -0.34929758310317993\n",
      "tensor([0])\n",
      "tensor([[ 0.5717,  0.3693, -0.7419]], grad_fn=<AddmmBackward0>)\n",
      "Batch 155, Loss: -0.5716977715492249\n",
      "tensor([1])\n",
      "tensor([[ 0.5547,  0.3790, -0.7660]], grad_fn=<AddmmBackward0>)\n",
      "Batch 156, Loss: -0.3790134787559509\n",
      "tensor([0])\n",
      "tensor([[ 0.5674,  0.3720, -0.7480]], grad_fn=<AddmmBackward0>)\n",
      "Batch 157, Loss: -0.5674282908439636\n",
      "tensor([2])\n",
      "tensor([[ 0.5253,  0.3958, -0.8076]], grad_fn=<AddmmBackward0>)\n",
      "Batch 158, Loss: 0.807628870010376\n",
      "tensor([0])\n",
      "tensor([[ 0.5859,  0.3618, -0.7218]], grad_fn=<AddmmBackward0>)\n",
      "Batch 159, Loss: -0.5859227180480957\n",
      "tensor([0])\n",
      "tensor([[ 0.5666,  0.3728, -0.7492]], grad_fn=<AddmmBackward0>)\n",
      "Batch 160, Loss: -0.5666024088859558\n",
      "tensor([1])\n",
      "tensor([[ 0.5414,  0.3871, -0.7849]], grad_fn=<AddmmBackward0>)\n",
      "Batch 161, Loss: -0.38712015748023987\n",
      "tensor([0])\n",
      "tensor([[ 0.5587,  0.3775, -0.7605]], grad_fn=<AddmmBackward0>)\n",
      "Batch 162, Loss: -0.5586996078491211\n",
      "tensor([1])\n",
      "tensor([[ 0.5213,  0.3987, -0.8134]], grad_fn=<AddmmBackward0>)\n",
      "Batch 163, Loss: -0.3986506462097168\n",
      "tensor([0])\n",
      "tensor([[ 0.4901,  0.4163, -0.8577]], grad_fn=<AddmmBackward0>)\n",
      "Batch 164, Loss: -0.49006223678588867\n",
      "tensor([2])\n",
      "tensor([[ 0.5308,  0.3936, -0.8002]], grad_fn=<AddmmBackward0>)\n",
      "Batch 165, Loss: 0.8002267479896545\n",
      "tensor([2])\n",
      "tensor([[ 0.5958,  0.3572, -0.7083]], grad_fn=<AddmmBackward0>)\n",
      "Batch 166, Loss: 0.7083443403244019\n",
      "tensor([2])\n",
      "tensor([[ 0.6061,  0.3516, -0.6939]], grad_fn=<AddmmBackward0>)\n",
      "Batch 167, Loss: 0.693888783454895\n",
      "tensor([1])\n",
      "tensor([[ 0.5646,  0.3750, -0.7525]], grad_fn=<AddmmBackward0>)\n",
      "Batch 168, Loss: -0.3749575614929199\n",
      "tensor([1])\n",
      "tensor([[ 0.4823,  0.4212, -0.8688]], grad_fn=<AddmmBackward0>)\n",
      "Batch 169, Loss: -0.42119985818862915\n",
      "tensor([1])\n",
      "tensor([[ 0.5982,  0.3563, -0.7050]], grad_fn=<AddmmBackward0>)\n",
      "Batch 170, Loss: -0.3563130497932434\n",
      "tensor([1])\n",
      "tensor([[ 0.5839,  0.3645, -0.7253]], grad_fn=<AddmmBackward0>)\n",
      "Batch 171, Loss: -0.36449331045150757\n",
      "tensor([2])\n",
      "tensor([[ 0.5798,  0.3669, -0.7310]], grad_fn=<AddmmBackward0>)\n",
      "Batch 172, Loss: 0.7310019731521606\n",
      "tensor([0])\n",
      "tensor([[ 0.5874,  0.3628, -0.7204]], grad_fn=<AddmmBackward0>)\n",
      "Batch 173, Loss: -0.5873618125915527\n",
      "tensor([1])\n",
      "tensor([[ 0.4291,  0.4515, -0.9441]], grad_fn=<AddmmBackward0>)\n",
      "Batch 174, Loss: -0.4515363872051239\n",
      "tensor([0])\n",
      "tensor([[ 0.4786,  0.4239, -0.8741]], grad_fn=<AddmmBackward0>)\n",
      "Batch 175, Loss: -0.478609561920166\n",
      "tensor([2])\n",
      "tensor([[ 0.5849,  0.3646, -0.7239]], grad_fn=<AddmmBackward0>)\n",
      "Batch 176, Loss: 0.7239100933074951\n",
      "tensor([2])\n",
      "tensor([[ 0.6190,  0.3457, -0.6758]], grad_fn=<AddmmBackward0>)\n",
      "Batch 177, Loss: 0.6758179664611816\n",
      "tensor([0])\n",
      "tensor([[ 0.5838,  0.3654, -0.7255]], grad_fn=<AddmmBackward0>)\n",
      "Batch 178, Loss: -0.5837880373001099\n",
      "tensor([0])\n",
      "tensor([[ 0.5747,  0.3706, -0.7384]], grad_fn=<AddmmBackward0>)\n",
      "Batch 179, Loss: -0.5747084617614746\n",
      "tensor([1])\n",
      "tensor([[ 0.4552,  0.4375, -0.9073]], grad_fn=<AddmmBackward0>)\n",
      "Batch 180, Loss: -0.4375160336494446\n",
      "tensor([1])\n",
      "tensor([[ 0.5001,  0.4125, -0.8440]], grad_fn=<AddmmBackward0>)\n",
      "Batch 181, Loss: -0.41254815459251404\n",
      "tensor([1])\n",
      "tensor([[ 0.5294,  0.3963, -0.8026]], grad_fn=<AddmmBackward0>)\n",
      "Batch 182, Loss: -0.39627504348754883\n",
      "tensor([0])\n",
      "tensor([[ 0.6035,  0.3550, -0.6979]], grad_fn=<AddmmBackward0>)\n",
      "Batch 183, Loss: -0.6035146713256836\n",
      "tensor([2])\n",
      "tensor([[ 0.5460,  0.3873, -0.7793]], grad_fn=<AddmmBackward0>)\n",
      "Batch 184, Loss: 0.7793030142784119\n",
      "tensor([1])\n",
      "tensor([[ 0.5574,  0.3810, -0.7631]], grad_fn=<AddmmBackward0>)\n",
      "Batch 185, Loss: -0.3810020685195923\n",
      "tensor([2])\n",
      "tensor([[ 0.5908,  0.3625, -0.7160]], grad_fn=<AddmmBackward0>)\n",
      "Batch 186, Loss: 0.7159959077835083\n",
      "tensor([0])\n",
      "tensor([[ 0.5710,  0.3737, -0.7441]], grad_fn=<AddmmBackward0>)\n",
      "Batch 187, Loss: -0.5709773302078247\n",
      "tensor([1])\n",
      "tensor([[ 0.6061,  0.3542, -0.6945]], grad_fn=<AddmmBackward0>)\n",
      "Batch 188, Loss: -0.35419949889183044\n",
      "tensor([0])\n",
      "tensor([[ 0.4993,  0.4139, -0.8454]], grad_fn=<AddmmBackward0>)\n",
      "Batch 189, Loss: -0.49930793046951294\n",
      "tensor([2])\n",
      "tensor([[ 0.4512,  0.4408, -0.9135]], grad_fn=<AddmmBackward0>)\n",
      "Batch 190, Loss: 0.9134596586227417\n",
      "tensor([1])\n",
      "tensor([[ 0.6091,  0.3529, -0.6903]], grad_fn=<AddmmBackward0>)\n",
      "Batch 191, Loss: -0.352901816368103\n",
      "tensor([2])\n",
      "tensor([[ 0.5771,  0.3709, -0.7356]], grad_fn=<AddmmBackward0>)\n",
      "Batch 192, Loss: 0.7356372475624084\n",
      "tensor([0])\n",
      "tensor([[ 0.5596,  0.3808, -0.7604]], grad_fn=<AddmmBackward0>)\n",
      "Batch 193, Loss: -0.5595767498016357\n",
      "tensor([1])\n",
      "tensor([[ 0.6162,  0.3494, -0.6805]], grad_fn=<AddmmBackward0>)\n",
      "Batch 194, Loss: -0.349358469247818\n",
      "tensor([0])\n",
      "tensor([[ 0.3946,  0.4728, -0.9935]], grad_fn=<AddmmBackward0>)\n",
      "Batch 195, Loss: -0.3945840001106262\n",
      "tensor([2])\n",
      "tensor([[ 0.5404,  0.3918, -0.7876]], grad_fn=<AddmmBackward0>)\n",
      "Batch 196, Loss: 0.787639856338501\n",
      "tensor([2])\n",
      "tensor([[ 0.4577,  0.4379, -0.9044]], grad_fn=<AddmmBackward0>)\n",
      "Batch 197, Loss: 0.9044346809387207\n",
      "tensor([2])\n",
      "tensor([[ 0.6153,  0.3503, -0.6819]], grad_fn=<AddmmBackward0>)\n",
      "Batch 198, Loss: 0.6818733811378479\n",
      "tensor([2])\n",
      "tensor([[ 0.5957,  0.3613, -0.7096]], grad_fn=<AddmmBackward0>)\n",
      "Batch 199, Loss: 0.709557294845581\n",
      "tensor([1])\n",
      "tensor([[ 0.5887,  0.3653, -0.7195]], grad_fn=<AddmmBackward0>)\n",
      "Batch 200, Loss: -0.3653162717819214\n",
      "tensor([2])\n",
      "tensor([[ 0.5539,  0.3847, -0.7685]], grad_fn=<AddmmBackward0>)\n",
      "Batch 201, Loss: 0.7685036659240723\n",
      "tensor([2])\n",
      "tensor([[ 0.5942,  0.3624, -0.7116]], grad_fn=<AddmmBackward0>)\n",
      "Batch 202, Loss: 0.7115889191627502\n",
      "tensor([1])\n",
      "tensor([[ 0.3930,  0.4742, -0.9957]], grad_fn=<AddmmBackward0>)\n",
      "Batch 203, Loss: -0.474229097366333\n",
      "tensor([2])\n",
      "tensor([[ 0.5886,  0.3657, -0.7194]], grad_fn=<AddmmBackward0>)\n",
      "Batch 204, Loss: 0.7193998694419861\n",
      "tensor([2])\n",
      "tensor([[ 0.5358,  0.3951, -0.7939]], grad_fn=<AddmmBackward0>)\n",
      "Batch 205, Loss: 0.7939005494117737\n",
      "tensor([0])\n",
      "tensor([[ 0.5742,  0.3739, -0.7396]], grad_fn=<AddmmBackward0>)\n",
      "Batch 206, Loss: -0.5741602182388306\n",
      "tensor([2])\n",
      "tensor([[ 0.4701,  0.4317, -0.8867]], grad_fn=<AddmmBackward0>)\n",
      "Batch 207, Loss: 0.8866680860519409\n",
      "tensor([1])\n",
      "tensor([[ 0.5810,  0.3702, -0.7299]], grad_fn=<AddmmBackward0>)\n",
      "Batch 208, Loss: -0.3702203631401062\n",
      "tensor([1])\n",
      "tensor([[ 0.5687,  0.3771, -0.7471]], grad_fn=<AddmmBackward0>)\n",
      "Batch 209, Loss: -0.37710005044937134\n",
      "tensor([0])\n",
      "tensor([[ 0.5669,  0.3782, -0.7497]], grad_fn=<AddmmBackward0>)\n",
      "Batch 210, Loss: -0.5669329166412354\n",
      "tensor([1])\n",
      "tensor([[ 0.6014,  0.3592, -0.7009]], grad_fn=<AddmmBackward0>)\n",
      "Batch 211, Loss: -0.3591649532318115\n",
      "tensor([1])\n",
      "tensor([[ 0.6041,  0.3578, -0.6971]], grad_fn=<AddmmBackward0>)\n",
      "Batch 212, Loss: -0.3577861487865448\n",
      "tensor([2])\n",
      "tensor([[ 0.6011,  0.3596, -0.7014]], grad_fn=<AddmmBackward0>)\n",
      "Batch 213, Loss: 0.7013508081436157\n",
      "tensor([2])\n",
      "tensor([[ 0.6038,  0.3582, -0.6975]], grad_fn=<AddmmBackward0>)\n",
      "Batch 214, Loss: 0.6974982023239136\n",
      "tensor([2])\n",
      "tensor([[ 0.4190,  0.4607, -0.9587]], grad_fn=<AddmmBackward0>)\n",
      "Batch 215, Loss: 0.9587427973747253\n",
      "tensor([1])\n",
      "tensor([[ 0.5912,  0.3654, -0.7152]], grad_fn=<AddmmBackward0>)\n",
      "Batch 216, Loss: -0.3653913736343384\n",
      "tensor([2])\n",
      "tensor([[ 0.5945,  0.3637, -0.7105]], grad_fn=<AddmmBackward0>)\n",
      "Batch 217, Loss: 0.7104704976081848\n",
      "tensor([2])\n",
      "tensor([[ 0.6002,  0.3606, -0.7023]], grad_fn=<AddmmBackward0>)\n",
      "Batch 218, Loss: 0.7023342847824097\n",
      "tensor([0])\n",
      "tensor([[ 0.5756,  0.3743, -0.7371]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 219, Loss: -0.5755831003189087\n",
      "tensor([1])\n",
      "tensor([[ 0.5949,  0.3638, -0.7098]], grad_fn=<AddmmBackward0>)\n",
      "Batch 220, Loss: -0.36375167965888977\n",
      "tensor([1])\n",
      "tensor([[ 0.5855,  0.3691, -0.7231]], grad_fn=<AddmmBackward0>)\n",
      "Batch 221, Loss: -0.369052529335022\n",
      "tensor([2])\n",
      "tensor([[ 0.6090,  0.3562, -0.6898]], grad_fn=<AddmmBackward0>)\n",
      "Batch 222, Loss: 0.6898277997970581\n",
      "tensor([2])\n",
      "tensor([[ 0.5921,  0.3656, -0.7137]], grad_fn=<AddmmBackward0>)\n",
      "Batch 223, Loss: 0.7136733531951904\n",
      "tensor([1])\n",
      "tensor([[ 0.5865,  0.3688, -0.7214]], grad_fn=<AddmmBackward0>)\n",
      "Batch 224, Loss: -0.36877503991127014\n",
      "tensor([2])\n",
      "tensor([[ 0.5860,  0.3692, -0.7221]], grad_fn=<AddmmBackward0>)\n",
      "Batch 225, Loss: 0.7221145629882812\n",
      "tensor([1])\n",
      "tensor([[ 0.4136,  0.4644, -0.9658]], grad_fn=<AddmmBackward0>)\n",
      "Batch 226, Loss: -0.46444204449653625\n",
      "tensor([0])\n",
      "tensor([[ 0.5280,  0.4014, -0.8039]], grad_fn=<AddmmBackward0>)\n",
      "Batch 227, Loss: -0.5280389785766602\n",
      "tensor([2])\n",
      "tensor([[ 0.6106,  0.3559, -0.6871]], grad_fn=<AddmmBackward0>)\n",
      "Batch 228, Loss: 0.6871065497398376\n",
      "tensor([2])\n",
      "tensor([[ 0.5959,  0.3641, -0.7079]], grad_fn=<AddmmBackward0>)\n",
      "Batch 229, Loss: 0.7079352140426636\n",
      "tensor([1])\n",
      "tensor([[ 0.5662,  0.3806, -0.7498]], grad_fn=<AddmmBackward0>)\n",
      "Batch 230, Loss: -0.380583792924881\n",
      "tensor([0])\n",
      "tensor([[ 0.5090,  0.4122, -0.8307]], grad_fn=<AddmmBackward0>)\n",
      "Batch 231, Loss: -0.5089865922927856\n",
      "tensor([0])\n",
      "tensor([[ 0.5947,  0.3651, -0.7095]], grad_fn=<AddmmBackward0>)\n",
      "Batch 232, Loss: -0.5946609973907471\n",
      "tensor([2])\n",
      "tensor([[ 0.4916,  0.4220, -0.8554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 233, Loss: 0.8553879261016846\n",
      "tensor([2])\n",
      "tensor([[ 0.4855,  0.4254, -0.8639]], grad_fn=<AddmmBackward0>)\n",
      "Batch 234, Loss: 0.8639153838157654\n",
      "tensor([2])\n",
      "tensor([[ 0.5224,  0.4052, -0.8118]], grad_fn=<AddmmBackward0>)\n",
      "Batch 235, Loss: 0.811763346195221\n",
      "tensor([0])\n",
      "tensor([[ 0.5999,  0.3625, -0.7020]], grad_fn=<AddmmBackward0>)\n",
      "Batch 236, Loss: -0.5999465584754944\n",
      "tensor([2])\n",
      "tensor([[ 0.6186,  0.3524, -0.6756]], grad_fn=<AddmmBackward0>)\n",
      "Batch 237, Loss: 0.6756346821784973\n",
      "tensor([0])\n",
      "tensor([[ 0.5673,  0.3807, -0.7482]], grad_fn=<AddmmBackward0>)\n",
      "Batch 238, Loss: -0.5672585368156433\n",
      "tensor([2])\n",
      "tensor([[ 0.5939,  0.3661, -0.7106]], grad_fn=<AddmmBackward0>)\n",
      "Batch 239, Loss: 0.7106063365936279\n",
      "tensor([0])\n",
      "tensor([[ 0.6082,  0.3583, -0.6904]], grad_fn=<AddmmBackward0>)\n",
      "Batch 240, Loss: -0.608158528804779\n",
      "tensor([2])\n",
      "tensor([[ 0.4828,  0.4274, -0.8678]], grad_fn=<AddmmBackward0>)\n",
      "Batch 241, Loss: 0.8678094744682312\n",
      "tensor([0])\n",
      "tensor([[ 0.5425,  0.3946, -0.7832]], grad_fn=<AddmmBackward0>)\n",
      "Batch 242, Loss: -0.5425481796264648\n",
      "tensor([1])\n",
      "tensor([[ 0.5826,  0.3726, -0.7266]], grad_fn=<AddmmBackward0>)\n",
      "Batch 243, Loss: -0.3726240396499634\n",
      "tensor([2])\n",
      "tensor([[ 0.4976,  0.4195, -0.8470]], grad_fn=<AddmmBackward0>)\n",
      "Batch 244, Loss: 0.8469535112380981\n",
      "tensor([1])\n",
      "tensor([[ 0.5566,  0.3871, -0.7635]], grad_fn=<AddmmBackward0>)\n",
      "Batch 245, Loss: -0.38712823390960693\n",
      "tensor([0])\n",
      "tensor([[ 0.6263,  0.3489, -0.6649]], grad_fn=<AddmmBackward0>)\n",
      "Batch 246, Loss: -0.6262938380241394\n",
      "tensor([1])\n",
      "tensor([[ 0.5986,  0.3642, -0.7042]], grad_fn=<AddmmBackward0>)\n",
      "Batch 247, Loss: -0.3642314672470093\n",
      "tensor([2])\n",
      "tensor([[ 0.6148,  0.3554, -0.6813]], grad_fn=<AddmmBackward0>)\n",
      "Batch 248, Loss: 0.6812530159950256\n",
      "tensor([0])\n",
      "tensor([[ 0.6044,  0.3613, -0.6960]], grad_fn=<AddmmBackward0>)\n",
      "Batch 249, Loss: -0.6043636202812195\n",
      "tensor([2])\n",
      "tensor([[ 0.4875,  0.4255, -0.8614]], grad_fn=<AddmmBackward0>)\n",
      "Batch 250, Loss: 0.8613643050193787\n",
      "tensor([2])\n",
      "tensor([[ 0.6173,  0.3544, -0.6778]], grad_fn=<AddmmBackward0>)\n",
      "Batch 251, Loss: 0.6778194904327393\n",
      "tensor([0])\n",
      "tensor([[ 0.5681,  0.3814, -0.7473]], grad_fn=<AddmmBackward0>)\n",
      "Batch 252, Loss: -0.5681450366973877\n",
      "tensor([0])\n",
      "tensor([[ 0.5837,  0.3730, -0.7253]], grad_fn=<AddmmBackward0>)\n",
      "Batch 253, Loss: -0.5837480425834656\n",
      "tensor([1])\n",
      "tensor([[ 0.5578,  0.3873, -0.7620]], grad_fn=<AddmmBackward0>)\n",
      "Batch 254, Loss: -0.38728177547454834\n",
      "tensor([1])\n",
      "tensor([[ 0.4802,  0.4299, -0.8718]], grad_fn=<AddmmBackward0>)\n",
      "Batch 255, Loss: -0.4299314320087433\n",
      "tensor([2])\n",
      "tensor([[ 0.5856,  0.3723, -0.7230]], grad_fn=<AddmmBackward0>)\n",
      "Batch 256, Loss: 0.722952663898468\n",
      "tensor([1])\n",
      "tensor([[ 0.5079,  0.4150, -0.8328]], grad_fn=<AddmmBackward0>)\n",
      "Batch 257, Loss: -0.4149746596813202\n",
      "tensor([0])\n",
      "tensor([[ 0.6140,  0.3569, -0.6828]], grad_fn=<AddmmBackward0>)\n",
      "Batch 258, Loss: -0.614024817943573\n",
      "tensor([2])\n",
      "tensor([[ 0.5529,  0.3905, -0.7692]], grad_fn=<AddmmBackward0>)\n",
      "Batch 259, Loss: 0.7692219018936157\n",
      "tensor([1])\n",
      "tensor([[ 0.4654,  0.4385, -0.8929]], grad_fn=<AddmmBackward0>)\n",
      "Batch 260, Loss: -0.43850916624069214\n",
      "tensor([1])\n",
      "tensor([[ 0.4704,  0.4359, -0.8858]], grad_fn=<AddmmBackward0>)\n",
      "Batch 261, Loss: -0.43586990237236023\n",
      "tensor([1])\n",
      "tensor([[ 0.5943,  0.3682, -0.7108]], grad_fn=<AddmmBackward0>)\n",
      "Batch 262, Loss: -0.3681964576244354\n",
      "tensor([1])\n",
      "tensor([[ 0.4994,  0.4202, -0.8449]], grad_fn=<AddmmBackward0>)\n",
      "Batch 263, Loss: -0.4202103912830353\n",
      "tensor([2])\n",
      "tensor([[ 0.5648,  0.3846, -0.7526]], grad_fn=<AddmmBackward0>)\n",
      "Batch 264, Loss: 0.7526100873947144\n",
      "tensor([0])\n",
      "tensor([[ 0.5863,  0.3730, -0.7222]], grad_fn=<AddmmBackward0>)\n",
      "Batch 265, Loss: -0.5862800478935242\n",
      "tensor([2])\n",
      "tensor([[ 0.6090,  0.3607, -0.6901]], grad_fn=<AddmmBackward0>)\n",
      "Batch 266, Loss: 0.69012451171875\n",
      "tensor([1])\n",
      "tensor([[ 0.5255,  0.4064, -0.8081]], grad_fn=<AddmmBackward0>)\n",
      "Batch 267, Loss: -0.40639713406562805\n",
      "tensor([1])\n",
      "tensor([[ 0.5858,  0.3736, -0.7230]], grad_fn=<AddmmBackward0>)\n",
      "Batch 268, Loss: -0.3736085891723633\n",
      "tensor([0])\n",
      "tensor([[ 0.6202,  0.3549, -0.6743]], grad_fn=<AddmmBackward0>)\n",
      "Batch 269, Loss: -0.620233416557312\n",
      "tensor([2])\n",
      "tensor([[ 0.6462,  0.3409, -0.6377]], grad_fn=<AddmmBackward0>)\n",
      "Batch 270, Loss: 0.6376889944076538\n",
      "tensor([1])\n",
      "tensor([[ 0.6086,  0.3615, -0.6908]], grad_fn=<AddmmBackward0>)\n",
      "Batch 271, Loss: -0.36151421070098877\n",
      "tensor([2])\n",
      "tensor([[ 0.6123,  0.3596, -0.6856]], grad_fn=<AddmmBackward0>)\n",
      "Batch 272, Loss: 0.6855616569519043\n",
      "tensor([1])\n",
      "tensor([[ 0.5920,  0.3708, -0.7143]], grad_fn=<AddmmBackward0>)\n",
      "Batch 273, Loss: -0.3708285689353943\n",
      "tensor([1])\n",
      "tensor([[ 0.6095,  0.3614, -0.6896]], grad_fn=<AddmmBackward0>)\n",
      "Batch 274, Loss: -0.3614332973957062\n",
      "tensor([1])\n",
      "tensor([[ 0.5207,  0.4099, -0.8149]], grad_fn=<AddmmBackward0>)\n",
      "Batch 275, Loss: -0.4099302887916565\n",
      "tensor([1])\n",
      "tensor([[ 0.4761,  0.4344, -0.8779]], grad_fn=<AddmmBackward0>)\n",
      "Batch 276, Loss: -0.43435314297676086\n",
      "tensor([1])\n",
      "tensor([[ 0.4481,  0.4498, -0.9175]], grad_fn=<AddmmBackward0>)\n",
      "Batch 277, Loss: -0.4497610628604889\n",
      "tensor([1])\n",
      "tensor([[ 0.6172,  0.3578, -0.6787]], grad_fn=<AddmmBackward0>)\n",
      "Batch 278, Loss: -0.357847660779953\n",
      "tensor([2])\n",
      "tensor([[ 0.6182,  0.3574, -0.6772]], grad_fn=<AddmmBackward0>)\n",
      "Batch 279, Loss: 0.6771799325942993\n",
      "tensor([2])\n",
      "tensor([[ 0.6353,  0.3483, -0.6531]], grad_fn=<AddmmBackward0>)\n",
      "Batch 280, Loss: 0.6530786752700806\n",
      "tensor([0])\n",
      "tensor([[ 0.5873,  0.3745, -0.7208]], grad_fn=<AddmmBackward0>)\n",
      "Batch 281, Loss: -0.5873317122459412\n",
      "tensor([1])\n",
      "tensor([[ 0.6438,  0.3439, -0.6410]], grad_fn=<AddmmBackward0>)\n",
      "Batch 282, Loss: -0.34393709897994995\n",
      "tensor([2])\n",
      "tensor([[ 0.3811,  0.4868, -1.0120]], grad_fn=<AddmmBackward0>)\n",
      "Batch 283, Loss: 1.011983871459961\n",
      "tensor([2])\n",
      "tensor([[ 0.6313,  0.3510, -0.6587]], grad_fn=<AddmmBackward0>)\n",
      "Batch 284, Loss: 0.6586769819259644\n",
      "tensor([1])\n",
      "tensor([[ 0.4691,  0.4392, -0.8877]], grad_fn=<AddmmBackward0>)\n",
      "Batch 285, Loss: -0.43920791149139404\n",
      "tensor([1])\n",
      "tensor([[ 0.6295,  0.3522, -0.6611]], grad_fn=<AddmmBackward0>)\n",
      "Batch 286, Loss: -0.35222670435905457\n",
      "tensor([1])\n",
      "tensor([[ 0.5495,  0.3958, -0.7740]], grad_fn=<AddmmBackward0>)\n",
      "Batch 287, Loss: -0.3957735300064087\n",
      "tensor([2])\n",
      "tensor([[ 0.6000,  0.3685, -0.7027]], grad_fn=<AddmmBackward0>)\n",
      "Batch 288, Loss: 0.7026792764663696\n",
      "tensor([1])\n",
      "tensor([[ 0.5762,  0.3815, -0.7362]], grad_fn=<AddmmBackward0>)\n",
      "Batch 289, Loss: -0.3815327286720276\n",
      "tensor([0])\n",
      "tensor([[ 0.6052,  0.3660, -0.6953]], grad_fn=<AddmmBackward0>)\n",
      "Batch 290, Loss: -0.6051534414291382\n",
      "tensor([2])\n",
      "tensor([[ 0.5399,  0.4015, -0.7876]], grad_fn=<AddmmBackward0>)\n",
      "Batch 291, Loss: 0.7875527143478394\n",
      "tensor([1])\n",
      "tensor([[ 0.6054,  0.3661, -0.6949]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 292, Loss: -0.3661046028137207\n",
      "tensor([2])\n",
      "tensor([[ 0.6233,  0.3566, -0.6697]], grad_fn=<AddmmBackward0>)\n",
      "Batch 293, Loss: 0.6697278022766113\n",
      "tensor([1])\n",
      "tensor([[ 0.6044,  0.3669, -0.6963]], grad_fn=<AddmmBackward0>)\n",
      "Batch 294, Loss: -0.3668983280658722\n",
      "tensor([0])\n",
      "tensor([[ 0.6122,  0.3628, -0.6853]], grad_fn=<AddmmBackward0>)\n",
      "Batch 295, Loss: -0.6122077703475952\n",
      "tensor([1])\n",
      "tensor([[ 0.6101,  0.3641, -0.6882]], grad_fn=<AddmmBackward0>)\n",
      "Batch 296, Loss: -0.36407649517059326\n",
      "tensor([0])\n",
      "tensor([[ 0.5650,  0.3886, -0.7520]], grad_fn=<AddmmBackward0>)\n",
      "Batch 297, Loss: -0.5649808645248413\n",
      "tensor([0])\n",
      "tensor([[ 0.6078,  0.3656, -0.6916]], grad_fn=<AddmmBackward0>)\n",
      "Batch 298, Loss: -0.607784628868103\n",
      "tensor([2])\n",
      "tensor([[ 0.6006,  0.3696, -0.7018]], grad_fn=<AddmmBackward0>)\n",
      "Batch 299, Loss: 0.7017620205879211\n",
      "tensor([1])\n",
      "tensor([[ 0.6113,  0.3640, -0.6867]], grad_fn=<AddmmBackward0>)\n",
      "Batch 300, Loss: -0.3639835715293884\n",
      "tensor([0])\n",
      "tensor([[ 0.4896,  0.4299, -0.8586]], grad_fn=<AddmmBackward0>)\n",
      "Batch 301, Loss: -0.48959800601005554\n",
      "tensor([2])\n",
      "tensor([[ 0.6231,  0.3579, -0.6701]], grad_fn=<AddmmBackward0>)\n",
      "Batch 302, Loss: 0.6701309680938721\n",
      "tensor([0])\n",
      "tensor([[ 0.6191,  0.3602, -0.6759]], grad_fn=<AddmmBackward0>)\n",
      "Batch 303, Loss: -0.6190789937973022\n",
      "tensor([0])\n",
      "tensor([[ 0.6045,  0.3682, -0.6965]], grad_fn=<AddmmBackward0>)\n",
      "Batch 304, Loss: -0.6045204401016235\n",
      "tensor([2])\n",
      "tensor([[ 0.5464,  0.3996, -0.7786]], grad_fn=<AddmmBackward0>)\n",
      "Batch 305, Loss: 0.7786030769348145\n",
      "tensor([2])\n",
      "tensor([[ 0.5945,  0.3738, -0.7108]], grad_fn=<AddmmBackward0>)\n",
      "Batch 306, Loss: 0.7108280658721924\n",
      "tensor([2])\n",
      "tensor([[ 0.5776,  0.3831, -0.7348]], grad_fn=<AddmmBackward0>)\n",
      "Batch 307, Loss: 0.7347921133041382\n",
      "tensor([2])\n",
      "tensor([[ 0.6209,  0.3598, -0.6736]], grad_fn=<AddmmBackward0>)\n",
      "Batch 308, Loss: 0.6735741496086121\n",
      "tensor([0])\n",
      "tensor([[ 0.6442,  0.3473, -0.6407]], grad_fn=<AddmmBackward0>)\n",
      "Batch 309, Loss: -0.6442059874534607\n",
      "tensor([0])\n",
      "tensor([[ 0.4433,  0.4556, -0.9244]], grad_fn=<AddmmBackward0>)\n",
      "Batch 310, Loss: -0.44328075647354126\n",
      "tensor([1])\n",
      "tensor([[ 0.6317,  0.3542, -0.6584]], grad_fn=<AddmmBackward0>)\n",
      "Batch 311, Loss: -0.35424119234085083\n",
      "tensor([1])\n",
      "tensor([[ 0.6482,  0.3455, -0.6353]], grad_fn=<AddmmBackward0>)\n",
      "Batch 312, Loss: -0.3455163240432739\n",
      "tensor([2])\n",
      "tensor([[ 0.4300,  0.4630, -0.9432]], grad_fn=<AddmmBackward0>)\n",
      "Batch 313, Loss: 0.9431726932525635\n",
      "tensor([1])\n",
      "tensor([[ 0.5553,  0.3957, -0.7665]], grad_fn=<AddmmBackward0>)\n",
      "Batch 314, Loss: -0.3957101106643677\n",
      "tensor([0])\n",
      "tensor([[ 0.6233,  0.3593, -0.6705]], grad_fn=<AddmmBackward0>)\n",
      "Batch 315, Loss: -0.6232671737670898\n",
      "tensor([2])\n",
      "tensor([[ 0.6420,  0.3493, -0.6441]], grad_fn=<AddmmBackward0>)\n",
      "Batch 316, Loss: 0.6440883874893188\n",
      "tensor([2])\n",
      "tensor([[ 0.6066,  0.3684, -0.6941]], grad_fn=<AddmmBackward0>)\n",
      "Batch 317, Loss: 0.6940520405769348\n",
      "tensor([0])\n",
      "tensor([[ 0.6258,  0.3582, -0.6670]], grad_fn=<AddmmBackward0>)\n",
      "Batch 318, Loss: -0.625827431678772\n",
      "tensor([1])\n",
      "tensor([[ 0.5134,  0.4187, -0.8256]], grad_fn=<AddmmBackward0>)\n",
      "Batch 319, Loss: -0.4186643362045288\n",
      "tensor([2])\n",
      "tensor([[ 0.6318,  0.3553, -0.6587]], grad_fn=<AddmmBackward0>)\n",
      "Batch 320, Loss: 0.6587017774581909\n",
      "tensor([1])\n",
      "tensor([[ 0.6377,  0.3522, -0.6503]], grad_fn=<AddmmBackward0>)\n",
      "Batch 321, Loss: -0.3521874248981476\n",
      "tensor([0])\n",
      "tensor([[ 0.6187,  0.3625, -0.6772]], grad_fn=<AddmmBackward0>)\n",
      "Batch 322, Loss: -0.6186724901199341\n",
      "tensor([0])\n",
      "tensor([[ 0.6083,  0.3682, -0.6919]], grad_fn=<AddmmBackward0>)\n",
      "Batch 323, Loss: -0.6083085536956787\n",
      "tensor([0])\n",
      "tensor([[ 0.6504,  0.3458, -0.6326]], grad_fn=<AddmmBackward0>)\n",
      "Batch 324, Loss: -0.6503911018371582\n",
      "tensor([1])\n",
      "tensor([[ 0.6145,  0.3651, -0.6834]], grad_fn=<AddmmBackward0>)\n",
      "Batch 325, Loss: -0.3651323914527893\n",
      "tensor([0])\n",
      "tensor([[ 0.6249,  0.3597, -0.6687]], grad_fn=<AddmmBackward0>)\n",
      "Batch 326, Loss: -0.624927282333374\n",
      "tensor([2])\n",
      "tensor([[ 0.6009,  0.3727, -0.7027]], grad_fn=<AddmmBackward0>)\n",
      "Batch 327, Loss: 0.7027127742767334\n",
      "tensor([0])\n",
      "tensor([[ 0.5453,  0.4025, -0.7812]], grad_fn=<AddmmBackward0>)\n",
      "Batch 328, Loss: -0.5453062057495117\n",
      "tensor([2])\n",
      "tensor([[ 0.5882,  0.3797, -0.7208]], grad_fn=<AddmmBackward0>)\n",
      "Batch 329, Loss: 0.7208428978919983\n",
      "tensor([1])\n",
      "tensor([[ 0.5738,  0.3875, -0.7412]], grad_fn=<AddmmBackward0>)\n",
      "Batch 330, Loss: -0.3874902129173279\n",
      "tensor([0])\n",
      "tensor([[ 0.5858,  0.3812, -0.7243]], grad_fn=<AddmmBackward0>)\n",
      "Batch 331, Loss: -0.5858272910118103\n",
      "tensor([2])\n",
      "tensor([[ 0.5660,  0.3919, -0.7524]], grad_fn=<AddmmBackward0>)\n",
      "Batch 332, Loss: 0.7524313926696777\n",
      "tensor([0])\n",
      "tensor([[ 0.5917,  0.3783, -0.7162]], grad_fn=<AddmmBackward0>)\n",
      "Batch 333, Loss: -0.5917298197746277\n",
      "tensor([0])\n",
      "tensor([[ 0.6274,  0.3593, -0.6661]], grad_fn=<AddmmBackward0>)\n",
      "Batch 334, Loss: -0.627351701259613\n",
      "tensor([2])\n",
      "tensor([[ 0.6186,  0.3641, -0.6785]], grad_fn=<AddmmBackward0>)\n",
      "Batch 335, Loss: 0.6784749031066895\n",
      "tensor([0])\n",
      "tensor([[ 0.6578,  0.3433, -0.6234]], grad_fn=<AddmmBackward0>)\n",
      "Batch 336, Loss: -0.6577742099761963\n",
      "tensor([1])\n",
      "tensor([[ 0.6634,  0.3405, -0.6157]], grad_fn=<AddmmBackward0>)\n",
      "Batch 337, Loss: -0.34046393632888794\n",
      "tensor([2])\n",
      "tensor([[ 0.6246,  0.3613, -0.6704]], grad_fn=<AddmmBackward0>)\n",
      "Batch 338, Loss: 0.6704013347625732\n",
      "tensor([0])\n",
      "tensor([[ 0.6671,  0.3387, -0.6106]], grad_fn=<AddmmBackward0>)\n",
      "Batch 339, Loss: -0.6671051383018494\n",
      "tensor([0])\n",
      "tensor([[ 0.6956,  0.3237, -0.5707]], grad_fn=<AddmmBackward0>)\n",
      "Batch 340, Loss: -0.6955602169036865\n",
      "tensor([2])\n",
      "tensor([[ 0.6088,  0.3700, -0.6930]], grad_fn=<AddmmBackward0>)\n",
      "Batch 341, Loss: 0.6929636001586914\n",
      "tensor([2])\n",
      "tensor([[ 0.5962,  0.3769, -0.7109]], grad_fn=<AddmmBackward0>)\n",
      "Batch 342, Loss: 0.7108503580093384\n",
      "tensor([0])\n",
      "tensor([[ 0.6536,  0.3464, -0.6300]], grad_fn=<AddmmBackward0>)\n",
      "Batch 343, Loss: -0.6536425948143005\n",
      "tensor([2])\n",
      "tensor([[ 0.4852,  0.4361, -0.8672]], grad_fn=<AddmmBackward0>)\n",
      "Batch 344, Loss: 0.8671732544898987\n",
      "tensor([1])\n",
      "tensor([[ 0.7118,  0.3156, -0.5484]], grad_fn=<AddmmBackward0>)\n",
      "Batch 345, Loss: -0.315643310546875\n",
      "tensor([1])\n",
      "tensor([[ 0.5206,  0.4174, -0.8175]], grad_fn=<AddmmBackward0>)\n",
      "Batch 346, Loss: -0.4174381494522095\n",
      "tensor([0])\n",
      "tensor([[ 0.4847,  0.4366, -0.8680]], grad_fn=<AddmmBackward0>)\n",
      "Batch 347, Loss: -0.4847097396850586\n",
      "tensor([2])\n",
      "tensor([[ 0.5029,  0.4270, -0.8424]], grad_fn=<AddmmBackward0>)\n",
      "Batch 348, Loss: 0.8424202799797058\n",
      "tensor([1])\n",
      "tensor([[ 0.5329,  0.4112, -0.8004]], grad_fn=<AddmmBackward0>)\n",
      "Batch 349, Loss: -0.41120070219039917\n",
      "tensor([0])\n",
      "tensor([[ 0.6452,  0.3517, -0.6424]], grad_fn=<AddmmBackward0>)\n",
      "Batch 350, Loss: -0.6451799869537354\n",
      "tensor([0])\n",
      "tensor([[ 0.4912,  0.4335, -0.8590]], grad_fn=<AddmmBackward0>)\n",
      "Batch 351, Loss: -0.4912257492542267\n",
      "tensor([2])\n",
      "tensor([[ 0.6581,  0.3451, -0.6245]], grad_fn=<AddmmBackward0>)\n",
      "Batch 352, Loss: 0.624483048915863\n",
      "tensor([0])\n",
      "tensor([[ 0.6797,  0.3337, -0.5941]], grad_fn=<AddmmBackward0>)\n",
      "Batch 353, Loss: -0.6797378063201904\n",
      "tensor([0])\n",
      "tensor([[ 0.7082,  0.3187, -0.5542]], grad_fn=<AddmmBackward0>)\n",
      "Batch 354, Loss: -0.7082188725471497\n",
      "tensor([1])\n",
      "tensor([[ 0.7064,  0.3198, -0.5569]], grad_fn=<AddmmBackward0>)\n",
      "Batch 355, Loss: -0.3198288083076477\n",
      "tensor([1])\n",
      "tensor([[ 0.6569,  0.3462, -0.6266]], grad_fn=<AddmmBackward0>)\n",
      "Batch 356, Loss: -0.34618067741394043\n",
      "tensor([1])\n",
      "tensor([[ 0.4658,  0.4475, -0.8952]], grad_fn=<AddmmBackward0>)\n",
      "Batch 357, Loss: -0.4475287199020386\n",
      "tensor([1])\n",
      "tensor([[ 0.6126,  0.3699, -0.6890]], grad_fn=<AddmmBackward0>)\n",
      "Batch 358, Loss: -0.369913786649704\n",
      "tensor([2])\n",
      "tensor([[ 0.4905,  0.4347, -0.8606]], grad_fn=<AddmmBackward0>)\n",
      "Batch 359, Loss: 0.8606307506561279\n",
      "tensor([0])\n",
      "tensor([[ 0.3865,  0.4898, -1.0067]], grad_fn=<AddmmBackward0>)\n",
      "Batch 360, Loss: -0.3865203261375427\n",
      "tensor([1])\n",
      "tensor([[ 0.6242,  0.3642, -0.6729]], grad_fn=<AddmmBackward0>)\n",
      "Batch 361, Loss: -0.3642158508300781\n",
      "tensor([1])\n",
      "tensor([[ 0.6690,  0.3407, -0.6101]], grad_fn=<AddmmBackward0>)\n",
      "Batch 362, Loss: -0.34068694710731506\n",
      "tensor([1])\n",
      "tensor([[ 0.5746,  0.3907, -0.7427]], grad_fn=<AddmmBackward0>)\n",
      "Batch 363, Loss: -0.39072656631469727\n",
      "tensor([1])\n",
      "tensor([[ 0.5969,  0.3791, -0.7115]], grad_fn=<AddmmBackward0>)\n",
      "Batch 364, Loss: -0.37911590933799744\n",
      "tensor([0])\n",
      "tensor([[ 0.7162,  0.3163, -0.5440]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 365, Loss: -0.716219425201416\n",
      "tensor([0])\n",
      "tensor([[ 0.6247,  0.3648, -0.6726]], grad_fn=<AddmmBackward0>)\n",
      "Batch 366, Loss: -0.6246584057807922\n",
      "tensor([0])\n",
      "tensor([[ 0.6712,  0.3404, -0.6073]], grad_fn=<AddmmBackward0>)\n",
      "Batch 367, Loss: -0.6712478995323181\n",
      "tensor([0])\n",
      "tensor([[ 0.5221,  0.4192, -0.8169]], grad_fn=<AddmmBackward0>)\n",
      "Batch 368, Loss: -0.5220502018928528\n",
      "tensor([0])\n",
      "tensor([[ 0.6990,  0.3261, -0.5687]], grad_fn=<AddmmBackward0>)\n",
      "Batch 369, Loss: -0.6989967226982117\n",
      "tensor([0])\n",
      "tensor([[ 0.6698,  0.3416, -0.6098]], grad_fn=<AddmmBackward0>)\n",
      "Batch 370, Loss: -0.6698204278945923\n",
      "tensor([0])\n",
      "tensor([[ 0.6345,  0.3604, -0.6596]], grad_fn=<AddmmBackward0>)\n",
      "Batch 371, Loss: -0.6345041394233704\n",
      "tensor([1])\n",
      "tensor([[ 0.6867,  0.3331, -0.5867]], grad_fn=<AddmmBackward0>)\n",
      "Batch 372, Loss: -0.3331124186515808\n",
      "tensor([0])\n",
      "tensor([[ 0.5860,  0.3862, -0.7280]], grad_fn=<AddmmBackward0>)\n",
      "Batch 373, Loss: -0.5860358476638794\n",
      "tensor([1])\n",
      "tensor([[ 0.6195,  0.3688, -0.6813]], grad_fn=<AddmmBackward0>)\n",
      "Batch 374, Loss: -0.36876460909843445\n",
      "tensor([0])\n",
      "tensor([[ 0.6536,  0.3510, -0.6336]], grad_fn=<AddmmBackward0>)\n",
      "Batch 375, Loss: -0.6536070108413696\n",
      "tensor([2])\n",
      "tensor([[ 0.6619,  0.3468, -0.6222]], grad_fn=<AddmmBackward0>)\n",
      "Batch 376, Loss: 0.6222038269042969\n",
      "tensor([1])\n",
      "tensor([[ 0.6862,  0.3342, -0.5883]], grad_fn=<AddmmBackward0>)\n",
      "Batch 377, Loss: -0.3342122733592987\n",
      "tensor([0])\n",
      "tensor([[ 0.6806,  0.3374, -0.5964]], grad_fn=<AddmmBackward0>)\n",
      "Batch 378, Loss: -0.680568277835846\n",
      "tensor([0])\n",
      "tensor([[ 0.6918,  0.3317, -0.5809]], grad_fn=<AddmmBackward0>)\n",
      "Batch 379, Loss: -0.6917751431465149\n",
      "tensor([2])\n",
      "tensor([[ 0.6931,  0.3311, -0.5792]], grad_fn=<AddmmBackward0>)\n",
      "Batch 380, Loss: 0.579248309135437\n",
      "tensor([0])\n",
      "tensor([[ 0.5560,  0.4031, -0.7713]], grad_fn=<AddmmBackward0>)\n",
      "Batch 381, Loss: -0.555989682674408\n",
      "tensor([2])\n",
      "tensor([[ 0.7103,  0.3224, -0.5554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 382, Loss: 0.5554463863372803\n",
      "tensor([0])\n",
      "tensor([[ 0.6622,  0.3478, -0.6230]], grad_fn=<AddmmBackward0>)\n",
      "Batch 383, Loss: -0.6621612310409546\n",
      "tensor([0])\n",
      "tensor([[ 0.7263,  0.3144, -0.5333]], grad_fn=<AddmmBackward0>)\n",
      "Batch 384, Loss: -0.7263069152832031\n",
      "tensor([1])\n",
      "tensor([[ 0.6870,  0.3350, -0.5885]], grad_fn=<AddmmBackward0>)\n",
      "Batch 385, Loss: -0.3350384533405304\n",
      "tensor([0])\n",
      "tensor([[ 0.6842,  0.3367, -0.5926]], grad_fn=<AddmmBackward0>)\n",
      "Batch 386, Loss: -0.684178352355957\n",
      "tensor([2])\n",
      "tensor([[ 0.5947,  0.3836, -0.7179]], grad_fn=<AddmmBackward0>)\n",
      "Batch 387, Loss: 0.7179251909255981\n",
      "tensor([0])\n",
      "tensor([[ 0.6678,  0.3455, -0.6158]], grad_fn=<AddmmBackward0>)\n",
      "Batch 388, Loss: -0.6678184270858765\n",
      "tensor([2])\n",
      "tensor([[ 0.6887,  0.3347, -0.5868]], grad_fn=<AddmmBackward0>)\n",
      "Batch 389, Loss: 0.5868039727210999\n",
      "tensor([0])\n",
      "tensor([[ 0.6706,  0.3443, -0.6122]], grad_fn=<AddmmBackward0>)\n",
      "Batch 390, Loss: -0.6706299781799316\n",
      "tensor([0])\n",
      "tensor([[ 0.6113,  0.3753, -0.6952]], grad_fn=<AddmmBackward0>)\n",
      "Batch 391, Loss: -0.6113389730453491\n",
      "tensor([0])\n",
      "tensor([[ 0.6926,  0.3331, -0.5818]], grad_fn=<AddmmBackward0>)\n",
      "Batch 392, Loss: -0.6926047205924988\n",
      "tensor([2])\n",
      "tensor([[ 0.7821,  0.2866, -0.4569]], grad_fn=<AddmmBackward0>)\n",
      "Batch 393, Loss: 0.4569290280342102\n",
      "tensor([0])\n",
      "tensor([[ 0.7410,  0.3081, -0.5145]], grad_fn=<AddmmBackward0>)\n",
      "Batch 394, Loss: -0.7410195469856262\n",
      "tensor([0])\n",
      "tensor([[ 0.7338,  0.3120, -0.5248]], grad_fn=<AddmmBackward0>)\n",
      "Batch 395, Loss: -0.7337683439254761\n",
      "tensor([1])\n",
      "tensor([[ 0.7971,  0.2792, -0.4366]], grad_fn=<AddmmBackward0>)\n",
      "Batch 396, Loss: -0.27921628952026367\n",
      "tensor([0])\n",
      "tensor([[ 0.6590,  0.3512, -0.6295]], grad_fn=<AddmmBackward0>)\n",
      "Batch 397, Loss: -0.6590027809143066\n",
      "tensor([2])\n",
      "tensor([[ 0.7554,  0.3012, -0.4952]], grad_fn=<AddmmBackward0>)\n",
      "Batch 398, Loss: 0.49516814947128296\n",
      "tensor([2])\n",
      "tensor([[ 0.8331,  0.2610, -0.3869]], grad_fn=<AddmmBackward0>)\n",
      "Batch 399, Loss: 0.38688695430755615\n",
      "tensor([2])\n",
      "tensor([[ 0.6618,  0.3501, -0.6260]], grad_fn=<AddmmBackward0>)\n",
      "Batch 400, Loss: 0.6260294914245605\n",
      "tensor([0])\n",
      "tensor([[ 0.8416,  0.2569, -0.3752]], grad_fn=<AddmmBackward0>)\n",
      "Batch 401, Loss: -0.8416358232498169\n",
      "tensor([0])\n",
      "tensor([[ 0.8140,  0.2714, -0.4139]], grad_fn=<AddmmBackward0>)\n",
      "Batch 402, Loss: -0.8139514327049255\n",
      "tensor([0])\n",
      "tensor([[ 0.8330,  0.2616, -0.3874]], grad_fn=<AddmmBackward0>)\n",
      "Batch 403, Loss: -0.8330482244491577\n",
      "tensor([0])\n",
      "tensor([[ 0.7963,  0.2808, -0.4389]], grad_fn=<AddmmBackward0>)\n",
      "Batch 404, Loss: -0.7963100671768188\n",
      "tensor([1])\n",
      "tensor([[ 0.7981,  0.2800, -0.4366]], grad_fn=<AddmmBackward0>)\n",
      "Batch 405, Loss: -0.28003638982772827\n",
      "tensor([2])\n",
      "tensor([[ 0.5374,  0.4153, -0.8003]], grad_fn=<AddmmBackward0>)\n",
      "Batch 406, Loss: 0.8002657294273376\n",
      "tensor([2])\n",
      "tensor([[ 0.7853,  0.2870, -0.4548]], grad_fn=<AddmmBackward0>)\n",
      "Batch 407, Loss: 0.454829603433609\n",
      "tensor([0])\n",
      "tensor([[ 0.6833,  0.3399, -0.5971]], grad_fn=<AddmmBackward0>)\n",
      "Batch 408, Loss: -0.6833160519599915\n",
      "tensor([2])\n",
      "tensor([[ 0.8499,  0.2539, -0.3651]], grad_fn=<AddmmBackward0>)\n",
      "Batch 409, Loss: 0.3650699257850647\n",
      "tensor([2])\n",
      "tensor([[ 0.7056,  0.3286, -0.5661]], grad_fn=<AddmmBackward0>)\n",
      "Batch 410, Loss: 0.5661477446556091\n",
      "tensor([0])\n",
      "tensor([[ 0.6704,  0.3470, -0.6153]], grad_fn=<AddmmBackward0>)\n",
      "Batch 411, Loss: -0.670380711555481\n",
      "tensor([1])\n",
      "tensor([[ 0.7291,  0.3167, -0.5336]], grad_fn=<AddmmBackward0>)\n",
      "Batch 412, Loss: -0.3167460560798645\n",
      "tensor([2])\n",
      "tensor([[ 0.4831,  0.4439, -0.8763]], grad_fn=<AddmmBackward0>)\n",
      "Batch 413, Loss: 0.8763149380683899\n",
      "tensor([2])\n",
      "tensor([[ 0.8231,  0.2684, -0.4027]], grad_fn=<AddmmBackward0>)\n",
      "Batch 414, Loss: 0.40271878242492676\n",
      "tensor([0])\n",
      "tensor([[ 0.8283,  0.2659, -0.3955]], grad_fn=<AddmmBackward0>)\n",
      "Batch 415, Loss: -0.828309178352356\n",
      "tensor([1])\n",
      "tensor([[ 0.8277,  0.2663, -0.3964]], grad_fn=<AddmmBackward0>)\n",
      "Batch 416, Loss: -0.2663378417491913\n",
      "tensor([1])\n",
      "tensor([[ 0.8359,  0.2623, -0.3850]], grad_fn=<AddmmBackward0>)\n",
      "Batch 417, Loss: -0.2622566521167755\n",
      "tensor([0])\n",
      "tensor([[ 0.8672,  0.2463, -0.3415]], grad_fn=<AddmmBackward0>)\n",
      "Batch 418, Loss: -0.867186427116394\n",
      "tensor([1])\n",
      "tensor([[ 0.6127,  0.3776, -0.6959]], grad_fn=<AddmmBackward0>)\n",
      "Batch 419, Loss: -0.37760984897613525\n",
      "tensor([2])\n",
      "tensor([[ 0.8274,  0.2672, -0.3971]], grad_fn=<AddmmBackward0>)\n",
      "Batch 420, Loss: 0.397056519985199\n",
      "tensor([1])\n",
      "tensor([[ 0.7157,  0.3249, -0.5527]], grad_fn=<AddmmBackward0>)\n",
      "Batch 421, Loss: -0.3249031901359558\n",
      "tensor([2])\n",
      "tensor([[ 0.7452,  0.3099, -0.5117]], grad_fn=<AddmmBackward0>)\n",
      "Batch 422, Loss: 0.5116522312164307\n",
      "tensor([0])\n",
      "tensor([[ 0.8605,  0.2508, -0.3511]], grad_fn=<AddmmBackward0>)\n",
      "Batch 423, Loss: -0.8604809045791626\n",
      "tensor([1])\n",
      "tensor([[ 0.6388,  0.3649, -0.6598]], grad_fn=<AddmmBackward0>)\n",
      "Batch 424, Loss: -0.3649001121520996\n",
      "tensor([2])\n",
      "tensor([[ 0.8238,  0.2700, -0.4022]], grad_fn=<AddmmBackward0>)\n",
      "Batch 425, Loss: 0.40223515033721924\n",
      "tensor([0])\n",
      "tensor([[ 0.7005,  0.3335, -0.5739]], grad_fn=<AddmmBackward0>)\n",
      "Batch 426, Loss: -0.700545608997345\n",
      "tensor([2])\n",
      "tensor([[ 0.8645,  0.2496, -0.3457]], grad_fn=<AddmmBackward0>)\n",
      "Batch 427, Loss: 0.34573203325271606\n",
      "tensor([0])\n",
      "tensor([[ 0.7836,  0.2912, -0.4583]], grad_fn=<AddmmBackward0>)\n",
      "Batch 428, Loss: -0.7836278676986694\n",
      "tensor([2])\n",
      "tensor([[ 0.7548,  0.3061, -0.4985]], grad_fn=<AddmmBackward0>)\n",
      "Batch 429, Loss: 0.49849388003349304\n",
      "tensor([0])\n",
      "tensor([[ 0.6592,  0.3552, -0.6316]], grad_fn=<AddmmBackward0>)\n",
      "Batch 430, Loss: -0.6591595411300659\n",
      "tensor([0])\n",
      "tensor([[ 0.8367,  0.2644, -0.3845]], grad_fn=<AddmmBackward0>)\n",
      "Batch 431, Loss: -0.8366854190826416\n",
      "tensor([1])\n",
      "tensor([[ 0.8132,  0.2766, -0.4174]], grad_fn=<AddmmBackward0>)\n",
      "Batch 432, Loss: -0.27664172649383545\n",
      "tensor([1])\n",
      "tensor([[ 0.7623,  0.3029, -0.4883]], grad_fn=<AddmmBackward0>)\n",
      "Batch 433, Loss: -0.30285385251045227\n",
      "tensor([1])\n",
      "tensor([[ 0.8692,  0.2484, -0.3396]], grad_fn=<AddmmBackward0>)\n",
      "Batch 434, Loss: -0.24838267266750336\n",
      "tensor([1])\n",
      "tensor([[ 0.7908,  0.2887, -0.4487]], grad_fn=<AddmmBackward0>)\n",
      "Batch 435, Loss: -0.28865301609039307\n",
      "tensor([1])\n",
      "tensor([[ 0.6592,  0.3561, -0.6320]], grad_fn=<AddmmBackward0>)\n",
      "Batch 436, Loss: -0.35608088970184326\n",
      "tensor([2])\n",
      "tensor([[ 0.7980,  0.2854, -0.4388]], grad_fn=<AddmmBackward0>)\n",
      "Batch 437, Loss: 0.4388048052787781\n",
      "tensor([2])\n",
      "tensor([[ 0.6379,  0.3673, -0.6616]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 438, Loss: 0.6616186499595642\n",
      "tensor([0])\n",
      "tensor([[ 0.8553,  0.2567, -0.3592]], grad_fn=<AddmmBackward0>)\n",
      "Batch 439, Loss: -0.8552747964859009\n",
      "tensor([2])\n",
      "tensor([[ 0.5828,  0.3956, -0.7382]], grad_fn=<AddmmBackward0>)\n",
      "Batch 440, Loss: 0.7382091283798218\n",
      "tensor([1])\n",
      "tensor([[ 0.5747,  0.3999, -0.7495]], grad_fn=<AddmmBackward0>)\n",
      "Batch 441, Loss: -0.39985543489456177\n",
      "tensor([0])\n",
      "tensor([[ 0.6466,  0.3634, -0.6496]], grad_fn=<AddmmBackward0>)\n",
      "Batch 442, Loss: -0.6465527415275574\n",
      "tensor([2])\n",
      "tensor([[ 0.7546,  0.3087, -0.4993]], grad_fn=<AddmmBackward0>)\n",
      "Batch 443, Loss: 0.4993410110473633\n",
      "tensor([2])\n",
      "tensor([[ 0.8173,  0.2770, -0.4121]], grad_fn=<AddmmBackward0>)\n",
      "Batch 444, Loss: 0.41214418411254883\n",
      "tensor([1])\n",
      "tensor([[ 0.6202,  0.3772, -0.6863]], grad_fn=<AddmmBackward0>)\n",
      "Batch 445, Loss: -0.37722817063331604\n",
      "tensor([1])\n",
      "tensor([[ 0.7968,  0.2877, -0.4406]], grad_fn=<AddmmBackward0>)\n",
      "Batch 446, Loss: -0.28768280148506165\n",
      "tensor([1])\n",
      "tensor([[ 0.7896,  0.2915, -0.4505]], grad_fn=<AddmmBackward0>)\n",
      "Batch 447, Loss: -0.29148316383361816\n",
      "tensor([2])\n",
      "tensor([[ 0.6265,  0.3744, -0.6775]], grad_fn=<AddmmBackward0>)\n",
      "Batch 448, Loss: 0.6774532794952393\n",
      "tensor([0])\n",
      "tensor([[ 0.8640,  0.2542, -0.3470]], grad_fn=<AddmmBackward0>)\n",
      "Batch 449, Loss: -0.8639922142028809\n",
      "tensor([2])\n",
      "tensor([[ 0.7370,  0.3187, -0.5238]], grad_fn=<AddmmBackward0>)\n",
      "Batch 450, Loss: 0.5237880945205688\n",
      "tensor([0])\n",
      "tensor([[ 0.8466,  0.2633, -0.3712]], grad_fn=<AddmmBackward0>)\n",
      "Batch 451, Loss: -0.8466229438781738\n",
      "tensor([1])\n",
      "tensor([[ 0.8768,  0.2482, -0.3292]], grad_fn=<AddmmBackward0>)\n",
      "Batch 452, Loss: -0.24823695421218872\n",
      "tensor([0])\n",
      "tensor([[ 0.8458,  0.2641, -0.3724]], grad_fn=<AddmmBackward0>)\n",
      "Batch 453, Loss: -0.8458395600318909\n",
      "tensor([2])\n",
      "tensor([[ 0.5631,  0.4072, -0.7656]], grad_fn=<AddmmBackward0>)\n",
      "Batch 454, Loss: 0.7656424045562744\n",
      "tensor([2])\n",
      "tensor([[ 0.6513,  0.3628, -0.6430]], grad_fn=<AddmmBackward0>)\n",
      "Batch 455, Loss: 0.6430174708366394\n",
      "tensor([0])\n",
      "tensor([[ 0.8276,  0.2739, -0.3979]], grad_fn=<AddmmBackward0>)\n",
      "Batch 456, Loss: -0.8275706768035889\n",
      "tensor([1])\n",
      "tensor([[ 0.8555,  0.2600, -0.3592]], grad_fn=<AddmmBackward0>)\n",
      "Batch 457, Loss: -0.2599937617778778\n",
      "tensor([1])\n",
      "tensor([[ 0.8601,  0.2579, -0.3528]], grad_fn=<AddmmBackward0>)\n",
      "Batch 458, Loss: -0.2578524053096771\n",
      "tensor([2])\n",
      "tensor([[ 0.6745,  0.3517, -0.6110]], grad_fn=<AddmmBackward0>)\n",
      "Batch 459, Loss: 0.610986590385437\n",
      "tensor([1])\n",
      "tensor([[ 0.6018,  0.3884, -0.7120]], grad_fn=<AddmmBackward0>)\n",
      "Batch 460, Loss: -0.3884217143058777\n",
      "tensor([0])\n",
      "tensor([[ 0.8202,  0.2786, -0.4083]], grad_fn=<AddmmBackward0>)\n",
      "Batch 461, Loss: -0.8202493190765381\n",
      "tensor([1])\n",
      "tensor([[ 0.8560,  0.2608, -0.3588]], grad_fn=<AddmmBackward0>)\n",
      "Batch 462, Loss: -0.2607800364494324\n",
      "tensor([2])\n",
      "tensor([[ 0.8369,  0.2706, -0.3853]], grad_fn=<AddmmBackward0>)\n",
      "Batch 463, Loss: 0.3852936625480652\n",
      "tensor([0])\n",
      "tensor([[ 0.8668,  0.2558, -0.3438]], grad_fn=<AddmmBackward0>)\n",
      "Batch 464, Loss: -0.8667677044868469\n",
      "tensor([1])\n",
      "tensor([[ 0.8220,  0.2785, -0.4062]], grad_fn=<AddmmBackward0>)\n",
      "Batch 465, Loss: -0.278507798910141\n",
      "tensor([0])\n",
      "tensor([[ 0.6480,  0.3660, -0.6479]], grad_fn=<AddmmBackward0>)\n",
      "Batch 466, Loss: -0.648048460483551\n",
      "tensor([1])\n",
      "tensor([[ 0.8557,  0.2620, -0.3595]], grad_fn=<AddmmBackward0>)\n",
      "Batch 467, Loss: -0.2620154023170471\n",
      "tensor([0])\n",
      "tensor([[ 0.8226,  0.2788, -0.4056]], grad_fn=<AddmmBackward0>)\n",
      "Batch 468, Loss: -0.8225804567337036\n",
      "tensor([0])\n",
      "tensor([[ 0.8651,  0.2577, -0.3466]], grad_fn=<AddmmBackward0>)\n",
      "Batch 469, Loss: -0.865134596824646\n",
      "tensor([2])\n",
      "tensor([[ 0.7466,  0.3173, -0.5114]], grad_fn=<AddmmBackward0>)\n",
      "Batch 470, Loss: 0.5113646984100342\n",
      "tensor([0])\n",
      "tensor([[ 0.8404,  0.2706, -0.3812]], grad_fn=<AddmmBackward0>)\n",
      "Batch 471, Loss: -0.840435266494751\n",
      "tensor([0])\n",
      "tensor([[ 0.7412,  0.3204, -0.5192]], grad_fn=<AddmmBackward0>)\n",
      "Batch 472, Loss: -0.7411506772041321\n",
      "tensor([0])\n",
      "tensor([[ 0.7983,  0.2920, -0.4401]], grad_fn=<AddmmBackward0>)\n",
      "Batch 473, Loss: -0.7982832789421082\n",
      "tensor([2])\n",
      "tensor([[ 0.8071,  0.2878, -0.4280]], grad_fn=<AddmmBackward0>)\n",
      "Batch 474, Loss: 0.4279918968677521\n",
      "tensor([0])\n",
      "tensor([[ 0.8720,  0.2555, -0.3381]], grad_fn=<AddmmBackward0>)\n",
      "Batch 475, Loss: -0.8720334768295288\n",
      "tensor([0])\n",
      "tensor([[ 0.6147,  0.3842, -0.6954]], grad_fn=<AddmmBackward0>)\n",
      "Batch 476, Loss: -0.6146575212478638\n",
      "tensor([0])\n",
      "tensor([[ 0.8138,  0.2850, -0.4193]], grad_fn=<AddmmBackward0>)\n",
      "Batch 477, Loss: -0.8137508630752563\n",
      "tensor([1])\n",
      "tensor([[ 0.8607,  0.2617, -0.3544]], grad_fn=<AddmmBackward0>)\n",
      "Batch 478, Loss: -0.26172935962677\n",
      "tensor([1])\n",
      "tensor([[ 0.8504,  0.2671, -0.3689]], grad_fn=<AddmmBackward0>)\n",
      "Batch 479, Loss: -0.2670760154724121\n",
      "tensor([0])\n",
      "tensor([[ 0.8029,  0.2910, -0.4350]], grad_fn=<AddmmBackward0>)\n",
      "Batch 480, Loss: -0.8029196262359619\n",
      "tensor([2])\n",
      "tensor([[ 0.7581,  0.3134, -0.4973]], grad_fn=<AddmmBackward0>)\n",
      "Batch 481, Loss: 0.49725955724716187\n",
      "tensor([0])\n",
      "tensor([[ 0.7456,  0.3198, -0.5148]], grad_fn=<AddmmBackward0>)\n",
      "Batch 482, Loss: -0.7456146478652954\n",
      "tensor([1])\n",
      "tensor([[ 0.7961,  0.2949, -0.4450]], grad_fn=<AddmmBackward0>)\n",
      "Batch 483, Loss: -0.2949073314666748\n",
      "tensor([1])\n",
      "tensor([[ 0.8759,  0.2555, -0.3346]], grad_fn=<AddmmBackward0>)\n",
      "Batch 484, Loss: -0.2554512619972229\n",
      "tensor([0])\n",
      "tensor([[ 0.8022,  0.2923, -0.4368]], grad_fn=<AddmmBackward0>)\n",
      "Batch 485, Loss: -0.8022089600563049\n",
      "tensor([0])\n",
      "tensor([[ 0.8543,  0.2666, -0.3649]], grad_fn=<AddmmBackward0>)\n",
      "Batch 486, Loss: -0.8543045520782471\n",
      "tensor([1])\n",
      "tensor([[ 0.8316,  0.2781, -0.3965]], grad_fn=<AddmmBackward0>)\n",
      "Batch 487, Loss: -0.2781115174293518\n",
      "tensor([2])\n",
      "tensor([[ 0.8665,  0.2611, -0.3484]], grad_fn=<AddmmBackward0>)\n",
      "Batch 488, Loss: 0.3484306335449219\n",
      "tensor([2])\n",
      "tensor([[ 0.8109,  0.2888, -0.4255]], grad_fn=<AddmmBackward0>)\n",
      "Batch 489, Loss: 0.42550021409988403\n",
      "tensor([1])\n",
      "tensor([[ 0.8165,  0.2862, -0.4178]], grad_fn=<AddmmBackward0>)\n",
      "Batch 490, Loss: -0.2862265110015869\n",
      "tensor([0])\n",
      "tensor([[ 0.8157,  0.2868, -0.4190]], grad_fn=<AddmmBackward0>)\n",
      "Batch 491, Loss: -0.8157325983047485\n",
      "tensor([1])\n",
      "tensor([[ 0.7774,  0.3060, -0.4721]], grad_fn=<AddmmBackward0>)\n",
      "Batch 492, Loss: -0.305955708026886\n",
      "tensor([1])\n",
      "tensor([[ 0.6796,  0.3545, -0.6075]], grad_fn=<AddmmBackward0>)\n",
      "Batch 493, Loss: -0.3544764220714569\n",
      "tensor([1])\n",
      "tensor([[ 0.8265,  0.2821, -0.4044]], grad_fn=<AddmmBackward0>)\n",
      "Batch 494, Loss: -0.282113641500473\n",
      "tensor([2])\n",
      "tensor([[ 0.8601,  0.2658, -0.3580]], grad_fn=<AddmmBackward0>)\n",
      "Batch 495, Loss: 0.3579840660095215\n",
      "tensor([0])\n",
      "tensor([[ 0.7636,  0.3136, -0.4915]], grad_fn=<AddmmBackward0>)\n",
      "Batch 496, Loss: -0.7636394500732422\n",
      "tensor([1])\n",
      "tensor([[ 0.8378,  0.2772, -0.3890]], grad_fn=<AddmmBackward0>)\n",
      "Batch 497, Loss: -0.27721768617630005\n",
      "tensor([2])\n",
      "tensor([[ 0.8559,  0.2685, -0.3640]], grad_fn=<AddmmBackward0>)\n",
      "Batch 498, Loss: 0.36397266387939453\n",
      "tensor([2])\n",
      "tensor([[ 0.6077,  0.3909, -0.7073]], grad_fn=<AddmmBackward0>)\n",
      "Batch 499, Loss: 0.7072915434837341\n",
      "tensor([0])\n",
      "tensor([[ 0.7705,  0.3109, -0.4821]], grad_fn=<AddmmBackward0>)\n",
      "Batch 500, Loss: -0.7704960703849792\n",
      "tensor([2])\n",
      "tensor([[ 0.7173,  0.3373, -0.5558]], grad_fn=<AddmmBackward0>)\n",
      "Batch 501, Loss: 0.555768609046936\n",
      "tensor([1])\n",
      "tensor([[ 0.8628,  0.2659, -0.3545]], grad_fn=<AddmmBackward0>)\n",
      "Batch 502, Loss: -0.2658841609954834\n",
      "tensor([0])\n",
      "tensor([[ 0.8480,  0.2734, -0.3750]], grad_fn=<AddmmBackward0>)\n",
      "Batch 503, Loss: -0.8479815721511841\n",
      "tensor([0])\n",
      "tensor([[ 0.8645,  0.2654, -0.3522]], grad_fn=<AddmmBackward0>)\n",
      "Batch 504, Loss: -0.8645278811454773\n",
      "tensor([1])\n",
      "tensor([[ 0.8798,  0.2581, -0.3311]], grad_fn=<AddmmBackward0>)\n",
      "Batch 505, Loss: -0.25809919834136963\n",
      "tensor([0])\n",
      "tensor([[ 0.7221,  0.3357, -0.5493]], grad_fn=<AddmmBackward0>)\n",
      "Batch 506, Loss: -0.722089409828186\n",
      "tensor([1])\n",
      "tensor([[ 0.6902,  0.3515, -0.5935]], grad_fn=<AddmmBackward0>)\n",
      "Batch 507, Loss: -0.3515028655529022\n",
      "tensor([2])\n",
      "tensor([[ 0.8108,  0.2926, -0.4269]], grad_fn=<AddmmBackward0>)\n",
      "Batch 508, Loss: 0.42691802978515625\n",
      "tensor([2])\n",
      "tensor([[ 0.8340,  0.2814, -0.3949]], grad_fn=<AddmmBackward0>)\n",
      "Batch 509, Loss: 0.3949244022369385\n",
      "tensor([0])\n",
      "tensor([[ 0.7854,  0.3054, -0.4621]], grad_fn=<AddmmBackward0>)\n",
      "Batch 510, Loss: -0.7853758335113525\n",
      "tensor([1])\n",
      "tensor([[ 0.8276,  0.2849, -0.4038]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 511, Loss: -0.2848823666572571\n",
      "tensor([2])\n",
      "tensor([[ 0.8728,  0.2630, -0.3414]], grad_fn=<AddmmBackward0>)\n",
      "Batch 512, Loss: 0.3413587808609009\n",
      "tensor([1])\n",
      "tensor([[ 0.7815,  0.3078, -0.4676]], grad_fn=<AddmmBackward0>)\n",
      "Batch 513, Loss: -0.3077569603919983\n",
      "tensor([1])\n",
      "tensor([[ 0.5464,  0.4228, -0.7926]], grad_fn=<AddmmBackward0>)\n",
      "Batch 514, Loss: -0.42277926206588745\n",
      "tensor([2])\n",
      "tensor([[ 0.8476,  0.2758, -0.3763]], grad_fn=<AddmmBackward0>)\n",
      "Batch 515, Loss: 0.3762769103050232\n",
      "tensor([0])\n",
      "tensor([[ 0.8672,  0.2664, -0.3491]], grad_fn=<AddmmBackward0>)\n",
      "Batch 516, Loss: -0.8672089576721191\n",
      "tensor([2])\n",
      "tensor([[ 0.7129,  0.3419, -0.5625]], grad_fn=<AddmmBackward0>)\n",
      "Batch 517, Loss: 0.5624613761901855\n",
      "tensor([2])\n",
      "tensor([[ 0.8824,  0.2594, -0.3281]], grad_fn=<AddmmBackward0>)\n",
      "Batch 518, Loss: 0.32805031538009644\n",
      "tensor([2])\n",
      "tensor([[ 0.6947,  0.3510, -0.5876]], grad_fn=<AddmmBackward0>)\n",
      "Batch 519, Loss: 0.5875871181488037\n",
      "tensor([0])\n",
      "tensor([[ 0.6388,  0.3784, -0.6648]], grad_fn=<AddmmBackward0>)\n",
      "Batch 520, Loss: -0.6388223767280579\n",
      "tensor([0])\n",
      "tensor([[ 0.7307,  0.3337, -0.5377]], grad_fn=<AddmmBackward0>)\n",
      "Batch 521, Loss: -0.7307004928588867\n",
      "tensor([0])\n",
      "tensor([[ 0.8026,  0.2988, -0.4383]], grad_fn=<AddmmBackward0>)\n",
      "Batch 522, Loss: -0.8026202917098999\n",
      "tensor([0])\n",
      "tensor([[ 0.8168,  0.2920, -0.4186]], grad_fn=<AddmmBackward0>)\n",
      "Batch 523, Loss: -0.8168483972549438\n",
      "tensor([1])\n",
      "tensor([[ 0.7062,  0.3460, -0.5717]], grad_fn=<AddmmBackward0>)\n",
      "Batch 524, Loss: -0.34595686197280884\n",
      "tensor([0])\n",
      "tensor([[ 0.8850,  0.2591, -0.3246]], grad_fn=<AddmmBackward0>)\n",
      "Batch 525, Loss: -0.8850082159042358\n",
      "tensor([1])\n",
      "tensor([[ 0.5526,  0.4209, -0.7842]], grad_fn=<AddmmBackward0>)\n",
      "Batch 526, Loss: -0.42090097069740295\n",
      "tensor([1])\n",
      "tensor([[ 0.7591,  0.3206, -0.4988]], grad_fn=<AddmmBackward0>)\n",
      "Batch 527, Loss: -0.3206135928630829\n",
      "tensor([1])\n",
      "tensor([[ 0.8405,  0.2812, -0.3865]], grad_fn=<AddmmBackward0>)\n",
      "Batch 528, Loss: -0.2812379002571106\n",
      "tensor([2])\n",
      "tensor([[ 0.8810,  0.2618, -0.3307]], grad_fn=<AddmmBackward0>)\n",
      "Batch 529, Loss: 0.33067184686660767\n",
      "tensor([0])\n",
      "tensor([[ 0.8879,  0.2586, -0.3211]], grad_fn=<AddmmBackward0>)\n",
      "Batch 530, Loss: -0.8879098892211914\n",
      "tensor([2])\n",
      "tensor([[ 0.8025,  0.3002, -0.4392]], grad_fn=<AddmmBackward0>)\n",
      "Batch 531, Loss: 0.43921026587486267\n",
      "tensor([2])\n",
      "tensor([[ 0.7958,  0.3036, -0.4485]], grad_fn=<AddmmBackward0>)\n",
      "Batch 532, Loss: 0.4485153555870056\n",
      "tensor([1])\n",
      "tensor([[ 0.8750,  0.2654, -0.3391]], grad_fn=<AddmmBackward0>)\n",
      "Batch 533, Loss: -0.2653695344924927\n",
      "tensor([2])\n",
      "tensor([[ 0.8721,  0.2669, -0.3431]], grad_fn=<AddmmBackward0>)\n",
      "Batch 534, Loss: 0.34311455488204956\n",
      "tensor([1])\n",
      "tensor([[ 0.7765,  0.3134, -0.4752]], grad_fn=<AddmmBackward0>)\n",
      "Batch 535, Loss: -0.3134123682975769\n",
      "tensor([0])\n",
      "tensor([[ 0.8786,  0.2641, -0.3341]], grad_fn=<AddmmBackward0>)\n",
      "Batch 536, Loss: -0.8786046504974365\n",
      "tensor([1])\n",
      "tensor([[ 0.7035,  0.3491, -0.5761]], grad_fn=<AddmmBackward0>)\n",
      "Batch 537, Loss: -0.34907612204551697\n",
      "tensor([1])\n",
      "tensor([[ 0.8662,  0.2705, -0.3513]], grad_fn=<AddmmBackward0>)\n",
      "Batch 538, Loss: -0.2705153226852417\n",
      "tensor([1])\n",
      "tensor([[ 0.7561,  0.3240, -0.5034]], grad_fn=<AddmmBackward0>)\n",
      "Batch 539, Loss: -0.3239578604698181\n",
      "tensor([2])\n",
      "tensor([[ 0.8243,  0.2912, -0.4092]], grad_fn=<AddmmBackward0>)\n",
      "Batch 540, Loss: 0.40917152166366577\n",
      "tensor([2])\n",
      "tensor([[ 0.8035,  0.3014, -0.4380]], grad_fn=<AddmmBackward0>)\n",
      "Batch 541, Loss: 0.43796300888061523\n",
      "tensor([0])\n",
      "tensor([[ 0.7730,  0.3163, -0.4800]], grad_fn=<AddmmBackward0>)\n",
      "Batch 542, Loss: -0.7729895114898682\n",
      "tensor([0])\n",
      "tensor([[ 0.7933,  0.3067, -0.4520]], grad_fn=<AddmmBackward0>)\n",
      "Batch 543, Loss: -0.7932958602905273\n",
      "tensor([1])\n",
      "tensor([[ 0.8956,  0.2575, -0.3107]], grad_fn=<AddmmBackward0>)\n",
      "Batch 544, Loss: -0.25750136375427246\n",
      "tensor([1])\n",
      "tensor([[ 0.8540,  0.2778, -0.3682]], grad_fn=<AddmmBackward0>)\n",
      "Batch 545, Loss: -0.27777934074401855\n",
      "tensor([2])\n",
      "tensor([[ 0.8340,  0.2876, -0.3959]], grad_fn=<AddmmBackward0>)\n",
      "Batch 546, Loss: 0.39587944746017456\n",
      "tensor([0])\n",
      "tensor([[ 0.8750,  0.2681, -0.3392]], grad_fn=<AddmmBackward0>)\n",
      "Batch 547, Loss: -0.8749719262123108\n",
      "tensor([0])\n",
      "tensor([[ 0.7665,  0.3204, -0.4891]], grad_fn=<AddmmBackward0>)\n",
      "Batch 548, Loss: -0.7665436267852783\n",
      "tensor([0])\n",
      "tensor([[ 0.8735,  0.2692, -0.3415]], grad_fn=<AddmmBackward0>)\n",
      "Batch 549, Loss: -0.8734549283981323\n",
      "tensor([0])\n",
      "tensor([[ 0.7966,  0.3063, -0.4478]], grad_fn=<AddmmBackward0>)\n",
      "Batch 550, Loss: -0.7965942621231079\n",
      "tensor([1])\n",
      "tensor([[ 0.6918,  0.3569, -0.5927]], grad_fn=<AddmmBackward0>)\n",
      "Batch 551, Loss: -0.3568606972694397\n",
      "tensor([1])\n",
      "tensor([[ 0.8876,  0.2630, -0.3224]], grad_fn=<AddmmBackward0>)\n",
      "Batch 552, Loss: -0.26298385858535767\n",
      "tensor([0])\n",
      "tensor([[ 0.7829,  0.3134, -0.4671]], grad_fn=<AddmmBackward0>)\n",
      "Batch 553, Loss: -0.7829071283340454\n",
      "tensor([0])\n",
      "tensor([[ 0.6624,  0.3715, -0.6337]], grad_fn=<AddmmBackward0>)\n",
      "Batch 554, Loss: -0.6623525619506836\n",
      "tensor([2])\n",
      "tensor([[ 0.7303,  0.3390, -0.5400]], grad_fn=<AddmmBackward0>)\n",
      "Batch 555, Loss: 0.5399576425552368\n",
      "tensor([2])\n",
      "tensor([[ 0.8450,  0.2842, -0.3818]], grad_fn=<AddmmBackward0>)\n",
      "Batch 556, Loss: 0.38184022903442383\n",
      "tensor([1])\n",
      "tensor([[ 0.6025,  0.4006, -0.7167]], grad_fn=<AddmmBackward0>)\n",
      "Batch 557, Loss: -0.40058305859565735\n",
      "tensor([0])\n",
      "tensor([[ 0.8468,  0.2837, -0.3795]], grad_fn=<AddmmBackward0>)\n",
      "Batch 558, Loss: -0.8467658758163452\n",
      "tensor([1])\n",
      "tensor([[ 0.8621,  0.2765, -0.3585]], grad_fn=<AddmmBackward0>)\n",
      "Batch 559, Loss: -0.27654263377189636\n",
      "tensor([0])\n",
      "tensor([[ 0.8287,  0.2927, -0.4047]], grad_fn=<AddmmBackward0>)\n",
      "Batch 560, Loss: -0.8286610841751099\n",
      "tensor([0])\n",
      "tensor([[ 0.6172,  0.3940, -0.6966]], grad_fn=<AddmmBackward0>)\n",
      "Batch 561, Loss: -0.6172413229942322\n",
      "tensor([0])\n",
      "tensor([[ 0.8491,  0.2833, -0.3768]], grad_fn=<AddmmBackward0>)\n",
      "Batch 562, Loss: -0.8491246104240417\n",
      "tensor([1])\n",
      "tensor([[ 0.8414,  0.2872, -0.3876]], grad_fn=<AddmmBackward0>)\n",
      "Batch 563, Loss: -0.2871664762496948\n",
      "tensor([2])\n",
      "tensor([[ 0.7914,  0.3112, -0.4567]], grad_fn=<AddmmBackward0>)\n",
      "Batch 564, Loss: 0.45673009753227234\n",
      "tensor([2])\n",
      "tensor([[ 0.8550,  0.2810, -0.3692]], grad_fn=<AddmmBackward0>)\n",
      "Batch 565, Loss: 0.36916083097457886\n",
      "tensor([1])\n",
      "tensor([[ 0.8545,  0.2814, -0.3699]], grad_fn=<AddmmBackward0>)\n",
      "Batch 566, Loss: -0.2814415693283081\n",
      "tensor([2])\n",
      "tensor([[ 0.8383,  0.2894, -0.3923]], grad_fn=<AddmmBackward0>)\n",
      "Batch 567, Loss: 0.39231300354003906\n",
      "tensor([1])\n",
      "tensor([[ 0.8253,  0.2957, -0.4103]], grad_fn=<AddmmBackward0>)\n",
      "Batch 568, Loss: -0.29570668935775757\n",
      "tensor([0])\n",
      "tensor([[ 0.8502,  0.2840, -0.3759]], grad_fn=<AddmmBackward0>)\n",
      "Batch 569, Loss: -0.8502109050750732\n",
      "tensor([2])\n",
      "tensor([[ 0.6652,  0.3723, -0.6311]], grad_fn=<AddmmBackward0>)\n",
      "Batch 570, Loss: 0.631115198135376\n",
      "tensor([0])\n",
      "tensor([[ 0.8874,  0.2667, -0.3248]], grad_fn=<AddmmBackward0>)\n",
      "Batch 571, Loss: -0.8873528242111206\n",
      "tensor([1])\n",
      "tensor([[ 0.7109,  0.3508, -0.5681]], grad_fn=<AddmmBackward0>)\n",
      "Batch 572, Loss: -0.350778192281723\n",
      "tensor([2])\n",
      "tensor([[ 0.8452,  0.2871, -0.3830]], grad_fn=<AddmmBackward0>)\n",
      "Batch 573, Loss: 0.38302719593048096\n",
      "tensor([2])\n",
      "tensor([[ 0.8363,  0.2915, -0.3954]], grad_fn=<AddmmBackward0>)\n",
      "Batch 574, Loss: 0.395355761051178\n",
      "tensor([1])\n",
      "tensor([[ 0.8759,  0.2728, -0.3407]], grad_fn=<AddmmBackward0>)\n",
      "Batch 575, Loss: -0.27277565002441406\n",
      "tensor([1])\n",
      "tensor([[ 0.8154,  0.3017, -0.4242]], grad_fn=<AddmmBackward0>)\n",
      "Batch 576, Loss: -0.30168992280960083\n",
      "tensor([1])\n",
      "tensor([[ 0.8133,  0.3029, -0.4271]], grad_fn=<AddmmBackward0>)\n",
      "Batch 577, Loss: -0.3028671145439148\n",
      "tensor([0])\n",
      "tensor([[ 0.7220,  0.3464, -0.5530]], grad_fn=<AddmmBackward0>)\n",
      "Batch 578, Loss: -0.7219539284706116\n",
      "tensor([2])\n",
      "tensor([[ 0.7118,  0.3513, -0.5669]], grad_fn=<AddmmBackward0>)\n",
      "Batch 579, Loss: 0.5669193267822266\n",
      "tensor([2])\n",
      "tensor([[ 0.9028,  0.2610, -0.3037]], grad_fn=<AddmmBackward0>)\n",
      "Batch 580, Loss: 0.3036752939224243\n",
      "tensor([1])\n",
      "tensor([[ 0.8256,  0.2977, -0.4100]], grad_fn=<AddmmBackward0>)\n",
      "Batch 581, Loss: -0.29770636558532715\n",
      "tensor([0])\n",
      "tensor([[ 0.7315,  0.3425, -0.5398]], grad_fn=<AddmmBackward0>)\n",
      "Batch 582, Loss: -0.7314530611038208\n",
      "tensor([0])\n",
      "tensor([[ 0.8742,  0.2750, -0.3430]], grad_fn=<AddmmBackward0>)\n",
      "Batch 583, Loss: -0.8742207288742065\n",
      "tensor([1])\n",
      "tensor([[ 0.8562,  0.2837, -0.3679]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 584, Loss: -0.28374791145324707\n",
      "tensor([1])\n",
      "tensor([[ 0.8589,  0.2826, -0.3642]], grad_fn=<AddmmBackward0>)\n",
      "Batch 585, Loss: -0.2826320230960846\n",
      "tensor([1])\n",
      "tensor([[ 0.8213,  0.3006, -0.4161]], grad_fn=<AddmmBackward0>)\n",
      "Batch 586, Loss: -0.30062830448150635\n",
      "tensor([0])\n",
      "tensor([[ 0.8485,  0.2880, -0.3786]], grad_fn=<AddmmBackward0>)\n",
      "Batch 587, Loss: -0.8485086560249329\n",
      "tensor([0])\n",
      "tensor([[ 0.8675,  0.2792, -0.3526]], grad_fn=<AddmmBackward0>)\n",
      "Batch 588, Loss: -0.8674579858779907\n",
      "tensor([1])\n",
      "tensor([[ 0.8050,  0.3089, -0.4388]], grad_fn=<AddmmBackward0>)\n",
      "Batch 589, Loss: -0.30892401933670044\n",
      "tensor([2])\n",
      "tensor([[ 0.8861,  0.2709, -0.3271]], grad_fn=<AddmmBackward0>)\n",
      "Batch 590, Loss: 0.3270986080169678\n",
      "tensor([1])\n",
      "tensor([[ 0.8942,  0.2673, -0.3160]], grad_fn=<AddmmBackward0>)\n",
      "Batch 591, Loss: -0.26727092266082764\n",
      "tensor([1])\n",
      "tensor([[ 0.8768,  0.2757, -0.3400]], grad_fn=<AddmmBackward0>)\n",
      "Batch 592, Loss: -0.27568548917770386\n",
      "tensor([0])\n",
      "tensor([[ 0.8467,  0.2901, -0.3816]], grad_fn=<AddmmBackward0>)\n",
      "Batch 593, Loss: -0.8467303514480591\n",
      "tensor([0])\n",
      "tensor([[ 0.6841,  0.3668, -0.6057]], grad_fn=<AddmmBackward0>)\n",
      "Batch 594, Loss: -0.6841025352478027\n",
      "tensor([2])\n",
      "tensor([[ 0.7684,  0.3274, -0.4897]], grad_fn=<AddmmBackward0>)\n",
      "Batch 595, Loss: 0.4896699786186218\n",
      "tensor([0])\n",
      "tensor([[ 0.8051,  0.3103, -0.4392]], grad_fn=<AddmmBackward0>)\n",
      "Batch 596, Loss: -0.8050971031188965\n",
      "tensor([2])\n",
      "tensor([[ 0.8746,  0.2778, -0.3436]], grad_fn=<AddmmBackward0>)\n",
      "Batch 597, Loss: 0.34357279539108276\n",
      "tensor([0])\n",
      "tensor([[ 0.8264,  0.3006, -0.4100]], grad_fn=<AddmmBackward0>)\n",
      "Batch 598, Loss: -0.8264389038085938\n",
      "tensor([1])\n",
      "tensor([[ 0.8521,  0.2887, -0.3747]], grad_fn=<AddmmBackward0>)\n",
      "Batch 599, Loss: -0.28873318433761597\n",
      "Training [20%]\tLoss: -0.1012\n",
      "tensor([0])\n",
      "tensor([[ 0.7936,  0.3164, -0.4554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 0, Loss: -0.7935519218444824\n",
      "tensor([0])\n",
      "tensor([[ 0.8835,  0.2744, -0.3317]], grad_fn=<AddmmBackward0>)\n",
      "Batch 1, Loss: -0.8834957480430603\n",
      "tensor([0])\n",
      "tensor([[ 0.8446,  0.2928, -0.3854]], grad_fn=<AddmmBackward0>)\n",
      "Batch 2, Loss: -0.8446063995361328\n",
      "tensor([2])\n",
      "tensor([[ 0.7616,  0.3318, -0.4999]], grad_fn=<AddmmBackward0>)\n",
      "Batch 3, Loss: 0.4999064803123474\n",
      "tensor([1])\n",
      "tensor([[ 0.8534,  0.2890, -0.3736]], grad_fn=<AddmmBackward0>)\n",
      "Batch 4, Loss: -0.28898271918296814\n",
      "tensor([0])\n",
      "tensor([[ 0.8245,  0.3027, -0.4136]], grad_fn=<AddmmBackward0>)\n",
      "Batch 5, Loss: -0.8244803547859192\n",
      "tensor([2])\n",
      "tensor([[ 0.8489,  0.2915, -0.3801]], grad_fn=<AddmmBackward0>)\n",
      "Batch 6, Loss: 0.3800853490829468\n",
      "tensor([0])\n",
      "tensor([[ 0.6692,  0.3756, -0.6274]], grad_fn=<AddmmBackward0>)\n",
      "Batch 7, Loss: -0.6692492961883545\n",
      "tensor([2])\n",
      "tensor([[ 0.8661,  0.2837, -0.3566]], grad_fn=<AddmmBackward0>)\n",
      "Batch 8, Loss: 0.3566429018974304\n",
      "tensor([2])\n",
      "tensor([[ 0.7995,  0.3150, -0.4484]], grad_fn=<AddmmBackward0>)\n",
      "Batch 9, Loss: 0.4483916163444519\n",
      "tensor([2])\n",
      "tensor([[ 0.8630,  0.2855, -0.3610]], grad_fn=<AddmmBackward0>)\n",
      "Batch 10, Loss: 0.36104899644851685\n",
      "tensor([0])\n",
      "tensor([[ 0.8580,  0.2879, -0.3679]], grad_fn=<AddmmBackward0>)\n",
      "Batch 11, Loss: -0.8580247163772583\n",
      "tensor([0])\n",
      "tensor([[ 0.8405,  0.2962, -0.3921]], grad_fn=<AddmmBackward0>)\n",
      "Batch 12, Loss: -0.8404766321182251\n",
      "tensor([2])\n",
      "tensor([[ 0.8518,  0.2910, -0.3766]], grad_fn=<AddmmBackward0>)\n",
      "Batch 13, Loss: 0.3765948414802551\n",
      "tensor([2])\n",
      "tensor([[ 0.9032,  0.2672, -0.3059]], grad_fn=<AddmmBackward0>)\n",
      "Batch 14, Loss: 0.3058963418006897\n",
      "tensor([2])\n",
      "tensor([[ 0.8030,  0.3140, -0.4437]], grad_fn=<AddmmBackward0>)\n",
      "Batch 15, Loss: 0.44374099373817444\n",
      "tensor([2])\n",
      "tensor([[ 0.9066,  0.2658, -0.3012]], grad_fn=<AddmmBackward0>)\n",
      "Batch 16, Loss: 0.3012470602989197\n",
      "tensor([2])\n",
      "tensor([[ 0.8391,  0.2973, -0.3940]], grad_fn=<AddmmBackward0>)\n",
      "Batch 17, Loss: 0.39403724670410156\n",
      "tensor([0])\n",
      "tensor([[ 0.8732,  0.2815, -0.3470]], grad_fn=<AddmmBackward0>)\n",
      "Batch 18, Loss: -0.8731734156608582\n",
      "tensor([0])\n",
      "tensor([[ 0.8927,  0.2725, -0.3201]], grad_fn=<AddmmBackward0>)\n",
      "Batch 19, Loss: -0.8927403092384338\n",
      "tensor([2])\n",
      "tensor([[ 0.7514,  0.3384, -0.5146]], grad_fn=<AddmmBackward0>)\n",
      "Batch 20, Loss: 0.5145900845527649\n",
      "tensor([0])\n",
      "tensor([[ 0.9042,  0.2673, -0.3043]], grad_fn=<AddmmBackward0>)\n",
      "Batch 21, Loss: -0.9041848182678223\n",
      "tensor([1])\n",
      "tensor([[ 0.8910,  0.2736, -0.3225]], grad_fn=<AddmmBackward0>)\n",
      "Batch 22, Loss: -0.2735714316368103\n",
      "tensor([1])\n",
      "tensor([[ 0.5861,  0.4156, -0.7421]], grad_fn=<AddmmBackward0>)\n",
      "Batch 23, Loss: -0.41558122634887695\n",
      "tensor([0])\n",
      "tensor([[ 0.8556,  0.2903, -0.3713]], grad_fn=<AddmmBackward0>)\n",
      "Batch 24, Loss: -0.8555690050125122\n",
      "tensor([2])\n",
      "tensor([[ 0.8771,  0.2804, -0.3418]], grad_fn=<AddmmBackward0>)\n",
      "Batch 25, Loss: 0.341800332069397\n",
      "tensor([1])\n",
      "tensor([[ 0.7652,  0.3326, -0.4958]], grad_fn=<AddmmBackward0>)\n",
      "Batch 26, Loss: -0.332577109336853\n",
      "tensor([0])\n",
      "tensor([[ 0.9028,  0.2687, -0.3064]], grad_fn=<AddmmBackward0>)\n",
      "Batch 27, Loss: -0.9028342962265015\n",
      "tensor([0])\n",
      "tensor([[ 0.8968,  0.2717, -0.3148]], grad_fn=<AddmmBackward0>)\n",
      "Batch 28, Loss: -0.896782398223877\n",
      "tensor([2])\n",
      "tensor([[ 0.8909,  0.2746, -0.3230]], grad_fn=<AddmmBackward0>)\n",
      "Batch 29, Loss: 0.3230167627334595\n",
      "tensor([0])\n",
      "tensor([[ 0.7995,  0.3171, -0.4487]], grad_fn=<AddmmBackward0>)\n",
      "Batch 30, Loss: -0.7995412349700928\n",
      "tensor([1])\n",
      "tensor([[ 0.8784,  0.2806, -0.3404]], grad_fn=<AddmmBackward0>)\n",
      "Batch 31, Loss: -0.2806360721588135\n",
      "tensor([1])\n",
      "tensor([[ 0.8231,  0.3064, -0.4165]], grad_fn=<AddmmBackward0>)\n",
      "Batch 32, Loss: -0.30643725395202637\n",
      "tensor([2])\n",
      "tensor([[ 0.9018,  0.2701, -0.3083]], grad_fn=<AddmmBackward0>)\n",
      "Batch 33, Loss: 0.30833572149276733\n",
      "tensor([0])\n",
      "tensor([[ 0.6634,  0.3808, -0.6363]], grad_fn=<AddmmBackward0>)\n",
      "Batch 34, Loss: -0.6633882522583008\n",
      "tensor([2])\n",
      "tensor([[ 0.8398,  0.2992, -0.3937]], grad_fn=<AddmmBackward0>)\n",
      "Batch 35, Loss: 0.3937169313430786\n",
      "tensor([1])\n",
      "tensor([[ 0.8689,  0.2858, -0.3538]], grad_fn=<AddmmBackward0>)\n",
      "Batch 36, Loss: -0.2858322262763977\n",
      "tensor([2])\n",
      "tensor([[ 0.7135,  0.3579, -0.5675]], grad_fn=<AddmmBackward0>)\n",
      "Batch 37, Loss: 0.5674641132354736\n",
      "tensor([1])\n",
      "tensor([[ 0.8423,  0.2984, -0.3903]], grad_fn=<AddmmBackward0>)\n",
      "Batch 38, Loss: -0.29842084646224976\n",
      "tensor([0])\n",
      "tensor([[ 0.7339,  0.3487, -0.5393]], grad_fn=<AddmmBackward0>)\n",
      "Batch 39, Loss: -0.7339491844177246\n",
      "tensor([2])\n",
      "tensor([[ 0.8394,  0.3001, -0.3943]], grad_fn=<AddmmBackward0>)\n",
      "Batch 40, Loss: 0.3943474292755127\n",
      "tensor([1])\n",
      "tensor([[ 0.6887,  0.3699, -0.6015]], grad_fn=<AddmmBackward0>)\n",
      "Batch 41, Loss: -0.36986392736434937\n",
      "tensor([0])\n",
      "tensor([[ 0.9003,  0.2722, -0.3105]], grad_fn=<AddmmBackward0>)\n",
      "Batch 42, Loss: -0.9003125429153442\n",
      "tensor([2])\n",
      "tensor([[ 0.8852,  0.2794, -0.3313]], grad_fn=<AddmmBackward0>)\n",
      "Batch 43, Loss: 0.33133602142333984\n",
      "tensor([2])\n",
      "tensor([[ 0.9003,  0.2725, -0.3105]], grad_fn=<AddmmBackward0>)\n",
      "Batch 44, Loss: 0.3105499744415283\n",
      "tensor([1])\n",
      "tensor([[ 0.9036,  0.2711, -0.3060]], grad_fn=<AddmmBackward0>)\n",
      "Batch 45, Loss: -0.27114665508270264\n",
      "tensor([2])\n",
      "tensor([[ 0.8827,  0.2810, -0.3347]], grad_fn=<AddmmBackward0>)\n",
      "Batch 46, Loss: 0.33474332094192505\n",
      "tensor([2])\n",
      "tensor([[ 0.5519,  0.4337, -0.7897]], grad_fn=<AddmmBackward0>)\n",
      "Batch 47, Loss: 0.7897276878356934\n",
      "tensor([2])\n",
      "tensor([[ 0.8775,  0.2836, -0.3417]], grad_fn=<AddmmBackward0>)\n",
      "Batch 48, Loss: 0.3417432904243469\n",
      "tensor([2])\n",
      "tensor([[ 0.8853,  0.2801, -0.3309]], grad_fn=<AddmmBackward0>)\n",
      "Batch 49, Loss: 0.3309016227722168\n",
      "tensor([1])\n",
      "tensor([[ 0.8263,  0.3074, -0.4120]], grad_fn=<AddmmBackward0>)\n",
      "Batch 50, Loss: -0.30744606256484985\n",
      "tensor([2])\n",
      "tensor([[ 0.6750,  0.3773, -0.6201]], grad_fn=<AddmmBackward0>)\n",
      "Batch 51, Loss: 0.6200711727142334\n",
      "tensor([1])\n",
      "tensor([[ 0.6248,  0.4005, -0.6890]], grad_fn=<AddmmBackward0>)\n",
      "Batch 52, Loss: -0.4004542827606201\n",
      "tensor([2])\n",
      "tensor([[ 0.8928,  0.2772, -0.3201]], grad_fn=<AddmmBackward0>)\n",
      "Batch 53, Loss: 0.320081889629364\n",
      "tensor([0])\n",
      "tensor([[ 0.7942,  0.3227, -0.4557]], grad_fn=<AddmmBackward0>)\n",
      "Batch 54, Loss: -0.794207751750946\n",
      "tensor([1])\n",
      "tensor([[ 0.8385,  0.3024, -0.3946]], grad_fn=<AddmmBackward0>)\n",
      "Batch 55, Loss: -0.3023754358291626\n",
      "tensor([2])\n",
      "tensor([[ 0.8747,  0.2859, -0.3447]], grad_fn=<AddmmBackward0>)\n",
      "Batch 56, Loss: 0.3447158932685852\n",
      "tensor([0])\n",
      "tensor([[ 0.7445,  0.3459, -0.5239]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 57, Loss: -0.7444624900817871\n",
      "tensor([1])\n",
      "tensor([[ 0.8683,  0.2891, -0.3533]], grad_fn=<AddmmBackward0>)\n",
      "Batch 58, Loss: -0.2890613079071045\n",
      "tensor([1])\n",
      "tensor([[ 0.9081,  0.2709, -0.2985]], grad_fn=<AddmmBackward0>)\n",
      "Batch 59, Loss: -0.27091169357299805\n",
      "tensor([2])\n",
      "tensor([[ 0.8751,  0.2863, -0.3439]], grad_fn=<AddmmBackward0>)\n",
      "Batch 60, Loss: 0.3438642621040344\n",
      "tensor([0])\n",
      "tensor([[ 0.8858,  0.2815, -0.3291]], grad_fn=<AddmmBackward0>)\n",
      "Batch 61, Loss: -0.8858011960983276\n",
      "tensor([0])\n",
      "tensor([[ 0.8620,  0.2926, -0.3618]], grad_fn=<AddmmBackward0>)\n",
      "Batch 62, Loss: -0.8620198965072632\n",
      "tensor([1])\n",
      "tensor([[ 0.8352,  0.3050, -0.3987]], grad_fn=<AddmmBackward0>)\n",
      "Batch 63, Loss: -0.3050115406513214\n",
      "tensor([1])\n",
      "tensor([[ 0.9065,  0.2725, -0.3006]], grad_fn=<AddmmBackward0>)\n",
      "Batch 64, Loss: -0.2725127637386322\n",
      "tensor([2])\n",
      "tensor([[ 0.8097,  0.3171, -0.4340]], grad_fn=<AddmmBackward0>)\n",
      "Batch 65, Loss: 0.4339865744113922\n",
      "tensor([2])\n",
      "tensor([[ 0.8968,  0.2773, -0.3140]], grad_fn=<AddmmBackward0>)\n",
      "Batch 66, Loss: 0.31395405530929565\n",
      "tensor([2])\n",
      "tensor([[ 0.7519,  0.3439, -0.5135]], grad_fn=<AddmmBackward0>)\n",
      "Batch 67, Loss: 0.5135258436203003\n",
      "tensor([1])\n",
      "tensor([[ 0.8972,  0.2774, -0.3133]], grad_fn=<AddmmBackward0>)\n",
      "Batch 68, Loss: -0.277446985244751\n",
      "tensor([0])\n",
      "tensor([[ 0.5970,  0.4149, -0.7266]], grad_fn=<AddmmBackward0>)\n",
      "Batch 69, Loss: -0.5970411896705627\n",
      "tensor([1])\n",
      "tensor([[ 0.8016,  0.3215, -0.4449]], grad_fn=<AddmmBackward0>)\n",
      "Batch 70, Loss: -0.3214973509311676\n",
      "tensor([0])\n",
      "tensor([[ 0.8646,  0.2929, -0.3581]], grad_fn=<AddmmBackward0>)\n",
      "Batch 71, Loss: -0.8646261692047119\n",
      "tensor([1])\n",
      "tensor([[ 0.8322,  0.3079, -0.4028]], grad_fn=<AddmmBackward0>)\n",
      "Batch 72, Loss: -0.3078662157058716\n",
      "tensor([2])\n",
      "tensor([[ 0.9035,  0.2755, -0.3046]], grad_fn=<AddmmBackward0>)\n",
      "Batch 73, Loss: 0.30463820695877075\n",
      "tensor([2])\n",
      "tensor([[ 0.8972,  0.2785, -0.3132]], grad_fn=<AddmmBackward0>)\n",
      "Batch 74, Loss: 0.3131939172744751\n",
      "tensor([2])\n",
      "tensor([[ 0.7594,  0.3415, -0.5030]], grad_fn=<AddmmBackward0>)\n",
      "Batch 75, Loss: 0.503002405166626\n",
      "tensor([1])\n",
      "tensor([[ 0.8778,  0.2877, -0.3398]], grad_fn=<AddmmBackward0>)\n",
      "Batch 76, Loss: -0.287665992975235\n",
      "tensor([0])\n",
      "tensor([[ 0.6540,  0.3899, -0.6481]], grad_fn=<AddmmBackward0>)\n",
      "Batch 77, Loss: -0.6539793014526367\n",
      "tensor([0])\n",
      "tensor([[ 0.8884,  0.2831, -0.3251]], grad_fn=<AddmmBackward0>)\n",
      "Batch 78, Loss: -0.8884099721908569\n",
      "tensor([1])\n",
      "tensor([[ 0.8698,  0.2918, -0.3508]], grad_fn=<AddmmBackward0>)\n",
      "Batch 79, Loss: -0.29178333282470703\n",
      "tensor([2])\n",
      "tensor([[ 0.8209,  0.3142, -0.4181]], grad_fn=<AddmmBackward0>)\n",
      "Batch 80, Loss: 0.4181225597858429\n",
      "tensor([1])\n",
      "tensor([[ 0.8251,  0.3125, -0.4124]], grad_fn=<AddmmBackward0>)\n",
      "Batch 81, Loss: -0.3124503195285797\n",
      "tensor([2])\n",
      "tensor([[ 0.6166,  0.4075, -0.6997]], grad_fn=<AddmmBackward0>)\n",
      "Batch 82, Loss: 0.6996515989303589\n",
      "tensor([2])\n",
      "tensor([[ 0.7976,  0.3253, -0.4502]], grad_fn=<AddmmBackward0>)\n",
      "Batch 83, Loss: 0.4501740634441376\n",
      "tensor([1])\n",
      "tensor([[ 0.8688,  0.2931, -0.3521]], grad_fn=<AddmmBackward0>)\n",
      "Batch 84, Loss: -0.2930571436882019\n",
      "tensor([2])\n",
      "tensor([[ 0.7997,  0.3246, -0.4472]], grad_fn=<AddmmBackward0>)\n",
      "Batch 85, Loss: 0.4472276568412781\n",
      "tensor([0])\n",
      "tensor([[ 0.8629,  0.2960, -0.3601]], grad_fn=<AddmmBackward0>)\n",
      "Batch 86, Loss: -0.8628963232040405\n",
      "tensor([1])\n",
      "tensor([[ 0.8483,  0.3028, -0.3801]], grad_fn=<AddmmBackward0>)\n",
      "Batch 87, Loss: -0.30279648303985596\n",
      "tensor([2])\n",
      "tensor([[ 0.8529,  0.3009, -0.3738]], grad_fn=<AddmmBackward0>)\n",
      "Batch 88, Loss: 0.3738333582878113\n",
      "tensor([2])\n",
      "tensor([[ 0.7089,  0.3664, -0.5723]], grad_fn=<AddmmBackward0>)\n",
      "Batch 89, Loss: 0.5722577571868896\n",
      "tensor([2])\n",
      "tensor([[ 0.8592,  0.2983, -0.3650]], grad_fn=<AddmmBackward0>)\n",
      "Batch 90, Loss: 0.3650054931640625\n",
      "tensor([2])\n",
      "tensor([[ 0.9110,  0.2749, -0.2934]], grad_fn=<AddmmBackward0>)\n",
      "Batch 91, Loss: 0.29343730211257935\n",
      "tensor([2])\n",
      "tensor([[ 0.9088,  0.2761, -0.2965]], grad_fn=<AddmmBackward0>)\n",
      "Batch 92, Loss: 0.2964608669281006\n",
      "tensor([1])\n",
      "tensor([[ 0.7552,  0.3458, -0.5080]], grad_fn=<AddmmBackward0>)\n",
      "Batch 93, Loss: -0.3457525372505188\n",
      "tensor([2])\n",
      "tensor([[ 0.8916,  0.2841, -0.3199]], grad_fn=<AddmmBackward0>)\n",
      "Batch 94, Loss: 0.31985509395599365\n",
      "tensor([1])\n",
      "tensor([[ 0.9002,  0.2803, -0.3077]], grad_fn=<AddmmBackward0>)\n",
      "Batch 95, Loss: -0.2802618145942688\n",
      "tensor([0])\n",
      "tensor([[ 0.8849,  0.2873, -0.3287]], grad_fn=<AddmmBackward0>)\n",
      "Batch 96, Loss: -0.8849307298660278\n",
      "tensor([1])\n",
      "tensor([[ 0.8603,  0.2986, -0.3626]], grad_fn=<AddmmBackward0>)\n",
      "Batch 97, Loss: -0.2986159324645996\n",
      "tensor([2])\n",
      "tensor([[ 0.7520,  0.3477, -0.5119]], grad_fn=<AddmmBackward0>)\n",
      "Batch 98, Loss: 0.5118511915206909\n",
      "tensor([1])\n",
      "tensor([[ 0.6587,  0.3901, -0.6406]], grad_fn=<AddmmBackward0>)\n",
      "Batch 99, Loss: -0.39006179571151733\n",
      "tensor([2])\n",
      "tensor([[ 0.8447,  0.3061, -0.3839]], grad_fn=<AddmmBackward0>)\n",
      "Batch 100, Loss: 0.38387537002563477\n",
      "tensor([2])\n",
      "tensor([[ 0.8980,  0.2822, -0.3103]], grad_fn=<AddmmBackward0>)\n",
      "Batch 101, Loss: 0.3102940320968628\n",
      "tensor([0])\n",
      "tensor([[ 0.8349,  0.3108, -0.3972]], grad_fn=<AddmmBackward0>)\n",
      "Batch 102, Loss: -0.8348736763000488\n",
      "tensor([0])\n",
      "tensor([[ 0.7345,  0.3562, -0.5357]], grad_fn=<AddmmBackward0>)\n",
      "Batch 103, Loss: -0.7344911694526672\n",
      "tensor([1])\n",
      "tensor([[ 0.7494,  0.3496, -0.5150]], grad_fn=<AddmmBackward0>)\n",
      "Batch 104, Loss: -0.3495904207229614\n",
      "tensor([2])\n",
      "tensor([[ 0.9122,  0.2763, -0.2904]], grad_fn=<AddmmBackward0>)\n",
      "Batch 105, Loss: 0.290424644947052\n",
      "tensor([2])\n",
      "tensor([[ 0.8572,  0.3012, -0.3663]], grad_fn=<AddmmBackward0>)\n",
      "Batch 106, Loss: 0.3662843704223633\n",
      "tensor([1])\n",
      "tensor([[ 0.8962,  0.2837, -0.3123]], grad_fn=<AddmmBackward0>)\n",
      "Batch 107, Loss: -0.283743679523468\n",
      "tensor([0])\n",
      "tensor([[ 0.9090,  0.2781, -0.2946]], grad_fn=<AddmmBackward0>)\n",
      "Batch 108, Loss: -0.9090184569358826\n",
      "tensor([0])\n",
      "tensor([[ 0.8297,  0.3140, -0.4041]], grad_fn=<AddmmBackward0>)\n",
      "Batch 109, Loss: -0.8296833634376526\n",
      "tensor([2])\n",
      "tensor([[ 0.6352,  0.4018, -0.6725]], grad_fn=<AddmmBackward0>)\n",
      "Batch 110, Loss: 0.6725245714187622\n",
      "tensor([1])\n",
      "tensor([[ 0.7210,  0.3632, -0.5540]], grad_fn=<AddmmBackward0>)\n",
      "Batch 111, Loss: -0.36319079995155334\n",
      "tensor([0])\n",
      "tensor([[ 0.9017,  0.2820, -0.3047]], grad_fn=<AddmmBackward0>)\n",
      "Batch 112, Loss: -0.9016736745834351\n",
      "tensor([1])\n",
      "tensor([[ 0.8260,  0.3162, -0.4092]], grad_fn=<AddmmBackward0>)\n",
      "Batch 113, Loss: -0.3162161707878113\n",
      "tensor([1])\n",
      "tensor([[ 0.7613,  0.3454, -0.4985]], grad_fn=<AddmmBackward0>)\n",
      "Batch 114, Loss: -0.3454466462135315\n",
      "tensor([2])\n",
      "tensor([[ 0.9053,  0.2809, -0.2999]], grad_fn=<AddmmBackward0>)\n",
      "Batch 115, Loss: 0.29987668991088867\n",
      "tensor([1])\n",
      "tensor([[ 0.8862,  0.2896, -0.3263]], grad_fn=<AddmmBackward0>)\n",
      "Batch 116, Loss: -0.28964728116989136\n",
      "tensor([2])\n",
      "tensor([[ 0.8403,  0.3104, -0.3896]], grad_fn=<AddmmBackward0>)\n",
      "Batch 117, Loss: 0.3896116018295288\n",
      "tensor([2])\n",
      "tensor([[ 0.8980,  0.2847, -0.3100]], grad_fn=<AddmmBackward0>)\n",
      "Batch 118, Loss: 0.3099519610404968\n",
      "tensor([2])\n",
      "tensor([[ 0.8714,  0.2968, -0.3466]], grad_fn=<AddmmBackward0>)\n",
      "Batch 119, Loss: 0.34657472372055054\n",
      "tensor([2])\n",
      "tensor([[ 0.8167,  0.3214, -0.4219]], grad_fn=<AddmmBackward0>)\n",
      "Batch 120, Loss: 0.4219253659248352\n",
      "tensor([1])\n",
      "tensor([[ 0.7945,  0.3315, -0.4525]], grad_fn=<AddmmBackward0>)\n",
      "Batch 121, Loss: -0.3314841687679291\n",
      "tensor([2])\n",
      "tensor([[ 0.6707,  0.3871, -0.6233]], grad_fn=<AddmmBackward0>)\n",
      "Batch 122, Loss: 0.6232897639274597\n",
      "tensor([2])\n",
      "tensor([[ 0.8391,  0.3118, -0.3908]], grad_fn=<AddmmBackward0>)\n",
      "Batch 123, Loss: 0.39077770709991455\n",
      "tensor([0])\n",
      "tensor([[ 0.9127,  0.2789, -0.2890]], grad_fn=<AddmmBackward0>)\n",
      "Batch 124, Loss: -0.9127057194709778\n",
      "tensor([1])\n",
      "tensor([[ 0.8659,  0.3000, -0.3536]], grad_fn=<AddmmBackward0>)\n",
      "Batch 125, Loss: -0.29998403787612915\n",
      "tensor([1])\n",
      "tensor([[ 0.8554,  0.3048, -0.3680]], grad_fn=<AddmmBackward0>)\n",
      "Batch 126, Loss: -0.30480194091796875\n",
      "tensor([1])\n",
      "tensor([[ 0.8705,  0.2982, -0.3470]], grad_fn=<AddmmBackward0>)\n",
      "Batch 127, Loss: -0.2981884479522705\n",
      "tensor([0])\n",
      "tensor([[ 0.8250,  0.3187, -0.4098]], grad_fn=<AddmmBackward0>)\n",
      "Batch 128, Loss: -0.8250476121902466\n",
      "tensor([2])\n",
      "tensor([[ 0.8969,  0.2867, -0.3105]], grad_fn=<AddmmBackward0>)\n",
      "Batch 129, Loss: 0.3105241060256958\n",
      "tensor([0])\n",
      "tensor([[ 0.8103,  0.3256, -0.4302]], grad_fn=<AddmmBackward0>)\n",
      "Batch 130, Loss: -0.8102501034736633\n",
      "tensor([1])\n",
      "tensor([[ 0.8779,  0.2955, -0.3368]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 131, Loss: -0.2955447733402252\n",
      "tensor([0])\n",
      "tensor([[ 0.9068,  0.2828, -0.2969]], grad_fn=<AddmmBackward0>)\n",
      "Batch 132, Loss: -0.9067946672439575\n",
      "tensor([0])\n",
      "tensor([[ 0.9104,  0.2814, -0.2920]], grad_fn=<AddmmBackward0>)\n",
      "Batch 133, Loss: -0.9103896617889404\n",
      "tensor([1])\n",
      "tensor([[ 0.8646,  0.3020, -0.3554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 134, Loss: -0.3020211160182953\n",
      "tensor([1])\n",
      "tensor([[ 0.6694,  0.3892, -0.6249]], grad_fn=<AddmmBackward0>)\n",
      "Batch 135, Loss: -0.3892461657524109\n",
      "tensor([1])\n",
      "tensor([[ 0.8807,  0.2952, -0.3333]], grad_fn=<AddmmBackward0>)\n",
      "Batch 136, Loss: -0.29523032903671265\n",
      "tensor([2])\n",
      "tensor([[ 0.8856,  0.2932, -0.3265]], grad_fn=<AddmmBackward0>)\n",
      "Batch 137, Loss: 0.32650983333587646\n",
      "tensor([1])\n",
      "tensor([[ 0.8574,  0.3060, -0.3655]], grad_fn=<AddmmBackward0>)\n",
      "Batch 138, Loss: -0.30598291754722595\n",
      "tensor([2])\n",
      "tensor([[ 0.8078,  0.3283, -0.4340]], grad_fn=<AddmmBackward0>)\n",
      "Batch 139, Loss: 0.43400922417640686\n",
      "tensor([0])\n",
      "tensor([[ 0.6865,  0.3824, -0.6015]], grad_fn=<AddmmBackward0>)\n",
      "Batch 140, Loss: -0.6865035891532898\n",
      "tensor([0])\n",
      "tensor([[ 0.8462,  0.3115, -0.3811]], grad_fn=<AddmmBackward0>)\n",
      "Batch 141, Loss: -0.8461941480636597\n",
      "tensor([2])\n",
      "tensor([[ 0.8905,  0.2920, -0.3200]], grad_fn=<AddmmBackward0>)\n",
      "Batch 142, Loss: 0.3200175166130066\n",
      "tensor([2])\n",
      "tensor([[ 0.7973,  0.3336, -0.4487]], grad_fn=<AddmmBackward0>)\n",
      "Batch 143, Loss: 0.4486946165561676\n",
      "tensor([0])\n",
      "tensor([[ 0.9190,  0.2797, -0.2807]], grad_fn=<AddmmBackward0>)\n",
      "Batch 144, Loss: -0.9189524054527283\n",
      "tensor([0])\n",
      "tensor([[ 0.8281,  0.3202, -0.4062]], grad_fn=<AddmmBackward0>)\n",
      "Batch 145, Loss: -0.8280776143074036\n",
      "tensor([0])\n",
      "tensor([[ 0.8626,  0.3050, -0.3587]], grad_fn=<AddmmBackward0>)\n",
      "Batch 146, Loss: -0.8625915050506592\n",
      "tensor([2])\n",
      "tensor([[ 0.9167,  0.2811, -0.2841]], grad_fn=<AddmmBackward0>)\n",
      "Batch 147, Loss: 0.2840666174888611\n",
      "tensor([2])\n",
      "tensor([[ 0.8295,  0.3199, -0.4045]], grad_fn=<AddmmBackward0>)\n",
      "Batch 148, Loss: 0.40447384119033813\n",
      "tensor([1])\n",
      "tensor([[ 0.8707,  0.3018, -0.3478]], grad_fn=<AddmmBackward0>)\n",
      "Batch 149, Loss: -0.3017734885215759\n",
      "tensor([0])\n",
      "tensor([[ 0.9164,  0.2816, -0.2847]], grad_fn=<AddmmBackward0>)\n",
      "Batch 150, Loss: -0.9164190888404846\n",
      "tensor([1])\n",
      "tensor([[ 0.8880,  0.2944, -0.3240]], grad_fn=<AddmmBackward0>)\n",
      "Batch 151, Loss: -0.2943848669528961\n",
      "tensor([0])\n",
      "tensor([[ 0.8158,  0.3265, -0.4237]], grad_fn=<AddmmBackward0>)\n",
      "Batch 152, Loss: -0.8157541751861572\n",
      "tensor([2])\n",
      "tensor([[ 0.8430,  0.3146, -0.3863]], grad_fn=<AddmmBackward0>)\n",
      "Batch 153, Loss: 0.3863070011138916\n",
      "tensor([2])\n",
      "tensor([[ 0.8921,  0.2931, -0.3186]], grad_fn=<AddmmBackward0>)\n",
      "Batch 154, Loss: 0.31864070892333984\n",
      "tensor([0])\n",
      "tensor([[ 0.8691,  0.3033, -0.3503]], grad_fn=<AddmmBackward0>)\n",
      "Batch 155, Loss: -0.8691120147705078\n",
      "tensor([1])\n",
      "tensor([[ 0.7714,  0.3467, -0.4852]], grad_fn=<AddmmBackward0>)\n",
      "Batch 156, Loss: -0.3466668725013733\n",
      "tensor([1])\n",
      "tensor([[ 0.8817,  0.2981, -0.3331]], grad_fn=<AddmmBackward0>)\n",
      "Batch 157, Loss: -0.29805290699005127\n",
      "tensor([0])\n",
      "tensor([[ 0.7594,  0.3522, -0.5018]], grad_fn=<AddmmBackward0>)\n",
      "Batch 158, Loss: -0.7594084739685059\n",
      "tensor([1])\n",
      "tensor([[ 0.8611,  0.3075, -0.3617]], grad_fn=<AddmmBackward0>)\n",
      "Batch 159, Loss: -0.3074866831302643\n",
      "tensor([2])\n",
      "tensor([[ 0.8583,  0.3089, -0.3656]], grad_fn=<AddmmBackward0>)\n",
      "Batch 160, Loss: 0.3655892610549927\n",
      "tensor([1])\n",
      "tensor([[ 0.9118,  0.2854, -0.2919]], grad_fn=<AddmmBackward0>)\n",
      "Batch 161, Loss: -0.2854434847831726\n",
      "tensor([0])\n",
      "tensor([[ 0.9221,  0.2811, -0.2777]], grad_fn=<AddmmBackward0>)\n",
      "Batch 162, Loss: -0.9220897555351257\n",
      "tensor([1])\n",
      "tensor([[ 0.9130,  0.2853, -0.2903]], grad_fn=<AddmmBackward0>)\n",
      "Batch 163, Loss: -0.28529006242752075\n",
      "tensor([0])\n",
      "tensor([[ 0.8646,  0.3068, -0.3571]], grad_fn=<AddmmBackward0>)\n",
      "Batch 164, Loss: -0.8646368980407715\n",
      "tensor([0])\n",
      "tensor([[ 0.7527,  0.3563, -0.5116]], grad_fn=<AddmmBackward0>)\n",
      "Batch 165, Loss: -0.7526618242263794\n",
      "tensor([1])\n",
      "tensor([[ 0.8403,  0.3179, -0.3909]], grad_fn=<AddmmBackward0>)\n",
      "Batch 166, Loss: -0.3178972005844116\n",
      "tensor([1])\n",
      "tensor([[ 0.7423,  0.3612, -0.5261]], grad_fn=<AddmmBackward0>)\n",
      "Batch 167, Loss: -0.3611820936203003\n",
      "tensor([2])\n",
      "tensor([[ 0.8927,  0.2952, -0.3189]], grad_fn=<AddmmBackward0>)\n",
      "Batch 168, Loss: 0.3189448118209839\n",
      "tensor([0])\n",
      "tensor([[ 0.8875,  0.2977, -0.3261]], grad_fn=<AddmmBackward0>)\n",
      "Batch 169, Loss: -0.8874852657318115\n",
      "tensor([2])\n",
      "tensor([[ 0.9111,  0.2875, -0.2937]], grad_fn=<AddmmBackward0>)\n",
      "Batch 170, Loss: 0.2936936020851135\n",
      "tensor([1])\n",
      "tensor([[ 0.9207,  0.2835, -0.2806]], grad_fn=<AddmmBackward0>)\n",
      "Batch 171, Loss: -0.2834998369216919\n",
      "tensor([1])\n",
      "tensor([[ 0.9225,  0.2829, -0.2781]], grad_fn=<AddmmBackward0>)\n",
      "Batch 172, Loss: -0.28289157152175903\n",
      "tensor([2])\n",
      "tensor([[ 0.7684,  0.3507, -0.4905]], grad_fn=<AddmmBackward0>)\n",
      "Batch 173, Loss: 0.49050620198249817\n",
      "tensor([2])\n",
      "tensor([[ 0.9227,  0.2832, -0.2779]], grad_fn=<AddmmBackward0>)\n",
      "Batch 174, Loss: 0.2779359221458435\n",
      "tensor([2])\n",
      "tensor([[ 0.8856,  0.2996, -0.3290]], grad_fn=<AddmmBackward0>)\n",
      "Batch 175, Loss: 0.3289700150489807\n",
      "tensor([2])\n",
      "tensor([[ 0.8537,  0.3137, -0.3729]], grad_fn=<AddmmBackward0>)\n",
      "Batch 176, Loss: 0.372938871383667\n",
      "tensor([1])\n",
      "tensor([[ 0.8456,  0.3174, -0.3840]], grad_fn=<AddmmBackward0>)\n",
      "Batch 177, Loss: -0.3174089789390564\n",
      "tensor([0])\n",
      "tensor([[ 0.9171,  0.2863, -0.2854]], grad_fn=<AddmmBackward0>)\n",
      "Batch 178, Loss: -0.917096734046936\n",
      "tensor([0])\n",
      "tensor([[ 0.8630,  0.3101, -0.3599]], grad_fn=<AddmmBackward0>)\n",
      "Batch 179, Loss: -0.8630424737930298\n",
      "tensor([2])\n",
      "tensor([[ 0.8608,  0.3112, -0.3630]], grad_fn=<AddmmBackward0>)\n",
      "Batch 180, Loss: 0.363025963306427\n",
      "tensor([0])\n",
      "tensor([[ 0.8565,  0.3132, -0.3689]], grad_fn=<AddmmBackward0>)\n",
      "Batch 181, Loss: -0.8565270304679871\n",
      "tensor([1])\n",
      "tensor([[ 0.7877,  0.3434, -0.4638]], grad_fn=<AddmmBackward0>)\n",
      "Batch 182, Loss: -0.3434080481529236\n",
      "tensor([2])\n",
      "tensor([[ 0.8974,  0.2956, -0.3127]], grad_fn=<AddmmBackward0>)\n",
      "Batch 183, Loss: 0.3126901388168335\n",
      "tensor([2])\n",
      "tensor([[ 0.9253,  0.2836, -0.2742]], grad_fn=<AddmmBackward0>)\n",
      "Batch 184, Loss: 0.2742401361465454\n",
      "tensor([0])\n",
      "tensor([[ 0.8790,  0.3039, -0.3380]], grad_fn=<AddmmBackward0>)\n",
      "Batch 185, Loss: -0.87903892993927\n",
      "tensor([0])\n",
      "tensor([[ 0.9007,  0.2946, -0.3081]], grad_fn=<AddmmBackward0>)\n",
      "Batch 186, Loss: -0.9006809592247009\n",
      "tensor([2])\n",
      "tensor([[ 0.9080,  0.2915, -0.2982]], grad_fn=<AddmmBackward0>)\n",
      "Batch 187, Loss: 0.2981504201889038\n",
      "tensor([2])\n",
      "tensor([[ 0.9294,  0.2823, -0.2687]], grad_fn=<AddmmBackward0>)\n",
      "Batch 188, Loss: 0.26867491006851196\n",
      "tensor([1])\n",
      "tensor([[ 0.8142,  0.3326, -0.4274]], grad_fn=<AddmmBackward0>)\n",
      "Batch 189, Loss: -0.3326442837715149\n",
      "tensor([1])\n",
      "tensor([[ 0.8999,  0.2954, -0.3093]], grad_fn=<AddmmBackward0>)\n",
      "Batch 190, Loss: -0.29542914032936096\n",
      "tensor([0])\n",
      "tensor([[ 0.9287,  0.2830, -0.2695]], grad_fn=<AddmmBackward0>)\n",
      "Batch 191, Loss: -0.928743302822113\n",
      "tensor([1])\n",
      "tensor([[ 0.9260,  0.2844, -0.2733]], grad_fn=<AddmmBackward0>)\n",
      "Batch 192, Loss: -0.2843531370162964\n",
      "tensor([1])\n",
      "tensor([[ 0.8736,  0.3073, -0.3455]], grad_fn=<AddmmBackward0>)\n",
      "Batch 193, Loss: -0.3073248267173767\n",
      "tensor([1])\n",
      "tensor([[ 0.7374,  0.3668, -0.5333]], grad_fn=<AddmmBackward0>)\n",
      "Batch 194, Loss: -0.366773396730423\n",
      "tensor([2])\n",
      "tensor([[ 0.8164,  0.3326, -0.4244]], grad_fn=<AddmmBackward0>)\n",
      "Batch 195, Loss: 0.4243980944156647\n",
      "tensor([2])\n",
      "tensor([[ 0.8004,  0.3397, -0.4465]], grad_fn=<AddmmBackward0>)\n",
      "Batch 196, Loss: 0.44653940200805664\n",
      "tensor([0])\n",
      "tensor([[ 0.7077,  0.3801, -0.5742]], grad_fn=<AddmmBackward0>)\n",
      "Batch 197, Loss: -0.7076524496078491\n",
      "tensor([2])\n",
      "tensor([[ 0.9312,  0.2832, -0.2662]], grad_fn=<AddmmBackward0>)\n",
      "Batch 198, Loss: 0.26619815826416016\n",
      "tensor([1])\n",
      "tensor([[ 0.8737,  0.3083, -0.3454]], grad_fn=<AddmmBackward0>)\n",
      "Batch 199, Loss: -0.30827465653419495\n",
      "tensor([2])\n",
      "tensor([[ 0.8008,  0.3401, -0.4459]], grad_fn=<AddmmBackward0>)\n",
      "Batch 200, Loss: 0.4458869993686676\n",
      "tensor([1])\n",
      "tensor([[ 0.8200,  0.3319, -0.4194]], grad_fn=<AddmmBackward0>)\n",
      "Batch 201, Loss: -0.3318909704685211\n",
      "tensor([0])\n",
      "tensor([[ 0.8833,  0.3046, -0.3322]], grad_fn=<AddmmBackward0>)\n",
      "Batch 202, Loss: -0.883253812789917\n",
      "tensor([2])\n",
      "tensor([[ 0.7210,  0.3751, -0.5557]], grad_fn=<AddmmBackward0>)\n",
      "Batch 203, Loss: 0.5556995868682861\n",
      "tensor([2])\n",
      "tensor([[ 0.6776,  0.3940, -0.6156]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 204, Loss: 0.6155743598937988\n",
      "tensor([0])\n",
      "tensor([[ 0.9062,  0.2951, -0.3005]], grad_fn=<AddmmBackward0>)\n",
      "Batch 205, Loss: -0.9061592817306519\n",
      "tensor([0])\n",
      "tensor([[ 0.9107,  0.2933, -0.2942]], grad_fn=<AddmmBackward0>)\n",
      "Batch 206, Loss: -0.9107450246810913\n",
      "tensor([0])\n",
      "tensor([[ 0.7559,  0.3604, -0.5077]], grad_fn=<AddmmBackward0>)\n",
      "Batch 207, Loss: -0.7558577060699463\n",
      "tensor([0])\n",
      "tensor([[ 0.9140,  0.2921, -0.2899]], grad_fn=<AddmmBackward0>)\n",
      "Batch 208, Loss: -0.9140294790267944\n",
      "tensor([0])\n",
      "tensor([[ 0.9302,  0.2853, -0.2678]], grad_fn=<AddmmBackward0>)\n",
      "Batch 209, Loss: -0.9301642775535583\n",
      "tensor([0])\n",
      "tensor([[ 0.8165,  0.3346, -0.4245]], grad_fn=<AddmmBackward0>)\n",
      "Batch 210, Loss: -0.8164995908737183\n",
      "tensor([2])\n",
      "tensor([[ 0.8248,  0.3311, -0.4133]], grad_fn=<AddmmBackward0>)\n",
      "Batch 211, Loss: 0.413270503282547\n",
      "tensor([0])\n",
      "tensor([[ 0.8709,  0.3113, -0.3499]], grad_fn=<AddmmBackward0>)\n",
      "Batch 212, Loss: -0.8709083795547485\n",
      "tensor([1])\n",
      "tensor([[ 0.7633,  0.3579, -0.4982]], grad_fn=<AddmmBackward0>)\n",
      "Batch 213, Loss: -0.35788142681121826\n",
      "tensor([1])\n",
      "tensor([[ 0.7994,  0.3424, -0.4486]], grad_fn=<AddmmBackward0>)\n",
      "Batch 214, Loss: -0.34240883588790894\n",
      "tensor([1])\n",
      "tensor([[ 0.9244,  0.2886, -0.2767]], grad_fn=<AddmmBackward0>)\n",
      "Batch 215, Loss: -0.28861892223358154\n",
      "tensor([1])\n",
      "tensor([[ 0.7815,  0.3505, -0.4736]], grad_fn=<AddmmBackward0>)\n",
      "Batch 216, Loss: -0.3504515290260315\n",
      "tensor([2])\n",
      "tensor([[ 0.6659,  0.4004, -0.6327]], grad_fn=<AddmmBackward0>)\n",
      "Batch 217, Loss: 0.6327216625213623\n",
      "tensor([1])\n",
      "tensor([[ 0.8364,  0.3271, -0.3982]], grad_fn=<AddmmBackward0>)\n",
      "Batch 218, Loss: -0.3271105885505676\n",
      "tensor([1])\n",
      "tensor([[ 0.8865,  0.3057, -0.3293]], grad_fn=<AddmmBackward0>)\n",
      "Batch 219, Loss: -0.3057019114494324\n",
      "tensor([0])\n",
      "tensor([[ 0.6778,  0.3957, -0.6165]], grad_fn=<AddmmBackward0>)\n",
      "Batch 220, Loss: -0.6777825355529785\n",
      "tensor([0])\n",
      "tensor([[ 0.9202,  0.2916, -0.2830]], grad_fn=<AddmmBackward0>)\n",
      "Batch 221, Loss: -0.9202430844306946\n",
      "tensor([1])\n",
      "tensor([[ 0.9119,  0.2954, -0.2947]], grad_fn=<AddmmBackward0>)\n",
      "Batch 222, Loss: -0.2953646779060364\n",
      "tensor([0])\n",
      "tensor([[ 0.9080,  0.2972, -0.3001]], grad_fn=<AddmmBackward0>)\n",
      "Batch 223, Loss: -0.9079654812812805\n",
      "tensor([2])\n",
      "tensor([[ 0.8872,  0.3063, -0.3288]], grad_fn=<AddmmBackward0>)\n",
      "Batch 224, Loss: 0.32877033948898315\n",
      "tensor([2])\n",
      "tensor([[ 0.8535,  0.3210, -0.3753]], grad_fn=<AddmmBackward0>)\n",
      "Batch 225, Loss: 0.3753366470336914\n",
      "tensor([1])\n",
      "tensor([[ 0.9144,  0.2950, -0.2916]], grad_fn=<AddmmBackward0>)\n",
      "Batch 226, Loss: -0.2950388491153717\n",
      "tensor([0])\n",
      "tensor([[ 0.8271,  0.3326, -0.4117]], grad_fn=<AddmmBackward0>)\n",
      "Batch 227, Loss: -0.8271260857582092\n",
      "tensor([1])\n",
      "tensor([[ 0.7361,  0.3718, -0.5369]], grad_fn=<AddmmBackward0>)\n",
      "Batch 228, Loss: -0.3718334436416626\n",
      "tensor([2])\n",
      "tensor([[ 0.9140,  0.2957, -0.2923]], grad_fn=<AddmmBackward0>)\n",
      "Batch 229, Loss: 0.2923179268836975\n",
      "tensor([1])\n",
      "tensor([[ 0.8671,  0.3160, -0.3569]], grad_fn=<AddmmBackward0>)\n",
      "Batch 230, Loss: -0.3160054087638855\n",
      "tensor([2])\n",
      "tensor([[ 0.9064,  0.2993, -0.3029]], grad_fn=<AddmmBackward0>)\n",
      "Batch 231, Loss: 0.30285435914993286\n",
      "tensor([2])\n",
      "tensor([[ 0.8574,  0.3205, -0.3703]], grad_fn=<AddmmBackward0>)\n",
      "Batch 232, Loss: 0.37034183740615845\n",
      "tensor([1])\n",
      "tensor([[ 0.8833,  0.3095, -0.3346]], grad_fn=<AddmmBackward0>)\n",
      "Batch 233, Loss: -0.3095387816429138\n",
      "tensor([2])\n",
      "tensor([[ 0.8211,  0.3363, -0.4202]], grad_fn=<AddmmBackward0>)\n",
      "Batch 234, Loss: 0.4201531708240509\n",
      "tensor([1])\n",
      "tensor([[ 0.8831,  0.3100, -0.3349]], grad_fn=<AddmmBackward0>)\n",
      "Batch 235, Loss: -0.3099687993526459\n",
      "tensor([0])\n",
      "tensor([[ 0.9019,  0.3021, -0.3089]], grad_fn=<AddmmBackward0>)\n",
      "Batch 236, Loss: -0.9019404053688049\n",
      "tensor([2])\n",
      "tensor([[ 0.8891,  0.3077, -0.3266]], grad_fn=<AddmmBackward0>)\n",
      "Batch 237, Loss: 0.32655853033065796\n",
      "tensor([0])\n",
      "tensor([[ 0.8852,  0.3095, -0.3318]], grad_fn=<AddmmBackward0>)\n",
      "Batch 238, Loss: -0.8852261900901794\n",
      "tensor([2])\n",
      "tensor([[ 0.9234,  0.2934, -0.2793]], grad_fn=<AddmmBackward0>)\n",
      "Batch 239, Loss: 0.2792937159538269\n",
      "tensor([0])\n",
      "tensor([[ 0.9104,  0.2991, -0.2973]], grad_fn=<AddmmBackward0>)\n",
      "Batch 240, Loss: -0.9103636741638184\n",
      "tensor([2])\n",
      "tensor([[ 0.9185,  0.2958, -0.2861]], grad_fn=<AddmmBackward0>)\n",
      "Batch 241, Loss: 0.2861363887786865\n",
      "tensor([2])\n",
      "tensor([[ 0.9195,  0.2954, -0.2847]], grad_fn=<AddmmBackward0>)\n",
      "Batch 242, Loss: 0.28471624851226807\n",
      "tensor([2])\n",
      "tensor([[ 0.8607,  0.3206, -0.3656]], grad_fn=<AddmmBackward0>)\n",
      "Batch 243, Loss: 0.36559629440307617\n",
      "tensor([1])\n",
      "tensor([[ 0.8268,  0.3352, -0.4121]], grad_fn=<AddmmBackward0>)\n",
      "Batch 244, Loss: -0.3351720869541168\n",
      "tensor([0])\n",
      "tensor([[ 0.7696,  0.3596, -0.4907]], grad_fn=<AddmmBackward0>)\n",
      "Batch 245, Loss: -0.7696223855018616\n",
      "tensor([1])\n",
      "tensor([[ 0.8409,  0.3294, -0.3926]], grad_fn=<AddmmBackward0>)\n",
      "Batch 246, Loss: -0.3293851613998413\n",
      "tensor([1])\n",
      "tensor([[ 0.9335,  0.2901, -0.2653]], grad_fn=<AddmmBackward0>)\n",
      "Batch 247, Loss: -0.29009324312210083\n",
      "tensor([0])\n",
      "tensor([[ 0.9130,  0.2990, -0.2935]], grad_fn=<AddmmBackward0>)\n",
      "Batch 248, Loss: -0.9129730463027954\n",
      "tensor([2])\n",
      "tensor([[ 0.9143,  0.2986, -0.2916]], grad_fn=<AddmmBackward0>)\n",
      "Batch 249, Loss: 0.2916485071182251\n",
      "tensor([1])\n",
      "tensor([[ 0.9091,  0.3010, -0.2988]], grad_fn=<AddmmBackward0>)\n",
      "Batch 250, Loss: -0.3009534776210785\n",
      "tensor([1])\n",
      "tensor([[ 0.8451,  0.3283, -0.3868]], grad_fn=<AddmmBackward0>)\n",
      "Batch 251, Loss: -0.3283044099807739\n",
      "tensor([1])\n",
      "tensor([[ 0.8976,  0.3062, -0.3147]], grad_fn=<AddmmBackward0>)\n",
      "Batch 252, Loss: -0.30619144439697266\n",
      "tensor([0])\n",
      "tensor([[ 0.9040,  0.3037, -0.3059]], grad_fn=<AddmmBackward0>)\n",
      "Batch 253, Loss: -0.9040147066116333\n",
      "tensor([1])\n",
      "tensor([[ 0.8445,  0.3291, -0.3878]], grad_fn=<AddmmBackward0>)\n",
      "Batch 254, Loss: -0.3291279673576355\n",
      "tensor([1])\n",
      "tensor([[ 0.7280,  0.3787, -0.5480]], grad_fn=<AddmmBackward0>)\n",
      "Batch 255, Loss: -0.3787120580673218\n",
      "tensor([0])\n",
      "tensor([[ 0.8381,  0.3322, -0.3967]], grad_fn=<AddmmBackward0>)\n",
      "Batch 256, Loss: -0.8380742073059082\n",
      "tensor([1])\n",
      "tensor([[ 0.9251,  0.2955, -0.2771]], grad_fn=<AddmmBackward0>)\n",
      "Batch 257, Loss: -0.29554349184036255\n",
      "tensor([2])\n",
      "tensor([[ 0.6492,  0.4126, -0.6565]], grad_fn=<AddmmBackward0>)\n",
      "Batch 258, Loss: 0.6565419435501099\n",
      "tensor([2])\n",
      "tensor([[ 0.8168,  0.3418, -0.4261]], grad_fn=<AddmmBackward0>)\n",
      "Batch 259, Loss: 0.4260740280151367\n",
      "tensor([0])\n",
      "tensor([[ 0.9196,  0.2985, -0.2847]], grad_fn=<AddmmBackward0>)\n",
      "Batch 260, Loss: -0.9196441173553467\n",
      "tensor([0])\n",
      "tensor([[ 0.9422,  0.2891, -0.2538]], grad_fn=<AddmmBackward0>)\n",
      "Batch 261, Loss: -0.942171037197113\n",
      "tensor([1])\n",
      "tensor([[ 0.9208,  0.2983, -0.2832]], grad_fn=<AddmmBackward0>)\n",
      "Batch 262, Loss: -0.2983080744743347\n",
      "tensor([2])\n",
      "tensor([[ 0.9376,  0.2914, -0.2603]], grad_fn=<AddmmBackward0>)\n",
      "Batch 263, Loss: 0.26030534505844116\n",
      "tensor([1])\n",
      "tensor([[ 0.9088,  0.3038, -0.2999]], grad_fn=<AddmmBackward0>)\n",
      "Batch 264, Loss: -0.3037605583667755\n",
      "tensor([0])\n",
      "tensor([[ 0.8903,  0.3117, -0.3253]], grad_fn=<AddmmBackward0>)\n",
      "Batch 265, Loss: -0.8903262615203857\n",
      "tensor([1])\n",
      "tensor([[ 0.9227,  0.2983, -0.2810]], grad_fn=<AddmmBackward0>)\n",
      "Batch 266, Loss: -0.29827553033828735\n",
      "tensor([1])\n",
      "tensor([[ 0.9257,  0.2972, -0.2768]], grad_fn=<AddmmBackward0>)\n",
      "Batch 267, Loss: -0.29718339443206787\n",
      "tensor([1])\n",
      "tensor([[ 0.9330,  0.2944, -0.2669]], grad_fn=<AddmmBackward0>)\n",
      "Batch 268, Loss: -0.2943548560142517\n",
      "tensor([1])\n",
      "tensor([[ 0.8792,  0.3172, -0.3408]], grad_fn=<AddmmBackward0>)\n",
      "Batch 269, Loss: -0.3172149360179901\n",
      "tensor([0])\n",
      "tensor([[ 0.7356,  0.3779, -0.5383]], grad_fn=<AddmmBackward0>)\n",
      "Batch 270, Loss: -0.7355796694755554\n",
      "tensor([1])\n",
      "tensor([[ 0.9179,  0.3014, -0.2879]], grad_fn=<AddmmBackward0>)\n",
      "Batch 271, Loss: -0.30141937732696533\n",
      "tensor([0])\n",
      "tensor([[ 0.9393,  0.2927, -0.2586]], grad_fn=<AddmmBackward0>)\n",
      "Batch 272, Loss: -0.939257025718689\n",
      "tensor([2])\n",
      "tensor([[ 0.8346,  0.3368, -0.4024]], grad_fn=<AddmmBackward0>)\n",
      "Batch 273, Loss: 0.4024416208267212\n",
      "tensor([1])\n",
      "tensor([[ 0.8947,  0.3118, -0.3200]], grad_fn=<AddmmBackward0>)\n",
      "Batch 274, Loss: -0.3118252754211426\n",
      "tensor([0])\n",
      "tensor([[ 0.7996,  0.3519, -0.4507]], grad_fn=<AddmmBackward0>)\n",
      "Batch 275, Loss: -0.7995600700378418\n",
      "tensor([0])\n",
      "tensor([[ 0.9145,  0.3039, -0.2930]], grad_fn=<AddmmBackward0>)\n",
      "Batch 276, Loss: -0.9144780039787292\n",
      "tensor([0])\n",
      "tensor([[ 0.8475,  0.3322, -0.3851]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 277, Loss: -0.8475457429885864\n",
      "tensor([1])\n",
      "tensor([[ 0.9278,  0.2988, -0.2750]], grad_fn=<AddmmBackward0>)\n",
      "Batch 278, Loss: -0.29877567291259766\n",
      "tensor([1])\n",
      "tensor([[ 0.8880,  0.3157, -0.3298]], grad_fn=<AddmmBackward0>)\n",
      "Batch 279, Loss: -0.3156505227088928\n",
      "tensor([1])\n",
      "tensor([[ 0.8594,  0.3278, -0.3693]], grad_fn=<AddmmBackward0>)\n",
      "Batch 280, Loss: -0.3278404474258423\n",
      "tensor([0])\n",
      "tensor([[ 0.8906,  0.3150, -0.3265]], grad_fn=<AddmmBackward0>)\n",
      "Batch 281, Loss: -0.8906239867210388\n",
      "tensor([1])\n",
      "tensor([[ 0.8886,  0.3160, -0.3294]], grad_fn=<AddmmBackward0>)\n",
      "Batch 282, Loss: -0.31603866815567017\n",
      "tensor([2])\n",
      "tensor([[ 0.8588,  0.3287, -0.3704]], grad_fn=<AddmmBackward0>)\n",
      "Batch 283, Loss: 0.3703863024711609\n",
      "tensor([0])\n",
      "tensor([[ 0.8356,  0.3386, -0.4024]], grad_fn=<AddmmBackward0>)\n",
      "Batch 284, Loss: -0.8356090784072876\n",
      "tensor([0])\n",
      "tensor([[ 0.8900,  0.3161, -0.3279]], grad_fn=<AddmmBackward0>)\n",
      "Batch 285, Loss: -0.8899714946746826\n",
      "tensor([1])\n",
      "tensor([[ 0.8655,  0.3265, -0.3617]], grad_fn=<AddmmBackward0>)\n",
      "Batch 286, Loss: -0.3265143036842346\n",
      "tensor([1])\n",
      "tensor([[ 0.9417,  0.2950, -0.2573]], grad_fn=<AddmmBackward0>)\n",
      "Batch 287, Loss: -0.2949622869491577\n",
      "tensor([1])\n",
      "tensor([[ 0.9291,  0.3004, -0.2746]], grad_fn=<AddmmBackward0>)\n",
      "Batch 288, Loss: -0.3004131317138672\n",
      "tensor([0])\n",
      "tensor([[ 0.8624,  0.3284, -0.3662]], grad_fn=<AddmmBackward0>)\n",
      "Batch 289, Loss: -0.8624160289764404\n",
      "tensor([0])\n",
      "tensor([[ 0.8144,  0.3486, -0.4322]], grad_fn=<AddmmBackward0>)\n",
      "Batch 290, Loss: -0.8144224882125854\n",
      "tensor([0])\n",
      "tensor([[ 0.7682,  0.3680, -0.4957]], grad_fn=<AddmmBackward0>)\n",
      "Batch 291, Loss: -0.7682274580001831\n",
      "tensor([1])\n",
      "tensor([[ 0.9303,  0.3008, -0.2736]], grad_fn=<AddmmBackward0>)\n",
      "Batch 292, Loss: -0.3008049726486206\n",
      "tensor([0])\n",
      "tensor([[ 0.8925,  0.3167, -0.3256]], grad_fn=<AddmmBackward0>)\n",
      "Batch 293, Loss: -0.892545759677887\n",
      "tensor([1])\n",
      "tensor([[ 0.8457,  0.3364, -0.3901]], grad_fn=<AddmmBackward0>)\n",
      "Batch 294, Loss: -0.3363776206970215\n",
      "tensor([1])\n",
      "tensor([[ 0.8870,  0.3194, -0.3336]], grad_fn=<AddmmBackward0>)\n",
      "Batch 295, Loss: -0.31943583488464355\n",
      "tensor([1])\n",
      "tensor([[ 0.9231,  0.3047, -0.2843]], grad_fn=<AddmmBackward0>)\n",
      "Batch 296, Loss: -0.3047165870666504\n",
      "tensor([2])\n",
      "tensor([[ 0.8741,  0.3253, -0.3516]], grad_fn=<AddmmBackward0>)\n",
      "Batch 297, Loss: 0.35160887241363525\n",
      "tensor([0])\n",
      "tensor([[ 0.8936,  0.3174, -0.3250]], grad_fn=<AddmmBackward0>)\n",
      "Batch 298, Loss: -0.8935624957084656\n",
      "tensor([1])\n",
      "tensor([[ 0.8953,  0.3169, -0.3227]], grad_fn=<AddmmBackward0>)\n",
      "Batch 299, Loss: -0.31687015295028687\n",
      "tensor([0])\n",
      "tensor([[ 0.8024,  0.3555, -0.4502]], grad_fn=<AddmmBackward0>)\n",
      "Batch 300, Loss: -0.8023698329925537\n",
      "tensor([2])\n",
      "tensor([[ 0.8791,  0.3240, -0.3452]], grad_fn=<AddmmBackward0>)\n",
      "Batch 301, Loss: 0.34516316652297974\n",
      "tensor([2])\n",
      "tensor([[ 0.8582,  0.3328, -0.3740]], grad_fn=<AddmmBackward0>)\n",
      "Batch 302, Loss: 0.3739783763885498\n",
      "tensor([2])\n",
      "tensor([[ 0.9218,  0.3067, -0.2869]], grad_fn=<AddmmBackward0>)\n",
      "Batch 303, Loss: 0.28687697649002075\n",
      "tensor([2])\n",
      "tensor([[ 0.9412,  0.2989, -0.2603]], grad_fn=<AddmmBackward0>)\n",
      "Batch 304, Loss: 0.260312020778656\n",
      "tensor([0])\n",
      "tensor([[ 0.8542,  0.3349, -0.3794]], grad_fn=<AddmmBackward0>)\n",
      "Batch 305, Loss: -0.8542433977127075\n",
      "tensor([1])\n",
      "tensor([[ 0.8955,  0.3180, -0.3230]], grad_fn=<AddmmBackward0>)\n",
      "Batch 306, Loss: -0.3179979920387268\n",
      "tensor([2])\n",
      "tensor([[ 0.8790,  0.3250, -0.3456]], grad_fn=<AddmmBackward0>)\n",
      "Batch 307, Loss: 0.34561848640441895\n",
      "tensor([2])\n",
      "tensor([[ 0.8866,  0.3219, -0.3351]], grad_fn=<AddmmBackward0>)\n",
      "Batch 308, Loss: 0.3351302146911621\n",
      "tensor([2])\n",
      "tensor([[ 0.9197,  0.3084, -0.2897]], grad_fn=<AddmmBackward0>)\n",
      "Batch 309, Loss: 0.2897151708602905\n",
      "tensor([2])\n",
      "tensor([[ 0.7928,  0.3608, -0.4635]], grad_fn=<AddmmBackward0>)\n",
      "Batch 310, Loss: 0.4635138213634491\n",
      "tensor([0])\n",
      "tensor([[ 0.8186,  0.3503, -0.4281]], grad_fn=<AddmmBackward0>)\n",
      "Batch 311, Loss: -0.8186194896697998\n",
      "tensor([2])\n",
      "tensor([[ 0.9047,  0.3149, -0.3101]], grad_fn=<AddmmBackward0>)\n",
      "Batch 312, Loss: 0.31011784076690674\n",
      "tensor([0])\n",
      "tensor([[ 0.8419,  0.3409, -0.3961]], grad_fn=<AddmmBackward0>)\n",
      "Batch 313, Loss: -0.8418821096420288\n",
      "tensor([1])\n",
      "tensor([[ 0.9286,  0.3053, -0.2772]], grad_fn=<AddmmBackward0>)\n",
      "Batch 314, Loss: -0.30528515577316284\n",
      "tensor([1])\n",
      "tensor([[ 0.9036,  0.3157, -0.3115]], grad_fn=<AddmmBackward0>)\n",
      "Batch 315, Loss: -0.3157033324241638\n",
      "tensor([0])\n",
      "tensor([[ 0.8756,  0.3274, -0.3498]], grad_fn=<AddmmBackward0>)\n",
      "Batch 316, Loss: -0.8756110668182373\n",
      "tensor([0])\n",
      "tensor([[ 0.9355,  0.3029, -0.2678]], grad_fn=<AddmmBackward0>)\n",
      "Batch 317, Loss: -0.9355481863021851\n",
      "tensor([1])\n",
      "tensor([[ 0.9469,  0.2984, -0.2523]], grad_fn=<AddmmBackward0>)\n",
      "Batch 318, Loss: -0.2983522117137909\n",
      "tensor([2])\n",
      "tensor([[ 0.9340,  0.3038, -0.2701]], grad_fn=<AddmmBackward0>)\n",
      "Batch 319, Loss: 0.2700537443161011\n",
      "tensor([0])\n",
      "tensor([[ 0.8779,  0.3270, -0.3469]], grad_fn=<AddmmBackward0>)\n",
      "Batch 320, Loss: -0.8778932094573975\n",
      "tensor([1])\n",
      "tensor([[ 0.6198,  0.4330, -0.7004]], grad_fn=<AddmmBackward0>)\n",
      "Batch 321, Loss: -0.43298301100730896\n",
      "tensor([1])\n",
      "tensor([[ 0.9489,  0.2982, -0.2498]], grad_fn=<AddmmBackward0>)\n",
      "Batch 322, Loss: -0.29815226793289185\n",
      "tensor([0])\n",
      "tensor([[ 0.8484,  0.3395, -0.3874]], grad_fn=<AddmmBackward0>)\n",
      "Batch 323, Loss: -0.8484431505203247\n",
      "tensor([0])\n",
      "tensor([[ 0.9335,  0.3048, -0.2710]], grad_fn=<AddmmBackward0>)\n",
      "Batch 324, Loss: -0.933485746383667\n",
      "tensor([1])\n",
      "tensor([[ 0.9483,  0.2989, -0.2508]], grad_fn=<AddmmBackward0>)\n",
      "Batch 325, Loss: -0.2989090383052826\n",
      "tensor([0])\n",
      "tensor([[ 0.9069,  0.3161, -0.3077]], grad_fn=<AddmmBackward0>)\n",
      "Batch 326, Loss: -0.9068958759307861\n",
      "tensor([2])\n",
      "tensor([[ 0.8601,  0.3354, -0.3719]], grad_fn=<AddmmBackward0>)\n",
      "Batch 327, Loss: 0.3718577027320862\n",
      "tensor([2])\n",
      "tensor([[ 0.7655,  0.3742, -0.5015]], grad_fn=<AddmmBackward0>)\n",
      "Batch 328, Loss: 0.5015145540237427\n",
      "tensor([1])\n",
      "tensor([[ 0.9382,  0.3037, -0.2651]], grad_fn=<AddmmBackward0>)\n",
      "Batch 329, Loss: -0.30373239517211914\n",
      "tensor([2])\n",
      "tensor([[ 0.8444,  0.3422, -0.3937]], grad_fn=<AddmmBackward0>)\n",
      "Batch 330, Loss: 0.3936506509780884\n",
      "tensor([1])\n",
      "tensor([[ 0.7829,  0.3675, -0.4778]], grad_fn=<AddmmBackward0>)\n",
      "Batch 331, Loss: -0.36749067902565\n",
      "tensor([0])\n",
      "tensor([[ 0.9524,  0.2984, -0.2458]], grad_fn=<AddmmBackward0>)\n",
      "Batch 332, Loss: -0.9524109363555908\n",
      "tensor([1])\n",
      "tensor([[ 0.8873,  0.3252, -0.3350]], grad_fn=<AddmmBackward0>)\n",
      "Batch 333, Loss: -0.3251509666442871\n",
      "tensor([0])\n",
      "tensor([[ 0.7878,  0.3659, -0.4713]], grad_fn=<AddmmBackward0>)\n",
      "Batch 334, Loss: -0.7877758741378784\n",
      "tensor([0])\n",
      "tensor([[ 0.7909,  0.3648, -0.4670]], grad_fn=<AddmmBackward0>)\n",
      "Batch 335, Loss: -0.790910005569458\n",
      "tensor([0])\n",
      "tensor([[ 0.9408,  0.3038, -0.2620]], grad_fn=<AddmmBackward0>)\n",
      "Batch 336, Loss: -0.9408299922943115\n",
      "tensor([2])\n",
      "tensor([[ 0.9383,  0.3051, -0.2657]], grad_fn=<AddmmBackward0>)\n",
      "Batch 337, Loss: 0.26567745208740234\n",
      "tensor([1])\n",
      "tensor([[ 0.9527,  0.2993, -0.2461]], grad_fn=<AddmmBackward0>)\n",
      "Batch 338, Loss: -0.2993420958518982\n",
      "tensor([0])\n",
      "tensor([[ 0.7441,  0.3844, -0.5314]], grad_fn=<AddmmBackward0>)\n",
      "Batch 339, Loss: -0.7441362142562866\n",
      "tensor([2])\n",
      "tensor([[ 0.9485,  0.3014, -0.2520]], grad_fn=<AddmmBackward0>)\n",
      "Batch 340, Loss: 0.25198137760162354\n",
      "tensor([2])\n",
      "tensor([[ 0.7805,  0.3698, -0.4818]], grad_fn=<AddmmBackward0>)\n",
      "Batch 341, Loss: 0.4818265438079834\n",
      "tensor([2])\n",
      "tensor([[ 0.6550,  0.4210, -0.6535]], grad_fn=<AddmmBackward0>)\n",
      "Batch 342, Loss: 0.653485119342804\n",
      "tensor([0])\n",
      "tensor([[ 0.8579,  0.3386, -0.3761]], grad_fn=<AddmmBackward0>)\n",
      "Batch 343, Loss: -0.8579025864601135\n",
      "tensor([2])\n",
      "tensor([[ 0.9305,  0.3092, -0.2768]], grad_fn=<AddmmBackward0>)\n",
      "Batch 344, Loss: 0.2768194079399109\n",
      "tensor([2])\n",
      "tensor([[ 0.7987,  0.3628, -0.4570]], grad_fn=<AddmmBackward0>)\n",
      "Batch 345, Loss: 0.4570443630218506\n",
      "tensor([0])\n",
      "tensor([[ 0.9444,  0.3038, -0.2579]], grad_fn=<AddmmBackward0>)\n",
      "Batch 346, Loss: -0.9443690180778503\n",
      "tensor([1])\n",
      "tensor([[ 0.8557,  0.3399, -0.3792]], grad_fn=<AddmmBackward0>)\n",
      "Batch 347, Loss: -0.3398944139480591\n",
      "tensor([2])\n",
      "tensor([[ 0.8804,  0.3300, -0.3455]], grad_fn=<AddmmBackward0>)\n",
      "Batch 348, Loss: 0.34547120332717896\n",
      "tensor([0])\n",
      "tensor([[ 0.9424,  0.3049, -0.2606]], grad_fn=<AddmmBackward0>)\n",
      "Batch 349, Loss: -0.9423977732658386\n",
      "tensor([0])\n",
      "tensor([[ 0.9495,  0.3022, -0.2510]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 350, Loss: -0.9494788646697998\n",
      "tensor([2])\n",
      "tensor([[ 0.8682,  0.3353, -0.3622]], grad_fn=<AddmmBackward0>)\n",
      "Batch 351, Loss: 0.36224913597106934\n",
      "tensor([0])\n",
      "tensor([[ 0.9423,  0.3053, -0.2609]], grad_fn=<AddmmBackward0>)\n",
      "Batch 352, Loss: -0.9423321485519409\n",
      "tensor([0])\n",
      "tensor([[ 0.8989,  0.3230, -0.3204]], grad_fn=<AddmmBackward0>)\n",
      "Batch 353, Loss: -0.8988744020462036\n",
      "tensor([0])\n",
      "tensor([[ 0.9147,  0.3167, -0.2989]], grad_fn=<AddmmBackward0>)\n",
      "Batch 354, Loss: -0.914732813835144\n",
      "tensor([1])\n",
      "tensor([[ 0.8746,  0.3331, -0.3539]], grad_fn=<AddmmBackward0>)\n",
      "Batch 355, Loss: -0.33307868242263794\n",
      "tensor([1])\n",
      "tensor([[ 0.9022,  0.3220, -0.3162]], grad_fn=<AddmmBackward0>)\n",
      "Batch 356, Loss: -0.3220145106315613\n",
      "tensor([0])\n",
      "tensor([[ 0.8820,  0.3304, -0.3440]], grad_fn=<AddmmBackward0>)\n",
      "Batch 357, Loss: -0.8819720149040222\n",
      "tensor([1])\n",
      "tensor([[ 0.8546,  0.3416, -0.3815]], grad_fn=<AddmmBackward0>)\n",
      "Batch 358, Loss: -0.3415589928627014\n",
      "tensor([0])\n",
      "tensor([[ 0.9193,  0.3156, -0.2933]], grad_fn=<AddmmBackward0>)\n",
      "Batch 359, Loss: -0.9193088412284851\n",
      "tensor([0])\n",
      "tensor([[ 0.9483,  0.3040, -0.2539]], grad_fn=<AddmmBackward0>)\n",
      "Batch 360, Loss: -0.9482892751693726\n",
      "tensor([0])\n",
      "tensor([[ 0.8906,  0.3275, -0.3328]], grad_fn=<AddmmBackward0>)\n",
      "Batch 361, Loss: -0.8906498551368713\n",
      "tensor([0])\n",
      "tensor([[ 0.9201,  0.3157, -0.2928]], grad_fn=<AddmmBackward0>)\n",
      "Batch 362, Loss: -0.9201263785362244\n",
      "tensor([1])\n",
      "tensor([[ 0.8595,  0.3404, -0.3759]], grad_fn=<AddmmBackward0>)\n",
      "Batch 363, Loss: -0.34035903215408325\n",
      "tensor([1])\n",
      "tensor([[ 0.8435,  0.3470, -0.3979]], grad_fn=<AddmmBackward0>)\n",
      "Batch 364, Loss: -0.34697574377059937\n",
      "tensor([0])\n",
      "tensor([[ 0.8314,  0.3520, -0.4146]], grad_fn=<AddmmBackward0>)\n",
      "Batch 365, Loss: -0.8313634395599365\n",
      "tensor([2])\n",
      "tensor([[ 0.8907,  0.3283, -0.3338]], grad_fn=<AddmmBackward0>)\n",
      "Batch 366, Loss: 0.33380627632141113\n",
      "tensor([1])\n",
      "tensor([[ 0.8301,  0.3528, -0.4167]], grad_fn=<AddmmBackward0>)\n",
      "Batch 367, Loss: -0.3528274595737457\n",
      "tensor([2])\n",
      "tensor([[ 0.8982,  0.3256, -0.3239]], grad_fn=<AddmmBackward0>)\n",
      "Batch 368, Loss: 0.32391244173049927\n",
      "tensor([2])\n",
      "tensor([[ 0.8875,  0.3300, -0.3386]], grad_fn=<AddmmBackward0>)\n",
      "Batch 369, Loss: 0.3386268615722656\n",
      "tensor([0])\n",
      "tensor([[ 0.8117,  0.3607, -0.4421]], grad_fn=<AddmmBackward0>)\n",
      "Batch 370, Loss: -0.8116796016693115\n",
      "tensor([0])\n",
      "tensor([[ 0.8701,  0.3373, -0.3625]], grad_fn=<AddmmBackward0>)\n",
      "Batch 371, Loss: -0.870130181312561\n",
      "tensor([1])\n",
      "tensor([[ 0.8561,  0.3430, -0.3817]], grad_fn=<AddmmBackward0>)\n",
      "Batch 372, Loss: -0.34304624795913696\n",
      "tensor([1])\n",
      "tensor([[ 0.8243,  0.3560, -0.4251]], grad_fn=<AddmmBackward0>)\n",
      "Batch 373, Loss: -0.35597437620162964\n",
      "tensor([1])\n",
      "tensor([[ 0.9247,  0.3158, -0.2884]], grad_fn=<AddmmBackward0>)\n",
      "Batch 374, Loss: -0.3158036470413208\n",
      "tensor([1])\n",
      "tensor([[ 0.9501,  0.3058, -0.2539]], grad_fn=<AddmmBackward0>)\n",
      "Batch 375, Loss: -0.3057900667190552\n",
      "tensor([0])\n",
      "tensor([[ 0.8255,  0.3560, -0.4238]], grad_fn=<AddmmBackward0>)\n",
      "Batch 376, Loss: -0.8254750967025757\n",
      "tensor([0])\n",
      "tensor([[ 0.8378,  0.3512, -0.4071]], grad_fn=<AddmmBackward0>)\n",
      "Batch 377, Loss: -0.8378353118896484\n",
      "tensor([1])\n",
      "tensor([[ 0.9492,  0.3067, -0.2553]], grad_fn=<AddmmBackward0>)\n",
      "Batch 378, Loss: -0.30671626329421997\n",
      "tensor([2])\n",
      "tensor([[ 0.9336,  0.3132, -0.2768]], grad_fn=<AddmmBackward0>)\n",
      "Batch 379, Loss: 0.27676552534103394\n",
      "tensor([1])\n",
      "tensor([[ 0.8939,  0.3293, -0.3310]], grad_fn=<AddmmBackward0>)\n",
      "Batch 380, Loss: -0.3292868733406067\n",
      "tensor([1])\n",
      "tensor([[ 0.9456,  0.3088, -0.2606]], grad_fn=<AddmmBackward0>)\n",
      "Batch 381, Loss: -0.30875420570373535\n",
      "tensor([0])\n",
      "tensor([[ 0.9322,  0.3143, -0.2789]], grad_fn=<AddmmBackward0>)\n",
      "Batch 382, Loss: -0.9322082996368408\n",
      "tensor([0])\n",
      "tensor([[ 0.9106,  0.3232, -0.3085]], grad_fn=<AddmmBackward0>)\n",
      "Batch 383, Loss: -0.9105789661407471\n",
      "tensor([1])\n",
      "tensor([[ 0.9237,  0.3181, -0.2908]], grad_fn=<AddmmBackward0>)\n",
      "Batch 384, Loss: -0.31814560294151306\n",
      "tensor([1])\n",
      "tensor([[ 0.9014,  0.3273, -0.3213]], grad_fn=<AddmmBackward0>)\n",
      "Batch 385, Loss: -0.32726171612739563\n",
      "tensor([2])\n",
      "tensor([[ 0.9186,  0.3206, -0.2980]], grad_fn=<AddmmBackward0>)\n",
      "Batch 386, Loss: 0.29797452688217163\n",
      "tensor([2])\n",
      "tensor([[ 0.8918,  0.3315, -0.3345]], grad_fn=<AddmmBackward0>)\n",
      "Batch 387, Loss: 0.3344762921333313\n",
      "tensor([0])\n",
      "tensor([[ 0.9506,  0.3082, -0.2545]], grad_fn=<AddmmBackward0>)\n",
      "Batch 388, Loss: -0.9505654573440552\n",
      "tensor([0])\n",
      "tensor([[ 0.9495,  0.3088, -0.2560]], grad_fn=<AddmmBackward0>)\n",
      "Batch 389, Loss: -0.949499785900116\n",
      "tensor([1])\n",
      "tensor([[ 0.9101,  0.3247, -0.3098]], grad_fn=<AddmmBackward0>)\n",
      "Batch 390, Loss: -0.3246981203556061\n",
      "tensor([0])\n",
      "tensor([[ 0.9170,  0.3221, -0.3004]], grad_fn=<AddmmBackward0>)\n",
      "Batch 391, Loss: -0.9170454740524292\n",
      "tensor([2])\n",
      "tensor([[ 0.9409,  0.3128, -0.2681]], grad_fn=<AddmmBackward0>)\n",
      "Batch 392, Loss: 0.2681339383125305\n",
      "tensor([0])\n",
      "tensor([[ 0.9494,  0.3096, -0.2566]], grad_fn=<AddmmBackward0>)\n",
      "Batch 393, Loss: -0.9494383931159973\n",
      "tensor([2])\n",
      "tensor([[ 0.9142,  0.3238, -0.3047]], grad_fn=<AddmmBackward0>)\n",
      "Batch 394, Loss: 0.30469441413879395\n",
      "tensor([2])\n",
      "tensor([[ 0.9390,  0.3140, -0.2709]], grad_fn=<AddmmBackward0>)\n",
      "Batch 395, Loss: 0.2709154486656189\n",
      "tensor([0])\n",
      "tensor([[ 0.9538,  0.3083, -0.2509]], grad_fn=<AddmmBackward0>)\n",
      "Batch 396, Loss: -0.9537665247917175\n",
      "tensor([1])\n",
      "tensor([[ 0.9465,  0.3113, -0.2609]], grad_fn=<AddmmBackward0>)\n",
      "Batch 397, Loss: -0.31131207942962646\n",
      "tensor([1])\n",
      "tensor([[ 0.8586,  0.3464, -0.3806]], grad_fn=<AddmmBackward0>)\n",
      "Batch 398, Loss: -0.34637320041656494\n",
      "tensor([2])\n",
      "tensor([[ 0.8727,  0.3409, -0.3614]], grad_fn=<AddmmBackward0>)\n",
      "Batch 399, Loss: 0.36140066385269165\n",
      "tensor([2])\n",
      "tensor([[ 0.9572,  0.3075, -0.2464]], grad_fn=<AddmmBackward0>)\n",
      "Batch 400, Loss: 0.24643123149871826\n",
      "tensor([0])\n",
      "tensor([[ 0.8834,  0.3370, -0.3469]], grad_fn=<AddmmBackward0>)\n",
      "Batch 401, Loss: -0.8834069967269897\n",
      "tensor([1])\n",
      "tensor([[ 0.9178,  0.3235, -0.3002]], grad_fn=<AddmmBackward0>)\n",
      "Batch 402, Loss: -0.3234609365463257\n",
      "tensor([0])\n",
      "tensor([[ 0.9049,  0.3287, -0.3177]], grad_fn=<AddmmBackward0>)\n",
      "Batch 403, Loss: -0.9049143195152283\n",
      "tensor([2])\n",
      "tensor([[ 0.9544,  0.3092, -0.2504]], grad_fn=<AddmmBackward0>)\n",
      "Batch 404, Loss: 0.2504163384437561\n",
      "tensor([1])\n",
      "tensor([[ 0.9188,  0.3235, -0.2988]], grad_fn=<AddmmBackward0>)\n",
      "Batch 405, Loss: -0.3234666883945465\n",
      "tensor([2])\n",
      "tensor([[ 0.9608,  0.3070, -0.2417]], grad_fn=<AddmmBackward0>)\n",
      "Batch 406, Loss: 0.24171584844589233\n",
      "tensor([2])\n",
      "tensor([[ 0.9501,  0.3114, -0.2563]], grad_fn=<AddmmBackward0>)\n",
      "Batch 407, Loss: 0.256330668926239\n",
      "tensor([1])\n",
      "tensor([[ 0.9629,  0.3064, -0.2388]], grad_fn=<AddmmBackward0>)\n",
      "Batch 408, Loss: -0.3064359724521637\n",
      "tensor([1])\n",
      "tensor([[ 0.8309,  0.3588, -0.4185]], grad_fn=<AddmmBackward0>)\n",
      "Batch 409, Loss: -0.35880303382873535\n",
      "tensor([0])\n",
      "tensor([[ 0.9578,  0.3088, -0.2457]], grad_fn=<AddmmBackward0>)\n",
      "Batch 410, Loss: -0.9577870965003967\n",
      "tensor([1])\n",
      "tensor([[ 0.9010,  0.3314, -0.3230]], grad_fn=<AddmmBackward0>)\n",
      "Batch 411, Loss: -0.33139467239379883\n",
      "tensor([1])\n",
      "tensor([[ 0.9610,  0.3079, -0.2414]], grad_fn=<AddmmBackward0>)\n",
      "Batch 412, Loss: -0.3078795373439789\n",
      "tensor([0])\n",
      "tensor([[ 0.9040,  0.3306, -0.3190]], grad_fn=<AddmmBackward0>)\n",
      "Batch 413, Loss: -0.9039710760116577\n",
      "tensor([2])\n",
      "tensor([[ 0.8762,  0.3417, -0.3568]], grad_fn=<AddmmBackward0>)\n",
      "Batch 414, Loss: 0.35683226585388184\n",
      "tensor([2])\n",
      "tensor([[ 0.9325,  0.3197, -0.2802]], grad_fn=<AddmmBackward0>)\n",
      "Batch 415, Loss: 0.28019124269485474\n",
      "tensor([0])\n",
      "tensor([[ 0.9496,  0.3131, -0.2570]], grad_fn=<AddmmBackward0>)\n",
      "Batch 416, Loss: -0.949567437171936\n",
      "tensor([0])\n",
      "tensor([[ 0.9555,  0.3109, -0.2489]], grad_fn=<AddmmBackward0>)\n",
      "Batch 417, Loss: -0.9554968476295471\n",
      "tensor([1])\n",
      "tensor([[ 0.8609,  0.3483, -0.3778]], grad_fn=<AddmmBackward0>)\n",
      "Batch 418, Loss: -0.34832292795181274\n",
      "tensor([1])\n",
      "tensor([[ 0.9434,  0.3160, -0.2655]], grad_fn=<AddmmBackward0>)\n",
      "Batch 419, Loss: -0.315990686416626\n",
      "tensor([1])\n",
      "tensor([[ 0.9411,  0.3171, -0.2687]], grad_fn=<AddmmBackward0>)\n",
      "Batch 420, Loss: -0.3170616626739502\n",
      "tensor([2])\n",
      "tensor([[ 0.9549,  0.3118, -0.2500]], grad_fn=<AddmmBackward0>)\n",
      "Batch 421, Loss: 0.24995362758636475\n",
      "tensor([0])\n",
      "tensor([[ 0.9529,  0.3128, -0.2527]], grad_fn=<AddmmBackward0>)\n",
      "Batch 422, Loss: -0.9528940916061401\n",
      "tensor([1])\n",
      "tensor([[ 0.9543,  0.3124, -0.2509]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 423, Loss: -0.31244441866874695\n",
      "tensor([0])\n",
      "tensor([[ 0.9068,  0.3313, -0.3156]], grad_fn=<AddmmBackward0>)\n",
      "Batch 424, Loss: -0.906805694103241\n",
      "tensor([0])\n",
      "tensor([[ 0.9321,  0.3215, -0.2813]], grad_fn=<AddmmBackward0>)\n",
      "Batch 425, Loss: -0.9320632815361023\n",
      "tensor([0])\n",
      "tensor([[ 0.9087,  0.3309, -0.3132]], grad_fn=<AddmmBackward0>)\n",
      "Batch 426, Loss: -0.9086857438087463\n",
      "tensor([2])\n",
      "tensor([[ 0.9199,  0.3267, -0.2981]], grad_fn=<AddmmBackward0>)\n",
      "Batch 427, Loss: 0.2980867624282837\n",
      "tensor([1])\n",
      "tensor([[ 0.9661,  0.3087, -0.2354]], grad_fn=<AddmmBackward0>)\n",
      "Batch 428, Loss: -0.30871033668518066\n",
      "tensor([2])\n",
      "tensor([[ 0.8803,  0.3425, -0.3521]], grad_fn=<AddmmBackward0>)\n",
      "Batch 429, Loss: 0.35211533308029175\n",
      "tensor([0])\n",
      "tensor([[ 0.9027,  0.3339, -0.3218]], grad_fn=<AddmmBackward0>)\n",
      "Batch 430, Loss: -0.9026950597763062\n",
      "tensor([0])\n",
      "tensor([[ 0.7379,  0.3985, -0.5459]], grad_fn=<AddmmBackward0>)\n",
      "Batch 431, Loss: -0.7379058599472046\n",
      "tensor([1])\n",
      "tensor([[ 0.8558,  0.3525, -0.3857]], grad_fn=<AddmmBackward0>)\n",
      "Batch 432, Loss: -0.35251057147979736\n",
      "tensor([2])\n",
      "tensor([[ 0.9415,  0.3191, -0.2693]], grad_fn=<AddmmBackward0>)\n",
      "Batch 433, Loss: 0.2692853808403015\n",
      "tensor([0])\n",
      "tensor([[ 0.9497,  0.3161, -0.2582]], grad_fn=<AddmmBackward0>)\n",
      "Batch 434, Loss: -0.9496853351593018\n",
      "tensor([1])\n",
      "tensor([[ 0.9531,  0.3149, -0.2537]], grad_fn=<AddmmBackward0>)\n",
      "Batch 435, Loss: -0.314917653799057\n",
      "tensor([1])\n",
      "tensor([[ 0.9356,  0.3219, -0.2775]], grad_fn=<AddmmBackward0>)\n",
      "Batch 436, Loss: -0.3219037652015686\n",
      "tensor([0])\n",
      "tensor([[ 0.9149,  0.3302, -0.3058]], grad_fn=<AddmmBackward0>)\n",
      "Batch 437, Loss: -0.9149089455604553\n",
      "tensor([2])\n",
      "tensor([[ 0.9439,  0.3190, -0.2665]], grad_fn=<AddmmBackward0>)\n",
      "Batch 438, Loss: 0.2665303945541382\n",
      "tensor([2])\n",
      "tensor([[ 0.8293,  0.3639, -0.4223]], grad_fn=<AddmmBackward0>)\n",
      "Batch 439, Loss: 0.42231863737106323\n",
      "tensor([2])\n",
      "tensor([[ 0.9525,  0.3160, -0.2549]], grad_fn=<AddmmBackward0>)\n",
      "Batch 440, Loss: 0.25493019819259644\n",
      "tensor([0])\n",
      "tensor([[ 0.9669,  0.3105, -0.2354]], grad_fn=<AddmmBackward0>)\n",
      "Batch 441, Loss: -0.9668523073196411\n",
      "tensor([1])\n",
      "tensor([[ 0.9228,  0.3278, -0.2954]], grad_fn=<AddmmBackward0>)\n",
      "Batch 442, Loss: -0.32782769203186035\n",
      "tensor([1])\n",
      "tensor([[ 0.9350,  0.3232, -0.2788]], grad_fn=<AddmmBackward0>)\n",
      "Batch 443, Loss: -0.3232339024543762\n",
      "tensor([0])\n",
      "tensor([[ 0.9588,  0.3141, -0.2464]], grad_fn=<AddmmBackward0>)\n",
      "Batch 444, Loss: -0.9588230848312378\n",
      "tensor([1])\n",
      "tensor([[ 0.9707,  0.3097, -0.2304]], grad_fn=<AddmmBackward0>)\n",
      "Batch 445, Loss: -0.30966395139694214\n",
      "tensor([2])\n",
      "tensor([[ 0.9263,  0.3271, -0.2907]], grad_fn=<AddmmBackward0>)\n",
      "Batch 446, Loss: 0.290730357170105\n",
      "tensor([0])\n",
      "tensor([[ 0.9407,  0.3217, -0.2712]], grad_fn=<AddmmBackward0>)\n",
      "Batch 447, Loss: -0.9407247304916382\n",
      "tensor([1])\n",
      "tensor([[ 0.9201,  0.3298, -0.2993]], grad_fn=<AddmmBackward0>)\n",
      "Batch 448, Loss: -0.3298398554325104\n",
      "tensor([1])\n",
      "tensor([[ 0.9490,  0.3188, -0.2602]], grad_fn=<AddmmBackward0>)\n",
      "Batch 449, Loss: -0.3188025653362274\n",
      "tensor([0])\n",
      "tensor([[ 0.9187,  0.3307, -0.3013]], grad_fn=<AddmmBackward0>)\n",
      "Batch 450, Loss: -0.9187430143356323\n",
      "tensor([2])\n",
      "tensor([[ 0.9549,  0.3169, -0.2522]], grad_fn=<AddmmBackward0>)\n",
      "Batch 451, Loss: 0.2522130608558655\n",
      "tensor([1])\n",
      "tensor([[ 0.9487,  0.3194, -0.2607]], grad_fn=<AddmmBackward0>)\n",
      "Batch 452, Loss: -0.3194388151168823\n",
      "tensor([0])\n",
      "tensor([[ 0.9491,  0.3195, -0.2602]], grad_fn=<AddmmBackward0>)\n",
      "Batch 453, Loss: -0.9491161108016968\n",
      "tensor([2])\n",
      "tensor([[ 0.8585,  0.3548, -0.3834]], grad_fn=<AddmmBackward0>)\n",
      "Batch 454, Loss: 0.3833586573600769\n",
      "tensor([0])\n",
      "tensor([[ 0.9355,  0.3251, -0.2788]], grad_fn=<AddmmBackward0>)\n",
      "Batch 455, Loss: -0.9355068206787109\n",
      "tensor([2])\n",
      "tensor([[ 0.9233,  0.3300, -0.2955]], grad_fn=<AddmmBackward0>)\n",
      "Batch 456, Loss: 0.2955482602119446\n",
      "tensor([1])\n",
      "tensor([[ 0.8197,  0.3702, -0.4363]], grad_fn=<AddmmBackward0>)\n",
      "Batch 457, Loss: -0.37019360065460205\n",
      "tensor([1])\n",
      "tensor([[ 0.9717,  0.3116, -0.2299]], grad_fn=<AddmmBackward0>)\n",
      "Batch 458, Loss: -0.31155282258987427\n",
      "tensor([2])\n",
      "tensor([[ 0.9090,  0.3359, -0.3150]], grad_fn=<AddmmBackward0>)\n",
      "Batch 459, Loss: 0.3149694800376892\n",
      "tensor([2])\n",
      "tensor([[ 0.6946,  0.4189, -0.6062]], grad_fn=<AddmmBackward0>)\n",
      "Batch 460, Loss: 0.6061991453170776\n",
      "tensor([0])\n",
      "tensor([[ 0.9447,  0.3225, -0.2665]], grad_fn=<AddmmBackward0>)\n",
      "Batch 461, Loss: -0.9446985721588135\n",
      "tensor([0])\n",
      "tensor([[ 0.7698,  0.3901, -0.5040]], grad_fn=<AddmmBackward0>)\n",
      "Batch 462, Loss: -0.7698396444320679\n",
      "tensor([2])\n",
      "tensor([[ 0.8827,  0.3467, -0.3508]], grad_fn=<AddmmBackward0>)\n",
      "Batch 463, Loss: 0.3507848381996155\n",
      "tensor([2])\n",
      "tensor([[ 0.9668,  0.3143, -0.2366]], grad_fn=<AddmmBackward0>)\n",
      "Batch 464, Loss: 0.23660200834274292\n",
      "tensor([2])\n",
      "tensor([[ 0.9538,  0.3195, -0.2543]], grad_fn=<AddmmBackward0>)\n",
      "Batch 465, Loss: 0.25430142879486084\n",
      "tensor([0])\n",
      "tensor([[ 0.9720,  0.3126, -0.2295]], grad_fn=<AddmmBackward0>)\n",
      "Batch 466, Loss: -0.9719982147216797\n",
      "tensor([2])\n",
      "tensor([[ 0.9265,  0.3302, -0.2913]], grad_fn=<AddmmBackward0>)\n",
      "Batch 467, Loss: 0.2913281321525574\n",
      "tensor([1])\n",
      "tensor([[ 0.9160,  0.3343, -0.3055]], grad_fn=<AddmmBackward0>)\n",
      "Batch 468, Loss: -0.33432480692863464\n",
      "tensor([2])\n",
      "tensor([[ 0.8049,  0.3772, -0.4563]], grad_fn=<AddmmBackward0>)\n",
      "Batch 469, Loss: 0.4562852084636688\n",
      "tensor([1])\n",
      "tensor([[ 0.9692,  0.3141, -0.2332]], grad_fn=<AddmmBackward0>)\n",
      "Batch 470, Loss: -0.31407052278518677\n",
      "tensor([0])\n",
      "tensor([[ 0.9739,  0.3124, -0.2267]], grad_fn=<AddmmBackward0>)\n",
      "Batch 471, Loss: -0.9738856554031372\n",
      "tensor([0])\n",
      "tensor([[ 0.9594,  0.3181, -0.2464]], grad_fn=<AddmmBackward0>)\n",
      "Batch 472, Loss: -0.9594096541404724\n",
      "tensor([2])\n",
      "tensor([[ 0.9526,  0.3208, -0.2557]], grad_fn=<AddmmBackward0>)\n",
      "Batch 473, Loss: 0.2557380199432373\n",
      "tensor([1])\n",
      "tensor([[ 0.8034,  0.3783, -0.4584]], grad_fn=<AddmmBackward0>)\n",
      "Batch 474, Loss: -0.3783460259437561\n",
      "tensor([2])\n",
      "tensor([[ 0.9141,  0.3359, -0.3081]], grad_fn=<AddmmBackward0>)\n",
      "Batch 475, Loss: 0.30806535482406616\n",
      "tensor([0])\n",
      "tensor([[ 0.7899,  0.3837, -0.4766]], grad_fn=<AddmmBackward0>)\n",
      "Batch 476, Loss: -0.7899072170257568\n",
      "tensor([0])\n",
      "tensor([[ 0.9122,  0.3369, -0.3106]], grad_fn=<AddmmBackward0>)\n",
      "Batch 477, Loss: -0.9121595621109009\n",
      "tensor([2])\n",
      "tensor([[ 0.8439,  0.3632, -0.4034]], grad_fn=<AddmmBackward0>)\n",
      "Batch 478, Loss: 0.40344008803367615\n",
      "tensor([1])\n",
      "tensor([[ 0.9597,  0.3188, -0.2462]], grad_fn=<AddmmBackward0>)\n",
      "Batch 479, Loss: -0.31883305311203003\n",
      "tensor([0])\n",
      "tensor([[ 0.8810,  0.3492, -0.3531]], grad_fn=<AddmmBackward0>)\n",
      "Batch 480, Loss: -0.8809532523155212\n",
      "tensor([0])\n",
      "tensor([[ 0.8749,  0.3516, -0.3614]], grad_fn=<AddmmBackward0>)\n",
      "Batch 481, Loss: -0.8748936057090759\n",
      "tensor([1])\n",
      "tensor([[ 0.8826,  0.3488, -0.3509]], grad_fn=<AddmmBackward0>)\n",
      "Batch 482, Loss: -0.3487773835659027\n",
      "tensor([2])\n",
      "tensor([[ 0.9676,  0.3163, -0.2356]], grad_fn=<AddmmBackward0>)\n",
      "Batch 483, Loss: 0.23564040660858154\n",
      "tensor([1])\n",
      "tensor([[ 0.8543,  0.3599, -0.3895]], grad_fn=<AddmmBackward0>)\n",
      "Batch 484, Loss: -0.35988491773605347\n",
      "tensor([2])\n",
      "tensor([[ 0.9718,  0.3150, -0.2301]], grad_fn=<AddmmBackward0>)\n",
      "Batch 485, Loss: 0.23007410764694214\n",
      "tensor([0])\n",
      "tensor([[ 0.9490,  0.3239, -0.2611]], grad_fn=<AddmmBackward0>)\n",
      "Batch 486, Loss: -0.9489542245864868\n",
      "tensor([1])\n",
      "tensor([[ 0.9639,  0.3183, -0.2408]], grad_fn=<AddmmBackward0>)\n",
      "Batch 487, Loss: -0.3183193802833557\n",
      "tensor([0])\n",
      "tensor([[ 0.9576,  0.3209, -0.2494]], grad_fn=<AddmmBackward0>)\n",
      "Batch 488, Loss: -0.9576051235198975\n",
      "tensor([1])\n",
      "tensor([[ 0.9412,  0.3273, -0.2718]], grad_fn=<AddmmBackward0>)\n",
      "Batch 489, Loss: -0.32731926441192627\n",
      "tensor([1])\n",
      "tensor([[ 0.9302,  0.3317, -0.2868]], grad_fn=<AddmmBackward0>)\n",
      "Batch 490, Loss: -0.33170637488365173\n",
      "tensor([2])\n",
      "tensor([[ 0.8441,  0.3648, -0.4037]], grad_fn=<AddmmBackward0>)\n",
      "Batch 491, Loss: 0.4036850333213806\n",
      "tensor([1])\n",
      "tensor([[ 0.9602,  0.3206, -0.2461]], grad_fn=<AddmmBackward0>)\n",
      "Batch 492, Loss: -0.32056504487991333\n",
      "tensor([2])\n",
      "tensor([[ 0.9625,  0.3199, -0.2431]], grad_fn=<AddmmBackward0>)\n",
      "Batch 493, Loss: 0.243053138256073\n",
      "tensor([0])\n",
      "tensor([[ 0.9395,  0.3288, -0.2743]], grad_fn=<AddmmBackward0>)\n",
      "Batch 494, Loss: -0.9394944906234741\n",
      "tensor([1])\n",
      "tensor([[ 0.8729,  0.3544, -0.3647]], grad_fn=<AddmmBackward0>)\n",
      "Batch 495, Loss: -0.3543945550918579\n",
      "tensor([0])\n",
      "tensor([[ 0.9208,  0.3363, -0.2997]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 496, Loss: -0.92082279920578\n",
      "tensor([0])\n",
      "tensor([[ 0.8676,  0.3567, -0.3719]], grad_fn=<AddmmBackward0>)\n",
      "Batch 497, Loss: -0.8676441311836243\n",
      "tensor([1])\n",
      "tensor([[ 0.8742,  0.3543, -0.3630]], grad_fn=<AddmmBackward0>)\n",
      "Batch 498, Loss: -0.3543495237827301\n",
      "tensor([2])\n",
      "tensor([[ 0.9535,  0.3244, -0.2556]], grad_fn=<AddmmBackward0>)\n",
      "Batch 499, Loss: 0.25563716888427734\n",
      "tensor([0])\n",
      "tensor([[ 0.9318,  0.3328, -0.2851]], grad_fn=<AddmmBackward0>)\n",
      "Batch 500, Loss: -0.9317852258682251\n",
      "tensor([1])\n",
      "tensor([[ 0.9502,  0.3259, -0.2602]], grad_fn=<AddmmBackward0>)\n",
      "Batch 501, Loss: -0.3259184956550598\n",
      "tensor([1])\n",
      "tensor([[ 0.9330,  0.3326, -0.2837]], grad_fn=<AddmmBackward0>)\n",
      "Batch 502, Loss: -0.332641065120697\n",
      "tensor([0])\n",
      "tensor([[ 0.9782,  0.3156, -0.2224]], grad_fn=<AddmmBackward0>)\n",
      "Batch 503, Loss: -0.9781914353370667\n",
      "tensor([2])\n",
      "tensor([[ 0.7264,  0.4115, -0.5640]], grad_fn=<AddmmBackward0>)\n",
      "Batch 504, Loss: 0.5640177130699158\n",
      "tensor([1])\n",
      "tensor([[ 0.9776,  0.3162, -0.2234]], grad_fn=<AddmmBackward0>)\n",
      "Batch 505, Loss: -0.3162249028682709\n",
      "tensor([2])\n",
      "tensor([[ 0.9595,  0.3233, -0.2479]], grad_fn=<AddmmBackward0>)\n",
      "Batch 506, Loss: 0.24793756008148193\n",
      "tensor([2])\n",
      "tensor([[ 0.9655,  0.3212, -0.2399]], grad_fn=<AddmmBackward0>)\n",
      "Batch 507, Loss: 0.23989659547805786\n",
      "tensor([2])\n",
      "tensor([[ 0.9232,  0.3374, -0.2973]], grad_fn=<AddmmBackward0>)\n",
      "Batch 508, Loss: 0.2972717881202698\n",
      "tensor([0])\n",
      "tensor([[ 0.9581,  0.3243, -0.2498]], grad_fn=<AddmmBackward0>)\n",
      "Batch 509, Loss: -0.9581292867660522\n",
      "tensor([0])\n",
      "tensor([[ 0.9281,  0.3358, -0.2906]], grad_fn=<AddmmBackward0>)\n",
      "Batch 510, Loss: -0.9281152486801147\n",
      "tensor([0])\n",
      "tensor([[ 0.9598,  0.3239, -0.2476]], grad_fn=<AddmmBackward0>)\n",
      "Batch 511, Loss: -0.9598051309585571\n",
      "tensor([0])\n",
      "tensor([[ 0.8065,  0.3820, -0.4556]], grad_fn=<AddmmBackward0>)\n",
      "Batch 512, Loss: -0.8065363168716431\n",
      "tensor([1])\n",
      "tensor([[ 0.9743,  0.3187, -0.2282]], grad_fn=<AddmmBackward0>)\n",
      "Batch 513, Loss: -0.3186776638031006\n",
      "tensor([0])\n",
      "tensor([[ 0.9814,  0.3161, -0.2186]], grad_fn=<AddmmBackward0>)\n",
      "Batch 514, Loss: -0.9814339280128479\n",
      "tensor([0])\n",
      "tensor([[ 0.9404,  0.3318, -0.2744]], grad_fn=<AddmmBackward0>)\n",
      "Batch 515, Loss: -0.9404157996177673\n",
      "tensor([1])\n",
      "tensor([[ 0.8510,  0.3657, -0.3958]], grad_fn=<AddmmBackward0>)\n",
      "Batch 516, Loss: -0.36573633551597595\n",
      "tensor([1])\n",
      "tensor([[ 0.8868,  0.3524, -0.3474]], grad_fn=<AddmmBackward0>)\n",
      "Batch 517, Loss: -0.3523576259613037\n",
      "tensor([2])\n",
      "tensor([[ 0.9721,  0.3203, -0.2318]], grad_fn=<AddmmBackward0>)\n",
      "Batch 518, Loss: 0.23183101415634155\n",
      "tensor([1])\n",
      "tensor([[ 0.7302,  0.4118, -0.5599]], grad_fn=<AddmmBackward0>)\n",
      "Batch 519, Loss: -0.41178515553474426\n",
      "tensor([0])\n",
      "tensor([[ 0.9542,  0.3274, -0.2563]], grad_fn=<AddmmBackward0>)\n",
      "Batch 520, Loss: -0.9541906118392944\n",
      "tensor([1])\n",
      "tensor([[ 0.7934,  0.3882, -0.4743]], grad_fn=<AddmmBackward0>)\n",
      "Batch 521, Loss: -0.3881886303424835\n",
      "tensor([1])\n",
      "tensor([[ 0.9224,  0.3397, -0.2996]], grad_fn=<AddmmBackward0>)\n",
      "Batch 522, Loss: -0.339706689119339\n",
      "tensor([0])\n",
      "tensor([[ 0.7854,  0.3915, -0.4854]], grad_fn=<AddmmBackward0>)\n",
      "Batch 523, Loss: -0.7853610515594482\n",
      "tensor([0])\n",
      "tensor([[ 0.8817,  0.3554, -0.3549]], grad_fn=<AddmmBackward0>)\n",
      "Batch 524, Loss: -0.8817330598831177\n",
      "tensor([0])\n",
      "tensor([[ 0.9663,  0.3237, -0.2405]], grad_fn=<AddmmBackward0>)\n",
      "Batch 525, Loss: -0.9663349390029907\n",
      "tensor([2])\n",
      "tensor([[ 0.9502,  0.3299, -0.2625]], grad_fn=<AddmmBackward0>)\n",
      "Batch 526, Loss: 0.26245230436325073\n",
      "tensor([2])\n",
      "tensor([[ 0.9752,  0.3207, -0.2288]], grad_fn=<AddmmBackward0>)\n",
      "Batch 527, Loss: 0.22875046730041504\n",
      "tensor([2])\n",
      "tensor([[ 0.8964,  0.3505, -0.3356]], grad_fn=<AddmmBackward0>)\n",
      "Batch 528, Loss: 0.33557993173599243\n",
      "tensor([1])\n",
      "tensor([[ 0.7832,  0.3931, -0.4889]], grad_fn=<AddmmBackward0>)\n",
      "Batch 529, Loss: -0.3931303024291992\n",
      "tensor([1])\n",
      "tensor([[ 0.9458,  0.3321, -0.2687]], grad_fn=<AddmmBackward0>)\n",
      "Batch 530, Loss: -0.3321339786052704\n",
      "tensor([2])\n",
      "tensor([[ 0.9720,  0.3225, -0.2333]], grad_fn=<AddmmBackward0>)\n",
      "Batch 531, Loss: 0.23329216241836548\n",
      "tensor([0])\n",
      "tensor([[ 0.9344,  0.3367, -0.2842]], grad_fn=<AddmmBackward0>)\n",
      "Batch 532, Loss: -0.9344158172607422\n",
      "tensor([2])\n",
      "tensor([[ 0.9263,  0.3399, -0.2953]], grad_fn=<AddmmBackward0>)\n",
      "Batch 533, Loss: 0.2952954173088074\n",
      "tensor([2])\n",
      "tensor([[ 0.8321,  0.3754, -0.4228]], grad_fn=<AddmmBackward0>)\n",
      "Batch 534, Loss: 0.42284929752349854\n",
      "tensor([0])\n",
      "tensor([[ 0.9111,  0.3458, -0.3158]], grad_fn=<AddmmBackward0>)\n",
      "Batch 535, Loss: -0.9110753536224365\n",
      "tensor([0])\n",
      "tensor([[ 0.8806,  0.3574, -0.3571]], grad_fn=<AddmmBackward0>)\n",
      "Batch 536, Loss: -0.8806359767913818\n",
      "tensor([2])\n",
      "tensor([[ 0.9572,  0.3288, -0.2534]], grad_fn=<AddmmBackward0>)\n",
      "Batch 537, Loss: 0.2533761262893677\n",
      "tensor([0])\n",
      "tensor([[ 0.9552,  0.3296, -0.2562]], grad_fn=<AddmmBackward0>)\n",
      "Batch 538, Loss: -0.955161988735199\n",
      "tensor([2])\n",
      "tensor([[ 0.9401,  0.3354, -0.2766]], grad_fn=<AddmmBackward0>)\n",
      "Batch 539, Loss: 0.27664846181869507\n",
      "tensor([0])\n",
      "tensor([[ 0.9756,  0.3222, -0.2286]], grad_fn=<AddmmBackward0>)\n",
      "Batch 540, Loss: -0.9755901098251343\n",
      "tensor([2])\n",
      "tensor([[ 0.9777,  0.3215, -0.2259]], grad_fn=<AddmmBackward0>)\n",
      "Batch 541, Loss: 0.22589659690856934\n",
      "tensor([0])\n",
      "tensor([[ 0.9572,  0.3292, -0.2536]], grad_fn=<AddmmBackward0>)\n",
      "Batch 542, Loss: -0.9572338461875916\n",
      "tensor([0])\n",
      "tensor([[ 0.9634,  0.3270, -0.2453]], grad_fn=<AddmmBackward0>)\n",
      "Batch 543, Loss: -0.9633694887161255\n",
      "tensor([2])\n",
      "tensor([[ 0.9802,  0.3208, -0.2226]], grad_fn=<AddmmBackward0>)\n",
      "Batch 544, Loss: 0.2225993275642395\n",
      "tensor([1])\n",
      "tensor([[ 0.7896,  0.3922, -0.4807]], grad_fn=<AddmmBackward0>)\n",
      "Batch 545, Loss: -0.3922175467014313\n",
      "tensor([1])\n",
      "tensor([[ 0.9523,  0.3315, -0.2606]], grad_fn=<AddmmBackward0>)\n",
      "Batch 546, Loss: -0.3314793109893799\n",
      "tensor([0])\n",
      "tensor([[ 0.9779,  0.3220, -0.2259]], grad_fn=<AddmmBackward0>)\n",
      "Batch 547, Loss: -0.9778969287872314\n",
      "tensor([1])\n",
      "tensor([[ 0.9757,  0.3230, -0.2290]], grad_fn=<AddmmBackward0>)\n",
      "Batch 548, Loss: -0.3229793906211853\n",
      "tensor([1])\n",
      "tensor([[ 0.9072,  0.3487, -0.3218]], grad_fn=<AddmmBackward0>)\n",
      "Batch 549, Loss: -0.3487255573272705\n",
      "tensor([1])\n",
      "tensor([[ 0.9494,  0.3331, -0.2647]], grad_fn=<AddmmBackward0>)\n",
      "Batch 550, Loss: -0.3331088423728943\n",
      "tensor([2])\n",
      "tensor([[ 0.9665,  0.3269, -0.2417]], grad_fn=<AddmmBackward0>)\n",
      "Batch 551, Loss: 0.24172312021255493\n",
      "tensor([0])\n",
      "tensor([[ 0.9859,  0.3198, -0.2154]], grad_fn=<AddmmBackward0>)\n",
      "Batch 552, Loss: -0.9859145879745483\n",
      "tensor([0])\n",
      "tensor([[ 0.9540,  0.3319, -0.2586]], grad_fn=<AddmmBackward0>)\n",
      "Batch 553, Loss: -0.95404052734375\n",
      "tensor([1])\n",
      "tensor([[ 0.9648,  0.3280, -0.2442]], grad_fn=<AddmmBackward0>)\n",
      "Batch 554, Loss: -0.32803475856781006\n",
      "tensor([1])\n",
      "tensor([[ 0.9812,  0.3221, -0.2221]], grad_fn=<AddmmBackward0>)\n",
      "Batch 555, Loss: -0.32209399342536926\n",
      "tensor([2])\n",
      "tensor([[ 0.9585,  0.3307, -0.2529]], grad_fn=<AddmmBackward0>)\n",
      "Batch 556, Loss: 0.25287020206451416\n",
      "tensor([2])\n",
      "tensor([[ 0.9708,  0.3263, -0.2362]], grad_fn=<AddmmBackward0>)\n",
      "Batch 557, Loss: 0.23623692989349365\n",
      "tensor([1])\n",
      "tensor([[ 0.9844,  0.3214, -0.2179]], grad_fn=<AddmmBackward0>)\n",
      "Batch 558, Loss: -0.3214341998100281\n",
      "tensor([0])\n",
      "tensor([[ 0.9819,  0.3225, -0.2212]], grad_fn=<AddmmBackward0>)\n",
      "Batch 559, Loss: -0.9819134473800659\n",
      "tensor([2])\n",
      "tensor([[ 0.9763,  0.3248, -0.2289]], grad_fn=<AddmmBackward0>)\n",
      "Batch 560, Loss: 0.22887271642684937\n",
      "tensor([2])\n",
      "tensor([[ 0.9438,  0.3370, -0.2728]], grad_fn=<AddmmBackward0>)\n",
      "Batch 561, Loss: 0.2727910876274109\n",
      "tensor([2])\n",
      "tensor([[ 0.9806,  0.3235, -0.2230]], grad_fn=<AddmmBackward0>)\n",
      "Batch 562, Loss: 0.22301125526428223\n",
      "tensor([0])\n",
      "tensor([[ 0.8702,  0.3646, -0.3724]], grad_fn=<AddmmBackward0>)\n",
      "Batch 563, Loss: -0.8701513409614563\n",
      "tensor([2])\n",
      "tensor([[ 0.9658,  0.3292, -0.2430]], grad_fn=<AddmmBackward0>)\n",
      "Batch 564, Loss: 0.24295693635940552\n",
      "tensor([2])\n",
      "tensor([[ 0.9390,  0.3393, -0.2792]], grad_fn=<AddmmBackward0>)\n",
      "Batch 565, Loss: 0.27924734354019165\n",
      "tensor([0])\n",
      "tensor([[ 0.9825,  0.3232, -0.2202]], grad_fn=<AddmmBackward0>)\n",
      "Batch 566, Loss: -0.9825218915939331\n",
      "tensor([0])\n",
      "tensor([[ 0.9505,  0.3352, -0.2635]], grad_fn=<AddmmBackward0>)\n",
      "Batch 567, Loss: -0.9505100846290588\n",
      "tensor([0])\n",
      "tensor([[ 0.8573,  0.3699, -0.3896]], grad_fn=<AddmmBackward0>)\n",
      "Batch 568, Loss: -0.8573354482650757\n",
      "tensor([1])\n",
      "tensor([[ 0.9369,  0.3405, -0.2820]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 569, Loss: -0.340462327003479\n",
      "tensor([0])\n",
      "tensor([[ 0.9672,  0.3294, -0.2411]], grad_fn=<AddmmBackward0>)\n",
      "Batch 570, Loss: -0.967200756072998\n",
      "tensor([2])\n",
      "tensor([[ 0.8579,  0.3700, -0.3891]], grad_fn=<AddmmBackward0>)\n",
      "Batch 571, Loss: 0.38906994462013245\n",
      "tensor([2])\n",
      "tensor([[ 0.9922,  0.3203, -0.2074]], grad_fn=<AddmmBackward0>)\n",
      "Batch 572, Loss: 0.20742517709732056\n",
      "tensor([1])\n",
      "tensor([[ 0.9207,  0.3469, -0.3042]], grad_fn=<AddmmBackward0>)\n",
      "Batch 573, Loss: -0.3469350337982178\n",
      "tensor([1])\n",
      "tensor([[ 0.9671,  0.3299, -0.2414]], grad_fn=<AddmmBackward0>)\n",
      "Batch 574, Loss: -0.3298565149307251\n",
      "tensor([1])\n",
      "tensor([[ 0.9517,  0.3357, -0.2623]], grad_fn=<AddmmBackward0>)\n",
      "Batch 575, Loss: -0.3357192277908325\n",
      "tensor([1])\n",
      "tensor([[ 0.9843,  0.3238, -0.2182]], grad_fn=<AddmmBackward0>)\n",
      "Batch 576, Loss: -0.32382458448410034\n",
      "tensor([1])\n",
      "tensor([[ 0.8700,  0.3663, -0.3729]], grad_fn=<AddmmBackward0>)\n",
      "Batch 577, Loss: -0.3662545084953308\n",
      "tensor([0])\n",
      "tensor([[ 0.9697,  0.3296, -0.2379]], grad_fn=<AddmmBackward0>)\n",
      "Batch 578, Loss: -0.9697301387786865\n",
      "tensor([2])\n",
      "tensor([[ 0.9373,  0.3418, -0.2819]], grad_fn=<AddmmBackward0>)\n",
      "Batch 579, Loss: 0.28185731172561646\n",
      "tensor([0])\n",
      "tensor([[ 0.7742,  0.4021, -0.5025]], grad_fn=<AddmmBackward0>)\n",
      "Batch 580, Loss: -0.7742081880569458\n",
      "tensor([1])\n",
      "tensor([[ 0.9815,  0.3258, -0.2221]], grad_fn=<AddmmBackward0>)\n",
      "Batch 581, Loss: -0.32575732469558716\n",
      "tensor([1])\n",
      "tensor([[ 0.9143,  0.3507, -0.3130]], grad_fn=<AddmmBackward0>)\n",
      "Batch 582, Loss: -0.35071325302124023\n",
      "tensor([0])\n",
      "tensor([[ 0.7337,  0.4174, -0.5574]], grad_fn=<AddmmBackward0>)\n",
      "Batch 583, Loss: -0.7336815595626831\n",
      "tensor([0])\n",
      "tensor([[ 0.9623,  0.3334, -0.2483]], grad_fn=<AddmmBackward0>)\n",
      "Batch 584, Loss: -0.9622783660888672\n",
      "tensor([2])\n",
      "tensor([[ 0.9617,  0.3338, -0.2491]], grad_fn=<AddmmBackward0>)\n",
      "Batch 585, Loss: 0.24912846088409424\n",
      "tensor([0])\n",
      "tensor([[ 0.9402,  0.3419, -0.2783]], grad_fn=<AddmmBackward0>)\n",
      "Batch 586, Loss: -0.9402093291282654\n",
      "tensor([2])\n",
      "tensor([[ 0.8832,  0.3630, -0.3554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 587, Loss: 0.35539817810058594\n",
      "tensor([0])\n",
      "tensor([[ 0.9493,  0.3388, -0.2661]], grad_fn=<AddmmBackward0>)\n",
      "Batch 588, Loss: -0.9492883086204529\n",
      "tensor([1])\n",
      "tensor([[ 0.9347,  0.3443, -0.2859]], grad_fn=<AddmmBackward0>)\n",
      "Batch 589, Loss: -0.3442966639995575\n",
      "tensor([1])\n",
      "tensor([[ 0.9623,  0.3343, -0.2487]], grad_fn=<AddmmBackward0>)\n",
      "Batch 590, Loss: -0.3343092203140259\n",
      "tensor([2])\n",
      "tensor([[ 0.9832,  0.3268, -0.2205]], grad_fn=<AddmmBackward0>)\n",
      "Batch 591, Loss: 0.220514178276062\n",
      "tensor([2])\n",
      "tensor([[ 0.9533,  0.3380, -0.2610]], grad_fn=<AddmmBackward0>)\n",
      "Batch 592, Loss: 0.26103848218917847\n",
      "tensor([1])\n",
      "tensor([[ 0.9873,  0.3256, -0.2150]], grad_fn=<AddmmBackward0>)\n",
      "Batch 593, Loss: -0.325614869594574\n",
      "tensor([1])\n",
      "tensor([[ 0.9905,  0.3246, -0.2107]], grad_fn=<AddmmBackward0>)\n",
      "Batch 594, Loss: -0.3245936930179596\n",
      "tensor([0])\n",
      "tensor([[ 0.9655,  0.3340, -0.2446]], grad_fn=<AddmmBackward0>)\n",
      "Batch 595, Loss: -0.9654725790023804\n",
      "tensor([2])\n",
      "tensor([[ 0.9175,  0.3517, -0.3095]], grad_fn=<AddmmBackward0>)\n",
      "Batch 596, Loss: 0.30946362018585205\n",
      "tensor([2])\n",
      "tensor([[ 0.9707,  0.3324, -0.2375]], grad_fn=<AddmmBackward0>)\n",
      "Batch 597, Loss: 0.23752319812774658\n",
      "tensor([2])\n",
      "tensor([[ 0.9672,  0.3338, -0.2422]], grad_fn=<AddmmBackward0>)\n",
      "Batch 598, Loss: 0.2422173023223877\n",
      "tensor([2])\n",
      "tensor([[ 0.9862,  0.3270, -0.2165]], grad_fn=<AddmmBackward0>)\n",
      "Batch 599, Loss: 0.2164648175239563\n",
      "Training [40%]\tLoss: -0.2849\n",
      "tensor([0])\n",
      "tensor([[ 0.9110,  0.3546, -0.3181]], grad_fn=<AddmmBackward0>)\n",
      "Batch 0, Loss: -0.9109789133071899\n",
      "tensor([1])\n",
      "tensor([[ 0.9717,  0.3325, -0.2360]], grad_fn=<AddmmBackward0>)\n",
      "Batch 1, Loss: -0.3325173854827881\n",
      "tensor([1])\n",
      "tensor([[ 0.8467,  0.3783, -0.4050]], grad_fn=<AddmmBackward0>)\n",
      "Batch 2, Loss: -0.37833666801452637\n",
      "tensor([1])\n",
      "tensor([[ 0.8665,  0.3712, -0.3782]], grad_fn=<AddmmBackward0>)\n",
      "Batch 3, Loss: -0.3712490200996399\n",
      "tensor([1])\n",
      "tensor([[ 0.9157,  0.3534, -0.3117]], grad_fn=<AddmmBackward0>)\n",
      "Batch 4, Loss: -0.3534262180328369\n",
      "tensor([0])\n",
      "tensor([[ 0.9101,  0.3556, -0.3191]], grad_fn=<AddmmBackward0>)\n",
      "Batch 5, Loss: -0.9101331233978271\n",
      "tensor([0])\n",
      "tensor([[ 0.8800,  0.3668, -0.3599]], grad_fn=<AddmmBackward0>)\n",
      "Batch 6, Loss: -0.8800152540206909\n",
      "tensor([0])\n",
      "tensor([[ 0.9752,  0.3322, -0.2313]], grad_fn=<AddmmBackward0>)\n",
      "Batch 7, Loss: -0.9751578569412231\n",
      "tensor([0])\n",
      "tensor([[ 0.9917,  0.3264, -0.2090]], grad_fn=<AddmmBackward0>)\n",
      "Batch 8, Loss: -0.9916601181030273\n",
      "tensor([2])\n",
      "tensor([[ 0.9285,  0.3496, -0.2946]], grad_fn=<AddmmBackward0>)\n",
      "Batch 9, Loss: 0.29462724924087524\n",
      "tensor([0])\n",
      "tensor([[ 0.9478,  0.3427, -0.2685]], grad_fn=<AddmmBackward0>)\n",
      "Batch 10, Loss: -0.9478222727775574\n",
      "tensor([0])\n",
      "tensor([[ 0.8780,  0.3682, -0.3630]], grad_fn=<AddmmBackward0>)\n",
      "Batch 11, Loss: -0.8780467510223389\n",
      "tensor([2])\n",
      "tensor([[ 0.9338,  0.3480, -0.2877]], grad_fn=<AddmmBackward0>)\n",
      "Batch 12, Loss: 0.28773826360702515\n",
      "tensor([0])\n",
      "tensor([[ 0.9843,  0.3298, -0.2196]], grad_fn=<AddmmBackward0>)\n",
      "Batch 13, Loss: -0.9843016266822815\n",
      "tensor([0])\n",
      "tensor([[ 0.9945,  0.3262, -0.2060]], grad_fn=<AddmmBackward0>)\n",
      "Batch 14, Loss: -0.9945030212402344\n",
      "tensor([0])\n",
      "tensor([[ 0.9579,  0.3396, -0.2556]], grad_fn=<AddmmBackward0>)\n",
      "Batch 15, Loss: -0.9578769207000732\n",
      "tensor([1])\n",
      "tensor([[ 0.9857,  0.3296, -0.2182]], grad_fn=<AddmmBackward0>)\n",
      "Batch 16, Loss: -0.3296397924423218\n",
      "tensor([0])\n",
      "tensor([[ 0.9372,  0.3474, -0.2839]], grad_fn=<AddmmBackward0>)\n",
      "Batch 17, Loss: -0.9372143149375916\n",
      "tensor([1])\n",
      "tensor([[ 0.8046,  0.3957, -0.4632]], grad_fn=<AddmmBackward0>)\n",
      "Batch 18, Loss: -0.39570942521095276\n",
      "tensor([1])\n",
      "tensor([[ 0.9057,  0.3591, -0.3268]], grad_fn=<AddmmBackward0>)\n",
      "Batch 19, Loss: -0.3591233491897583\n",
      "tensor([0])\n",
      "tensor([[ 0.9510,  0.3429, -0.2658]], grad_fn=<AddmmBackward0>)\n",
      "Batch 20, Loss: -0.9509729146957397\n",
      "tensor([2])\n",
      "tensor([[ 0.9798,  0.3326, -0.2271]], grad_fn=<AddmmBackward0>)\n",
      "Batch 21, Loss: 0.22709941864013672\n",
      "tensor([2])\n",
      "tensor([[ 0.9941,  0.3275, -0.2080]], grad_fn=<AddmmBackward0>)\n",
      "Batch 22, Loss: 0.20795893669128418\n",
      "tensor([0])\n",
      "tensor([[ 0.9554,  0.3417, -0.2603]], grad_fn=<AddmmBackward0>)\n",
      "Batch 23, Loss: -0.9553743600845337\n",
      "tensor([1])\n",
      "tensor([[ 0.9453,  0.3455, -0.2740]], grad_fn=<AddmmBackward0>)\n",
      "Batch 24, Loss: -0.3454883098602295\n",
      "tensor([2])\n",
      "tensor([[ 0.9598,  0.3404, -0.2546]], grad_fn=<AddmmBackward0>)\n",
      "Batch 25, Loss: 0.2546294331550598\n",
      "tensor([1])\n",
      "tensor([[ 0.9575,  0.3414, -0.2578]], grad_fn=<AddmmBackward0>)\n",
      "Batch 26, Loss: -0.34137046337127686\n",
      "tensor([2])\n",
      "tensor([[ 0.9403,  0.3477, -0.2810]], grad_fn=<AddmmBackward0>)\n",
      "Batch 27, Loss: 0.2810271978378296\n",
      "tensor([0])\n",
      "tensor([[ 0.9895,  0.3301, -0.2147]], grad_fn=<AddmmBackward0>)\n",
      "Batch 28, Loss: -0.9894503355026245\n",
      "tensor([1])\n",
      "tensor([[ 0.6222,  0.4630, -0.7103]], grad_fn=<AddmmBackward0>)\n",
      "Batch 29, Loss: -0.463001012802124\n",
      "tensor([0])\n",
      "tensor([[ 0.9881,  0.3309, -0.2167]], grad_fn=<AddmmBackward0>)\n",
      "Batch 30, Loss: -0.9880536794662476\n",
      "tensor([0])\n",
      "tensor([[ 0.9757,  0.3355, -0.2335]], grad_fn=<AddmmBackward0>)\n",
      "Batch 31, Loss: -0.975663423538208\n",
      "tensor([2])\n",
      "tensor([[ 0.9903,  0.3304, -0.2139]], grad_fn=<AddmmBackward0>)\n",
      "Batch 32, Loss: 0.2139318585395813\n",
      "tensor([1])\n",
      "tensor([[ 0.6872,  0.4399, -0.6229]], grad_fn=<AddmmBackward0>)\n",
      "Batch 33, Loss: -0.43993282318115234\n",
      "tensor([2])\n",
      "tensor([[ 0.9287,  0.3529, -0.2971]], grad_fn=<AddmmBackward0>)\n",
      "Batch 34, Loss: 0.29711616039276123\n",
      "tensor([2])\n",
      "tensor([[ 0.9981,  0.3279, -0.2035]], grad_fn=<AddmmBackward0>)\n",
      "Batch 35, Loss: 0.20351797342300415\n",
      "tensor([2])\n",
      "tensor([[ 0.9685,  0.3388, -0.2436]], grad_fn=<AddmmBackward0>)\n",
      "Batch 36, Loss: 0.24356377124786377\n",
      "tensor([2])\n",
      "tensor([[ 0.9217,  0.3557, -0.3066]], grad_fn=<AddmmBackward0>)\n",
      "Batch 37, Loss: 0.3065769672393799\n",
      "tensor([1])\n",
      "tensor([[ 0.8898,  0.3673, -0.3496]], grad_fn=<AddmmBackward0>)\n",
      "Batch 38, Loss: -0.3673064708709717\n",
      "tensor([2])\n",
      "tensor([[ 0.9955,  0.3293, -0.2069]], grad_fn=<AddmmBackward0>)\n",
      "Batch 39, Loss: 0.20690500736236572\n",
      "tensor([0])\n",
      "tensor([[ 0.8822,  0.3703, -0.3597]], grad_fn=<AddmmBackward0>)\n",
      "Batch 40, Loss: -0.8821631669998169\n",
      "tensor([1])\n",
      "tensor([[ 0.9736,  0.3374, -0.2364]], grad_fn=<AddmmBackward0>)\n",
      "Batch 41, Loss: -0.3374209403991699\n",
      "tensor([1])\n",
      "tensor([[ 0.9882,  0.3323, -0.2166]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 42, Loss: -0.3322754502296448\n",
      "tensor([0])\n",
      "tensor([[ 0.9495,  0.3463, -0.2688]], grad_fn=<AddmmBackward0>)\n",
      "Batch 43, Loss: -0.9495185613632202\n",
      "tensor([0])\n",
      "tensor([[ 0.9984,  0.3289, -0.2029]], grad_fn=<AddmmBackward0>)\n",
      "Batch 44, Loss: -0.9983522891998291\n",
      "tensor([1])\n",
      "tensor([[ 0.8215,  0.3927, -0.4415]], grad_fn=<AddmmBackward0>)\n",
      "Batch 45, Loss: -0.39266616106033325\n",
      "tensor([2])\n",
      "tensor([[ 0.9892,  0.3325, -0.2153]], grad_fn=<AddmmBackward0>)\n",
      "Batch 46, Loss: 0.21527332067489624\n",
      "tensor([2])\n",
      "tensor([[ 0.9616,  0.3426, -0.2526]], grad_fn=<AddmmBackward0>)\n",
      "Batch 47, Loss: 0.2526344656944275\n",
      "tensor([2])\n",
      "tensor([[ 0.8002,  0.4007, -0.4704]], grad_fn=<AddmmBackward0>)\n",
      "Batch 48, Loss: 0.4704030752182007\n",
      "tensor([2])\n",
      "tensor([[ 0.9998,  0.3291, -0.2010]], grad_fn=<AddmmBackward0>)\n",
      "Batch 49, Loss: 0.20103704929351807\n",
      "tensor([2])\n",
      "tensor([[ 0.9949,  0.3310, -0.2076]], grad_fn=<AddmmBackward0>)\n",
      "Batch 50, Loss: 0.207558274269104\n",
      "tensor([1])\n",
      "tensor([[ 0.8238,  0.3925, -0.4383]], grad_fn=<AddmmBackward0>)\n",
      "Batch 51, Loss: -0.39246654510498047\n",
      "tensor([1])\n",
      "tensor([[ 0.9394,  0.3511, -0.2823]], grad_fn=<AddmmBackward0>)\n",
      "Batch 52, Loss: -0.3510899543762207\n",
      "tensor([0])\n",
      "tensor([[ 0.9603,  0.3437, -0.2539]], grad_fn=<AddmmBackward0>)\n",
      "Batch 53, Loss: -0.960303544998169\n",
      "tensor([0])\n",
      "tensor([[ 1.0030,  0.3285, -0.1962]], grad_fn=<AddmmBackward0>)\n",
      "Batch 54, Loss: -1.0030206441879272\n",
      "tensor([2])\n",
      "tensor([[ 0.9337,  0.3535, -0.2898]], grad_fn=<AddmmBackward0>)\n",
      "Batch 55, Loss: 0.2897832989692688\n",
      "tensor([1])\n",
      "tensor([[ 0.9950,  0.3317, -0.2071]], grad_fn=<AddmmBackward0>)\n",
      "Batch 56, Loss: -0.33165669441223145\n",
      "tensor([2])\n",
      "tensor([[ 0.8103,  0.3979, -0.4563]], grad_fn=<AddmmBackward0>)\n",
      "Batch 57, Loss: 0.45628970861434937\n",
      "tensor([0])\n",
      "tensor([[ 0.9699,  0.3409, -0.2409]], grad_fn=<AddmmBackward0>)\n",
      "Batch 58, Loss: -0.9698960781097412\n",
      "tensor([0])\n",
      "tensor([[ 0.9626,  0.3436, -0.2508]], grad_fn=<AddmmBackward0>)\n",
      "Batch 59, Loss: -0.9625930786132812\n",
      "tensor([1])\n",
      "tensor([[ 0.9319,  0.3547, -0.2922]], grad_fn=<AddmmBackward0>)\n",
      "Batch 60, Loss: -0.35474029183387756\n",
      "tensor([2])\n",
      "tensor([[ 0.9781,  0.3384, -0.2299]], grad_fn=<AddmmBackward0>)\n",
      "Batch 61, Loss: 0.2298862338066101\n",
      "tensor([1])\n",
      "tensor([[ 0.9819,  0.3371, -0.2248]], grad_fn=<AddmmBackward0>)\n",
      "Batch 62, Loss: -0.3371191620826721\n",
      "tensor([2])\n",
      "tensor([[ 0.5881,  0.4781, -0.7563]], grad_fn=<AddmmBackward0>)\n",
      "Batch 63, Loss: 0.7562731504440308\n",
      "tensor([2])\n",
      "tensor([[ 0.9457,  0.3504, -0.2737]], grad_fn=<AddmmBackward0>)\n",
      "Batch 64, Loss: 0.2737090587615967\n",
      "tensor([2])\n",
      "tensor([[ 0.8445,  0.3866, -0.4102]], grad_fn=<AddmmBackward0>)\n",
      "Batch 65, Loss: 0.4101583957672119\n",
      "tensor([2])\n",
      "tensor([[ 0.9993,  0.3314, -0.2012]], grad_fn=<AddmmBackward0>)\n",
      "Batch 66, Loss: 0.20123165845870972\n",
      "tensor([1])\n",
      "tensor([[ 0.9976,  0.3322, -0.2034]], grad_fn=<AddmmBackward0>)\n",
      "Batch 67, Loss: -0.3321523666381836\n",
      "tensor([1])\n",
      "tensor([[ 0.9904,  0.3348, -0.2130]], grad_fn=<AddmmBackward0>)\n",
      "Batch 68, Loss: -0.334835946559906\n",
      "tensor([1])\n",
      "tensor([[ 0.9942,  0.3336, -0.2078]], grad_fn=<AddmmBackward0>)\n",
      "Batch 69, Loss: -0.33362895250320435\n",
      "tensor([0])\n",
      "tensor([[ 0.9980,  0.3324, -0.2025]], grad_fn=<AddmmBackward0>)\n",
      "Batch 70, Loss: -0.998049259185791\n",
      "tensor([0])\n",
      "tensor([[ 0.9974,  0.3328, -0.2034]], grad_fn=<AddmmBackward0>)\n",
      "Batch 71, Loss: -0.9973784685134888\n",
      "tensor([2])\n",
      "tensor([[ 0.8116,  0.3992, -0.4543]], grad_fn=<AddmmBackward0>)\n",
      "Batch 72, Loss: 0.4542980492115021\n",
      "tensor([0])\n",
      "tensor([[ 0.9128,  0.3632, -0.3176]], grad_fn=<AddmmBackward0>)\n",
      "Batch 73, Loss: -0.9128400087356567\n",
      "tensor([2])\n",
      "tensor([[ 0.9826,  0.3385, -0.2234]], grad_fn=<AddmmBackward0>)\n",
      "Batch 74, Loss: 0.22343158721923828\n",
      "tensor([2])\n",
      "tensor([[ 0.9866,  0.3372, -0.2181]], grad_fn=<AddmmBackward0>)\n",
      "Batch 75, Loss: 0.21809536218643188\n",
      "tensor([2])\n",
      "tensor([[ 0.9971,  0.3336, -0.2039]], grad_fn=<AddmmBackward0>)\n",
      "Batch 76, Loss: 0.20385098457336426\n",
      "tensor([0])\n",
      "tensor([[ 0.9689,  0.3437, -0.2418]], grad_fn=<AddmmBackward0>)\n",
      "Batch 77, Loss: -0.9689404964447021\n",
      "tensor([2])\n",
      "tensor([[ 0.9950,  0.3346, -0.2066]], grad_fn=<AddmmBackward0>)\n",
      "Batch 78, Loss: 0.20655161142349243\n",
      "tensor([1])\n",
      "tensor([[ 1.0009,  0.3326, -0.1986]], grad_fn=<AddmmBackward0>)\n",
      "Batch 79, Loss: -0.3325657248497009\n",
      "tensor([2])\n",
      "tensor([[ 0.9869,  0.3377, -0.2175]], grad_fn=<AddmmBackward0>)\n",
      "Batch 80, Loss: 0.21745455265045166\n",
      "tensor([1])\n",
      "tensor([[ 0.9831,  0.3391, -0.2225]], grad_fn=<AddmmBackward0>)\n",
      "Batch 81, Loss: -0.33910199999809265\n",
      "tensor([2])\n",
      "tensor([[ 0.9659,  0.3453, -0.2456]], grad_fn=<AddmmBackward0>)\n",
      "Batch 82, Loss: 0.24561631679534912\n",
      "tensor([2])\n",
      "tensor([[ 0.9081,  0.3660, -0.3236]], grad_fn=<AddmmBackward0>)\n",
      "Batch 83, Loss: 0.32363516092300415\n",
      "tensor([2])\n",
      "tensor([[ 0.9930,  0.3360, -0.2089]], grad_fn=<AddmmBackward0>)\n",
      "Batch 84, Loss: 0.2088811993598938\n",
      "tensor([1])\n",
      "tensor([[ 1.0051,  0.3317, -0.1924]], grad_fn=<AddmmBackward0>)\n",
      "Batch 85, Loss: -0.3317483067512512\n",
      "tensor([1])\n",
      "tensor([[ 0.9674,  0.3452, -0.2432]], grad_fn=<AddmmBackward0>)\n",
      "Batch 86, Loss: -0.34524375200271606\n",
      "tensor([1])\n",
      "tensor([[ 1.0050,  0.3321, -0.1923]], grad_fn=<AddmmBackward0>)\n",
      "Batch 87, Loss: -0.3320593237876892\n",
      "tensor([0])\n",
      "tensor([[ 0.9831,  0.3400, -0.2217]], grad_fn=<AddmmBackward0>)\n",
      "Batch 88, Loss: -0.9831342101097107\n",
      "tensor([2])\n",
      "tensor([[ 0.9990,  0.3345, -0.2003]], grad_fn=<AddmmBackward0>)\n",
      "Batch 89, Loss: 0.20026367902755737\n",
      "tensor([1])\n",
      "tensor([[ 0.9809,  0.3410, -0.2246]], grad_fn=<AddmmBackward0>)\n",
      "Batch 90, Loss: -0.34103912115097046\n",
      "tensor([0])\n",
      "tensor([[ 0.8041,  0.4038, -0.4634]], grad_fn=<AddmmBackward0>)\n",
      "Batch 91, Loss: -0.8041406869888306\n",
      "tensor([2])\n",
      "tensor([[ 0.9729,  0.3442, -0.2353]], grad_fn=<AddmmBackward0>)\n",
      "Batch 92, Loss: 0.235293447971344\n",
      "tensor([2])\n",
      "tensor([[ 0.9987,  0.3352, -0.2004]], grad_fn=<AddmmBackward0>)\n",
      "Batch 93, Loss: 0.20036298036575317\n",
      "tensor([2])\n",
      "tensor([[ 0.9792,  0.3422, -0.2267]], grad_fn=<AddmmBackward0>)\n",
      "Batch 94, Loss: 0.2266772985458374\n",
      "tensor([1])\n",
      "tensor([[ 0.9882,  0.3392, -0.2145]], grad_fn=<AddmmBackward0>)\n",
      "Batch 95, Loss: -0.33918172121047974\n",
      "tensor([0])\n",
      "tensor([[ 0.9888,  0.3391, -0.2135]], grad_fn=<AddmmBackward0>)\n",
      "Batch 96, Loss: -0.9888340830802917\n",
      "tensor([1])\n",
      "tensor([[ 0.9952,  0.3370, -0.2049]], grad_fn=<AddmmBackward0>)\n",
      "Batch 97, Loss: -0.33697444200515747\n",
      "tensor([2])\n",
      "tensor([[ 0.9937,  0.3376, -0.2068]], grad_fn=<AddmmBackward0>)\n",
      "Batch 98, Loss: 0.20680350065231323\n",
      "tensor([2])\n",
      "tensor([[ 1.0079,  0.3328, -0.1876]], grad_fn=<AddmmBackward0>)\n",
      "Batch 99, Loss: 0.18757593631744385\n",
      "tensor([2])\n",
      "tensor([[ 0.9749,  0.3445, -0.2320]], grad_fn=<AddmmBackward0>)\n",
      "Batch 100, Loss: 0.2320476770401001\n",
      "tensor([0])\n",
      "tensor([[ 1.0074,  0.3332, -0.1880]], grad_fn=<AddmmBackward0>)\n",
      "Batch 101, Loss: -1.0073885917663574\n",
      "tensor([1])\n",
      "tensor([[ 0.9714,  0.3460, -0.2366]], grad_fn=<AddmmBackward0>)\n",
      "Batch 102, Loss: -0.34601205587387085\n",
      "tensor([2])\n",
      "tensor([[ 1.0079,  0.3333, -0.1872]], grad_fn=<AddmmBackward0>)\n",
      "Batch 103, Loss: 0.18723881244659424\n",
      "tensor([1])\n",
      "tensor([[ 0.8580,  0.3862, -0.3898]], grad_fn=<AddmmBackward0>)\n",
      "Batch 104, Loss: -0.386189341545105\n",
      "tensor([2])\n",
      "tensor([[ 0.9875,  0.3407, -0.2146]], grad_fn=<AddmmBackward0>)\n",
      "Batch 105, Loss: 0.21463602781295776\n",
      "tensor([2])\n",
      "tensor([[ 0.9725,  0.3461, -0.2348]], grad_fn=<AddmmBackward0>)\n",
      "Batch 106, Loss: 0.2348247766494751\n",
      "tensor([0])\n",
      "tensor([[ 0.8983,  0.3723, -0.3350]], grad_fn=<AddmmBackward0>)\n",
      "Batch 107, Loss: -0.8983486890792847\n",
      "tensor([1])\n",
      "tensor([[ 0.9795,  0.3439, -0.2251]], grad_fn=<AddmmBackward0>)\n",
      "Batch 108, Loss: -0.34388870000839233\n",
      "tensor([1])\n",
      "tensor([[ 0.9256,  0.3630, -0.2980]], grad_fn=<AddmmBackward0>)\n",
      "Batch 109, Loss: -0.3629697561264038\n",
      "tensor([1])\n",
      "tensor([[ 0.9281,  0.3622, -0.2946]], grad_fn=<AddmmBackward0>)\n",
      "Batch 110, Loss: -0.36224526166915894\n",
      "tensor([1])\n",
      "tensor([[ 0.9809,  0.3439, -0.2231]], grad_fn=<AddmmBackward0>)\n",
      "Batch 111, Loss: -0.3438623547554016\n",
      "tensor([0])\n",
      "tensor([[ 0.9925,  0.3400, -0.2074]], grad_fn=<AddmmBackward0>)\n",
      "Batch 112, Loss: -0.9925183057785034\n",
      "tensor([1])\n",
      "tensor([[ 0.9962,  0.3389, -0.2024]], grad_fn=<AddmmBackward0>)\n",
      "Batch 113, Loss: -0.3388622999191284\n",
      "tensor([1])\n",
      "tensor([[ 0.9691,  0.3486, -0.2391]], grad_fn=<AddmmBackward0>)\n",
      "Batch 114, Loss: -0.3485618233680725\n",
      "tensor([2])\n",
      "tensor([[ 0.8875,  0.3774, -0.3495]], grad_fn=<AddmmBackward0>)\n",
      "Batch 115, Loss: 0.34946203231811523\n",
      "tensor([2])\n",
      "tensor([[ 0.9730,  0.3476, -0.2338]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 116, Loss: 0.23376822471618652\n",
      "tensor([2])\n",
      "tensor([[ 0.9257,  0.3643, -0.2977]], grad_fn=<AddmmBackward0>)\n",
      "Batch 117, Loss: 0.29773402214050293\n",
      "tensor([0])\n",
      "tensor([[ 1.0046,  0.3368, -0.1908]], grad_fn=<AddmmBackward0>)\n",
      "Batch 118, Loss: -1.0046356916427612\n",
      "tensor([1])\n",
      "tensor([[ 0.9488,  0.3565, -0.2663]], grad_fn=<AddmmBackward0>)\n",
      "Batch 119, Loss: -0.356518030166626\n",
      "tensor([2])\n",
      "tensor([[ 0.9190,  0.3671, -0.3066]], grad_fn=<AddmmBackward0>)\n",
      "Batch 120, Loss: 0.30664634704589844\n",
      "tensor([0])\n",
      "tensor([[ 1.0025,  0.3380, -0.1936]], grad_fn=<AddmmBackward0>)\n",
      "Batch 121, Loss: -1.0024988651275635\n",
      "tensor([1])\n",
      "tensor([[ 0.7972,  0.4099, -0.4713]], grad_fn=<AddmmBackward0>)\n",
      "Batch 122, Loss: -0.4099122881889343\n",
      "tensor([0])\n",
      "tensor([[ 0.9929,  0.3417, -0.2065]], grad_fn=<AddmmBackward0>)\n",
      "Batch 123, Loss: -0.9929178953170776\n",
      "tensor([1])\n",
      "tensor([[ 1.0086,  0.3364, -0.1854]], grad_fn=<AddmmBackward0>)\n",
      "Batch 124, Loss: -0.3363841772079468\n",
      "tensor([1])\n",
      "tensor([[ 0.9288,  0.3644, -0.2933]], grad_fn=<AddmmBackward0>)\n",
      "Batch 125, Loss: -0.3643985986709595\n",
      "tensor([2])\n",
      "tensor([[ 0.9905,  0.3431, -0.2100]], grad_fn=<AddmmBackward0>)\n",
      "Batch 126, Loss: 0.20999091863632202\n",
      "tensor([1])\n",
      "tensor([[ 1.0064,  0.3377, -0.1885]], grad_fn=<AddmmBackward0>)\n",
      "Batch 127, Loss: -0.33770978450775146\n",
      "tensor([0])\n",
      "tensor([[ 0.9990,  0.3405, -0.1985]], grad_fn=<AddmmBackward0>)\n",
      "Batch 128, Loss: -0.9989778995513916\n",
      "tensor([1])\n",
      "tensor([[ 0.9847,  0.3456, -0.2178]], grad_fn=<AddmmBackward0>)\n",
      "Batch 129, Loss: -0.3456266224384308\n",
      "tensor([0])\n",
      "tensor([[ 0.9904,  0.3438, -0.2102]], grad_fn=<AddmmBackward0>)\n",
      "Batch 130, Loss: -0.9903714656829834\n",
      "tensor([0])\n",
      "tensor([[ 1.0116,  0.3366, -0.1815]], grad_fn=<AddmmBackward0>)\n",
      "Batch 131, Loss: -1.0116342306137085\n",
      "tensor([1])\n",
      "tensor([[ 0.9399,  0.3618, -0.2787]], grad_fn=<AddmmBackward0>)\n",
      "Batch 132, Loss: -0.3617587089538574\n",
      "tensor([0])\n",
      "tensor([[ 0.8637,  0.3884, -0.3818]], grad_fn=<AddmmBackward0>)\n",
      "Batch 133, Loss: -0.863680362701416\n",
      "tensor([2])\n",
      "tensor([[ 0.9997,  0.3414, -0.1980]], grad_fn=<AddmmBackward0>)\n",
      "Batch 134, Loss: 0.1980082392692566\n",
      "tensor([0])\n",
      "tensor([[ 0.9675,  0.3527, -0.2416]], grad_fn=<AddmmBackward0>)\n",
      "Batch 135, Loss: -0.9675370454788208\n",
      "tensor([0])\n",
      "tensor([[ 1.0067,  0.3392, -0.1887]], grad_fn=<AddmmBackward0>)\n",
      "Batch 136, Loss: -1.006731390953064\n",
      "tensor([2])\n",
      "tensor([[ 0.9005,  0.3762, -0.3325]], grad_fn=<AddmmBackward0>)\n",
      "Batch 137, Loss: 0.332461416721344\n",
      "tensor([2])\n",
      "tensor([[ 1.0134,  0.3372, -0.1800]], grad_fn=<AddmmBackward0>)\n",
      "Batch 138, Loss: 0.17995303869247437\n",
      "tensor([0])\n",
      "tensor([[ 1.0028,  0.3410, -0.1944]], grad_fn=<AddmmBackward0>)\n",
      "Batch 139, Loss: -1.0028300285339355\n",
      "tensor([2])\n",
      "tensor([[ 0.8145,  0.4064, -0.4490]], grad_fn=<AddmmBackward0>)\n",
      "Batch 140, Loss: 0.448978066444397\n",
      "tensor([0])\n",
      "tensor([[ 0.9759,  0.3506, -0.2310]], grad_fn=<AddmmBackward0>)\n",
      "Batch 141, Loss: -0.9758629202842712\n",
      "tensor([1])\n",
      "tensor([[ 0.9543,  0.3582, -0.2602]], grad_fn=<AddmmBackward0>)\n",
      "Batch 142, Loss: -0.3581801950931549\n",
      "tensor([2])\n",
      "tensor([[ 0.8889,  0.3810, -0.3487]], grad_fn=<AddmmBackward0>)\n",
      "Batch 143, Loss: 0.34873831272125244\n",
      "tensor([1])\n",
      "tensor([[ 0.9317,  0.3663, -0.2909]], grad_fn=<AddmmBackward0>)\n",
      "Batch 144, Loss: -0.3662567436695099\n",
      "tensor([0])\n",
      "tensor([[ 0.9794,  0.3499, -0.2265]], grad_fn=<AddmmBackward0>)\n",
      "Batch 145, Loss: -0.9793635606765747\n",
      "tensor([1])\n",
      "tensor([[ 0.9794,  0.3500, -0.2266]], grad_fn=<AddmmBackward0>)\n",
      "Batch 146, Loss: -0.3500255048274994\n",
      "tensor([2])\n",
      "tensor([[ 0.9446,  0.3622, -0.2737]], grad_fn=<AddmmBackward0>)\n",
      "Batch 147, Loss: 0.2737165689468384\n",
      "tensor([2])\n",
      "tensor([[ 0.8125,  0.4080, -0.4521]], grad_fn=<AddmmBackward0>)\n",
      "Batch 148, Loss: 0.45213213562965393\n",
      "tensor([2])\n",
      "tensor([[ 0.7287,  0.4370, -0.5654]], grad_fn=<AddmmBackward0>)\n",
      "Batch 149, Loss: 0.5654128789901733\n",
      "tensor([2])\n",
      "tensor([[ 1.0010,  0.3431, -0.1975]], grad_fn=<AddmmBackward0>)\n",
      "Batch 150, Loss: 0.19752901792526245\n",
      "tensor([1])\n",
      "tensor([[ 0.9982,  0.3442, -0.2013]], grad_fn=<AddmmBackward0>)\n",
      "Batch 151, Loss: -0.344205379486084\n",
      "tensor([0])\n",
      "tensor([[ 1.0138,  0.3389, -0.1801]], grad_fn=<AddmmBackward0>)\n",
      "Batch 152, Loss: -1.0138139724731445\n",
      "tensor([1])\n",
      "tensor([[ 0.9914,  0.3468, -0.2104]], grad_fn=<AddmmBackward0>)\n",
      "Batch 153, Loss: -0.3467968702316284\n",
      "tensor([0])\n",
      "tensor([[ 0.9368,  0.3658, -0.2842]], grad_fn=<AddmmBackward0>)\n",
      "Batch 154, Loss: -0.9367772340774536\n",
      "tensor([1])\n",
      "tensor([[ 1.0092,  0.3410, -0.1864]], grad_fn=<AddmmBackward0>)\n",
      "Batch 155, Loss: -0.3409538269042969\n",
      "tensor([2])\n",
      "tensor([[ 1.0069,  0.3419, -0.1895]], grad_fn=<AddmmBackward0>)\n",
      "Batch 156, Loss: 0.18953341245651245\n",
      "tensor([1])\n",
      "tensor([[ 0.9586,  0.3587, -0.2548]], grad_fn=<AddmmBackward0>)\n",
      "Batch 157, Loss: -0.35866644978523254\n",
      "tensor([0])\n",
      "tensor([[ 0.9435,  0.3640, -0.2752]], grad_fn=<AddmmBackward0>)\n",
      "Batch 158, Loss: -0.9434914588928223\n",
      "tensor([0])\n",
      "tensor([[ 0.8614,  0.3924, -0.3862]], grad_fn=<AddmmBackward0>)\n",
      "Batch 159, Loss: -0.8613519668579102\n",
      "tensor([0])\n",
      "tensor([[ 0.9977,  0.3457, -0.2022]], grad_fn=<AddmmBackward0>)\n",
      "Batch 160, Loss: -0.9976702332496643\n",
      "tensor([2])\n",
      "tensor([[ 0.9697,  0.3555, -0.2400]], grad_fn=<AddmmBackward0>)\n",
      "Batch 161, Loss: 0.2400113344192505\n",
      "tensor([2])\n",
      "tensor([[ 1.0124,  0.3409, -0.1824]], grad_fn=<AddmmBackward0>)\n",
      "Batch 162, Loss: 0.18240821361541748\n",
      "tensor([1])\n",
      "tensor([[ 0.9543,  0.3610, -0.2610]], grad_fn=<AddmmBackward0>)\n",
      "Batch 163, Loss: -0.36100512742996216\n",
      "tensor([0])\n",
      "tensor([[ 0.8892,  0.3835, -0.3489]], grad_fn=<AddmmBackward0>)\n",
      "Batch 164, Loss: -0.889244794845581\n",
      "tensor([2])\n",
      "tensor([[ 0.9549,  0.3611, -0.2602]], grad_fn=<AddmmBackward0>)\n",
      "Batch 165, Loss: 0.26023024320602417\n",
      "tensor([1])\n",
      "tensor([[ 0.9738,  0.3547, -0.2347]], grad_fn=<AddmmBackward0>)\n",
      "Batch 166, Loss: -0.35470056533813477\n",
      "tensor([1])\n",
      "tensor([[ 0.9317,  0.3693, -0.2916]], grad_fn=<AddmmBackward0>)\n",
      "Batch 167, Loss: -0.3692796230316162\n",
      "tensor([0])\n",
      "tensor([[ 0.8892,  0.3840, -0.3490]], grad_fn=<AddmmBackward0>)\n",
      "Batch 168, Loss: -0.8892201781272888\n",
      "tensor([2])\n",
      "tensor([[ 0.9383,  0.3673, -0.2828]], grad_fn=<AddmmBackward0>)\n",
      "Batch 169, Loss: 0.2827587127685547\n",
      "tensor([0])\n",
      "tensor([[ 1.0050,  0.3446, -0.1928]], grad_fn=<AddmmBackward0>)\n",
      "Batch 170, Loss: -1.0049878358840942\n",
      "tensor([0])\n",
      "tensor([[ 0.9658,  0.3582, -0.2458]], grad_fn=<AddmmBackward0>)\n",
      "Batch 171, Loss: -0.9657950401306152\n",
      "tensor([1])\n",
      "tensor([[ 1.0108,  0.3429, -0.1852]], grad_fn=<AddmmBackward0>)\n",
      "Batch 172, Loss: -0.3429279327392578\n",
      "tensor([1])\n",
      "tensor([[ 1.0080,  0.3440, -0.1890]], grad_fn=<AddmmBackward0>)\n",
      "Batch 173, Loss: -0.344044029712677\n",
      "tensor([0])\n",
      "tensor([[ 1.0111,  0.3432, -0.1849]], grad_fn=<AddmmBackward0>)\n",
      "Batch 174, Loss: -1.0110573768615723\n",
      "tensor([0])\n",
      "tensor([[ 1.0132,  0.3426, -0.1821]], grad_fn=<AddmmBackward0>)\n",
      "Batch 175, Loss: -1.0132477283477783\n",
      "tensor([0])\n",
      "tensor([[ 1.0162,  0.3417, -0.1783]], grad_fn=<AddmmBackward0>)\n",
      "Batch 176, Loss: -1.0161935091018677\n",
      "tensor([0])\n",
      "tensor([[ 0.9741,  0.3563, -0.2353]], grad_fn=<AddmmBackward0>)\n",
      "Batch 177, Loss: -0.9741047620773315\n",
      "tensor([1])\n",
      "tensor([[ 0.9536,  0.3634, -0.2632]], grad_fn=<AddmmBackward0>)\n",
      "Batch 178, Loss: -0.36341455578804016\n",
      "tensor([0])\n",
      "tensor([[ 0.9615,  0.3609, -0.2527]], grad_fn=<AddmmBackward0>)\n",
      "Batch 179, Loss: -0.9614691138267517\n",
      "tensor([0])\n",
      "tensor([[ 1.0178,  0.3419, -0.1770]], grad_fn=<AddmmBackward0>)\n",
      "Batch 180, Loss: -1.017763376235962\n",
      "tensor([2])\n",
      "tensor([[ 1.0084,  0.3452, -0.1899]], grad_fn=<AddmmBackward0>)\n",
      "Batch 181, Loss: 0.18985342979431152\n",
      "tensor([0])\n",
      "tensor([[ 1.0196,  0.3416, -0.1749]], grad_fn=<AddmmBackward0>)\n",
      "Batch 182, Loss: -1.019592523574829\n",
      "tensor([2])\n",
      "tensor([[ 1.0116,  0.3444, -0.1859]], grad_fn=<AddmmBackward0>)\n",
      "Batch 183, Loss: 0.1859140396118164\n",
      "tensor([1])\n",
      "tensor([[ 1.0128,  0.3441, -0.1845]], grad_fn=<AddmmBackward0>)\n",
      "Batch 184, Loss: -0.34414342045783997\n",
      "tensor([0])\n",
      "tensor([[ 0.9829,  0.3545, -0.2249]], grad_fn=<AddmmBackward0>)\n",
      "Batch 185, Loss: -0.9829097390174866\n",
      "tensor([1])\n",
      "tensor([[ 0.9536,  0.3646, -0.2646]], grad_fn=<AddmmBackward0>)\n",
      "Batch 186, Loss: -0.36456817388534546\n",
      "tensor([0])\n",
      "tensor([[ 1.0104,  0.3454, -0.1881]], grad_fn=<AddmmBackward0>)\n",
      "Batch 187, Loss: -1.0104174613952637\n",
      "tensor([2])\n",
      "tensor([[ 0.9277,  0.3737, -0.2998]], grad_fn=<AddmmBackward0>)\n",
      "Batch 188, Loss: 0.2998065948486328\n",
      "tensor([2])\n",
      "tensor([[ 1.0137,  0.3446, -0.1841]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 189, Loss: 0.18409675359725952\n",
      "tensor([0])\n",
      "tensor([[ 0.8263,  0.4083, -0.4366]], grad_fn=<AddmmBackward0>)\n",
      "Batch 190, Loss: -0.8262733817100525\n",
      "tensor([1])\n",
      "tensor([[ 0.9664,  0.3609, -0.2479]], grad_fn=<AddmmBackward0>)\n",
      "Batch 191, Loss: -0.3608946204185486\n",
      "tensor([0])\n",
      "tensor([[ 0.9880,  0.3537, -0.2190]], grad_fn=<AddmmBackward0>)\n",
      "Batch 192, Loss: -0.9879515171051025\n",
      "tensor([1])\n",
      "tensor([[ 0.9543,  0.3653, -0.2645]], grad_fn=<AddmmBackward0>)\n",
      "Batch 193, Loss: -0.36528754234313965\n",
      "tensor([2])\n",
      "tensor([[ 0.9213,  0.3766, -0.3090]], grad_fn=<AddmmBackward0>)\n",
      "Batch 194, Loss: 0.3090028762817383\n",
      "tensor([2])\n",
      "tensor([[ 1.0071,  0.3477, -0.1935]], grad_fn=<AddmmBackward0>)\n",
      "Batch 195, Loss: 0.19351792335510254\n",
      "tensor([0])\n",
      "tensor([[ 1.0083,  0.3474, -0.1919]], grad_fn=<AddmmBackward0>)\n",
      "Batch 196, Loss: -1.008342981338501\n",
      "tensor([1])\n",
      "tensor([[ 1.0202,  0.3435, -0.1760]], grad_fn=<AddmmBackward0>)\n",
      "Batch 197, Loss: -0.3434998691082001\n",
      "tensor([2])\n",
      "tensor([[ 0.8701,  0.3944, -0.3782]], grad_fn=<AddmmBackward0>)\n",
      "Batch 198, Loss: 0.378178209066391\n",
      "tensor([1])\n",
      "tensor([[ 0.9946,  0.3525, -0.2107]], grad_fn=<AddmmBackward0>)\n",
      "Batch 199, Loss: -0.35246604681015015\n",
      "tensor([1])\n",
      "tensor([[ 1.0235,  0.3428, -0.1717]], grad_fn=<AddmmBackward0>)\n",
      "Batch 200, Loss: -0.3428272306919098\n",
      "tensor([0])\n",
      "tensor([[ 0.9031,  0.3837, -0.3339]], grad_fn=<AddmmBackward0>)\n",
      "Batch 201, Loss: -0.9031039476394653\n",
      "tensor([0])\n",
      "tensor([[ 1.0188,  0.3448, -0.1781]], grad_fn=<AddmmBackward0>)\n",
      "Batch 202, Loss: -1.0188251733779907\n",
      "tensor([1])\n",
      "tensor([[ 1.0207,  0.3443, -0.1757]], grad_fn=<AddmmBackward0>)\n",
      "Batch 203, Loss: -0.34430915117263794\n",
      "tensor([1])\n",
      "tensor([[ 1.0081,  0.3487, -0.1927]], grad_fn=<AddmmBackward0>)\n",
      "Batch 204, Loss: -0.34871527552604675\n",
      "tensor([0])\n",
      "tensor([[ 0.8403,  0.4055, -0.4187]], grad_fn=<AddmmBackward0>)\n",
      "Batch 205, Loss: -0.840316653251648\n",
      "tensor([0])\n",
      "tensor([[ 0.9229,  0.3778, -0.3076]], grad_fn=<AddmmBackward0>)\n",
      "Batch 206, Loss: -0.9229130148887634\n",
      "tensor([1])\n",
      "tensor([[ 1.0067,  0.3498, -0.1949]], grad_fn=<AddmmBackward0>)\n",
      "Batch 207, Loss: -0.3497544527053833\n",
      "tensor([2])\n",
      "tensor([[ 0.9450,  0.3707, -0.2782]], grad_fn=<AddmmBackward0>)\n",
      "Batch 208, Loss: 0.2781681418418884\n",
      "tensor([2])\n",
      "tensor([[ 0.9907,  0.3555, -0.2168]], grad_fn=<AddmmBackward0>)\n",
      "Batch 209, Loss: 0.2167796492576599\n",
      "tensor([0])\n",
      "tensor([[ 1.0224,  0.3450, -0.1741]], grad_fn=<AddmmBackward0>)\n",
      "Batch 210, Loss: -1.0224337577819824\n",
      "tensor([1])\n",
      "tensor([[ 0.9337,  0.3750, -0.2935]], grad_fn=<AddmmBackward0>)\n",
      "Batch 211, Loss: -0.3749658465385437\n",
      "tensor([1])\n",
      "tensor([[ 1.0189,  0.3465, -0.1789]], grad_fn=<AddmmBackward0>)\n",
      "Batch 212, Loss: -0.34649670124053955\n",
      "tensor([2])\n",
      "tensor([[ 0.9986,  0.3535, -0.2063]], grad_fn=<AddmmBackward0>)\n",
      "Batch 213, Loss: 0.20632898807525635\n",
      "tensor([2])\n",
      "tensor([[ 1.0094,  0.3500, -0.1918]], grad_fn=<AddmmBackward0>)\n",
      "Batch 214, Loss: 0.19179904460906982\n",
      "tensor([2])\n",
      "tensor([[ 0.9489,  0.3705, -0.2732]], grad_fn=<AddmmBackward0>)\n",
      "Batch 215, Loss: 0.273201048374176\n",
      "tensor([2])\n",
      "tensor([[ 0.9765,  0.3614, -0.2360]], grad_fn=<AddmmBackward0>)\n",
      "Batch 216, Loss: 0.23604148626327515\n",
      "tensor([0])\n",
      "tensor([[ 1.0157,  0.3484, -0.1833]], grad_fn=<AddmmBackward0>)\n",
      "Batch 217, Loss: -1.015697956085205\n",
      "tensor([1])\n",
      "tensor([[ 1.0170,  0.3480, -0.1815]], grad_fn=<AddmmBackward0>)\n",
      "Batch 218, Loss: -0.3480382561683655\n",
      "tensor([2])\n",
      "tensor([[ 0.9811,  0.3602, -0.2298]], grad_fn=<AddmmBackward0>)\n",
      "Batch 219, Loss: 0.22978925704956055\n",
      "tensor([0])\n",
      "tensor([[ 0.9993,  0.3543, -0.2053]], grad_fn=<AddmmBackward0>)\n",
      "Batch 220, Loss: -0.9992716908454895\n",
      "tensor([2])\n",
      "tensor([[ 0.9756,  0.3623, -0.2371]], grad_fn=<AddmmBackward0>)\n",
      "Batch 221, Loss: 0.23711156845092773\n",
      "tensor([0])\n",
      "tensor([[ 0.9810,  0.3606, -0.2298]], grad_fn=<AddmmBackward0>)\n",
      "Batch 222, Loss: -0.9810417294502258\n",
      "tensor([2])\n",
      "tensor([[ 0.9989,  0.3547, -0.2058]], grad_fn=<AddmmBackward0>)\n",
      "Batch 223, Loss: 0.20583850145339966\n",
      "tensor([2])\n",
      "tensor([[ 0.9556,  0.3693, -0.2641]], grad_fn=<AddmmBackward0>)\n",
      "Batch 224, Loss: 0.2640536427497864\n",
      "tensor([0])\n",
      "tensor([[ 0.9587,  0.3684, -0.2598]], grad_fn=<AddmmBackward0>)\n",
      "Batch 225, Loss: -0.9587000608444214\n",
      "tensor([1])\n",
      "tensor([[ 0.9530,  0.3704, -0.2675]], grad_fn=<AddmmBackward0>)\n",
      "Batch 226, Loss: -0.37035977840423584\n",
      "tensor([2])\n",
      "tensor([[ 0.8361,  0.4095, -0.4247]], grad_fn=<AddmmBackward0>)\n",
      "Batch 227, Loss: 0.42474716901779175\n",
      "tensor([0])\n",
      "tensor([[ 1.0072,  0.3525, -0.1946]], grad_fn=<AddmmBackward0>)\n",
      "Batch 228, Loss: -1.0071513652801514\n",
      "tensor([1])\n",
      "tensor([[ 1.0236,  0.3471, -0.1725]], grad_fn=<AddmmBackward0>)\n",
      "Batch 229, Loss: -0.3470947742462158\n",
      "tensor([1])\n",
      "tensor([[ 0.9772,  0.3627, -0.2349]], grad_fn=<AddmmBackward0>)\n",
      "Batch 230, Loss: -0.3627058267593384\n",
      "tensor([0])\n",
      "tensor([[ 1.0119,  0.3513, -0.1883]], grad_fn=<AddmmBackward0>)\n",
      "Batch 231, Loss: -1.0118770599365234\n",
      "tensor([0])\n",
      "tensor([[ 1.0057,  0.3535, -0.1967]], grad_fn=<AddmmBackward0>)\n",
      "Batch 232, Loss: -1.0056730508804321\n",
      "tensor([0])\n",
      "tensor([[ 0.9921,  0.3582, -0.2150]], grad_fn=<AddmmBackward0>)\n",
      "Batch 233, Loss: -0.99211585521698\n",
      "tensor([0])\n",
      "tensor([[ 0.9451,  0.3739, -0.2783]], grad_fn=<AddmmBackward0>)\n",
      "Batch 234, Loss: -0.9451433420181274\n",
      "tensor([0])\n",
      "tensor([[ 1.0169,  0.3502, -0.1820]], grad_fn=<AddmmBackward0>)\n",
      "Batch 235, Loss: -1.0169062614440918\n",
      "tensor([1])\n",
      "tensor([[ 1.0208,  0.3490, -0.1769]], grad_fn=<AddmmBackward0>)\n",
      "Batch 236, Loss: -0.3490075469017029\n",
      "tensor([2])\n",
      "tensor([[ 1.0195,  0.3496, -0.1788]], grad_fn=<AddmmBackward0>)\n",
      "Batch 237, Loss: 0.17877620458602905\n",
      "tensor([2])\n",
      "tensor([[ 1.0287,  0.3467, -0.1665]], grad_fn=<AddmmBackward0>)\n",
      "Batch 238, Loss: 0.16651296615600586\n",
      "tensor([2])\n",
      "tensor([[ 0.9557,  0.3711, -0.2648]], grad_fn=<AddmmBackward0>)\n",
      "Batch 239, Loss: 0.26479268074035645\n",
      "tensor([2])\n",
      "tensor([[ 1.0186,  0.3503, -0.1803]], grad_fn=<AddmmBackward0>)\n",
      "Batch 240, Loss: 0.18031048774719238\n",
      "tensor([0])\n",
      "tensor([[ 1.0201,  0.3499, -0.1783]], grad_fn=<AddmmBackward0>)\n",
      "Batch 241, Loss: -1.020094633102417\n",
      "tensor([0])\n",
      "tensor([[ 0.9756,  0.3648, -0.2382]], grad_fn=<AddmmBackward0>)\n",
      "Batch 242, Loss: -0.975572943687439\n",
      "tensor([1])\n",
      "tensor([[ 1.0239,  0.3488, -0.1733]], grad_fn=<AddmmBackward0>)\n",
      "Batch 243, Loss: -0.34884482622146606\n",
      "tensor([0])\n",
      "tensor([[ 1.0261,  0.3482, -0.1703]], grad_fn=<AddmmBackward0>)\n",
      "Batch 244, Loss: -1.0261218547821045\n",
      "tensor([1])\n",
      "tensor([[ 0.7408,  0.4430, -0.5541]], grad_fn=<AddmmBackward0>)\n",
      "Batch 245, Loss: -0.4430239796638489\n",
      "tensor([0])\n",
      "tensor([[ 0.9162,  0.3849, -0.3183]], grad_fn=<AddmmBackward0>)\n",
      "Batch 246, Loss: -0.9162082076072693\n",
      "tensor([1])\n",
      "tensor([[ 1.0233,  0.3496, -0.1745]], grad_fn=<AddmmBackward0>)\n",
      "Batch 247, Loss: -0.34956443309783936\n",
      "tensor([2])\n",
      "tensor([[ 1.0290,  0.3478, -0.1670]], grad_fn=<AddmmBackward0>)\n",
      "Batch 248, Loss: 0.16700541973114014\n",
      "tensor([2])\n",
      "tensor([[ 0.8921,  0.3933, -0.3511]], grad_fn=<AddmmBackward0>)\n",
      "Batch 249, Loss: 0.3510645031929016\n",
      "tensor([0])\n",
      "tensor([[ 1.0181,  0.3517, -0.1817]], grad_fn=<AddmmBackward0>)\n",
      "Batch 250, Loss: -1.0181201696395874\n",
      "tensor([0])\n",
      "tensor([[ 0.8178,  0.4181, -0.4509]], grad_fn=<AddmmBackward0>)\n",
      "Batch 251, Loss: -0.8178113698959351\n",
      "tensor([2])\n",
      "tensor([[ 1.0115,  0.3541, -0.1908]], grad_fn=<AddmmBackward0>)\n",
      "Batch 252, Loss: 0.19075632095336914\n",
      "tensor([2])\n",
      "tensor([[ 0.9796,  0.3648, -0.2337]], grad_fn=<AddmmBackward0>)\n",
      "Batch 253, Loss: 0.23367655277252197\n",
      "tensor([0])\n",
      "tensor([[ 1.0264,  0.3494, -0.1708]], grad_fn=<AddmmBackward0>)\n",
      "Batch 254, Loss: -1.0264228582382202\n",
      "tensor([1])\n",
      "tensor([[ 0.9537,  0.3736, -0.2686]], grad_fn=<AddmmBackward0>)\n",
      "Batch 255, Loss: -0.373561292886734\n",
      "tensor([0])\n",
      "tensor([[ 0.9962,  0.3596, -0.2116]], grad_fn=<AddmmBackward0>)\n",
      "Batch 256, Loss: -0.9961636066436768\n",
      "tensor([1])\n",
      "tensor([[ 1.0107,  0.3549, -0.1921]], grad_fn=<AddmmBackward0>)\n",
      "Batch 257, Loss: -0.35494470596313477\n",
      "tensor([0])\n",
      "tensor([[ 0.7317,  0.4473, -0.5671]], grad_fn=<AddmmBackward0>)\n",
      "Batch 258, Loss: -0.7316539287567139\n",
      "tensor([1])\n",
      "tensor([[ 0.9899,  0.3621, -0.2202]], grad_fn=<AddmmBackward0>)\n",
      "Batch 259, Loss: -0.3620849549770355\n",
      "tensor([2])\n",
      "tensor([[ 1.0208,  0.3521, -0.1789]], grad_fn=<AddmmBackward0>)\n",
      "Batch 260, Loss: 0.17888730764389038\n",
      "tensor([1])\n",
      "tensor([[ 1.0288,  0.3495, -0.1681]], grad_fn=<AddmmBackward0>)\n",
      "Batch 261, Loss: -0.3495471477508545\n",
      "tensor([0])\n",
      "tensor([[ 1.0221,  0.3519, -0.1772]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 262, Loss: -1.0221211910247803\n",
      "tensor([2])\n",
      "tensor([[ 0.9865,  0.3638, -0.2252]], grad_fn=<AddmmBackward0>)\n",
      "Batch 263, Loss: 0.22515326738357544\n",
      "tensor([2])\n",
      "tensor([[ 0.9355,  0.3808, -0.2937]], grad_fn=<AddmmBackward0>)\n",
      "Batch 264, Loss: 0.29369956254959106\n",
      "tensor([0])\n",
      "tensor([[ 1.0167,  0.3541, -0.1846]], grad_fn=<AddmmBackward0>)\n",
      "Batch 265, Loss: -1.0167288780212402\n",
      "tensor([2])\n",
      "tensor([[ 0.9424,  0.3787, -0.2845]], grad_fn=<AddmmBackward0>)\n",
      "Batch 266, Loss: 0.2844863533973694\n",
      "tensor([2])\n",
      "tensor([[ 0.9693,  0.3700, -0.2483]], grad_fn=<AddmmBackward0>)\n",
      "Batch 267, Loss: 0.2483016848564148\n",
      "tensor([2])\n",
      "tensor([[ 0.9691,  0.3701, -0.2486]], grad_fn=<AddmmBackward0>)\n",
      "Batch 268, Loss: 0.2486191987991333\n",
      "tensor([1])\n",
      "tensor([[ 1.0186,  0.3539, -0.1820]], grad_fn=<AddmmBackward0>)\n",
      "Batch 269, Loss: -0.3539223372936249\n",
      "tensor([0])\n",
      "tensor([[ 1.0174,  0.3545, -0.1837]], grad_fn=<AddmmBackward0>)\n",
      "Batch 270, Loss: -1.0173683166503906\n",
      "tensor([1])\n",
      "tensor([[ 1.0279,  0.3511, -0.1696]], grad_fn=<AddmmBackward0>)\n",
      "Batch 271, Loss: -0.3511151969432831\n",
      "tensor([0])\n",
      "tensor([[ 0.9679,  0.3710, -0.2502]], grad_fn=<AddmmBackward0>)\n",
      "Batch 272, Loss: -0.9678770303726196\n",
      "tensor([1])\n",
      "tensor([[ 1.0028,  0.3596, -0.2033]], grad_fn=<AddmmBackward0>)\n",
      "Batch 273, Loss: -0.3596276640892029\n",
      "tensor([2])\n",
      "tensor([[ 1.0206,  0.3539, -0.1795]], grad_fn=<AddmmBackward0>)\n",
      "Batch 274, Loss: 0.17946863174438477\n",
      "tensor([2])\n",
      "tensor([[ 1.0365,  0.3488, -0.1580]], grad_fn=<AddmmBackward0>)\n",
      "Batch 275, Loss: 0.15804439783096313\n",
      "tensor([2])\n",
      "tensor([[ 1.0020,  0.3603, -0.2044]], grad_fn=<AddmmBackward0>)\n",
      "Batch 276, Loss: 0.20443987846374512\n",
      "tensor([2])\n",
      "tensor([[ 1.0098,  0.3579, -0.1939]], grad_fn=<AddmmBackward0>)\n",
      "Batch 277, Loss: 0.19389545917510986\n",
      "tensor([1])\n",
      "tensor([[ 1.0138,  0.3566, -0.1883]], grad_fn=<AddmmBackward0>)\n",
      "Batch 278, Loss: -0.3566340208053589\n",
      "tensor([0])\n",
      "tensor([[ 1.0305,  0.3513, -0.1659]], grad_fn=<AddmmBackward0>)\n",
      "Batch 279, Loss: -1.0304887294769287\n",
      "tensor([2])\n",
      "tensor([[ 0.9686,  0.3717, -0.2490]], grad_fn=<AddmmBackward0>)\n",
      "Batch 280, Loss: 0.24898576736450195\n",
      "tensor([0])\n",
      "tensor([[ 1.0230,  0.3540, -0.1759]], grad_fn=<AddmmBackward0>)\n",
      "Batch 281, Loss: -1.0230231285095215\n",
      "tensor([2])\n",
      "tensor([[ 1.0332,  0.3508, -0.1622]], grad_fn=<AddmmBackward0>)\n",
      "Batch 282, Loss: 0.16217899322509766\n",
      "tensor([2])\n",
      "tensor([[ 1.0231,  0.3542, -0.1758]], grad_fn=<AddmmBackward0>)\n",
      "Batch 283, Loss: 0.1757543683052063\n",
      "tensor([0])\n",
      "tensor([[ 1.0334,  0.3509, -0.1618]], grad_fn=<AddmmBackward0>)\n",
      "Batch 284, Loss: -1.0334193706512451\n",
      "tensor([1])\n",
      "tensor([[ 1.0041,  0.3606, -0.2011]], grad_fn=<AddmmBackward0>)\n",
      "Batch 285, Loss: -0.36057448387145996\n",
      "tensor([0])\n",
      "tensor([[ 1.0057,  0.3602, -0.1990]], grad_fn=<AddmmBackward0>)\n",
      "Batch 286, Loss: -1.0056737661361694\n",
      "tensor([1])\n",
      "tensor([[ 1.0120,  0.3582, -0.1905]], grad_fn=<AddmmBackward0>)\n",
      "Batch 287, Loss: -0.35822388529777527\n",
      "tensor([0])\n",
      "tensor([[ 1.0189,  0.3561, -0.1813]], grad_fn=<AddmmBackward0>)\n",
      "Batch 288, Loss: -1.0189170837402344\n",
      "tensor([0])\n",
      "tensor([[ 1.0139,  0.3579, -0.1881]], grad_fn=<AddmmBackward0>)\n",
      "Batch 289, Loss: -1.0138682126998901\n",
      "tensor([2])\n",
      "tensor([[ 0.9772,  0.3700, -0.2375]], grad_fn=<AddmmBackward0>)\n",
      "Batch 290, Loss: 0.23754507303237915\n",
      "tensor([1])\n",
      "tensor([[ 0.9619,  0.3751, -0.2581]], grad_fn=<AddmmBackward0>)\n",
      "Batch 291, Loss: -0.37509071826934814\n",
      "tensor([0])\n",
      "tensor([[ 1.0066,  0.3607, -0.1981]], grad_fn=<AddmmBackward0>)\n",
      "Batch 292, Loss: -1.0065864324569702\n",
      "tensor([2])\n",
      "tensor([[ 1.0304,  0.3530, -0.1663]], grad_fn=<AddmmBackward0>)\n",
      "Batch 293, Loss: 0.1662607192993164\n",
      "tensor([1])\n",
      "tensor([[ 1.0212,  0.3562, -0.1787]], grad_fn=<AddmmBackward0>)\n",
      "Batch 294, Loss: -0.3561546802520752\n",
      "tensor([0])\n",
      "tensor([[ 0.8939,  0.3978, -0.3497]], grad_fn=<AddmmBackward0>)\n",
      "Batch 295, Loss: -0.8938820362091064\n",
      "tensor([0])\n",
      "tensor([[ 1.0386,  0.3508, -0.1554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 296, Loss: -1.0386046171188354\n",
      "tensor([1])\n",
      "tensor([[ 0.9934,  0.3656, -0.2163]], grad_fn=<AddmmBackward0>)\n",
      "Batch 297, Loss: -0.3656303286552429\n",
      "tensor([1])\n",
      "tensor([[ 0.9859,  0.3682, -0.2265]], grad_fn=<AddmmBackward0>)\n",
      "Batch 298, Loss: -0.3682296872138977\n",
      "tensor([0])\n",
      "tensor([[ 1.0080,  0.3612, -0.1969]], grad_fn=<AddmmBackward0>)\n",
      "Batch 299, Loss: -1.0079821348190308\n",
      "tensor([1])\n",
      "tensor([[ 1.0260,  0.3555, -0.1728]], grad_fn=<AddmmBackward0>)\n",
      "Batch 300, Loss: -0.3554966151714325\n",
      "tensor([1])\n",
      "tensor([[ 1.0070,  0.3619, -0.1985]], grad_fn=<AddmmBackward0>)\n",
      "Batch 301, Loss: -0.36188119649887085\n",
      "tensor([1])\n",
      "tensor([[ 0.8641,  0.4085, -0.3904]], grad_fn=<AddmmBackward0>)\n",
      "Batch 302, Loss: -0.4084659814834595\n",
      "tensor([0])\n",
      "tensor([[ 1.0305,  0.3546, -0.1671]], grad_fn=<AddmmBackward0>)\n",
      "Batch 303, Loss: -1.0305187702178955\n",
      "tensor([2])\n",
      "tensor([[ 1.0257,  0.3564, -0.1736]], grad_fn=<AddmmBackward0>)\n",
      "Batch 304, Loss: 0.1736099123954773\n",
      "tensor([2])\n",
      "tensor([[ 0.9832,  0.3704, -0.2308]], grad_fn=<AddmmBackward0>)\n",
      "Batch 305, Loss: 0.23079711198806763\n",
      "tensor([2])\n",
      "tensor([[ 0.9681,  0.3754, -0.2511]], grad_fn=<AddmmBackward0>)\n",
      "Batch 306, Loss: 0.2511388063430786\n",
      "tensor([2])\n",
      "tensor([[ 1.0184,  0.3593, -0.1836]], grad_fn=<AddmmBackward0>)\n",
      "Batch 307, Loss: 0.18359333276748657\n",
      "tensor([0])\n",
      "tensor([[ 1.0393,  0.3526, -0.1556]], grad_fn=<AddmmBackward0>)\n",
      "Batch 308, Loss: -1.0392696857452393\n",
      "tensor([1])\n",
      "tensor([[ 1.0267,  0.3568, -0.1724]], grad_fn=<AddmmBackward0>)\n",
      "Batch 309, Loss: -0.35682302713394165\n",
      "tensor([2])\n",
      "tensor([[ 1.0397,  0.3528, -0.1551]], grad_fn=<AddmmBackward0>)\n",
      "Batch 310, Loss: 0.15505099296569824\n",
      "tensor([0])\n",
      "tensor([[ 1.0397,  0.3529, -0.1550]], grad_fn=<AddmmBackward0>)\n",
      "Batch 311, Loss: -1.039712905883789\n",
      "tensor([0])\n",
      "tensor([[ 0.9870,  0.3701, -0.2258]], grad_fn=<AddmmBackward0>)\n",
      "Batch 312, Loss: -0.9869915843009949\n",
      "tensor([2])\n",
      "tensor([[ 1.0177,  0.3603, -0.1846]], grad_fn=<AddmmBackward0>)\n",
      "Batch 313, Loss: 0.18457430601119995\n",
      "tensor([1])\n",
      "tensor([[ 1.0282,  0.3570, -0.1705]], grad_fn=<AddmmBackward0>)\n",
      "Batch 314, Loss: -0.3569920063018799\n",
      "tensor([0])\n",
      "tensor([[ 1.0258,  0.3579, -0.1738]], grad_fn=<AddmmBackward0>)\n",
      "Batch 315, Loss: -1.0257799625396729\n",
      "tensor([2])\n",
      "tensor([[ 1.0305,  0.3565, -0.1676]], grad_fn=<AddmmBackward0>)\n",
      "Batch 316, Loss: 0.16757512092590332\n",
      "tensor([1])\n",
      "tensor([[ 1.0412,  0.3532, -0.1533]], grad_fn=<AddmmBackward0>)\n",
      "Batch 317, Loss: -0.3532211184501648\n",
      "tensor([0])\n",
      "tensor([[ 1.0234,  0.3591, -0.1772]], grad_fn=<AddmmBackward0>)\n",
      "Batch 318, Loss: -1.0233649015426636\n",
      "tensor([1])\n",
      "tensor([[ 0.9942,  0.3687, -0.2164]], grad_fn=<AddmmBackward0>)\n",
      "Batch 319, Loss: -0.36865997314453125\n",
      "tensor([1])\n",
      "tensor([[ 1.0291,  0.3576, -0.1696]], grad_fn=<AddmmBackward0>)\n",
      "Batch 320, Loss: -0.3575688600540161\n",
      "tensor([1])\n",
      "tensor([[ 0.8988,  0.3997, -0.3445]], grad_fn=<AddmmBackward0>)\n",
      "Batch 321, Loss: -0.39970606565475464\n",
      "tensor([1])\n",
      "tensor([[ 0.9665,  0.3781, -0.2537]], grad_fn=<AddmmBackward0>)\n",
      "Batch 322, Loss: -0.3780783414840698\n",
      "tensor([0])\n",
      "tensor([[ 0.7250,  0.4560, -0.5777]], grad_fn=<AddmmBackward0>)\n",
      "Batch 323, Loss: -0.7249739170074463\n",
      "tensor([2])\n",
      "tensor([[ 1.0276,  0.3588, -0.1719]], grad_fn=<AddmmBackward0>)\n",
      "Batch 324, Loss: 0.1718572974205017\n",
      "tensor([1])\n",
      "tensor([[ 1.0204,  0.3613, -0.1815]], grad_fn=<AddmmBackward0>)\n",
      "Batch 325, Loss: -0.3612869679927826\n",
      "tensor([2])\n",
      "tensor([[ 1.0192,  0.3619, -0.1832]], grad_fn=<AddmmBackward0>)\n",
      "Batch 326, Loss: 0.18321514129638672\n",
      "tensor([1])\n",
      "tensor([[ 0.7835,  0.4377, -0.4993]], grad_fn=<AddmmBackward0>)\n",
      "Batch 327, Loss: -0.43769633769989014\n",
      "tensor([0])\n",
      "tensor([[ 1.0364,  0.3567, -0.1601]], grad_fn=<AddmmBackward0>)\n",
      "Batch 328, Loss: -1.0364197492599487\n",
      "tensor([0])\n",
      "tensor([[ 1.0402,  0.3556, -0.1550]], grad_fn=<AddmmBackward0>)\n",
      "Batch 329, Loss: -1.0402405261993408\n",
      "tensor([0])\n",
      "tensor([[ 0.9412,  0.3876, -0.2879]], grad_fn=<AddmmBackward0>)\n",
      "Batch 330, Loss: -0.9411686658859253\n",
      "tensor([0])\n",
      "tensor([[ 1.0008,  0.3686, -0.2081]], grad_fn=<AddmmBackward0>)\n",
      "Batch 331, Loss: -1.000808596611023\n",
      "tensor([1])\n",
      "tensor([[ 0.9151,  0.3962, -0.3232]], grad_fn=<AddmmBackward0>)\n",
      "Batch 332, Loss: -0.3962075114250183\n",
      "tensor([1])\n",
      "tensor([[ 1.0417,  0.3558, -0.1535]], grad_fn=<AddmmBackward0>)\n",
      "Batch 333, Loss: -0.3558332324028015\n",
      "tensor([2])\n",
      "tensor([[ 1.0385,  0.3571, -0.1580]], grad_fn=<AddmmBackward0>)\n",
      "Batch 334, Loss: 0.15795868635177612\n",
      "tensor([1])\n",
      "tensor([[ 1.0442,  0.3554, -0.1503]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 335, Loss: -0.3553840219974518\n",
      "tensor([1])\n",
      "tensor([[ 0.9154,  0.3967, -0.3230]], grad_fn=<AddmmBackward0>)\n",
      "Batch 336, Loss: -0.3967335820198059\n",
      "tensor([0])\n",
      "tensor([[ 1.0327,  0.3594, -0.1658]], grad_fn=<AddmmBackward0>)\n",
      "Batch 337, Loss: -1.032741904258728\n",
      "tensor([2])\n",
      "tensor([[ 1.0280,  0.3611, -0.1723]], grad_fn=<AddmmBackward0>)\n",
      "Batch 338, Loss: 0.17226195335388184\n",
      "tensor([2])\n",
      "tensor([[ 1.0421,  0.3568, -0.1534]], grad_fn=<AddmmBackward0>)\n",
      "Batch 339, Loss: 0.15339475870132446\n",
      "tensor([2])\n",
      "tensor([[ 0.7790,  0.4409, -0.5061]], grad_fn=<AddmmBackward0>)\n",
      "Batch 340, Loss: 0.5061118602752686\n",
      "tensor([1])\n",
      "tensor([[ 1.0419,  0.3572, -0.1537]], grad_fn=<AddmmBackward0>)\n",
      "Batch 341, Loss: -0.3571706712245941\n",
      "tensor([1])\n",
      "tensor([[ 1.0227,  0.3635, -0.1794]], grad_fn=<AddmmBackward0>)\n",
      "Batch 342, Loss: -0.36345216631889343\n",
      "tensor([0])\n",
      "tensor([[ 0.9880,  0.3747, -0.2259]], grad_fn=<AddmmBackward0>)\n",
      "Batch 343, Loss: -0.9880368709564209\n",
      "tensor([1])\n",
      "tensor([[ 1.0428,  0.3574, -0.1524]], grad_fn=<AddmmBackward0>)\n",
      "Batch 344, Loss: -0.3573783040046692\n",
      "tensor([1])\n",
      "tensor([[ 1.0443,  0.3571, -0.1505]], grad_fn=<AddmmBackward0>)\n",
      "Batch 345, Loss: -0.35710346698760986\n",
      "tensor([0])\n",
      "tensor([[ 1.0417,  0.3581, -0.1540]], grad_fn=<AddmmBackward0>)\n",
      "Batch 346, Loss: -1.041689395904541\n",
      "tensor([0])\n",
      "tensor([[ 1.0434,  0.3578, -0.1518]], grad_fn=<AddmmBackward0>)\n",
      "Batch 347, Loss: -1.043382167816162\n",
      "tensor([0])\n",
      "tensor([[ 1.0175,  0.3662, -0.1867]], grad_fn=<AddmmBackward0>)\n",
      "Batch 348, Loss: -1.0174801349639893\n",
      "tensor([0])\n",
      "tensor([[ 0.8639,  0.4151, -0.3926]], grad_fn=<AddmmBackward0>)\n",
      "Batch 349, Loss: -0.863895833492279\n",
      "tensor([0])\n",
      "tensor([[ 1.0437,  0.3582, -0.1518]], grad_fn=<AddmmBackward0>)\n",
      "Batch 350, Loss: -1.0437170267105103\n",
      "tensor([2])\n",
      "tensor([[ 0.9957,  0.3736, -0.2163]], grad_fn=<AddmmBackward0>)\n",
      "Batch 351, Loss: 0.21633392572402954\n",
      "tensor([0])\n",
      "tensor([[ 0.9969,  0.3734, -0.2149]], grad_fn=<AddmmBackward0>)\n",
      "Batch 352, Loss: -0.9968825578689575\n",
      "tensor([2])\n",
      "tensor([[ 1.0380,  0.3605, -0.1599]], grad_fn=<AddmmBackward0>)\n",
      "Batch 353, Loss: 0.15992021560668945\n",
      "tensor([0])\n",
      "tensor([[ 0.9728,  0.3813, -0.2475]], grad_fn=<AddmmBackward0>)\n",
      "Batch 354, Loss: -0.9727503061294556\n",
      "tensor([1])\n",
      "tensor([[ 1.0442,  0.3588, -0.1519]], grad_fn=<AddmmBackward0>)\n",
      "Batch 355, Loss: -0.3587808907032013\n",
      "tensor([1])\n",
      "tensor([[ 0.9902,  0.3760, -0.2243]], grad_fn=<AddmmBackward0>)\n",
      "Batch 356, Loss: -0.37600177526474\n",
      "tensor([2])\n",
      "tensor([[ 0.9964,  0.3742, -0.2161]], grad_fn=<AddmmBackward0>)\n",
      "Batch 357, Loss: 0.2161414623260498\n",
      "tensor([1])\n",
      "tensor([[ 0.9829,  0.3786, -0.2344]], grad_fn=<AddmmBackward0>)\n",
      "Batch 358, Loss: -0.37863364815711975\n",
      "tensor([2])\n",
      "tensor([[ 1.0486,  0.3580, -0.1464]], grad_fn=<AddmmBackward0>)\n",
      "Batch 359, Loss: 0.14638984203338623\n",
      "tensor([1])\n",
      "tensor([[ 1.0335,  0.3629, -0.1667]], grad_fn=<AddmmBackward0>)\n",
      "Batch 360, Loss: -0.3629448711872101\n",
      "tensor([2])\n",
      "tensor([[ 1.0186,  0.3678, -0.1866]], grad_fn=<AddmmBackward0>)\n",
      "Batch 361, Loss: 0.18663054704666138\n",
      "tensor([2])\n",
      "tensor([[ 1.0248,  0.3660, -0.1784]], grad_fn=<AddmmBackward0>)\n",
      "Batch 362, Loss: 0.17835533618927002\n",
      "tensor([0])\n",
      "tensor([[ 1.0345,  0.3631, -0.1653]], grad_fn=<AddmmBackward0>)\n",
      "Batch 363, Loss: -1.0345274209976196\n",
      "tensor([2])\n",
      "tensor([[ 1.0438,  0.3603, -0.1529]], grad_fn=<AddmmBackward0>)\n",
      "Batch 364, Loss: 0.15286362171173096\n",
      "tensor([1])\n",
      "tensor([[ 0.9772,  0.3814, -0.2420]], grad_fn=<AddmmBackward0>)\n",
      "Batch 365, Loss: -0.3814004957675934\n",
      "tensor([1])\n",
      "tensor([[ 0.9729,  0.3829, -0.2477]], grad_fn=<AddmmBackward0>)\n",
      "Batch 366, Loss: -0.38288527727127075\n",
      "tensor([2])\n",
      "tensor([[ 1.0473,  0.3596, -0.1481]], grad_fn=<AddmmBackward0>)\n",
      "Batch 367, Loss: 0.14812248945236206\n",
      "tensor([0])\n",
      "tensor([[ 0.9571,  0.3882, -0.2689]], grad_fn=<AddmmBackward0>)\n",
      "Batch 368, Loss: -0.9570730924606323\n",
      "tensor([2])\n",
      "tensor([[ 1.0082,  0.3722, -0.2004]], grad_fn=<AddmmBackward0>)\n",
      "Batch 369, Loss: 0.200439453125\n",
      "tensor([1])\n",
      "tensor([[ 0.9993,  0.3751, -0.2123]], grad_fn=<AddmmBackward0>)\n",
      "Batch 370, Loss: -0.37511059641838074\n",
      "tensor([0])\n",
      "tensor([[ 1.0065,  0.3730, -0.2026]], grad_fn=<AddmmBackward0>)\n",
      "Batch 371, Loss: -1.006518840789795\n",
      "tensor([1])\n",
      "tensor([[ 1.0435,  0.3615, -0.1531]], grad_fn=<AddmmBackward0>)\n",
      "Batch 372, Loss: -0.3614990711212158\n",
      "tensor([1])\n",
      "tensor([[ 1.0483,  0.3601, -0.1466]], grad_fn=<AddmmBackward0>)\n",
      "Batch 373, Loss: -0.36014777421951294\n",
      "tensor([1])\n",
      "tensor([[ 1.0366,  0.3640, -0.1623]], grad_fn=<AddmmBackward0>)\n",
      "Batch 374, Loss: -0.36401718854904175\n",
      "tensor([2])\n",
      "tensor([[ 1.0488,  0.3604, -0.1460]], grad_fn=<AddmmBackward0>)\n",
      "Batch 375, Loss: 0.14595448970794678\n",
      "tensor([1])\n",
      "tensor([[ 0.9641,  0.3871, -0.2593]], grad_fn=<AddmmBackward0>)\n",
      "Batch 376, Loss: -0.38710442185401917\n",
      "tensor([1])\n",
      "tensor([[ 0.9857,  0.3805, -0.2304]], grad_fn=<AddmmBackward0>)\n",
      "Batch 377, Loss: -0.38052549958229065\n",
      "tensor([0])\n",
      "tensor([[ 1.0165,  0.3711, -0.1892]], grad_fn=<AddmmBackward0>)\n",
      "Batch 378, Loss: -1.0164532661437988\n",
      "tensor([0])\n",
      "tensor([[ 1.0443,  0.3625, -0.1519]], grad_fn=<AddmmBackward0>)\n",
      "Batch 379, Loss: -1.0443100929260254\n",
      "tensor([0])\n",
      "tensor([[ 1.0483,  0.3615, -0.1466]], grad_fn=<AddmmBackward0>)\n",
      "Batch 380, Loss: -1.048306941986084\n",
      "tensor([1])\n",
      "tensor([[ 1.0500,  0.3611, -0.1445]], grad_fn=<AddmmBackward0>)\n",
      "Batch 381, Loss: -0.3611487150192261\n",
      "tensor([2])\n",
      "tensor([[ 1.0064,  0.3750, -0.2030]], grad_fn=<AddmmBackward0>)\n",
      "Batch 382, Loss: 0.2029860019683838\n",
      "tensor([1])\n",
      "tensor([[ 1.0439,  0.3634, -0.1528]], grad_fn=<AddmmBackward0>)\n",
      "Batch 383, Loss: -0.3634190559387207\n",
      "tensor([1])\n",
      "tensor([[ 1.0279,  0.3686, -0.1743]], grad_fn=<AddmmBackward0>)\n",
      "Batch 384, Loss: -0.3686012625694275\n",
      "tensor([1])\n",
      "tensor([[ 0.9870,  0.3816, -0.2291]], grad_fn=<AddmmBackward0>)\n",
      "Batch 385, Loss: -0.3815702199935913\n",
      "tensor([2])\n",
      "tensor([[ 0.9403,  0.3963, -0.2916]], grad_fn=<AddmmBackward0>)\n",
      "Batch 386, Loss: 0.29157036542892456\n",
      "tensor([2])\n",
      "tensor([[ 0.9864,  0.3821, -0.2299]], grad_fn=<AddmmBackward0>)\n",
      "Batch 387, Loss: 0.22991782426834106\n",
      "tensor([0])\n",
      "tensor([[ 1.0298,  0.3688, -0.1718]], grad_fn=<AddmmBackward0>)\n",
      "Batch 388, Loss: -1.0297707319259644\n",
      "tensor([2])\n",
      "tensor([[ 0.8402,  0.4280, -0.4256]], grad_fn=<AddmmBackward0>)\n",
      "Batch 389, Loss: 0.4256083071231842\n",
      "tensor([1])\n",
      "tensor([[ 1.0505,  0.3627, -0.1441]], grad_fn=<AddmmBackward0>)\n",
      "Batch 390, Loss: -0.36268389225006104\n",
      "tensor([1])\n",
      "tensor([[ 1.0479,  0.3637, -0.1476]], grad_fn=<AddmmBackward0>)\n",
      "Batch 391, Loss: -0.36365413665771484\n",
      "tensor([0])\n",
      "tensor([[ 0.9790,  0.3853, -0.2398]], grad_fn=<AddmmBackward0>)\n",
      "Batch 392, Loss: -0.9789596796035767\n",
      "tensor([1])\n",
      "tensor([[ 1.0414,  0.3660, -0.1563]], grad_fn=<AddmmBackward0>)\n",
      "Batch 393, Loss: -0.3660440742969513\n",
      "tensor([0])\n",
      "tensor([[ 1.0413,  0.3663, -0.1565]], grad_fn=<AddmmBackward0>)\n",
      "Batch 394, Loss: -1.0412793159484863\n",
      "tensor([0])\n",
      "tensor([[ 1.0415,  0.3664, -0.1563]], grad_fn=<AddmmBackward0>)\n",
      "Batch 395, Loss: -1.041450023651123\n",
      "tensor([2])\n",
      "tensor([[ 1.0033,  0.3784, -0.2074]], grad_fn=<AddmmBackward0>)\n",
      "Batch 396, Loss: 0.2074463963508606\n",
      "tensor([2])\n",
      "tensor([[ 1.0479,  0.3648, -0.1479]], grad_fn=<AddmmBackward0>)\n",
      "Batch 397, Loss: 0.14787638187408447\n",
      "tensor([1])\n",
      "tensor([[ 1.0356,  0.3687, -0.1643]], grad_fn=<AddmmBackward0>)\n",
      "Batch 398, Loss: -0.3687049150466919\n",
      "tensor([2])\n",
      "tensor([[ 0.9685,  0.3896, -0.2541]], grad_fn=<AddmmBackward0>)\n",
      "Batch 399, Loss: 0.254133939743042\n",
      "tensor([1])\n",
      "tensor([[ 1.0434,  0.3666, -0.1539]], grad_fn=<AddmmBackward0>)\n",
      "Batch 400, Loss: -0.36659932136535645\n",
      "tensor([0])\n",
      "tensor([[ 1.0441,  0.3666, -0.1530]], grad_fn=<AddmmBackward0>)\n",
      "Batch 401, Loss: -1.044050931930542\n",
      "tensor([2])\n",
      "tensor([[ 1.0126,  0.3765, -0.1952]], grad_fn=<AddmmBackward0>)\n",
      "Batch 402, Loss: 0.19520998001098633\n",
      "tensor([2])\n",
      "tensor([[ 0.9627,  0.3920, -0.2620]], grad_fn=<AddmmBackward0>)\n",
      "Batch 403, Loss: 0.2619873285293579\n",
      "tensor([1])\n",
      "tensor([[ 1.0354,  0.3697, -0.1647]], grad_fn=<AddmmBackward0>)\n",
      "Batch 404, Loss: -0.3696860074996948\n",
      "tensor([0])\n",
      "tensor([[ 1.0307,  0.3713, -0.1709]], grad_fn=<AddmmBackward0>)\n",
      "Batch 405, Loss: -1.0306870937347412\n",
      "tensor([0])\n",
      "tensor([[ 1.0471,  0.3663, -0.1489]], grad_fn=<AddmmBackward0>)\n",
      "Batch 406, Loss: -1.0471274852752686\n",
      "tensor([0])\n",
      "tensor([[ 1.0479,  0.3663, -0.1480]], grad_fn=<AddmmBackward0>)\n",
      "Batch 407, Loss: -1.0478575229644775\n",
      "tensor([0])\n",
      "tensor([[ 1.0269,  0.3729, -0.1761]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 408, Loss: -1.0269205570220947\n",
      "tensor([0])\n",
      "tensor([[ 1.0233,  0.3741, -0.1811]], grad_fn=<AddmmBackward0>)\n",
      "Batch 409, Loss: -1.0232744216918945\n",
      "tensor([1])\n",
      "tensor([[ 1.0520,  0.3654, -0.1428]], grad_fn=<AddmmBackward0>)\n",
      "Batch 410, Loss: -0.3653959631919861\n",
      "tensor([1])\n",
      "tensor([[ 1.0547,  0.3647, -0.1394]], grad_fn=<AddmmBackward0>)\n",
      "Batch 411, Loss: -0.3647315502166748\n",
      "tensor([2])\n",
      "tensor([[ 0.9820,  0.3873, -0.2368]], grad_fn=<AddmmBackward0>)\n",
      "Batch 412, Loss: 0.23675274848937988\n",
      "tensor([0])\n",
      "tensor([[ 1.0113,  0.3784, -0.1976]], grad_fn=<AddmmBackward0>)\n",
      "Batch 413, Loss: -1.0113459825515747\n",
      "tensor([2])\n",
      "tensor([[ 1.0542,  0.3653, -0.1403]], grad_fn=<AddmmBackward0>)\n",
      "Batch 414, Loss: 0.14032888412475586\n",
      "tensor([0])\n",
      "tensor([[ 1.0025,  0.3814, -0.2096]], grad_fn=<AddmmBackward0>)\n",
      "Batch 415, Loss: -1.0025368928909302\n",
      "tensor([1])\n",
      "tensor([[ 1.0403,  0.3699, -0.1592]], grad_fn=<AddmmBackward0>)\n",
      "Batch 416, Loss: -0.3698888123035431\n",
      "tensor([0])\n",
      "tensor([[ 1.0371,  0.3710, -0.1636]], grad_fn=<AddmmBackward0>)\n",
      "Batch 417, Loss: -1.0370674133300781\n",
      "tensor([2])\n",
      "tensor([[ 1.0545,  0.3658, -0.1405]], grad_fn=<AddmmBackward0>)\n",
      "Batch 418, Loss: 0.14045870304107666\n",
      "tensor([0])\n",
      "tensor([[ 1.0301,  0.3734, -0.1731]], grad_fn=<AddmmBackward0>)\n",
      "Batch 419, Loss: -1.030116319656372\n",
      "tensor([0])\n",
      "tensor([[ 1.0149,  0.3782, -0.1936]], grad_fn=<AddmmBackward0>)\n",
      "Batch 420, Loss: -1.0148875713348389\n",
      "tensor([1])\n",
      "tensor([[ 0.9818,  0.3885, -0.2380]], grad_fn=<AddmmBackward0>)\n",
      "Batch 421, Loss: -0.38852769136428833\n",
      "tensor([1])\n",
      "tensor([[ 0.9526,  0.3976, -0.2771]], grad_fn=<AddmmBackward0>)\n",
      "Batch 422, Loss: -0.3976069986820221\n",
      "tensor([1])\n",
      "tensor([[ 0.9708,  0.3922, -0.2530]], grad_fn=<AddmmBackward0>)\n",
      "Batch 423, Loss: -0.3921852707862854\n",
      "tensor([0])\n",
      "tensor([[ 1.0488,  0.3685, -0.1488]], grad_fn=<AddmmBackward0>)\n",
      "Batch 424, Loss: -1.0487626791000366\n",
      "tensor([2])\n",
      "tensor([[ 1.0251,  0.3759, -0.1806]], grad_fn=<AddmmBackward0>)\n",
      "Batch 425, Loss: 0.1805940866470337\n",
      "tensor([1])\n",
      "tensor([[ 0.9648,  0.3945, -0.2613]], grad_fn=<AddmmBackward0>)\n",
      "Batch 426, Loss: -0.3944740891456604\n",
      "tensor([1])\n",
      "tensor([[ 1.0519,  0.3680, -0.1449]], grad_fn=<AddmmBackward0>)\n",
      "Batch 427, Loss: -0.36799269914627075\n",
      "tensor([2])\n",
      "tensor([[ 1.0338,  0.3737, -0.1693]], grad_fn=<AddmmBackward0>)\n",
      "Batch 428, Loss: 0.16926461458206177\n",
      "tensor([0])\n",
      "tensor([[ 1.0545,  0.3676, -0.1416]], grad_fn=<AddmmBackward0>)\n",
      "Batch 429, Loss: -1.054531216621399\n",
      "tensor([1])\n",
      "tensor([[ 1.0166,  0.3793, -0.1923]], grad_fn=<AddmmBackward0>)\n",
      "Batch 430, Loss: -0.37929001450538635\n",
      "tensor([0])\n",
      "tensor([[ 1.0333,  0.3744, -0.1700]], grad_fn=<AddmmBackward0>)\n",
      "Batch 431, Loss: -1.0333447456359863\n",
      "tensor([1])\n",
      "tensor([[ 1.0548,  0.3680, -0.1415]], grad_fn=<AddmmBackward0>)\n",
      "Batch 432, Loss: -0.3679899573326111\n",
      "tensor([0])\n",
      "tensor([[ 1.0362,  0.3738, -0.1664]], grad_fn=<AddmmBackward0>)\n",
      "Batch 433, Loss: -1.0362143516540527\n",
      "tensor([0])\n",
      "tensor([[ 1.0507,  0.3696, -0.1472]], grad_fn=<AddmmBackward0>)\n",
      "Batch 434, Loss: -1.050672173500061\n",
      "tensor([2])\n",
      "tensor([[ 0.9802,  0.3912, -0.2414]], grad_fn=<AddmmBackward0>)\n",
      "Batch 435, Loss: 0.2414214015007019\n",
      "tensor([2])\n",
      "tensor([[ 0.9063,  0.4139, -0.3402]], grad_fn=<AddmmBackward0>)\n",
      "Batch 436, Loss: 0.3402256369590759\n",
      "tensor([2])\n",
      "tensor([[ 0.9941,  0.3873, -0.2232]], grad_fn=<AddmmBackward0>)\n",
      "Batch 437, Loss: 0.22315043210983276\n",
      "tensor([2])\n",
      "tensor([[ 0.9768,  0.3927, -0.2462]], grad_fn=<AddmmBackward0>)\n",
      "Batch 438, Loss: 0.24617373943328857\n",
      "tensor([1])\n",
      "tensor([[ 1.0501,  0.3705, -0.1483]], grad_fn=<AddmmBackward0>)\n",
      "Batch 439, Loss: -0.37048280239105225\n",
      "tensor([2])\n",
      "tensor([[ 1.0291,  0.3770, -0.1764]], grad_fn=<AddmmBackward0>)\n",
      "Batch 440, Loss: 0.17639648914337158\n",
      "tensor([1])\n",
      "tensor([[ 1.0115,  0.3825, -0.1999]], grad_fn=<AddmmBackward0>)\n",
      "Batch 441, Loss: -0.38247984647750854\n",
      "tensor([0])\n",
      "tensor([[ 1.0500,  0.3709, -0.1484]], grad_fn=<AddmmBackward0>)\n",
      "Batch 442, Loss: -1.0500390529632568\n",
      "tensor([1])\n",
      "tensor([[ 1.0475,  0.3718, -0.1517]], grad_fn=<AddmmBackward0>)\n",
      "Batch 443, Loss: -0.3718021512031555\n",
      "tensor([2])\n",
      "tensor([[ 0.9626,  0.3977, -0.2652]], grad_fn=<AddmmBackward0>)\n",
      "Batch 444, Loss: 0.265159547328949\n",
      "tensor([0])\n",
      "tensor([[ 0.9719,  0.3950, -0.2527]], grad_fn=<AddmmBackward0>)\n",
      "Batch 445, Loss: -0.9719047546386719\n",
      "tensor([2])\n",
      "tensor([[ 0.9305,  0.4077, -0.3079]], grad_fn=<AddmmBackward0>)\n",
      "Batch 446, Loss: 0.3079492449760437\n",
      "tensor([2])\n",
      "tensor([[ 0.9362,  0.4061, -0.3004]], grad_fn=<AddmmBackward0>)\n",
      "Batch 447, Loss: 0.30040866136550903\n",
      "tensor([0])\n",
      "tensor([[ 1.0534,  0.3707, -0.1438]], grad_fn=<AddmmBackward0>)\n",
      "Batch 448, Loss: -1.0534493923187256\n",
      "tensor([2])\n",
      "tensor([[ 1.0528,  0.3710, -0.1446]], grad_fn=<AddmmBackward0>)\n",
      "Batch 449, Loss: 0.14461392164230347\n",
      "tensor([1])\n",
      "tensor([[ 1.0243,  0.3797, -0.1826]], grad_fn=<AddmmBackward0>)\n",
      "Batch 450, Loss: -0.3797059655189514\n",
      "tensor([2])\n",
      "tensor([[ 1.0601,  0.3690, -0.1349]], grad_fn=<AddmmBackward0>)\n",
      "Batch 451, Loss: 0.13492244482040405\n",
      "tensor([0])\n",
      "tensor([[ 1.0467,  0.3732, -0.1527]], grad_fn=<AddmmBackward0>)\n",
      "Batch 452, Loss: -1.046699047088623\n",
      "tensor([0])\n",
      "tensor([[ 1.0197,  0.3814, -0.1887]], grad_fn=<AddmmBackward0>)\n",
      "Batch 453, Loss: -1.0197421312332153\n",
      "tensor([1])\n",
      "tensor([[ 1.0432,  0.3745, -0.1575]], grad_fn=<AddmmBackward0>)\n",
      "Batch 454, Loss: -0.374459445476532\n",
      "tensor([1])\n",
      "tensor([[ 1.0500,  0.3725, -0.1483]], grad_fn=<AddmmBackward0>)\n",
      "Batch 455, Loss: -0.372517853975296\n",
      "tensor([0])\n",
      "tensor([[ 1.0454,  0.3741, -0.1546]], grad_fn=<AddmmBackward0>)\n",
      "Batch 456, Loss: -1.045370101928711\n",
      "tensor([1])\n",
      "tensor([[ 1.0500,  0.3728, -0.1485]], grad_fn=<AddmmBackward0>)\n",
      "Batch 457, Loss: -0.3728204369544983\n",
      "tensor([2])\n",
      "tensor([[ 1.0540,  0.3718, -0.1432]], grad_fn=<AddmmBackward0>)\n",
      "Batch 458, Loss: 0.14323896169662476\n",
      "tensor([0])\n",
      "tensor([[ 1.0281,  0.3798, -0.1778]], grad_fn=<AddmmBackward0>)\n",
      "Batch 459, Loss: -1.0280832052230835\n",
      "tensor([2])\n",
      "tensor([[ 1.0602,  0.3702, -0.1350]], grad_fn=<AddmmBackward0>)\n",
      "Batch 460, Loss: 0.13498258590698242\n",
      "tensor([2])\n",
      "tensor([[ 0.9582,  0.4011, -0.2713]], grad_fn=<AddmmBackward0>)\n",
      "Batch 461, Loss: 0.2712681293487549\n",
      "tensor([2])\n",
      "tensor([[ 0.9801,  0.3946, -0.2420]], grad_fn=<AddmmBackward0>)\n",
      "Batch 462, Loss: 0.2419513463973999\n",
      "tensor([0])\n",
      "tensor([[ 1.0506,  0.3735, -0.1478]], grad_fn=<AddmmBackward0>)\n",
      "Batch 463, Loss: -1.050593376159668\n",
      "tensor([0])\n",
      "tensor([[ 1.0117,  0.3853, -0.1997]], grad_fn=<AddmmBackward0>)\n",
      "Batch 464, Loss: -1.0117348432540894\n",
      "tensor([0])\n",
      "tensor([[ 1.0115,  0.3854, -0.2001]], grad_fn=<AddmmBackward0>)\n",
      "Batch 465, Loss: -1.0114858150482178\n",
      "tensor([1])\n",
      "tensor([[ 1.0570,  0.3719, -0.1395]], grad_fn=<AddmmBackward0>)\n",
      "Batch 466, Loss: -0.37185579538345337\n",
      "tensor([0])\n",
      "tensor([[ 1.0281,  0.3807, -0.1780]], grad_fn=<AddmmBackward0>)\n",
      "Batch 467, Loss: -1.0281175374984741\n",
      "tensor([1])\n",
      "tensor([[ 1.0493,  0.3744, -0.1498]], grad_fn=<AddmmBackward0>)\n",
      "Batch 468, Loss: -0.37440693378448486\n",
      "tensor([1])\n",
      "tensor([[ 1.0555,  0.3727, -0.1418]], grad_fn=<AddmmBackward0>)\n",
      "Batch 469, Loss: -0.37270721793174744\n",
      "tensor([2])\n",
      "tensor([[ 1.0539,  0.3734, -0.1440]], grad_fn=<AddmmBackward0>)\n",
      "Batch 470, Loss: 0.1439657211303711\n",
      "tensor([2])\n",
      "tensor([[ 1.0533,  0.3737, -0.1448]], grad_fn=<AddmmBackward0>)\n",
      "Batch 471, Loss: 0.14483988285064697\n",
      "tensor([0])\n",
      "tensor([[ 1.0113,  0.3864, -0.2009]], grad_fn=<AddmmBackward0>)\n",
      "Batch 472, Loss: -1.0112528800964355\n",
      "tensor([1])\n",
      "tensor([[ 1.0537,  0.3738, -0.1444]], grad_fn=<AddmmBackward0>)\n",
      "Batch 473, Loss: -0.37383490800857544\n",
      "tensor([1])\n",
      "tensor([[ 1.0454,  0.3765, -0.1554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 474, Loss: -0.3764519691467285\n",
      "tensor([0])\n",
      "tensor([[ 1.0630,  0.3713, -0.1319]], grad_fn=<AddmmBackward0>)\n",
      "Batch 475, Loss: -1.063044786453247\n",
      "tensor([0])\n",
      "tensor([[ 1.0077,  0.3881, -0.2058]], grad_fn=<AddmmBackward0>)\n",
      "Batch 476, Loss: -1.007738471031189\n",
      "tensor([1])\n",
      "tensor([[ 1.0628,  0.3717, -0.1324]], grad_fn=<AddmmBackward0>)\n",
      "Batch 477, Loss: -0.37173059582710266\n",
      "tensor([2])\n",
      "tensor([[ 0.9997,  0.3908, -0.2167]], grad_fn=<AddmmBackward0>)\n",
      "Batch 478, Loss: 0.2166907787322998\n",
      "tensor([2])\n",
      "tensor([[ 1.0507,  0.3757, -0.1488]], grad_fn=<AddmmBackward0>)\n",
      "Batch 479, Loss: 0.148759663105011\n",
      "tensor([1])\n",
      "tensor([[ 0.9782,  0.3975, -0.2456]], grad_fn=<AddmmBackward0>)\n",
      "Batch 480, Loss: -0.39751458168029785\n",
      "tensor([1])\n",
      "tensor([[ 1.0400,  0.3792, -0.1631]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 481, Loss: -0.3791886568069458\n",
      "tensor([2])\n",
      "tensor([[ 1.0626,  0.3726, -0.1329]], grad_fn=<AddmmBackward0>)\n",
      "Batch 482, Loss: 0.13293182849884033\n",
      "tensor([1])\n",
      "tensor([[ 1.0363,  0.3806, -0.1681]], grad_fn=<AddmmBackward0>)\n",
      "Batch 483, Loss: -0.380633682012558\n",
      "tensor([2])\n",
      "tensor([[ 1.0517,  0.3762, -0.1475]], grad_fn=<AddmmBackward0>)\n",
      "Batch 484, Loss: 0.14745253324508667\n",
      "tensor([0])\n",
      "tensor([[ 0.9795,  0.3979, -0.2437]], grad_fn=<AddmmBackward0>)\n",
      "Batch 485, Loss: -0.979540228843689\n",
      "tensor([1])\n",
      "tensor([[ 1.0338,  0.3818, -0.1713]], grad_fn=<AddmmBackward0>)\n",
      "Batch 486, Loss: -0.3818414807319641\n",
      "tensor([2])\n",
      "tensor([[ 1.0411,  0.3799, -0.1616]], grad_fn=<AddmmBackward0>)\n",
      "Batch 487, Loss: 0.16160106658935547\n",
      "tensor([2])\n",
      "tensor([[ 1.0629,  0.3735, -0.1325]], grad_fn=<AddmmBackward0>)\n",
      "Batch 488, Loss: 0.1324940323829651\n",
      "tensor([1])\n",
      "tensor([[ 1.0463,  0.3786, -0.1545]], grad_fn=<AddmmBackward0>)\n",
      "Batch 489, Loss: -0.3785819709300995\n",
      "tensor([0])\n",
      "tensor([[ 1.0462,  0.3788, -0.1546]], grad_fn=<AddmmBackward0>)\n",
      "Batch 490, Loss: -1.0461695194244385\n",
      "tensor([0])\n",
      "tensor([[ 1.0503,  0.3777, -0.1491]], grad_fn=<AddmmBackward0>)\n",
      "Batch 491, Loss: -1.05031156539917\n",
      "tensor([0])\n",
      "tensor([[ 1.0398,  0.3810, -0.1632]], grad_fn=<AddmmBackward0>)\n",
      "Batch 492, Loss: -1.0398039817810059\n",
      "tensor([0])\n",
      "tensor([[ 1.0263,  0.3851, -0.1813]], grad_fn=<AddmmBackward0>)\n",
      "Batch 493, Loss: -1.026289701461792\n",
      "tensor([2])\n",
      "tensor([[ 0.9484,  0.4084, -0.2853]], grad_fn=<AddmmBackward0>)\n",
      "Batch 494, Loss: 0.28527605533599854\n",
      "tensor([0])\n",
      "tensor([[ 1.0475,  0.3791, -0.1531]], grad_fn=<AddmmBackward0>)\n",
      "Batch 495, Loss: -1.047520637512207\n",
      "tensor([1])\n",
      "tensor([[ 0.9643,  0.4039, -0.2643]], grad_fn=<AddmmBackward0>)\n",
      "Batch 496, Loss: -0.40388786792755127\n",
      "tensor([2])\n",
      "tensor([[ 1.0552,  0.3771, -0.1431]], grad_fn=<AddmmBackward0>)\n",
      "Batch 497, Loss: 0.1431083083152771\n",
      "tensor([1])\n",
      "tensor([[ 1.0359,  0.3829, -0.1689]], grad_fn=<AddmmBackward0>)\n",
      "Batch 498, Loss: -0.3829019069671631\n",
      "tensor([0])\n",
      "tensor([[ 1.0213,  0.3874, -0.1884]], grad_fn=<AddmmBackward0>)\n",
      "Batch 499, Loss: -1.0212788581848145\n",
      "tensor([2])\n",
      "tensor([[ 1.0694,  0.3733, -0.1244]], grad_fn=<AddmmBackward0>)\n",
      "Batch 500, Loss: 0.12435764074325562\n",
      "tensor([2])\n",
      "tensor([[ 1.0191,  0.3883, -0.1915]], grad_fn=<AddmmBackward0>)\n",
      "Batch 501, Loss: 0.19145303964614868\n",
      "tensor([0])\n",
      "tensor([[ 1.0690,  0.3736, -0.1249]], grad_fn=<AddmmBackward0>)\n",
      "Batch 502, Loss: -1.0689693689346313\n",
      "tensor([0])\n",
      "tensor([[ 1.0429,  0.3815, -0.1598]], grad_fn=<AddmmBackward0>)\n",
      "Batch 503, Loss: -1.0428876876831055\n",
      "tensor([2])\n",
      "tensor([[ 1.0576,  0.3772, -0.1403]], grad_fn=<AddmmBackward0>)\n",
      "Batch 504, Loss: 0.14025723934173584\n",
      "tensor([0])\n",
      "tensor([[ 1.0350,  0.3840, -0.1704]], grad_fn=<AddmmBackward0>)\n",
      "Batch 505, Loss: -1.0350368022918701\n",
      "tensor([1])\n",
      "tensor([[ 1.0614,  0.3763, -0.1354]], grad_fn=<AddmmBackward0>)\n",
      "Batch 506, Loss: -0.3763180375099182\n",
      "tensor([2])\n",
      "tensor([[ 1.0593,  0.3770, -0.1381]], grad_fn=<AddmmBackward0>)\n",
      "Batch 507, Loss: 0.1381128430366516\n",
      "tensor([1])\n",
      "tensor([[ 1.0715,  0.3736, -0.1220]], grad_fn=<AddmmBackward0>)\n",
      "Batch 508, Loss: -0.37357214093208313\n",
      "tensor([1])\n",
      "tensor([[ 1.0595,  0.3772, -0.1380]], grad_fn=<AddmmBackward0>)\n",
      "Batch 509, Loss: -0.37723758816719055\n",
      "tensor([0])\n",
      "tensor([[ 1.0684,  0.3748, -0.1262]], grad_fn=<AddmmBackward0>)\n",
      "Batch 510, Loss: -1.068350076675415\n",
      "tensor([2])\n",
      "tensor([[ 1.0346,  0.3849, -0.1713]], grad_fn=<AddmmBackward0>)\n",
      "Batch 511, Loss: 0.1712932586669922\n",
      "tensor([1])\n",
      "tensor([[ 1.0269,  0.3873, -0.1815]], grad_fn=<AddmmBackward0>)\n",
      "Batch 512, Loss: -0.38727807998657227\n",
      "tensor([2])\n",
      "tensor([[ 1.0619,  0.3771, -0.1350]], grad_fn=<AddmmBackward0>)\n",
      "Batch 513, Loss: 0.13495618104934692\n",
      "tensor([0])\n",
      "tensor([[ 1.0203,  0.3895, -0.1904]], grad_fn=<AddmmBackward0>)\n",
      "Batch 514, Loss: -1.0202841758728027\n",
      "tensor([2])\n",
      "tensor([[ 1.0692,  0.3752, -0.1252]], grad_fn=<AddmmBackward0>)\n",
      "Batch 515, Loss: 0.12517684698104858\n",
      "tensor([1])\n",
      "tensor([[ 1.0043,  0.3945, -0.2118]], grad_fn=<AddmmBackward0>)\n",
      "Batch 516, Loss: -0.3944871723651886\n",
      "tensor([0])\n",
      "tensor([[ 1.0594,  0.3784, -0.1383]], grad_fn=<AddmmBackward0>)\n",
      "Batch 517, Loss: -1.0593619346618652\n",
      "tensor([1])\n",
      "tensor([[ 1.0650,  0.3769, -0.1308]], grad_fn=<AddmmBackward0>)\n",
      "Batch 518, Loss: -0.37688276171684265\n",
      "tensor([1])\n",
      "tensor([[ 1.0399,  0.3844, -0.1643]], grad_fn=<AddmmBackward0>)\n",
      "Batch 519, Loss: -0.38441354036331177\n",
      "tensor([2])\n",
      "tensor([[ 0.9729,  0.4043, -0.2537]], grad_fn=<AddmmBackward0>)\n",
      "Batch 520, Loss: 0.25373828411102295\n",
      "tensor([0])\n",
      "tensor([[ 1.0571,  0.3797, -0.1414]], grad_fn=<AddmmBackward0>)\n",
      "Batch 521, Loss: -1.0571213960647583\n",
      "tensor([2])\n",
      "tensor([[ 1.0504,  0.3818, -0.1504]], grad_fn=<AddmmBackward0>)\n",
      "Batch 522, Loss: 0.1503770351409912\n",
      "tensor([1])\n",
      "tensor([[ 1.0668,  0.3771, -0.1286]], grad_fn=<AddmmBackward0>)\n",
      "Batch 523, Loss: -0.37714684009552\n",
      "tensor([2])\n",
      "tensor([[ 1.0679,  0.3770, -0.1271]], grad_fn=<AddmmBackward0>)\n",
      "Batch 524, Loss: 0.12705010175704956\n",
      "tensor([0])\n",
      "tensor([[ 1.0090,  0.3944, -0.2056]], grad_fn=<AddmmBackward0>)\n",
      "Batch 525, Loss: -1.008993148803711\n",
      "tensor([2])\n",
      "tensor([[ 0.9571,  0.4097, -0.2748]], grad_fn=<AddmmBackward0>)\n",
      "Batch 526, Loss: 0.2748206853866577\n",
      "tensor([1])\n",
      "tensor([[ 1.0631,  0.3788, -0.1335]], grad_fn=<AddmmBackward0>)\n",
      "Batch 527, Loss: -0.37879133224487305\n",
      "tensor([2])\n",
      "tensor([[ 1.0700,  0.3769, -0.1242]], grad_fn=<AddmmBackward0>)\n",
      "Batch 528, Loss: 0.12422484159469604\n",
      "tensor([0])\n",
      "tensor([[ 1.0347,  0.3874, -0.1712]], grad_fn=<AddmmBackward0>)\n",
      "Batch 529, Loss: -1.0347328186035156\n",
      "tensor([2])\n",
      "tensor([[ 1.0553,  0.3815, -0.1438]], grad_fn=<AddmmBackward0>)\n",
      "Batch 530, Loss: 0.14375364780426025\n",
      "tensor([2])\n",
      "tensor([[ 1.0414,  0.3856, -0.1623]], grad_fn=<AddmmBackward0>)\n",
      "Batch 531, Loss: 0.16225522756576538\n",
      "tensor([1])\n",
      "tensor([[ 1.0401,  0.3861, -0.1639]], grad_fn=<AddmmBackward0>)\n",
      "Batch 532, Loss: -0.38612544536590576\n",
      "tensor([0])\n",
      "tensor([[ 0.9975,  0.3987, -0.2207]], grad_fn=<AddmmBackward0>)\n",
      "Batch 533, Loss: -0.9974891543388367\n",
      "tensor([0])\n",
      "tensor([[ 1.0384,  0.3869, -0.1661]], grad_fn=<AddmmBackward0>)\n",
      "Batch 534, Loss: -1.0384184122085571\n",
      "tensor([1])\n",
      "tensor([[ 1.0713,  0.3774, -0.1223]], grad_fn=<AddmmBackward0>)\n",
      "Batch 535, Loss: -0.3773772418498993\n",
      "tensor([1])\n",
      "tensor([[ 1.0410,  0.3864, -0.1628]], grad_fn=<AddmmBackward0>)\n",
      "Batch 536, Loss: -0.38637956976890564\n",
      "tensor([0])\n",
      "tensor([[ 1.0492,  0.3841, -0.1518]], grad_fn=<AddmmBackward0>)\n",
      "Batch 537, Loss: -1.0492146015167236\n",
      "tensor([1])\n",
      "tensor([[ 1.0221,  0.3922, -0.1881]], grad_fn=<AddmmBackward0>)\n",
      "Batch 538, Loss: -0.39219218492507935\n",
      "tensor([2])\n",
      "tensor([[ 1.0702,  0.3783, -0.1240]], grad_fn=<AddmmBackward0>)\n",
      "Batch 539, Loss: 0.12400662899017334\n",
      "tensor([2])\n",
      "tensor([[ 1.0529,  0.3835, -0.1471]], grad_fn=<AddmmBackward0>)\n",
      "Batch 540, Loss: 0.1470605731010437\n",
      "tensor([1])\n",
      "tensor([[ 0.9259,  0.4206, -0.3163]], grad_fn=<AddmmBackward0>)\n",
      "Batch 541, Loss: -0.4206297993659973\n",
      "tensor([2])\n",
      "tensor([[ 1.0627,  0.3810, -0.1340]], grad_fn=<AddmmBackward0>)\n",
      "Batch 542, Loss: 0.1340402364730835\n",
      "tensor([2])\n",
      "tensor([[ 1.0250,  0.3921, -0.1842]], grad_fn=<AddmmBackward0>)\n",
      "Batch 543, Loss: 0.1842353343963623\n",
      "tensor([1])\n",
      "tensor([[ 1.0623,  0.3814, -0.1345]], grad_fn=<AddmmBackward0>)\n",
      "Batch 544, Loss: -0.3813721537590027\n",
      "tensor([2])\n",
      "tensor([[ 1.0157,  0.3950, -0.1964]], grad_fn=<AddmmBackward0>)\n",
      "Batch 545, Loss: 0.19641411304473877\n",
      "tensor([2])\n",
      "tensor([[ 1.0286,  0.3914, -0.1792]], grad_fn=<AddmmBackward0>)\n",
      "Batch 546, Loss: 0.17919141054153442\n",
      "tensor([1])\n",
      "tensor([[ 1.0588,  0.3828, -0.1388]], grad_fn=<AddmmBackward0>)\n",
      "Batch 547, Loss: -0.38278231024742126\n",
      "tensor([2])\n",
      "tensor([[ 1.0014,  0.3996, -0.2152]], grad_fn=<AddmmBackward0>)\n",
      "Batch 548, Loss: 0.21517908573150635\n",
      "tensor([1])\n",
      "tensor([[ 1.0716,  0.3793, -0.1215]], grad_fn=<AddmmBackward0>)\n",
      "Batch 549, Loss: -0.3793187737464905\n",
      "tensor([0])\n",
      "tensor([[ 1.0259,  0.3927, -0.1824]], grad_fn=<AddmmBackward0>)\n",
      "Batch 550, Loss: -1.0258634090423584\n",
      "tensor([1])\n",
      "tensor([[ 1.0657,  0.3813, -0.1292]], grad_fn=<AddmmBackward0>)\n",
      "Batch 551, Loss: -0.38132667541503906\n",
      "tensor([1])\n",
      "tensor([[ 1.0448,  0.3875, -0.1570]], grad_fn=<AddmmBackward0>)\n",
      "Batch 552, Loss: -0.38753658533096313\n",
      "tensor([0])\n",
      "tensor([[ 1.0427,  0.3883, -0.1597]], grad_fn=<AddmmBackward0>)\n",
      "Batch 553, Loss: -1.0427322387695312\n",
      "tensor([0])\n",
      "tensor([[ 1.0674,  0.3814, -0.1268]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 554, Loss: -1.0673969984054565\n",
      "tensor([0])\n",
      "tensor([[ 1.0756,  0.3791, -0.1159]], grad_fn=<AddmmBackward0>)\n",
      "Batch 555, Loss: -1.0756261348724365\n",
      "tensor([2])\n",
      "tensor([[ 1.0295,  0.3926, -0.1775]], grad_fn=<AddmmBackward0>)\n",
      "Batch 556, Loss: 0.17750895023345947\n",
      "tensor([2])\n",
      "tensor([[ 1.0590,  0.3842, -0.1382]], grad_fn=<AddmmBackward0>)\n",
      "Batch 557, Loss: 0.13818275928497314\n",
      "tensor([0])\n",
      "tensor([[ 0.9860,  0.4055, -0.2356]], grad_fn=<AddmmBackward0>)\n",
      "Batch 558, Loss: -0.9859834313392639\n",
      "tensor([2])\n",
      "tensor([[ 0.9636,  0.4120, -0.2655]], grad_fn=<AddmmBackward0>)\n",
      "Batch 559, Loss: 0.2654793858528137\n",
      "tensor([1])\n",
      "tensor([[ 1.0765,  0.3796, -0.1149]], grad_fn=<AddmmBackward0>)\n",
      "Batch 560, Loss: -0.3795512616634369\n",
      "tensor([2])\n",
      "tensor([[ 1.0562,  0.3856, -0.1420]], grad_fn=<AddmmBackward0>)\n",
      "Batch 561, Loss: 0.14203214645385742\n",
      "tensor([1])\n",
      "tensor([[ 1.0596,  0.3847, -0.1374]], grad_fn=<AddmmBackward0>)\n",
      "Batch 562, Loss: -0.384682834148407\n",
      "tensor([1])\n",
      "tensor([[ 1.0324,  0.3927, -0.1737]], grad_fn=<AddmmBackward0>)\n",
      "Batch 563, Loss: -0.3926636278629303\n",
      "tensor([2])\n",
      "tensor([[ 1.0330,  0.3927, -0.1729]], grad_fn=<AddmmBackward0>)\n",
      "Batch 564, Loss: 0.17292577028274536\n",
      "tensor([0])\n",
      "tensor([[ 1.0720,  0.3816, -0.1208]], grad_fn=<AddmmBackward0>)\n",
      "Batch 565, Loss: -1.0720157623291016\n",
      "tensor([2])\n",
      "tensor([[ 1.0541,  0.3869, -0.1447]], grad_fn=<AddmmBackward0>)\n",
      "Batch 566, Loss: 0.14472877979278564\n",
      "tensor([0])\n",
      "tensor([[ 1.0220,  0.3962, -0.1875]], grad_fn=<AddmmBackward0>)\n",
      "Batch 567, Loss: -1.021988868713379\n",
      "tensor([2])\n",
      "tensor([[ 1.0454,  0.3896, -0.1563]], grad_fn=<AddmmBackward0>)\n",
      "Batch 568, Loss: 0.15633779764175415\n",
      "tensor([0])\n",
      "tensor([[ 1.0207,  0.3968, -0.1892]], grad_fn=<AddmmBackward0>)\n",
      "Batch 569, Loss: -1.0206992626190186\n",
      "tensor([1])\n",
      "tensor([[ 1.0687,  0.3831, -0.1252]], grad_fn=<AddmmBackward0>)\n",
      "Batch 570, Loss: -0.3831207752227783\n",
      "tensor([1])\n",
      "tensor([[ 1.0528,  0.3878, -0.1465]], grad_fn=<AddmmBackward0>)\n",
      "Batch 571, Loss: -0.38783466815948486\n",
      "tensor([2])\n",
      "tensor([[ 1.0677,  0.3837, -0.1266]], grad_fn=<AddmmBackward0>)\n",
      "Batch 572, Loss: 0.12657231092453003\n",
      "tensor([0])\n",
      "tensor([[ 0.9885,  0.4066, -0.2323]], grad_fn=<AddmmBackward0>)\n",
      "Batch 573, Loss: -0.9884520769119263\n",
      "tensor([0])\n",
      "tensor([[ 1.0380,  0.3925, -0.1662]], grad_fn=<AddmmBackward0>)\n",
      "Batch 574, Loss: -1.038032054901123\n",
      "tensor([1])\n",
      "tensor([[ 1.0717,  0.3829, -0.1213]], grad_fn=<AddmmBackward0>)\n",
      "Batch 575, Loss: -0.3829421401023865\n",
      "tensor([2])\n",
      "tensor([[ 1.0430,  0.3913, -0.1597]], grad_fn=<AddmmBackward0>)\n",
      "Batch 576, Loss: 0.15965580940246582\n",
      "tensor([2])\n",
      "tensor([[ 1.0732,  0.3828, -0.1194]], grad_fn=<AddmmBackward0>)\n",
      "Batch 577, Loss: 0.11941200494766235\n",
      "tensor([1])\n",
      "tensor([[ 1.0666,  0.3848, -0.1282]], grad_fn=<AddmmBackward0>)\n",
      "Batch 578, Loss: -0.3848268389701843\n",
      "tensor([0])\n",
      "tensor([[ 1.0754,  0.3824, -0.1164]], grad_fn=<AddmmBackward0>)\n",
      "Batch 579, Loss: -1.075439453125\n",
      "tensor([0])\n",
      "tensor([[ 1.0025,  0.4034, -0.2137]], grad_fn=<AddmmBackward0>)\n",
      "Batch 580, Loss: -1.0025062561035156\n",
      "tensor([2])\n",
      "tensor([[ 1.0526,  0.3892, -0.1469]], grad_fn=<AddmmBackward0>)\n",
      "Batch 581, Loss: 0.14694833755493164\n",
      "tensor([1])\n",
      "tensor([[ 1.0505,  0.3900, -0.1498]], grad_fn=<AddmmBackward0>)\n",
      "Batch 582, Loss: -0.3899642825126648\n",
      "tensor([0])\n",
      "tensor([[ 1.0396,  0.3932, -0.1644]], grad_fn=<AddmmBackward0>)\n",
      "Batch 583, Loss: -1.039552092552185\n",
      "tensor([2])\n",
      "tensor([[ 0.9785,  0.4108, -0.2458]], grad_fn=<AddmmBackward0>)\n",
      "Batch 584, Loss: 0.2458363175392151\n",
      "tensor([2])\n",
      "tensor([[ 1.0306,  0.3960, -0.1764]], grad_fn=<AddmmBackward0>)\n",
      "Batch 585, Loss: 0.17643845081329346\n",
      "tensor([1])\n",
      "tensor([[ 1.0465,  0.3916, -0.1552]], grad_fn=<AddmmBackward0>)\n",
      "Batch 586, Loss: -0.3915966749191284\n",
      "tensor([1])\n",
      "tensor([[ 1.0417,  0.3931, -0.1617]], grad_fn=<AddmmBackward0>)\n",
      "Batch 587, Loss: -0.3931184411048889\n",
      "tensor([1])\n",
      "tensor([[ 1.0674,  0.3859, -0.1273]], grad_fn=<AddmmBackward0>)\n",
      "Batch 588, Loss: -0.38592445850372314\n",
      "tensor([0])\n",
      "tensor([[ 1.0696,  0.3855, -0.1244]], grad_fn=<AddmmBackward0>)\n",
      "Batch 589, Loss: -1.069589614868164\n",
      "tensor([1])\n",
      "tensor([[ 1.0750,  0.3841, -0.1173]], grad_fn=<AddmmBackward0>)\n",
      "Batch 590, Loss: -0.3841056525707245\n",
      "tensor([2])\n",
      "tensor([[ 1.0561,  0.3897, -0.1425]], grad_fn=<AddmmBackward0>)\n",
      "Batch 591, Loss: 0.1424793004989624\n",
      "tensor([1])\n",
      "tensor([[ 0.9509,  0.4198, -0.2827]], grad_fn=<AddmmBackward0>)\n",
      "Batch 592, Loss: -0.41975075006484985\n",
      "tensor([0])\n",
      "tensor([[ 1.0522,  0.3911, -0.1477]], grad_fn=<AddmmBackward0>)\n",
      "Batch 593, Loss: -1.0521814823150635\n",
      "tensor([2])\n",
      "tensor([[ 1.0568,  0.3900, -0.1416]], grad_fn=<AddmmBackward0>)\n",
      "Batch 594, Loss: 0.14161771535873413\n",
      "tensor([1])\n",
      "tensor([[ 1.0773,  0.3843, -0.1143]], grad_fn=<AddmmBackward0>)\n",
      "Batch 595, Loss: -0.38429689407348633\n",
      "tensor([1])\n",
      "tensor([[ 1.0733,  0.3856, -0.1197]], grad_fn=<AddmmBackward0>)\n",
      "Batch 596, Loss: -0.3856147229671478\n",
      "tensor([0])\n",
      "tensor([[ 1.0223,  0.4003, -0.1877]], grad_fn=<AddmmBackward0>)\n",
      "Batch 597, Loss: -1.0222675800323486\n",
      "tensor([0])\n",
      "tensor([[ 0.9882,  0.4101, -0.2331]], grad_fn=<AddmmBackward0>)\n",
      "Batch 598, Loss: -0.9882211685180664\n",
      "tensor([1])\n",
      "tensor([[ 1.0661,  0.3882, -0.1294]], grad_fn=<AddmmBackward0>)\n",
      "Batch 599, Loss: -0.38819217681884766\n",
      "Training [60%]\tLoss: -0.3803\n",
      "tensor([0])\n",
      "tensor([[ 0.9243,  0.4285, -0.3184]], grad_fn=<AddmmBackward0>)\n",
      "Batch 0, Loss: -0.9243443012237549\n",
      "tensor([0])\n",
      "tensor([[ 1.0091,  0.4047, -0.2055]], grad_fn=<AddmmBackward0>)\n",
      "Batch 1, Loss: -1.009143590927124\n",
      "tensor([1])\n",
      "tensor([[ 1.0805,  0.3846, -0.1105]], grad_fn=<AddmmBackward0>)\n",
      "Batch 2, Loss: -0.3846241235733032\n",
      "tensor([1])\n",
      "tensor([[ 1.0802,  0.3849, -0.1110]], grad_fn=<AddmmBackward0>)\n",
      "Batch 3, Loss: -0.38488858938217163\n",
      "tensor([2])\n",
      "tensor([[ 1.0773,  0.3859, -0.1149]], grad_fn=<AddmmBackward0>)\n",
      "Batch 4, Loss: 0.1149410605430603\n",
      "tensor([1])\n",
      "tensor([[ 1.0791,  0.3856, -0.1126]], grad_fn=<AddmmBackward0>)\n",
      "Batch 5, Loss: -0.38558685779571533\n",
      "tensor([1])\n",
      "tensor([[ 0.9924,  0.4103, -0.2282]], grad_fn=<AddmmBackward0>)\n",
      "Batch 6, Loss: -0.4102551341056824\n",
      "tensor([2])\n",
      "tensor([[ 1.0466,  0.3952, -0.1561]], grad_fn=<AddmmBackward0>)\n",
      "Batch 7, Loss: 0.1561034917831421\n",
      "tensor([1])\n",
      "tensor([[ 1.0216,  0.4024, -0.1894]], grad_fn=<AddmmBackward0>)\n",
      "Batch 8, Loss: -0.40240681171417236\n",
      "tensor([2])\n",
      "tensor([[ 1.0748,  0.3876, -0.1186]], grad_fn=<AddmmBackward0>)\n",
      "Batch 9, Loss: 0.11856544017791748\n",
      "tensor([2])\n",
      "tensor([[ 1.0685,  0.3895, -0.1269]], grad_fn=<AddmmBackward0>)\n",
      "Batch 10, Loss: 0.1268511414527893\n",
      "tensor([1])\n",
      "tensor([[ 1.0796,  0.3866, -0.1121]], grad_fn=<AddmmBackward0>)\n",
      "Batch 11, Loss: -0.3866019546985626\n",
      "tensor([0])\n",
      "tensor([[ 1.0679,  0.3901, -0.1276]], grad_fn=<AddmmBackward0>)\n",
      "Batch 12, Loss: -1.067932367324829\n",
      "tensor([0])\n",
      "tensor([[ 1.0718,  0.3891, -0.1224]], grad_fn=<AddmmBackward0>)\n",
      "Batch 13, Loss: -1.071781873703003\n",
      "tensor([2])\n",
      "tensor([[ 1.0628,  0.3918, -0.1345]], grad_fn=<AddmmBackward0>)\n",
      "Batch 14, Loss: 0.13449370861053467\n",
      "tensor([0])\n",
      "tensor([[ 1.0790,  0.3874, -0.1129]], grad_fn=<AddmmBackward0>)\n",
      "Batch 15, Loss: -1.0789823532104492\n",
      "tensor([2])\n",
      "tensor([[ 1.0649,  0.3915, -0.1317]], grad_fn=<AddmmBackward0>)\n",
      "Batch 16, Loss: 0.13169324398040771\n",
      "tensor([1])\n",
      "tensor([[ 1.0798,  0.3875, -0.1118]], grad_fn=<AddmmBackward0>)\n",
      "Batch 17, Loss: -0.38746291399002075\n",
      "tensor([2])\n",
      "tensor([[ 1.0164,  0.4054, -0.1963]], grad_fn=<AddmmBackward0>)\n",
      "Batch 18, Loss: 0.196308434009552\n",
      "tensor([2])\n",
      "tensor([[ 1.0780,  0.3883, -0.1142]], grad_fn=<AddmmBackward0>)\n",
      "Batch 19, Loss: 0.11421459913253784\n",
      "tensor([0])\n",
      "tensor([[ 1.0635,  0.3925, -0.1335]], grad_fn=<AddmmBackward0>)\n",
      "Batch 20, Loss: -1.0634996891021729\n",
      "tensor([2])\n",
      "tensor([[ 1.0567,  0.3945, -0.1426]], grad_fn=<AddmmBackward0>)\n",
      "Batch 21, Loss: 0.14260798692703247\n",
      "tensor([0])\n",
      "tensor([[ 1.0829,  0.3872, -0.1076]], grad_fn=<AddmmBackward0>)\n",
      "Batch 22, Loss: -1.0829029083251953\n",
      "tensor([1])\n",
      "tensor([[ 1.0761,  0.3892, -0.1167]], grad_fn=<AddmmBackward0>)\n",
      "Batch 23, Loss: -0.38923829793930054\n",
      "tensor([2])\n",
      "tensor([[ 0.9386,  0.4279, -0.2999]], grad_fn=<AddmmBackward0>)\n",
      "Batch 24, Loss: 0.2999492883682251\n",
      "tensor([0])\n",
      "tensor([[ 1.0215,  0.4048, -0.1895]], grad_fn=<AddmmBackward0>)\n",
      "Batch 25, Loss: -1.0214539766311646\n",
      "tensor([0])\n",
      "tensor([[ 1.0373,  0.4005, -0.1685]], grad_fn=<AddmmBackward0>)\n",
      "Batch 26, Loss: -1.0372636318206787\n",
      "tensor([1])\n",
      "tensor([[ 0.8832,  0.4436, -0.3738]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 27, Loss: -0.443644255399704\n",
      "tensor([1])\n",
      "tensor([[ 1.0211,  0.4052, -0.1901]], grad_fn=<AddmmBackward0>)\n",
      "Batch 28, Loss: -0.4052087068557739\n",
      "tensor([2])\n",
      "tensor([[ 1.0587,  0.3948, -0.1401]], grad_fn=<AddmmBackward0>)\n",
      "Batch 29, Loss: 0.14006423950195312\n",
      "tensor([2])\n",
      "tensor([[ 0.9296,  0.4310, -0.3121]], grad_fn=<AddmmBackward0>)\n",
      "Batch 30, Loss: 0.31209754943847656\n",
      "tensor([0])\n",
      "tensor([[ 1.0757,  0.3904, -0.1174]], grad_fn=<AddmmBackward0>)\n",
      "Batch 31, Loss: -1.0757315158843994\n",
      "tensor([0])\n",
      "tensor([[ 1.0366,  0.4014, -0.1696]], grad_fn=<AddmmBackward0>)\n",
      "Batch 32, Loss: -1.0365983247756958\n",
      "tensor([0])\n",
      "tensor([[ 1.0730,  0.3914, -0.1212]], grad_fn=<AddmmBackward0>)\n",
      "Batch 33, Loss: -1.0729777812957764\n",
      "tensor([0])\n",
      "tensor([[ 1.0852,  0.3880, -0.1050]], grad_fn=<AddmmBackward0>)\n",
      "Batch 34, Loss: -1.0852327346801758\n",
      "tensor([2])\n",
      "tensor([[ 0.9909,  0.4145, -0.2308]], grad_fn=<AddmmBackward0>)\n",
      "Batch 35, Loss: 0.2307834029197693\n",
      "tensor([0])\n",
      "tensor([[ 1.0353,  0.4022, -0.1717]], grad_fn=<AddmmBackward0>)\n",
      "Batch 36, Loss: -1.035339593887329\n",
      "tensor([0])\n",
      "tensor([[ 1.0845,  0.3886, -0.1063]], grad_fn=<AddmmBackward0>)\n",
      "Batch 37, Loss: -1.0844923257827759\n",
      "tensor([2])\n",
      "tensor([[ 1.0842,  0.3887, -0.1069]], grad_fn=<AddmmBackward0>)\n",
      "Batch 38, Loss: 0.10686981678009033\n",
      "tensor([2])\n",
      "tensor([[ 1.0842,  0.3888, -0.1070]], grad_fn=<AddmmBackward0>)\n",
      "Batch 39, Loss: 0.10703814029693604\n",
      "tensor([1])\n",
      "tensor([[ 1.0319,  0.4035, -0.1766]], grad_fn=<AddmmBackward0>)\n",
      "Batch 40, Loss: -0.4034578204154968\n",
      "tensor([2])\n",
      "tensor([[ 1.0604,  0.3956, -0.1388]], grad_fn=<AddmmBackward0>)\n",
      "Batch 41, Loss: 0.1387884020805359\n",
      "tensor([1])\n",
      "tensor([[ 1.0843,  0.3891, -0.1070]], grad_fn=<AddmmBackward0>)\n",
      "Batch 42, Loss: -0.38909608125686646\n",
      "tensor([0])\n",
      "tensor([[ 1.0569,  0.3968, -0.1435]], grad_fn=<AddmmBackward0>)\n",
      "Batch 43, Loss: -1.0568989515304565\n",
      "tensor([1])\n",
      "tensor([[ 1.0569,  0.3969, -0.1435]], grad_fn=<AddmmBackward0>)\n",
      "Batch 44, Loss: -0.396930992603302\n",
      "tensor([2])\n",
      "tensor([[ 1.0122,  0.4095, -0.2031]], grad_fn=<AddmmBackward0>)\n",
      "Batch 45, Loss: 0.20313358306884766\n",
      "tensor([1])\n",
      "tensor([[ 1.0651,  0.3949, -0.1327]], grad_fn=<AddmmBackward0>)\n",
      "Batch 46, Loss: -0.39491361379623413\n",
      "tensor([1])\n",
      "tensor([[ 1.0725,  0.3930, -0.1229]], grad_fn=<AddmmBackward0>)\n",
      "Batch 47, Loss: -0.3930146098136902\n",
      "tensor([2])\n",
      "tensor([[ 1.0148,  0.4092, -0.1997]], grad_fn=<AddmmBackward0>)\n",
      "Batch 48, Loss: 0.19969463348388672\n",
      "tensor([0])\n",
      "tensor([[ 1.0657,  0.3952, -0.1319]], grad_fn=<AddmmBackward0>)\n",
      "Batch 49, Loss: -1.0657494068145752\n",
      "tensor([0])\n",
      "tensor([[ 1.0596,  0.3970, -0.1400]], grad_fn=<AddmmBackward0>)\n",
      "Batch 50, Loss: -1.0596446990966797\n",
      "tensor([2])\n",
      "tensor([[ 1.0159,  0.4093, -0.1983]], grad_fn=<AddmmBackward0>)\n",
      "Batch 51, Loss: 0.19833582639694214\n",
      "tensor([0])\n",
      "tensor([[ 1.0724,  0.3937, -0.1231]], grad_fn=<AddmmBackward0>)\n",
      "Batch 52, Loss: -1.0723990201950073\n",
      "tensor([1])\n",
      "tensor([[ 1.0656,  0.3958, -0.1323]], grad_fn=<AddmmBackward0>)\n",
      "Batch 53, Loss: -0.39575278759002686\n",
      "tensor([2])\n",
      "tensor([[ 1.0154,  0.4098, -0.1992]], grad_fn=<AddmmBackward0>)\n",
      "Batch 54, Loss: 0.1991545557975769\n",
      "tensor([1])\n",
      "tensor([[ 1.0775,  0.3927, -0.1166]], grad_fn=<AddmmBackward0>)\n",
      "Batch 55, Loss: -0.39273568987846375\n",
      "tensor([2])\n",
      "tensor([[ 1.0843,  0.3910, -0.1074]], grad_fn=<AddmmBackward0>)\n",
      "Batch 56, Loss: 0.1074020266532898\n",
      "tensor([1])\n",
      "tensor([[ 0.9117,  0.4388, -0.3372]], grad_fn=<AddmmBackward0>)\n",
      "Batch 57, Loss: -0.4388204514980316\n",
      "tensor([0])\n",
      "tensor([[ 1.0899,  0.3897, -0.1000]], grad_fn=<AddmmBackward0>)\n",
      "Batch 58, Loss: -1.089893102645874\n",
      "tensor([0])\n",
      "tensor([[ 1.0868,  0.3907, -0.1042]], grad_fn=<AddmmBackward0>)\n",
      "Batch 59, Loss: -1.0867676734924316\n",
      "tensor([1])\n",
      "tensor([[ 1.0595,  0.3984, -0.1406]], grad_fn=<AddmmBackward0>)\n",
      "Batch 60, Loss: -0.3984113335609436\n",
      "tensor([0])\n",
      "tensor([[ 0.9910,  0.4175, -0.2318]], grad_fn=<AddmmBackward0>)\n",
      "Batch 61, Loss: -0.9909977912902832\n",
      "tensor([2])\n",
      "tensor([[ 1.0703,  0.3957, -0.1264]], grad_fn=<AddmmBackward0>)\n",
      "Batch 62, Loss: 0.1263698935508728\n",
      "tensor([1])\n",
      "tensor([[ 1.0798,  0.3933, -0.1138]], grad_fn=<AddmmBackward0>)\n",
      "Batch 63, Loss: -0.3932502567768097\n",
      "tensor([0])\n",
      "tensor([[ 1.0809,  0.3931, -0.1123]], grad_fn=<AddmmBackward0>)\n",
      "Batch 64, Loss: -1.0809333324432373\n",
      "tensor([1])\n",
      "tensor([[ 1.0377,  0.4052, -0.1700]], grad_fn=<AddmmBackward0>)\n",
      "Batch 65, Loss: -0.40515485405921936\n",
      "tensor([2])\n",
      "tensor([[ 1.0768,  0.3945, -0.1180]], grad_fn=<AddmmBackward0>)\n",
      "Batch 66, Loss: 0.1179547905921936\n",
      "tensor([0])\n",
      "tensor([[ 1.0789,  0.3941, -0.1152]], grad_fn=<AddmmBackward0>)\n",
      "Batch 67, Loss: -1.0789268016815186\n",
      "tensor([2])\n",
      "tensor([[ 0.9662,  0.4252, -0.2653]], grad_fn=<AddmmBackward0>)\n",
      "Batch 68, Loss: 0.2652745246887207\n",
      "tensor([1])\n",
      "tensor([[ 0.9477,  0.4304, -0.2899]], grad_fn=<AddmmBackward0>)\n",
      "Batch 69, Loss: -0.4304478168487549\n",
      "tensor([0])\n",
      "tensor([[ 1.0467,  0.4034, -0.1582]], grad_fn=<AddmmBackward0>)\n",
      "Batch 70, Loss: -1.0467309951782227\n",
      "tensor([0])\n",
      "tensor([[ 1.0563,  0.4009, -0.1455]], grad_fn=<AddmmBackward0>)\n",
      "Batch 71, Loss: -1.056340217590332\n",
      "tensor([2])\n",
      "tensor([[ 1.0800,  0.3945, -0.1141]], grad_fn=<AddmmBackward0>)\n",
      "Batch 72, Loss: 0.1140565276145935\n",
      "tensor([1])\n",
      "tensor([[ 1.0934,  0.3910, -0.0963]], grad_fn=<AddmmBackward0>)\n",
      "Batch 73, Loss: -0.39095914363861084\n",
      "tensor([0])\n",
      "tensor([[ 1.0398,  0.4058, -0.1677]], grad_fn=<AddmmBackward0>)\n",
      "Batch 74, Loss: -1.0398492813110352\n",
      "tensor([2])\n",
      "tensor([[ 1.0792,  0.3951, -0.1154]], grad_fn=<AddmmBackward0>)\n",
      "Batch 75, Loss: 0.11544597148895264\n",
      "tensor([1])\n",
      "tensor([[ 1.0896,  0.3924, -0.1016]], grad_fn=<AddmmBackward0>)\n",
      "Batch 76, Loss: -0.3924049735069275\n",
      "tensor([1])\n",
      "tensor([[ 1.0489,  0.4037, -0.1559]], grad_fn=<AddmmBackward0>)\n",
      "Batch 77, Loss: -0.403717577457428\n",
      "tensor([1])\n",
      "tensor([[ 1.0242,  0.4106, -0.1887]], grad_fn=<AddmmBackward0>)\n",
      "Batch 78, Loss: -0.4106258749961853\n",
      "tensor([1])\n",
      "tensor([[ 1.0696,  0.3984, -0.1283]], grad_fn=<AddmmBackward0>)\n",
      "Batch 79, Loss: -0.39837485551834106\n",
      "tensor([2])\n",
      "tensor([[ 1.0929,  0.3922, -0.0974]], grad_fn=<AddmmBackward0>)\n",
      "Batch 80, Loss: 0.09735721349716187\n",
      "tensor([0])\n",
      "tensor([[ 1.0273,  0.4103, -0.1846]], grad_fn=<AddmmBackward0>)\n",
      "Batch 81, Loss: -1.027313470840454\n",
      "tensor([2])\n",
      "tensor([[ 1.0579,  0.4021, -0.1441]], grad_fn=<AddmmBackward0>)\n",
      "Batch 82, Loss: 0.14405584335327148\n",
      "tensor([2])\n",
      "tensor([[ 1.0508,  0.4042, -0.1535]], grad_fn=<AddmmBackward0>)\n",
      "Batch 83, Loss: 0.15348714590072632\n",
      "tensor([2])\n",
      "tensor([[ 1.0885,  0.3940, -0.1033]], grad_fn=<AddmmBackward0>)\n",
      "Batch 84, Loss: 0.10329622030258179\n",
      "tensor([2])\n",
      "tensor([[ 0.9973,  0.4190, -0.2245]], grad_fn=<AddmmBackward0>)\n",
      "Batch 85, Loss: 0.2245025634765625\n",
      "tensor([1])\n",
      "tensor([[ 1.0674,  0.4000, -0.1312]], grad_fn=<AddmmBackward0>)\n",
      "Batch 86, Loss: -0.40003031492233276\n",
      "tensor([2])\n",
      "tensor([[ 1.0619,  0.4016, -0.1384]], grad_fn=<AddmmBackward0>)\n",
      "Batch 87, Loss: 0.13838070631027222\n",
      "tensor([0])\n",
      "tensor([[ 1.0943,  0.3929, -0.0952]], grad_fn=<AddmmBackward0>)\n",
      "Batch 88, Loss: -1.0943045616149902\n",
      "tensor([1])\n",
      "tensor([[ 1.0450,  0.4065, -0.1608]], grad_fn=<AddmmBackward0>)\n",
      "Batch 89, Loss: -0.4064931273460388\n",
      "tensor([2])\n",
      "tensor([[ 1.0081,  0.4167, -0.2099]], grad_fn=<AddmmBackward0>)\n",
      "Batch 90, Loss: 0.20985651016235352\n",
      "tensor([0])\n",
      "tensor([[ 1.0683,  0.4004, -0.1296]], grad_fn=<AddmmBackward0>)\n",
      "Batch 91, Loss: -1.068336844444275\n",
      "tensor([2])\n",
      "tensor([[ 1.0949,  0.3933, -0.0943]], grad_fn=<AddmmBackward0>)\n",
      "Batch 92, Loss: 0.09431624412536621\n",
      "tensor([1])\n",
      "tensor([[ 1.0272,  0.4118, -0.1843]], grad_fn=<AddmmBackward0>)\n",
      "Batch 93, Loss: -0.41180264949798584\n",
      "tensor([0])\n",
      "tensor([[ 1.0582,  0.4035, -0.1430]], grad_fn=<AddmmBackward0>)\n",
      "Batch 94, Loss: -1.058201789855957\n",
      "tensor([0])\n",
      "tensor([[ 1.0923,  0.3944, -0.0976]], grad_fn=<AddmmBackward0>)\n",
      "Batch 95, Loss: -1.0922919511795044\n",
      "tensor([2])\n",
      "tensor([[ 1.0920,  0.3946, -0.0981]], grad_fn=<AddmmBackward0>)\n",
      "Batch 96, Loss: 0.09805053472518921\n",
      "tensor([2])\n",
      "tensor([[ 0.9400,  0.4359, -0.3003]], grad_fn=<AddmmBackward0>)\n",
      "Batch 97, Loss: 0.30025434494018555\n",
      "tensor([0])\n",
      "tensor([[ 1.0957,  0.3938, -0.0932]], grad_fn=<AddmmBackward0>)\n",
      "Batch 98, Loss: -1.0956809520721436\n",
      "tensor([2])\n",
      "tensor([[ 0.9661,  0.4290, -0.2655]], grad_fn=<AddmmBackward0>)\n",
      "Batch 99, Loss: 0.26553648710250854\n",
      "tensor([1])\n",
      "tensor([[ 1.0554,  0.4049, -0.1468]], grad_fn=<AddmmBackward0>)\n",
      "Batch 100, Loss: -0.40489691495895386\n",
      "tensor([1])\n",
      "tensor([[ 1.0429,  0.4084, -0.1634]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101, Loss: -0.4083888530731201\n",
      "tensor([1])\n",
      "tensor([[ 1.0272,  0.4128, -0.1843]], grad_fn=<AddmmBackward0>)\n",
      "Batch 102, Loss: -0.4127708077430725\n",
      "tensor([2])\n",
      "tensor([[ 1.0520,  0.4062, -0.1512]], grad_fn=<AddmmBackward0>)\n",
      "Batch 103, Loss: 0.15122175216674805\n",
      "tensor([1])\n",
      "tensor([[ 1.0829,  0.3980, -0.1102]], grad_fn=<AddmmBackward0>)\n",
      "Batch 104, Loss: -0.3979821801185608\n",
      "tensor([0])\n",
      "tensor([[ 0.9840,  0.4249, -0.2416]], grad_fn=<AddmmBackward0>)\n",
      "Batch 105, Loss: -0.9840422868728638\n",
      "tensor([1])\n",
      "tensor([[ 1.0336,  0.4116, -0.1757]], grad_fn=<AddmmBackward0>)\n",
      "Batch 106, Loss: -0.41161566972732544\n",
      "tensor([2])\n",
      "tensor([[ 1.0784,  0.3996, -0.1160]], grad_fn=<AddmmBackward0>)\n",
      "Batch 107, Loss: 0.1160362958908081\n",
      "tensor([1])\n",
      "tensor([[ 1.0107,  0.4181, -0.2062]], grad_fn=<AddmmBackward0>)\n",
      "Batch 108, Loss: -0.41811078786849976\n",
      "tensor([0])\n",
      "tensor([[ 1.0655,  0.4035, -0.1332]], grad_fn=<AddmmBackward0>)\n",
      "Batch 109, Loss: -1.0655121803283691\n",
      "tensor([0])\n",
      "tensor([[ 1.0917,  0.3966, -0.0984]], grad_fn=<AddmmBackward0>)\n",
      "Batch 110, Loss: -1.091681718826294\n",
      "tensor([1])\n",
      "tensor([[ 1.0091,  0.4190, -0.2084]], grad_fn=<AddmmBackward0>)\n",
      "Batch 111, Loss: -0.41898518800735474\n",
      "tensor([1])\n",
      "tensor([[ 1.0916,  0.3969, -0.0986]], grad_fn=<AddmmBackward0>)\n",
      "Batch 112, Loss: -0.3968891501426697\n",
      "tensor([1])\n",
      "tensor([[ 1.0749,  0.4016, -0.1209]], grad_fn=<AddmmBackward0>)\n",
      "Batch 113, Loss: -0.4015786647796631\n",
      "tensor([2])\n",
      "tensor([[ 1.0908,  0.3975, -0.0998]], grad_fn=<AddmmBackward0>)\n",
      "Batch 114, Loss: 0.09975963830947876\n",
      "tensor([1])\n",
      "tensor([[ 1.0464,  0.4096, -0.1589]], grad_fn=<AddmmBackward0>)\n",
      "Batch 115, Loss: -0.40963590145111084\n",
      "tensor([1])\n",
      "tensor([[ 1.0854,  0.3993, -0.1070]], grad_fn=<AddmmBackward0>)\n",
      "Batch 116, Loss: -0.3993321359157562\n",
      "tensor([2])\n",
      "tensor([[ 1.0957,  0.3968, -0.0934]], grad_fn=<AddmmBackward0>)\n",
      "Batch 117, Loss: 0.09335595369338989\n",
      "tensor([2])\n",
      "tensor([[ 1.0489,  0.4095, -0.1555]], grad_fn=<AddmmBackward0>)\n",
      "Batch 118, Loss: 0.1555417776107788\n",
      "tensor([0])\n",
      "tensor([[ 1.0774,  0.4020, -0.1175]], grad_fn=<AddmmBackward0>)\n",
      "Batch 119, Loss: -1.0774309635162354\n",
      "tensor([0])\n",
      "tensor([[ 1.0456,  0.4107, -0.1598]], grad_fn=<AddmmBackward0>)\n",
      "Batch 120, Loss: -1.0456359386444092\n",
      "tensor([1])\n",
      "tensor([[ 0.9819,  0.4280, -0.2447]], grad_fn=<AddmmBackward0>)\n",
      "Batch 121, Loss: -0.42797136306762695\n",
      "tensor([2])\n",
      "tensor([[ 1.0621,  0.4066, -0.1380]], grad_fn=<AddmmBackward0>)\n",
      "Batch 122, Loss: 0.13798069953918457\n",
      "tensor([1])\n",
      "tensor([[ 1.0238,  0.4170, -0.1889]], grad_fn=<AddmmBackward0>)\n",
      "Batch 123, Loss: -0.4170237183570862\n",
      "tensor([0])\n",
      "tensor([[ 1.0695,  0.4050, -0.1281]], grad_fn=<AddmmBackward0>)\n",
      "Batch 124, Loss: -1.0694940090179443\n",
      "tensor([0])\n",
      "tensor([[ 1.0955,  0.3981, -0.0935]], grad_fn=<AddmmBackward0>)\n",
      "Batch 125, Loss: -1.0955429077148438\n",
      "tensor([2])\n",
      "tensor([[ 1.0870,  0.4006, -0.1049]], grad_fn=<AddmmBackward0>)\n",
      "Batch 126, Loss: 0.1049456000328064\n",
      "tensor([0])\n",
      "tensor([[ 0.9801,  0.4293, -0.2473]], grad_fn=<AddmmBackward0>)\n",
      "Batch 127, Loss: -0.9800525903701782\n",
      "tensor([2])\n",
      "tensor([[ 1.1011,  0.3971, -0.0863]], grad_fn=<AddmmBackward0>)\n",
      "Batch 128, Loss: 0.08633369207382202\n",
      "tensor([2])\n",
      "tensor([[ 1.0981,  0.3980, -0.0904]], grad_fn=<AddmmBackward0>)\n",
      "Batch 129, Loss: 0.09038364887237549\n",
      "tensor([2])\n",
      "tensor([[ 0.9460,  0.4387, -0.2926]], grad_fn=<AddmmBackward0>)\n",
      "Batch 130, Loss: 0.29262977838516235\n",
      "tensor([1])\n",
      "tensor([[ 1.0642,  0.4073, -0.1355]], grad_fn=<AddmmBackward0>)\n",
      "Batch 131, Loss: -0.40727758407592773\n",
      "tensor([1])\n",
      "tensor([[ 1.0750,  0.4045, -0.1210]], grad_fn=<AddmmBackward0>)\n",
      "Batch 132, Loss: -0.40450501441955566\n",
      "tensor([1])\n",
      "tensor([[ 1.0983,  0.3984, -0.0900]], grad_fn=<AddmmBackward0>)\n",
      "Batch 133, Loss: -0.3984330892562866\n",
      "tensor([2])\n",
      "tensor([[ 1.0106,  0.4220, -0.2066]], grad_fn=<AddmmBackward0>)\n",
      "Batch 134, Loss: 0.20663142204284668\n",
      "tensor([0])\n",
      "tensor([[ 1.0182,  0.4201, -0.1965]], grad_fn=<AddmmBackward0>)\n",
      "Batch 135, Loss: -1.018157958984375\n",
      "tensor([0])\n",
      "tensor([[ 1.0517,  0.4113, -0.1520]], grad_fn=<AddmmBackward0>)\n",
      "Batch 136, Loss: -1.0516719818115234\n",
      "tensor([1])\n",
      "tensor([[ 1.0867,  0.4021, -0.1054]], grad_fn=<AddmmBackward0>)\n",
      "Batch 137, Loss: -0.4020935893058777\n",
      "tensor([0])\n",
      "tensor([[ 1.0896,  0.4015, -0.1016]], grad_fn=<AddmmBackward0>)\n",
      "Batch 138, Loss: -1.0895981788635254\n",
      "tensor([0])\n",
      "tensor([[ 1.0644,  0.4083, -0.1351]], grad_fn=<AddmmBackward0>)\n",
      "Batch 139, Loss: -1.0644209384918213\n",
      "tensor([2])\n",
      "tensor([[ 0.9955,  0.4268, -0.2269]], grad_fn=<AddmmBackward0>)\n",
      "Batch 140, Loss: 0.2269226312637329\n",
      "tensor([1])\n",
      "tensor([[ 1.0974,  0.3998, -0.0915]], grad_fn=<AddmmBackward0>)\n",
      "Batch 141, Loss: -0.3998388350009918\n",
      "tensor([2])\n",
      "tensor([[ 1.0221,  0.4200, -0.1917]], grad_fn=<AddmmBackward0>)\n",
      "Batch 142, Loss: 0.19166380167007446\n",
      "tensor([2])\n",
      "tensor([[ 1.0990,  0.3997, -0.0894]], grad_fn=<AddmmBackward0>)\n",
      "Batch 143, Loss: 0.08937454223632812\n",
      "tensor([0])\n",
      "tensor([[ 1.0837,  0.4039, -0.1098]], grad_fn=<AddmmBackward0>)\n",
      "Batch 144, Loss: -1.0836718082427979\n",
      "tensor([0])\n",
      "tensor([[ 1.0814,  0.4046, -0.1129]], grad_fn=<AddmmBackward0>)\n",
      "Batch 145, Loss: -1.0814001560211182\n",
      "tensor([0])\n",
      "tensor([[ 1.0959,  0.4009, -0.0937]], grad_fn=<AddmmBackward0>)\n",
      "Batch 146, Loss: -1.0959019660949707\n",
      "tensor([2])\n",
      "tensor([[ 1.0990,  0.4002, -0.0896]], grad_fn=<AddmmBackward0>)\n",
      "Batch 147, Loss: 0.0896376371383667\n",
      "tensor([2])\n",
      "tensor([[ 1.0835,  0.4044, -0.1103]], grad_fn=<AddmmBackward0>)\n",
      "Batch 148, Loss: 0.11030566692352295\n",
      "tensor([1])\n",
      "tensor([[ 1.0368,  0.4168, -0.1724]], grad_fn=<AddmmBackward0>)\n",
      "Batch 149, Loss: -0.4168243408203125\n",
      "tensor([1])\n",
      "tensor([[ 1.0818,  0.4050, -0.1126]], grad_fn=<AddmmBackward0>)\n",
      "Batch 150, Loss: -0.4050149619579315\n",
      "tensor([2])\n",
      "tensor([[ 1.0728,  0.4075, -0.1246]], grad_fn=<AddmmBackward0>)\n",
      "Batch 151, Loss: 0.12459874153137207\n",
      "tensor([2])\n",
      "tensor([[ 1.0999,  0.4005, -0.0885]], grad_fn=<AddmmBackward0>)\n",
      "Batch 152, Loss: 0.088520348072052\n",
      "tensor([2])\n",
      "tensor([[ 1.1016,  0.4001, -0.0863]], grad_fn=<AddmmBackward0>)\n",
      "Batch 153, Loss: 0.0862550139427185\n",
      "tensor([2])\n",
      "tensor([[ 1.1045,  0.3995, -0.0824]], grad_fn=<AddmmBackward0>)\n",
      "Batch 154, Loss: 0.08238798379898071\n",
      "tensor([0])\n",
      "tensor([[ 1.0068,  0.4254, -0.2122]], grad_fn=<AddmmBackward0>)\n",
      "Batch 155, Loss: -1.006761074066162\n",
      "tensor([1])\n",
      "tensor([[ 1.0281,  0.4198, -0.1838]], grad_fn=<AddmmBackward0>)\n",
      "Batch 156, Loss: -0.41984987258911133\n",
      "tensor([0])\n",
      "tensor([[ 1.0666,  0.4098, -0.1325]], grad_fn=<AddmmBackward0>)\n",
      "Batch 157, Loss: -1.0665946006774902\n",
      "tensor([1])\n",
      "tensor([[ 1.1028,  0.4003, -0.0844]], grad_fn=<AddmmBackward0>)\n",
      "Batch 158, Loss: -0.40033891797065735\n",
      "tensor([0])\n",
      "tensor([[ 1.0922,  0.4033, -0.0985]], grad_fn=<AddmmBackward0>)\n",
      "Batch 159, Loss: -1.0922207832336426\n",
      "tensor([2])\n",
      "tensor([[ 1.0980,  0.4019, -0.0908]], grad_fn=<AddmmBackward0>)\n",
      "Batch 160, Loss: 0.09080469608306885\n",
      "tensor([2])\n",
      "tensor([[ 1.0075,  0.4259, -0.2112]], grad_fn=<AddmmBackward0>)\n",
      "Batch 161, Loss: 0.21121746301651\n",
      "tensor([2])\n",
      "tensor([[ 1.0834,  0.4059, -0.1103]], grad_fn=<AddmmBackward0>)\n",
      "Batch 162, Loss: 0.11028754711151123\n",
      "tensor([2])\n",
      "tensor([[ 1.0890,  0.4046, -0.1027]], grad_fn=<AddmmBackward0>)\n",
      "Batch 163, Loss: 0.10272479057312012\n",
      "tensor([0])\n",
      "tensor([[ 1.0771,  0.4078, -0.1185]], grad_fn=<AddmmBackward0>)\n",
      "Batch 164, Loss: -1.0770759582519531\n",
      "tensor([1])\n",
      "tensor([[ 1.1000,  0.4018, -0.0880]], grad_fn=<AddmmBackward0>)\n",
      "Batch 165, Loss: -0.4018329977989197\n",
      "tensor([2])\n",
      "tensor([[ 1.1022,  0.4014, -0.0850]], grad_fn=<AddmmBackward0>)\n",
      "Batch 166, Loss: 0.0850178599357605\n",
      "tensor([1])\n",
      "tensor([[ 0.8604,  0.4651, -0.4065]], grad_fn=<AddmmBackward0>)\n",
      "Batch 167, Loss: -0.4650835394859314\n",
      "tensor([2])\n",
      "tensor([[ 1.0762,  0.4084, -0.1195]], grad_fn=<AddmmBackward0>)\n",
      "Batch 168, Loss: 0.11953270435333252\n",
      "tensor([2])\n",
      "tensor([[ 1.0818,  0.4070, -0.1120]], grad_fn=<AddmmBackward0>)\n",
      "Batch 169, Loss: 0.11197441816329956\n",
      "tensor([1])\n",
      "tensor([[ 1.0967,  0.4032, -0.0921]], grad_fn=<AddmmBackward0>)\n",
      "Batch 170, Loss: -0.4032285809516907\n",
      "tensor([1])\n",
      "tensor([[ 1.0766,  0.4086, -0.1188]], grad_fn=<AddmmBackward0>)\n",
      "Batch 171, Loss: -0.4086328148841858\n",
      "tensor([1])\n",
      "tensor([[ 1.1015,  0.4022, -0.0855]], grad_fn=<AddmmBackward0>)\n",
      "Batch 172, Loss: -0.4022212028503418\n",
      "tensor([2])\n",
      "tensor([[ 1.0809,  0.4078, -0.1129]], grad_fn=<AddmmBackward0>)\n",
      "Batch 173, Loss: 0.11287134885787964\n",
      "tensor([0])\n",
      "tensor([[ 1.0763,  0.4091, -0.1188]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 174, Loss: -1.0763345956802368\n",
      "tensor([1])\n",
      "tensor([[ 1.0401,  0.4188, -0.1670]], grad_fn=<AddmmBackward0>)\n",
      "Batch 175, Loss: -0.4187837839126587\n",
      "tensor([2])\n",
      "tensor([[ 1.1044,  0.4021, -0.0814]], grad_fn=<AddmmBackward0>)\n",
      "Batch 176, Loss: 0.08136385679244995\n",
      "tensor([2])\n",
      "tensor([[ 1.1034,  0.4025, -0.0826]], grad_fn=<AddmmBackward0>)\n",
      "Batch 177, Loss: 0.0825965404510498\n",
      "tensor([1])\n",
      "tensor([[ 1.1044,  0.4023, -0.0812]], grad_fn=<AddmmBackward0>)\n",
      "Batch 178, Loss: -0.4023370146751404\n",
      "tensor([1])\n",
      "tensor([[ 1.1048,  0.4024, -0.0806]], grad_fn=<AddmmBackward0>)\n",
      "Batch 179, Loss: -0.4023873209953308\n",
      "tensor([2])\n",
      "tensor([[ 1.0844,  0.4079, -0.1077]], grad_fn=<AddmmBackward0>)\n",
      "Batch 180, Loss: 0.10769683122634888\n",
      "tensor([1])\n",
      "tensor([[ 1.1001,  0.4039, -0.0867]], grad_fn=<AddmmBackward0>)\n",
      "Batch 181, Loss: -0.4039410948753357\n",
      "tensor([0])\n",
      "tensor([[ 1.0546,  0.4160, -0.1471]], grad_fn=<AddmmBackward0>)\n",
      "Batch 182, Loss: -1.0545883178710938\n",
      "tensor([0])\n",
      "tensor([[ 1.1057,  0.4028, -0.0791]], grad_fn=<AddmmBackward0>)\n",
      "Batch 183, Loss: -1.1056811809539795\n",
      "tensor([0])\n",
      "tensor([[ 1.1004,  0.4043, -0.0861]], grad_fn=<AddmmBackward0>)\n",
      "Batch 184, Loss: -1.1004469394683838\n",
      "tensor([2])\n",
      "tensor([[ 1.0663,  0.4134, -0.1316]], grad_fn=<AddmmBackward0>)\n",
      "Batch 185, Loss: 0.13160520792007446\n",
      "tensor([0])\n",
      "tensor([[ 1.1035,  0.4038, -0.0821]], grad_fn=<AddmmBackward0>)\n",
      "Batch 186, Loss: -1.1034839153289795\n",
      "tensor([1])\n",
      "tensor([[ 1.1087,  0.4026, -0.0752]], grad_fn=<AddmmBackward0>)\n",
      "Batch 187, Loss: -0.40259379148483276\n",
      "tensor([2])\n",
      "tensor([[ 1.0336,  0.4223, -0.1752]], grad_fn=<AddmmBackward0>)\n",
      "Batch 188, Loss: 0.1752302050590515\n",
      "tensor([1])\n",
      "tensor([[ 0.8973,  0.4579, -0.3567]], grad_fn=<AddmmBackward0>)\n",
      "Batch 189, Loss: -0.4579499065876007\n",
      "tensor([1])\n",
      "tensor([[ 1.0856,  0.4090, -0.1060]], grad_fn=<AddmmBackward0>)\n",
      "Batch 190, Loss: -0.4090326428413391\n",
      "tensor([1])\n",
      "tensor([[ 1.1041,  0.4044, -0.0815]], grad_fn=<AddmmBackward0>)\n",
      "Batch 191, Loss: -0.40440768003463745\n",
      "tensor([2])\n",
      "tensor([[ 1.0996,  0.4057, -0.0874]], grad_fn=<AddmmBackward0>)\n",
      "Batch 192, Loss: 0.08737236261367798\n",
      "tensor([2])\n",
      "tensor([[ 1.1049,  0.4045, -0.0804]], grad_fn=<AddmmBackward0>)\n",
      "Batch 193, Loss: 0.08038932085037231\n",
      "tensor([0])\n",
      "tensor([[ 1.0500,  0.4189, -0.1533]], grad_fn=<AddmmBackward0>)\n",
      "Batch 194, Loss: -1.0500423908233643\n",
      "tensor([2])\n",
      "tensor([[ 1.0882,  0.4092, -0.1025]], grad_fn=<AddmmBackward0>)\n",
      "Batch 195, Loss: 0.10253804922103882\n",
      "tensor([2])\n",
      "tensor([[ 1.1063,  0.4046, -0.0784]], grad_fn=<AddmmBackward0>)\n",
      "Batch 196, Loss: 0.07841753959655762\n",
      "tensor([1])\n",
      "tensor([[ 1.0942,  0.4079, -0.0945]], grad_fn=<AddmmBackward0>)\n",
      "Batch 197, Loss: -0.40786588191986084\n",
      "tensor([1])\n",
      "tensor([[ 1.0364,  0.4230, -0.1714]], grad_fn=<AddmmBackward0>)\n",
      "Batch 198, Loss: -0.4229980707168579\n",
      "tensor([1])\n",
      "tensor([[ 1.0675,  0.4151, -0.1298]], grad_fn=<AddmmBackward0>)\n",
      "Batch 199, Loss: -0.4150693416595459\n",
      "tensor([2])\n",
      "tensor([[ 1.1086,  0.4046, -0.0750]], grad_fn=<AddmmBackward0>)\n",
      "Batch 200, Loss: 0.07503396272659302\n",
      "tensor([0])\n",
      "tensor([[ 1.1027,  0.4063, -0.0829]], grad_fn=<AddmmBackward0>)\n",
      "Batch 201, Loss: -1.1026893854141235\n",
      "tensor([1])\n",
      "tensor([[ 1.1044,  0.4060, -0.0806]], grad_fn=<AddmmBackward0>)\n",
      "Batch 202, Loss: -0.40599143505096436\n",
      "tensor([1])\n",
      "tensor([[ 1.1028,  0.4066, -0.0826]], grad_fn=<AddmmBackward0>)\n",
      "Batch 203, Loss: -0.4065682888031006\n",
      "tensor([0])\n",
      "tensor([[ 1.0413,  0.4227, -0.1645]], grad_fn=<AddmmBackward0>)\n",
      "Batch 204, Loss: -1.0413012504577637\n",
      "tensor([1])\n",
      "tensor([[ 1.1044,  0.4065, -0.0805]], grad_fn=<AddmmBackward0>)\n",
      "Batch 205, Loss: -0.4065181016921997\n",
      "tensor([2])\n",
      "tensor([[ 0.8867,  0.4629, -0.3704]], grad_fn=<AddmmBackward0>)\n",
      "Batch 206, Loss: 0.3703640401363373\n",
      "tensor([1])\n",
      "tensor([[ 1.1089,  0.4057, -0.0746]], grad_fn=<AddmmBackward0>)\n",
      "Batch 207, Loss: -0.40573954582214355\n",
      "tensor([2])\n",
      "tensor([[ 1.0981,  0.4087, -0.0890]], grad_fn=<AddmmBackward0>)\n",
      "Batch 208, Loss: 0.08900564908981323\n",
      "tensor([0])\n",
      "tensor([[ 1.0799,  0.4136, -0.1131]], grad_fn=<AddmmBackward0>)\n",
      "Batch 209, Loss: -1.0799072980880737\n",
      "tensor([0])\n",
      "tensor([[ 1.0703,  0.4162, -0.1259]], grad_fn=<AddmmBackward0>)\n",
      "Batch 210, Loss: -1.070343255996704\n",
      "tensor([2])\n",
      "tensor([[ 1.0130,  0.4311, -0.2022]], grad_fn=<AddmmBackward0>)\n",
      "Batch 211, Loss: 0.20223593711853027\n",
      "tensor([1])\n",
      "tensor([[ 1.1090,  0.4065, -0.0745]], grad_fn=<AddmmBackward0>)\n",
      "Batch 212, Loss: -0.4065336585044861\n",
      "tensor([0])\n",
      "tensor([[ 1.0537,  0.4209, -0.1481]], grad_fn=<AddmmBackward0>)\n",
      "Batch 213, Loss: -1.0537173748016357\n",
      "tensor([1])\n",
      "tensor([[ 1.1080,  0.4071, -0.0759]], grad_fn=<AddmmBackward0>)\n",
      "Batch 214, Loss: -0.40709391236305237\n",
      "tensor([1])\n",
      "tensor([[ 0.9942,  0.4365, -0.2274]], grad_fn=<AddmmBackward0>)\n",
      "Batch 215, Loss: -0.436490923166275\n",
      "tensor([0])\n",
      "tensor([[ 1.1081,  0.4074, -0.0758]], grad_fn=<AddmmBackward0>)\n",
      "Batch 216, Loss: -1.1081202030181885\n",
      "tensor([2])\n",
      "tensor([[ 1.1054,  0.4083, -0.0796]], grad_fn=<AddmmBackward0>)\n",
      "Batch 217, Loss: 0.07959914207458496\n",
      "tensor([2])\n",
      "tensor([[ 1.0819,  0.4145, -0.1109]], grad_fn=<AddmmBackward0>)\n",
      "Batch 218, Loss: 0.11085492372512817\n",
      "tensor([1])\n",
      "tensor([[ 1.0292,  0.4281, -0.1810]], grad_fn=<AddmmBackward0>)\n",
      "Batch 219, Loss: -0.42811545729637146\n",
      "tensor([2])\n",
      "tensor([[ 1.0917,  0.4123, -0.0979]], grad_fn=<AddmmBackward0>)\n",
      "Batch 220, Loss: 0.09789544343948364\n",
      "tensor([2])\n",
      "tensor([[ 1.0817,  0.4150, -0.1111]], grad_fn=<AddmmBackward0>)\n",
      "Batch 221, Loss: 0.11110538244247437\n",
      "tensor([0])\n",
      "tensor([[ 1.0738,  0.4171, -0.1216]], grad_fn=<AddmmBackward0>)\n",
      "Batch 222, Loss: -1.0737515687942505\n",
      "tensor([0])\n",
      "tensor([[ 1.0896,  0.4132, -0.1006]], grad_fn=<AddmmBackward0>)\n",
      "Batch 223, Loss: -1.089550495147705\n",
      "tensor([2])\n",
      "tensor([[ 1.0975,  0.4113, -0.0900]], grad_fn=<AddmmBackward0>)\n",
      "Batch 224, Loss: 0.08999258279800415\n",
      "tensor([0])\n",
      "tensor([[ 1.0972,  0.4115, -0.0905]], grad_fn=<AddmmBackward0>)\n",
      "Batch 225, Loss: -1.0971930027008057\n",
      "tensor([0])\n",
      "tensor([[ 1.1046,  0.4097, -0.0806]], grad_fn=<AddmmBackward0>)\n",
      "Batch 226, Loss: -1.1046065092086792\n",
      "tensor([0])\n",
      "tensor([[ 1.0625,  0.4205, -0.1368]], grad_fn=<AddmmBackward0>)\n",
      "Batch 227, Loss: -1.0625088214874268\n",
      "tensor([0])\n",
      "tensor([[ 1.0067,  0.4349, -0.2112]], grad_fn=<AddmmBackward0>)\n",
      "Batch 228, Loss: -1.006666898727417\n",
      "tensor([1])\n",
      "tensor([[ 1.1050,  0.4099, -0.0805]], grad_fn=<AddmmBackward0>)\n",
      "Batch 229, Loss: -0.40990644693374634\n",
      "tensor([1])\n",
      "tensor([[ 1.0804,  0.4163, -0.1134]], grad_fn=<AddmmBackward0>)\n",
      "Batch 230, Loss: -0.4163018763065338\n",
      "tensor([1])\n",
      "tensor([[ 1.1011,  0.4112, -0.0859]], grad_fn=<AddmmBackward0>)\n",
      "Batch 231, Loss: -0.41115492582321167\n",
      "tensor([0])\n",
      "tensor([[ 1.0994,  0.4118, -0.0883]], grad_fn=<AddmmBackward0>)\n",
      "Batch 232, Loss: -1.099410057067871\n",
      "tensor([0])\n",
      "tensor([[ 1.0825,  0.4162, -0.1109]], grad_fn=<AddmmBackward0>)\n",
      "Batch 233, Loss: -1.0825252532958984\n",
      "tensor([0])\n",
      "tensor([[ 1.1045,  0.4108, -0.0818]], grad_fn=<AddmmBackward0>)\n",
      "Batch 234, Loss: -1.1045165061950684\n",
      "tensor([1])\n",
      "tensor([[ 1.1039,  0.4111, -0.0828]], grad_fn=<AddmmBackward0>)\n",
      "Batch 235, Loss: -0.4110541343688965\n",
      "tensor([0])\n",
      "tensor([[ 1.1023,  0.4116, -0.0852]], grad_fn=<AddmmBackward0>)\n",
      "Batch 236, Loss: -1.1022882461547852\n",
      "tensor([1])\n",
      "tensor([[ 1.1107,  0.4097, -0.0742]], grad_fn=<AddmmBackward0>)\n",
      "Batch 237, Loss: -0.4096585810184479\n",
      "tensor([0])\n",
      "tensor([[ 1.0848,  0.4164, -0.1088]], grad_fn=<AddmmBackward0>)\n",
      "Batch 238, Loss: -1.0848376750946045\n",
      "tensor([1])\n",
      "tensor([[ 1.0605,  0.4227, -0.1413]], grad_fn=<AddmmBackward0>)\n",
      "Batch 239, Loss: -0.4227360486984253\n",
      "tensor([2])\n",
      "tensor([[ 1.1021,  0.4124, -0.0862]], grad_fn=<AddmmBackward0>)\n",
      "Batch 240, Loss: 0.0862160325050354\n",
      "tensor([0])\n",
      "tensor([[ 1.1076,  0.4111, -0.0790]], grad_fn=<AddmmBackward0>)\n",
      "Batch 241, Loss: -1.107628345489502\n",
      "tensor([0])\n",
      "tensor([[ 1.0497,  0.4259, -0.1561]], grad_fn=<AddmmBackward0>)\n",
      "Batch 242, Loss: -1.0497428178787231\n",
      "tensor([2])\n",
      "tensor([[ 1.1063,  0.4118, -0.0811]], grad_fn=<AddmmBackward0>)\n",
      "Batch 243, Loss: 0.0811346173286438\n",
      "tensor([2])\n",
      "tensor([[ 1.1157,  0.4095, -0.0688]], grad_fn=<AddmmBackward0>)\n",
      "Batch 244, Loss: 0.06876415014266968\n",
      "tensor([0])\n",
      "tensor([[ 1.0389,  0.4291, -0.1708]], grad_fn=<AddmmBackward0>)\n",
      "Batch 245, Loss: -1.0389485359191895\n",
      "tensor([0])\n",
      "tensor([[ 1.1003,  0.4136, -0.0895]], grad_fn=<AddmmBackward0>)\n",
      "Batch 246, Loss: -1.100315809249878\n",
      "tensor([1])\n",
      "tensor([[ 1.0689,  0.4217, -0.1313]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 247, Loss: -0.4216882586479187\n",
      "tensor([0])\n",
      "tensor([[ 1.0704,  0.4214, -0.1295]], grad_fn=<AddmmBackward0>)\n",
      "Batch 248, Loss: -1.07040274143219\n",
      "tensor([0])\n",
      "tensor([[ 1.0996,  0.4142, -0.0909]], grad_fn=<AddmmBackward0>)\n",
      "Batch 249, Loss: -1.099618673324585\n",
      "tensor([2])\n",
      "tensor([[ 1.0716,  0.4214, -0.1282]], grad_fn=<AddmmBackward0>)\n",
      "Batch 250, Loss: 0.12821239233016968\n",
      "tensor([1])\n",
      "tensor([[ 1.0703,  0.4218, -0.1301]], grad_fn=<AddmmBackward0>)\n",
      "Batch 251, Loss: -0.42182403802871704\n",
      "tensor([1])\n",
      "tensor([[ 1.0992,  0.4147, -0.0919]], grad_fn=<AddmmBackward0>)\n",
      "Batch 252, Loss: -0.41466495394706726\n",
      "tensor([2])\n",
      "tensor([[ 1.0903,  0.4171, -0.1039]], grad_fn=<AddmmBackward0>)\n",
      "Batch 253, Loss: 0.10385394096374512\n",
      "tensor([1])\n",
      "tensor([[ 1.0748,  0.4211, -0.1245]], grad_fn=<AddmmBackward0>)\n",
      "Batch 254, Loss: -0.4211121201515198\n",
      "tensor([0])\n",
      "tensor([[ 1.1107,  0.4122, -0.0768]], grad_fn=<AddmmBackward0>)\n",
      "Batch 255, Loss: -1.1107487678527832\n",
      "tensor([0])\n",
      "tensor([[ 1.1175,  0.4106, -0.0679]], grad_fn=<AddmmBackward0>)\n",
      "Batch 256, Loss: -1.1175477504730225\n",
      "tensor([1])\n",
      "tensor([[ 1.0591,  0.4255, -0.1456]], grad_fn=<AddmmBackward0>)\n",
      "Batch 257, Loss: -0.4254898428916931\n",
      "tensor([1])\n",
      "tensor([[ 1.0548,  0.4267, -0.1514]], grad_fn=<AddmmBackward0>)\n",
      "Batch 258, Loss: -0.4267352819442749\n",
      "tensor([0])\n",
      "tensor([[ 1.0745,  0.4219, -0.1254]], grad_fn=<AddmmBackward0>)\n",
      "Batch 259, Loss: -1.0745176076889038\n",
      "tensor([2])\n",
      "tensor([[ 1.0469,  0.4291, -0.1622]], grad_fn=<AddmmBackward0>)\n",
      "Batch 260, Loss: 0.16218435764312744\n",
      "tensor([0])\n",
      "tensor([[ 1.1076,  0.4139, -0.0817]], grad_fn=<AddmmBackward0>)\n",
      "Batch 261, Loss: -1.1076195240020752\n",
      "tensor([1])\n",
      "tensor([[ 1.1143,  0.4124, -0.0730]], grad_fn=<AddmmBackward0>)\n",
      "Batch 262, Loss: -0.41241562366485596\n",
      "tensor([2])\n",
      "tensor([[ 1.0704,  0.4236, -0.1313]], grad_fn=<AddmmBackward0>)\n",
      "Batch 263, Loss: 0.1313467025756836\n",
      "tensor([1])\n",
      "tensor([[ 1.1064,  0.4147, -0.0837]], grad_fn=<AddmmBackward0>)\n",
      "Batch 264, Loss: -0.41471415758132935\n",
      "tensor([1])\n",
      "tensor([[ 1.0901,  0.4190, -0.1053]], grad_fn=<AddmmBackward0>)\n",
      "Batch 265, Loss: -0.41895851492881775\n",
      "tensor([0])\n",
      "tensor([[ 1.1135,  0.4133, -0.0744]], grad_fn=<AddmmBackward0>)\n",
      "Batch 266, Loss: -1.1134778261184692\n",
      "tensor([2])\n",
      "tensor([[ 1.0895,  0.4195, -0.1063]], grad_fn=<AddmmBackward0>)\n",
      "Batch 267, Loss: 0.10627096891403198\n",
      "tensor([0])\n",
      "tensor([[ 1.1148,  0.4133, -0.0728]], grad_fn=<AddmmBackward0>)\n",
      "Batch 268, Loss: -1.1147668361663818\n",
      "tensor([0])\n",
      "tensor([[ 1.1012,  0.4168, -0.0909]], grad_fn=<AddmmBackward0>)\n",
      "Batch 269, Loss: -1.1011927127838135\n",
      "tensor([0])\n",
      "tensor([[ 1.0902,  0.4197, -0.1056]], grad_fn=<AddmmBackward0>)\n",
      "Batch 270, Loss: -1.0902159214019775\n",
      "tensor([0])\n",
      "tensor([[ 1.0950,  0.4187, -0.0994]], grad_fn=<AddmmBackward0>)\n",
      "Batch 271, Loss: -1.094995141029358\n",
      "tensor([0])\n",
      "tensor([[ 1.0910,  0.4198, -0.1049]], grad_fn=<AddmmBackward0>)\n",
      "Batch 272, Loss: -1.0909838676452637\n",
      "tensor([1])\n",
      "tensor([[ 1.0953,  0.4188, -0.0995]], grad_fn=<AddmmBackward0>)\n",
      "Batch 273, Loss: -0.41884666681289673\n",
      "tensor([0])\n",
      "tensor([[ 1.1154,  0.4140, -0.0730]], grad_fn=<AddmmBackward0>)\n",
      "Batch 274, Loss: -1.115391492843628\n",
      "tensor([2])\n",
      "tensor([[ 1.0932,  0.4196, -0.1026]], grad_fn=<AddmmBackward0>)\n",
      "Batch 275, Loss: 0.10257434844970703\n",
      "tensor([0])\n",
      "tensor([[ 1.0836,  0.4222, -0.1155]], grad_fn=<AddmmBackward0>)\n",
      "Batch 276, Loss: -1.083603858947754\n",
      "tensor([0])\n",
      "tensor([[ 1.1104,  0.4156, -0.0802]], grad_fn=<AddmmBackward0>)\n",
      "Batch 277, Loss: -1.1104077100753784\n",
      "tensor([2])\n",
      "tensor([[ 1.1153,  0.4145, -0.0739]], grad_fn=<AddmmBackward0>)\n",
      "Batch 278, Loss: 0.07389509677886963\n",
      "tensor([1])\n",
      "tensor([[ 1.1135,  0.4151, -0.0765]], grad_fn=<AddmmBackward0>)\n",
      "Batch 279, Loss: -0.41506701707839966\n",
      "tensor([1])\n",
      "tensor([[ 1.1238,  0.4126, -0.0630]], grad_fn=<AddmmBackward0>)\n",
      "Batch 280, Loss: -0.4126236140727997\n",
      "tensor([1])\n",
      "tensor([[ 1.1074,  0.4169, -0.0848]], grad_fn=<AddmmBackward0>)\n",
      "Batch 281, Loss: -0.4168532192707062\n",
      "tensor([1])\n",
      "tensor([[ 1.1221,  0.4134, -0.0655]], grad_fn=<AddmmBackward0>)\n",
      "Batch 282, Loss: -0.41338226199150085\n",
      "tensor([2])\n",
      "tensor([[ 1.1205,  0.4139, -0.0676]], grad_fn=<AddmmBackward0>)\n",
      "Batch 283, Loss: 0.06763201951980591\n",
      "tensor([2])\n",
      "tensor([[ 0.9992,  0.4442, -0.2283]], grad_fn=<AddmmBackward0>)\n",
      "Batch 284, Loss: 0.22834163904190063\n",
      "tensor([2])\n",
      "tensor([[ 1.1141,  0.4159, -0.0762]], grad_fn=<AddmmBackward0>)\n",
      "Batch 285, Loss: 0.0762220025062561\n",
      "tensor([0])\n",
      "tensor([[ 1.0801,  0.4244, -0.1213]], grad_fn=<AddmmBackward0>)\n",
      "Batch 286, Loss: -1.0800504684448242\n",
      "tensor([0])\n",
      "tensor([[ 1.1007,  0.4194, -0.0939]], grad_fn=<AddmmBackward0>)\n",
      "Batch 287, Loss: -1.1007421016693115\n",
      "tensor([1])\n",
      "tensor([[ 1.1019,  0.4193, -0.0925]], grad_fn=<AddmmBackward0>)\n",
      "Batch 288, Loss: -0.4192913770675659\n",
      "tensor([2])\n",
      "tensor([[ 1.1111,  0.4171, -0.0803]], grad_fn=<AddmmBackward0>)\n",
      "Batch 289, Loss: 0.08030486106872559\n",
      "tensor([2])\n",
      "tensor([[ 1.1224,  0.4145, -0.0655]], grad_fn=<AddmmBackward0>)\n",
      "Batch 290, Loss: 0.06548929214477539\n",
      "tensor([0])\n",
      "tensor([[ 1.1126,  0.4170, -0.0784]], grad_fn=<AddmmBackward0>)\n",
      "Batch 291, Loss: -1.11256742477417\n",
      "tensor([0])\n",
      "tensor([[ 1.1034,  0.4194, -0.0906]], grad_fn=<AddmmBackward0>)\n",
      "Batch 292, Loss: -1.1034191846847534\n",
      "tensor([2])\n",
      "tensor([[ 1.1025,  0.4198, -0.0919]], grad_fn=<AddmmBackward0>)\n",
      "Batch 293, Loss: 0.09192526340484619\n",
      "tensor([2])\n",
      "tensor([[ 1.0469,  0.4336, -0.1655]], grad_fn=<AddmmBackward0>)\n",
      "Batch 294, Loss: 0.16546612977981567\n",
      "tensor([2])\n",
      "tensor([[ 1.0792,  0.4257, -0.1228]], grad_fn=<AddmmBackward0>)\n",
      "Batch 295, Loss: 0.1227760910987854\n",
      "tensor([1])\n",
      "tensor([[ 1.1204,  0.4156, -0.0681]], grad_fn=<AddmmBackward0>)\n",
      "Batch 296, Loss: -0.41559484601020813\n",
      "tensor([1])\n",
      "tensor([[ 1.1208,  0.4156, -0.0676]], grad_fn=<AddmmBackward0>)\n",
      "Batch 297, Loss: -0.41561245918273926\n",
      "tensor([2])\n",
      "tensor([[ 1.1105,  0.4183, -0.0812]], grad_fn=<AddmmBackward0>)\n",
      "Batch 298, Loss: 0.08124339580535889\n",
      "tensor([0])\n",
      "tensor([[ 1.1174,  0.4167, -0.0721]], grad_fn=<AddmmBackward0>)\n",
      "Batch 299, Loss: -1.1173534393310547\n",
      "tensor([1])\n",
      "tensor([[ 1.0993,  0.4213, -0.0960]], grad_fn=<AddmmBackward0>)\n",
      "Batch 300, Loss: -0.4212803244590759\n",
      "tensor([2])\n",
      "tensor([[ 1.0732,  0.4278, -0.1305]], grad_fn=<AddmmBackward0>)\n",
      "Batch 301, Loss: 0.13053464889526367\n",
      "tensor([1])\n",
      "tensor([[ 1.1266,  0.4148, -0.0597]], grad_fn=<AddmmBackward0>)\n",
      "Batch 302, Loss: -0.4147983491420746\n",
      "tensor([2])\n",
      "tensor([[ 1.1069,  0.4198, -0.0858]], grad_fn=<AddmmBackward0>)\n",
      "Batch 303, Loss: 0.08583170175552368\n",
      "tensor([0])\n",
      "tensor([[ 1.0694,  0.4292, -0.1354]], grad_fn=<AddmmBackward0>)\n",
      "Batch 304, Loss: -1.0694376230239868\n",
      "tensor([1])\n",
      "tensor([[ 0.9219,  0.4656, -0.3307]], grad_fn=<AddmmBackward0>)\n",
      "Batch 305, Loss: -0.4655739665031433\n",
      "tensor([1])\n",
      "tensor([[ 1.1115,  0.4191, -0.0796]], grad_fn=<AddmmBackward0>)\n",
      "Batch 306, Loss: -0.4190629720687866\n",
      "tensor([1])\n",
      "tensor([[ 1.0810,  0.4267, -0.1200]], grad_fn=<AddmmBackward0>)\n",
      "Batch 307, Loss: -0.4267174005508423\n",
      "tensor([2])\n",
      "tensor([[ 1.1259,  0.4159, -0.0605]], grad_fn=<AddmmBackward0>)\n",
      "Batch 308, Loss: 0.06051599979400635\n",
      "tensor([2])\n",
      "tensor([[ 1.1246,  0.4163, -0.0622]], grad_fn=<AddmmBackward0>)\n",
      "Batch 309, Loss: 0.06223571300506592\n",
      "tensor([0])\n",
      "tensor([[ 1.1079,  0.4206, -0.0843]], grad_fn=<AddmmBackward0>)\n",
      "Batch 310, Loss: -1.107905626296997\n",
      "tensor([2])\n",
      "tensor([[ 1.1172,  0.4185, -0.0720]], grad_fn=<AddmmBackward0>)\n",
      "Batch 311, Loss: 0.07199782133102417\n",
      "tensor([1])\n",
      "tensor([[ 1.1125,  0.4197, -0.0781]], grad_fn=<AddmmBackward0>)\n",
      "Batch 312, Loss: -0.41971534490585327\n",
      "tensor([0])\n",
      "tensor([[ 1.1276,  0.4162, -0.0581]], grad_fn=<AddmmBackward0>)\n",
      "Batch 313, Loss: -1.1275520324707031\n",
      "tensor([1])\n",
      "tensor([[ 1.1090,  0.4208, -0.0826]], grad_fn=<AddmmBackward0>)\n",
      "Batch 314, Loss: -0.42084741592407227\n",
      "tensor([1])\n",
      "tensor([[ 1.1156,  0.4194, -0.0740]], grad_fn=<AddmmBackward0>)\n",
      "Batch 315, Loss: -0.41940823197364807\n",
      "tensor([2])\n",
      "tensor([[ 1.1202,  0.4185, -0.0679]], grad_fn=<AddmmBackward0>)\n",
      "Batch 316, Loss: 0.0678853988647461\n",
      "tensor([2])\n",
      "tensor([[ 1.1224,  0.4181, -0.0649]], grad_fn=<AddmmBackward0>)\n",
      "Batch 317, Loss: 0.06487423181533813\n",
      "tensor([1])\n",
      "tensor([[ 1.0892,  0.4263, -0.1088]], grad_fn=<AddmmBackward0>)\n",
      "Batch 318, Loss: -0.4263225197792053\n",
      "tensor([1])\n",
      "tensor([[ 1.1247,  0.4178, -0.0617]], grad_fn=<AddmmBackward0>)\n",
      "Batch 319, Loss: -0.41781580448150635\n",
      "tensor([0])\n",
      "tensor([[ 1.0998,  0.4241, -0.0947]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 320, Loss: -1.0997867584228516\n",
      "tensor([2])\n",
      "tensor([[ 1.1080,  0.4222, -0.0838]], grad_fn=<AddmmBackward0>)\n",
      "Batch 321, Loss: 0.08376067876815796\n",
      "tensor([1])\n",
      "tensor([[ 1.1273,  0.4177, -0.0583]], grad_fn=<AddmmBackward0>)\n",
      "Batch 322, Loss: -0.41769397258758545\n",
      "tensor([0])\n",
      "tensor([[ 1.0465,  0.4375, -0.1652]], grad_fn=<AddmmBackward0>)\n",
      "Batch 323, Loss: -1.0464876890182495\n",
      "tensor([2])\n",
      "tensor([[ 1.1207,  0.4196, -0.0670]], grad_fn=<AddmmBackward0>)\n",
      "Batch 324, Loss: 0.06699103116989136\n",
      "tensor([1])\n",
      "tensor([[ 0.9890,  0.4518, -0.2413]], grad_fn=<AddmmBackward0>)\n",
      "Batch 325, Loss: -0.4518185257911682\n",
      "tensor([0])\n",
      "tensor([[ 1.1136,  0.4217, -0.0762]], grad_fn=<AddmmBackward0>)\n",
      "Batch 326, Loss: -1.1136456727981567\n",
      "tensor([2])\n",
      "tensor([[ 1.1122,  0.4222, -0.0782]], grad_fn=<AddmmBackward0>)\n",
      "Batch 327, Loss: 0.07819139957427979\n",
      "tensor([1])\n",
      "tensor([[ 1.1289,  0.4182, -0.0560]], grad_fn=<AddmmBackward0>)\n",
      "Batch 328, Loss: -0.41823339462280273\n",
      "tensor([0])\n",
      "tensor([[ 1.1216,  0.4202, -0.0658]], grad_fn=<AddmmBackward0>)\n",
      "Batch 329, Loss: -1.1215641498565674\n",
      "tensor([0])\n",
      "tensor([[ 1.1178,  0.4212, -0.0708]], grad_fn=<AddmmBackward0>)\n",
      "Batch 330, Loss: -1.117789626121521\n",
      "tensor([0])\n",
      "tensor([[ 1.1231,  0.4201, -0.0639]], grad_fn=<AddmmBackward0>)\n",
      "Batch 331, Loss: -1.1230692863464355\n",
      "tensor([2])\n",
      "tensor([[ 1.1302,  0.4185, -0.0545]], grad_fn=<AddmmBackward0>)\n",
      "Batch 332, Loss: 0.05451446771621704\n",
      "tensor([0])\n",
      "tensor([[ 1.1139,  0.4226, -0.0761]], grad_fn=<AddmmBackward0>)\n",
      "Batch 333, Loss: -1.113945722579956\n",
      "tensor([2])\n",
      "tensor([[ 1.1196,  0.4213, -0.0688]], grad_fn=<AddmmBackward0>)\n",
      "Batch 334, Loss: 0.06881481409072876\n",
      "tensor([2])\n",
      "tensor([[ 1.0951,  0.4274, -0.1012]], grad_fn=<AddmmBackward0>)\n",
      "Batch 335, Loss: 0.1012144684791565\n",
      "tensor([0])\n",
      "tensor([[ 1.1161,  0.4224, -0.0735]], grad_fn=<AddmmBackward0>)\n",
      "Batch 336, Loss: -1.1160764694213867\n",
      "tensor([0])\n",
      "tensor([[ 1.0265,  0.4442, -0.1921]], grad_fn=<AddmmBackward0>)\n",
      "Batch 337, Loss: -1.0265345573425293\n",
      "tensor([0])\n",
      "tensor([[ 1.1237,  0.4207, -0.0636]], grad_fn=<AddmmBackward0>)\n",
      "Batch 338, Loss: -1.1236785650253296\n",
      "tensor([0])\n",
      "tensor([[ 1.1190,  0.4220, -0.0699]], grad_fn=<AddmmBackward0>)\n",
      "Batch 339, Loss: -1.119044303894043\n",
      "tensor([1])\n",
      "tensor([[ 1.1182,  0.4222, -0.0711]], grad_fn=<AddmmBackward0>)\n",
      "Batch 340, Loss: -0.4222358167171478\n",
      "tensor([2])\n",
      "tensor([[ 1.1285,  0.4199, -0.0576]], grad_fn=<AddmmBackward0>)\n",
      "Batch 341, Loss: 0.05764281749725342\n",
      "tensor([1])\n",
      "tensor([[ 1.0805,  0.4316, -0.1213]], grad_fn=<AddmmBackward0>)\n",
      "Batch 342, Loss: -0.4315720200538635\n",
      "tensor([1])\n",
      "tensor([[ 1.1189,  0.4224, -0.0706]], grad_fn=<AddmmBackward0>)\n",
      "Batch 343, Loss: -0.42242419719696045\n",
      "tensor([2])\n",
      "tensor([[ 1.1222,  0.4218, -0.0663]], grad_fn=<AddmmBackward0>)\n",
      "Batch 344, Loss: 0.06625831127166748\n",
      "tensor([1])\n",
      "tensor([[ 1.1203,  0.4224, -0.0689]], grad_fn=<AddmmBackward0>)\n",
      "Batch 345, Loss: -0.42236754298210144\n",
      "tensor([2])\n",
      "tensor([[ 1.1259,  0.4212, -0.0614]], grad_fn=<AddmmBackward0>)\n",
      "Batch 346, Loss: 0.06144571304321289\n",
      "tensor([0])\n",
      "tensor([[ 1.0870,  0.4307, -0.1129]], grad_fn=<AddmmBackward0>)\n",
      "Batch 347, Loss: -1.087026596069336\n",
      "tensor([0])\n",
      "tensor([[ 1.1317,  0.4200, -0.0539]], grad_fn=<AddmmBackward0>)\n",
      "Batch 348, Loss: -1.1316843032836914\n",
      "tensor([1])\n",
      "tensor([[ 1.1223,  0.4224, -0.0664]], grad_fn=<AddmmBackward0>)\n",
      "Batch 349, Loss: -0.42244648933410645\n",
      "tensor([2])\n",
      "tensor([[ 1.1317,  0.4203, -0.0540]], grad_fn=<AddmmBackward0>)\n",
      "Batch 350, Loss: 0.054007649421691895\n",
      "tensor([2])\n",
      "tensor([[ 1.0518,  0.4397, -0.1598]], grad_fn=<AddmmBackward0>)\n",
      "Batch 351, Loss: 0.15977299213409424\n",
      "tensor([0])\n",
      "tensor([[ 1.1244,  0.4223, -0.0637]], grad_fn=<AddmmBackward0>)\n",
      "Batch 352, Loss: -1.1244125366210938\n",
      "tensor([1])\n",
      "tensor([[ 1.1299,  0.4211, -0.0564]], grad_fn=<AddmmBackward0>)\n",
      "Batch 353, Loss: -0.4211185574531555\n",
      "tensor([2])\n",
      "tensor([[ 1.0644,  0.4370, -0.1431]], grad_fn=<AddmmBackward0>)\n",
      "Batch 354, Loss: 0.14311933517456055\n",
      "tensor([1])\n",
      "tensor([[ 1.1318,  0.4209, -0.0540]], grad_fn=<AddmmBackward0>)\n",
      "Batch 355, Loss: -0.4209441840648651\n",
      "tensor([2])\n",
      "tensor([[ 1.1327,  0.4209, -0.0528]], grad_fn=<AddmmBackward0>)\n",
      "Batch 356, Loss: 0.052773892879486084\n",
      "tensor([0])\n",
      "tensor([[ 1.0682,  0.4365, -0.1380]], grad_fn=<AddmmBackward0>)\n",
      "Batch 357, Loss: -1.068235158920288\n",
      "tensor([2])\n",
      "tensor([[ 1.0571,  0.4392, -0.1527]], grad_fn=<AddmmBackward0>)\n",
      "Batch 358, Loss: 0.1527348756790161\n",
      "tensor([2])\n",
      "tensor([[ 1.0618,  0.4382, -0.1465]], grad_fn=<AddmmBackward0>)\n",
      "Batch 359, Loss: 0.14651167392730713\n",
      "tensor([0])\n",
      "tensor([[ 1.1129,  0.4261, -0.0788]], grad_fn=<AddmmBackward0>)\n",
      "Batch 360, Loss: -1.1129355430603027\n",
      "tensor([2])\n",
      "tensor([[ 1.1328,  0.4214, -0.0526]], grad_fn=<AddmmBackward0>)\n",
      "Batch 361, Loss: 0.05260986089706421\n",
      "tensor([1])\n",
      "tensor([[ 1.0947,  0.4306, -0.1030]], grad_fn=<AddmmBackward0>)\n",
      "Batch 362, Loss: -0.43064647912979126\n",
      "tensor([0])\n",
      "tensor([[ 1.0750,  0.4355, -0.1290]], grad_fn=<AddmmBackward0>)\n",
      "Batch 363, Loss: -1.0749506950378418\n",
      "tensor([2])\n",
      "tensor([[ 1.1250,  0.4236, -0.0628]], grad_fn=<AddmmBackward0>)\n",
      "Batch 364, Loss: 0.0628167986869812\n",
      "tensor([0])\n",
      "tensor([[ 1.1318,  0.4221, -0.0538]], grad_fn=<AddmmBackward0>)\n",
      "Batch 365, Loss: -1.1317821741104126\n",
      "tensor([1])\n",
      "tensor([[ 1.1074,  0.4280, -0.0861]], grad_fn=<AddmmBackward0>)\n",
      "Batch 366, Loss: -0.4280015826225281\n",
      "tensor([2])\n",
      "tensor([[ 1.1357,  0.4214, -0.0487]], grad_fn=<AddmmBackward0>)\n",
      "Batch 367, Loss: 0.048712730407714844\n",
      "tensor([0])\n",
      "tensor([[ 1.1350,  0.4216, -0.0496]], grad_fn=<AddmmBackward0>)\n",
      "Batch 368, Loss: -1.1350456476211548\n",
      "tensor([0])\n",
      "tensor([[ 1.1125,  0.4271, -0.0795]], grad_fn=<AddmmBackward0>)\n",
      "Batch 369, Loss: -1.1124610900878906\n",
      "tensor([1])\n",
      "tensor([[ 1.0698,  0.4374, -0.1359]], grad_fn=<AddmmBackward0>)\n",
      "Batch 370, Loss: -0.43741053342819214\n",
      "tensor([2])\n",
      "tensor([[ 1.0905,  0.4326, -0.1087]], grad_fn=<AddmmBackward0>)\n",
      "Batch 371, Loss: 0.10865604877471924\n",
      "tensor([0])\n",
      "tensor([[ 1.1260,  0.4243, -0.0618]], grad_fn=<AddmmBackward0>)\n",
      "Batch 372, Loss: -1.1259658336639404\n",
      "tensor([1])\n",
      "tensor([[ 1.1332,  0.4226, -0.0522]], grad_fn=<AddmmBackward0>)\n",
      "Batch 373, Loss: -0.42263227701187134\n",
      "tensor([0])\n",
      "tensor([[ 1.1326,  0.4229, -0.0532]], grad_fn=<AddmmBackward0>)\n",
      "Batch 374, Loss: -1.1325626373291016\n",
      "tensor([1])\n",
      "tensor([[ 1.1327,  0.4230, -0.0531]], grad_fn=<AddmmBackward0>)\n",
      "Batch 375, Loss: -0.42302221059799194\n",
      "tensor([1])\n",
      "tensor([[ 1.1236,  0.4253, -0.0652]], grad_fn=<AddmmBackward0>)\n",
      "Batch 376, Loss: -0.42533066868782043\n",
      "tensor([1])\n",
      "tensor([[ 1.1342,  0.4230, -0.0513]], grad_fn=<AddmmBackward0>)\n",
      "Batch 377, Loss: -0.42298442125320435\n",
      "tensor([0])\n",
      "tensor([[ 1.1046,  0.4302, -0.0905]], grad_fn=<AddmmBackward0>)\n",
      "Batch 378, Loss: -1.104644536972046\n",
      "tensor([1])\n",
      "tensor([[ 1.1357,  0.4230, -0.0495]], grad_fn=<AddmmBackward0>)\n",
      "Batch 379, Loss: -0.4229871928691864\n",
      "tensor([1])\n",
      "tensor([[ 1.1359,  0.4231, -0.0493]], grad_fn=<AddmmBackward0>)\n",
      "Batch 380, Loss: -0.4231303334236145\n",
      "tensor([1])\n",
      "tensor([[ 1.1296,  0.4248, -0.0578]], grad_fn=<AddmmBackward0>)\n",
      "Batch 381, Loss: -0.42483988404273987\n",
      "tensor([0])\n",
      "tensor([[ 1.1185,  0.4277, -0.0725]], grad_fn=<AddmmBackward0>)\n",
      "Batch 382, Loss: -1.1184732913970947\n",
      "tensor([0])\n",
      "tensor([[ 1.1343,  0.4242, -0.0518]], grad_fn=<AddmmBackward0>)\n",
      "Batch 383, Loss: -1.1342695951461792\n",
      "tensor([0])\n",
      "tensor([[ 1.1323,  0.4248, -0.0546]], grad_fn=<AddmmBackward0>)\n",
      "Batch 384, Loss: -1.132285475730896\n",
      "tensor([2])\n",
      "tensor([[ 1.1329,  0.4249, -0.0539]], grad_fn=<AddmmBackward0>)\n",
      "Batch 385, Loss: 0.05388355255126953\n",
      "tensor([0])\n",
      "tensor([[ 1.1018,  0.4324, -0.0951]], grad_fn=<AddmmBackward0>)\n",
      "Batch 386, Loss: -1.1018482446670532\n",
      "tensor([0])\n",
      "tensor([[ 1.1349,  0.4247, -0.0516]], grad_fn=<AddmmBackward0>)\n",
      "Batch 387, Loss: -1.1348867416381836\n",
      "tensor([0])\n",
      "tensor([[ 1.1217,  0.4280, -0.0691]], grad_fn=<AddmmBackward0>)\n",
      "Batch 388, Loss: -1.1217446327209473\n",
      "tensor([1])\n",
      "tensor([[ 1.1296,  0.4262, -0.0590]], grad_fn=<AddmmBackward0>)\n",
      "Batch 389, Loss: -0.42624691128730774\n",
      "tensor([2])\n",
      "tensor([[ 1.0594,  0.4430, -0.1519]], grad_fn=<AddmmBackward0>)\n",
      "Batch 390, Loss: 0.1519237756729126\n",
      "tensor([0])\n",
      "tensor([[ 1.1274,  0.4271, -0.0622]], grad_fn=<AddmmBackward0>)\n",
      "Batch 391, Loss: -1.1273937225341797\n",
      "tensor([1])\n",
      "tensor([[ 1.0400,  0.4478, -0.1778]], grad_fn=<AddmmBackward0>)\n",
      "Batch 392, Loss: -0.44780170917510986\n",
      "tensor([2])\n",
      "tensor([[ 1.1169,  0.4298, -0.0763]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 393, Loss: 0.07633334398269653\n",
      "tensor([0])\n",
      "tensor([[ 1.0945,  0.4352, -0.1060]], grad_fn=<AddmmBackward0>)\n",
      "Batch 394, Loss: -1.094536542892456\n",
      "tensor([2])\n",
      "tensor([[ 1.0951,  0.4352, -0.1054]], grad_fn=<AddmmBackward0>)\n",
      "Batch 395, Loss: 0.10537219047546387\n",
      "tensor([0])\n",
      "tensor([[ 1.1102,  0.4318, -0.0856]], grad_fn=<AddmmBackward0>)\n",
      "Batch 396, Loss: -1.1101588010787964\n",
      "tensor([0])\n",
      "tensor([[ 1.1415,  0.4246, -0.0444]], grad_fn=<AddmmBackward0>)\n",
      "Batch 397, Loss: -1.1415019035339355\n",
      "tensor([2])\n",
      "tensor([[ 1.1320,  0.4269, -0.0570]], grad_fn=<AddmmBackward0>)\n",
      "Batch 398, Loss: 0.05702310800552368\n",
      "tensor([2])\n",
      "tensor([[ 1.1333,  0.4267, -0.0554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 399, Loss: 0.05539172887802124\n",
      "tensor([2])\n",
      "tensor([[ 1.1035,  0.4338, -0.0949]], grad_fn=<AddmmBackward0>)\n",
      "Batch 400, Loss: 0.09490394592285156\n",
      "tensor([1])\n",
      "tensor([[ 1.1381,  0.4258, -0.0493]], grad_fn=<AddmmBackward0>)\n",
      "Batch 401, Loss: -0.4257532060146332\n",
      "tensor([1])\n",
      "tensor([[ 1.1350,  0.4266, -0.0534]], grad_fn=<AddmmBackward0>)\n",
      "Batch 402, Loss: -0.42658495903015137\n",
      "tensor([1])\n",
      "tensor([[ 1.1390,  0.4258, -0.0481]], grad_fn=<AddmmBackward0>)\n",
      "Batch 403, Loss: -0.4257786273956299\n",
      "tensor([1])\n",
      "tensor([[ 1.1356,  0.4267, -0.0525]], grad_fn=<AddmmBackward0>)\n",
      "Batch 404, Loss: -0.4267193078994751\n",
      "tensor([1])\n",
      "tensor([[ 1.1405,  0.4257, -0.0461]], grad_fn=<AddmmBackward0>)\n",
      "Batch 405, Loss: -0.42574793100357056\n",
      "tensor([1])\n",
      "tensor([[ 0.9460,  0.4715, -0.3028]], grad_fn=<AddmmBackward0>)\n",
      "Batch 406, Loss: -0.471455454826355\n",
      "tensor([1])\n",
      "tensor([[ 1.1319,  0.4282, -0.0575]], grad_fn=<AddmmBackward0>)\n",
      "Batch 407, Loss: -0.4281521439552307\n",
      "tensor([1])\n",
      "tensor([[ 1.1169,  0.4319, -0.0772]], grad_fn=<AddmmBackward0>)\n",
      "Batch 408, Loss: -0.4318516254425049\n",
      "tensor([2])\n",
      "tensor([[ 1.1407,  0.4265, -0.0459]], grad_fn=<AddmmBackward0>)\n",
      "Batch 409, Loss: 0.04593193531036377\n",
      "tensor([0])\n",
      "tensor([[ 1.1407,  0.4267, -0.0458]], grad_fn=<AddmmBackward0>)\n",
      "Batch 410, Loss: -1.1407270431518555\n",
      "tensor([1])\n",
      "tensor([[ 1.0801,  0.4411, -0.1259]], grad_fn=<AddmmBackward0>)\n",
      "Batch 411, Loss: -0.4410605728626251\n",
      "tensor([2])\n",
      "tensor([[ 1.1363,  0.4281, -0.0517]], grad_fn=<AddmmBackward0>)\n",
      "Batch 412, Loss: 0.05173897743225098\n",
      "tensor([2])\n",
      "tensor([[ 1.1219,  0.4317, -0.0707]], grad_fn=<AddmmBackward0>)\n",
      "Batch 413, Loss: 0.07070881128311157\n",
      "tensor([0])\n",
      "tensor([[ 1.1386,  0.4279, -0.0486]], grad_fn=<AddmmBackward0>)\n",
      "Batch 414, Loss: -1.138635516166687\n",
      "tensor([0])\n",
      "tensor([[ 1.0951,  0.4382, -0.1060]], grad_fn=<AddmmBackward0>)\n",
      "Batch 415, Loss: -1.0951255559921265\n",
      "tensor([2])\n",
      "tensor([[ 1.1416,  0.4276, -0.0446]], grad_fn=<AddmmBackward0>)\n",
      "Batch 416, Loss: 0.04464620351791382\n",
      "tensor([0])\n",
      "tensor([[ 1.1259,  0.4314, -0.0655]], grad_fn=<AddmmBackward0>)\n",
      "Batch 417, Loss: -1.1258583068847656\n",
      "tensor([1])\n",
      "tensor([[ 1.1417,  0.4278, -0.0446]], grad_fn=<AddmmBackward0>)\n",
      "Batch 418, Loss: -0.42779892683029175\n",
      "tensor([0])\n",
      "tensor([[ 1.1388,  0.4286, -0.0485]], grad_fn=<AddmmBackward0>)\n",
      "Batch 419, Loss: -1.1387722492218018\n",
      "tensor([1])\n",
      "tensor([[ 0.9158,  0.4805, -0.3429]], grad_fn=<AddmmBackward0>)\n",
      "Batch 420, Loss: -0.4805179238319397\n",
      "tensor([2])\n",
      "tensor([[ 1.1387,  0.4289, -0.0487]], grad_fn=<AddmmBackward0>)\n",
      "Batch 421, Loss: 0.04872286319732666\n",
      "tensor([1])\n",
      "tensor([[ 1.1398,  0.4288, -0.0474]], grad_fn=<AddmmBackward0>)\n",
      "Batch 422, Loss: -0.4288373291492462\n",
      "tensor([2])\n",
      "tensor([[ 1.1388,  0.4292, -0.0487]], grad_fn=<AddmmBackward0>)\n",
      "Batch 423, Loss: 0.04873234033584595\n",
      "tensor([0])\n",
      "tensor([[ 1.1408,  0.4289, -0.0461]], grad_fn=<AddmmBackward0>)\n",
      "Batch 424, Loss: -1.1407897472381592\n",
      "tensor([0])\n",
      "tensor([[ 1.1342,  0.4306, -0.0549]], grad_fn=<AddmmBackward0>)\n",
      "Batch 425, Loss: -1.1341723203659058\n",
      "tensor([2])\n",
      "tensor([[ 1.0405,  0.4524, -0.1785]], grad_fn=<AddmmBackward0>)\n",
      "Batch 426, Loss: 0.17846786975860596\n",
      "tensor([1])\n",
      "tensor([[ 1.1214,  0.4338, -0.0718]], grad_fn=<AddmmBackward0>)\n",
      "Batch 427, Loss: -0.43376588821411133\n",
      "tensor([2])\n",
      "tensor([[ 1.0868,  0.4419, -0.1175]], grad_fn=<AddmmBackward0>)\n",
      "Batch 428, Loss: 0.11754673719406128\n",
      "tensor([1])\n",
      "tensor([[ 1.1371,  0.4304, -0.0512]], grad_fn=<AddmmBackward0>)\n",
      "Batch 429, Loss: -0.43042081594467163\n",
      "tensor([2])\n",
      "tensor([[ 1.1400,  0.4299, -0.0474]], grad_fn=<AddmmBackward0>)\n",
      "Batch 430, Loss: 0.04735386371612549\n",
      "tensor([1])\n",
      "tensor([[ 1.1365,  0.4308, -0.0519]], grad_fn=<AddmmBackward0>)\n",
      "Batch 431, Loss: -0.43083152174949646\n",
      "tensor([0])\n",
      "tensor([[ 1.1387,  0.4305, -0.0490]], grad_fn=<AddmmBackward0>)\n",
      "Batch 432, Loss: -1.13869047164917\n",
      "tensor([2])\n",
      "tensor([[ 1.1320,  0.4322, -0.0579]], grad_fn=<AddmmBackward0>)\n",
      "Batch 433, Loss: 0.05789095163345337\n",
      "tensor([0])\n",
      "tensor([[ 1.1123,  0.4368, -0.0838]], grad_fn=<AddmmBackward0>)\n",
      "Batch 434, Loss: -1.1123230457305908\n",
      "tensor([2])\n",
      "tensor([[ 1.1302,  0.4328, -0.0603]], grad_fn=<AddmmBackward0>)\n",
      "Batch 435, Loss: 0.06027776002883911\n",
      "tensor([1])\n",
      "tensor([[ 1.1291,  0.4332, -0.0616]], grad_fn=<AddmmBackward0>)\n",
      "Batch 436, Loss: -0.43319764733314514\n",
      "tensor([0])\n",
      "tensor([[ 1.1312,  0.4328, -0.0588]], grad_fn=<AddmmBackward0>)\n",
      "Batch 437, Loss: -1.1312323808670044\n",
      "tensor([2])\n",
      "tensor([[ 1.0833,  0.4440, -0.1221]], grad_fn=<AddmmBackward0>)\n",
      "Batch 438, Loss: 0.12212121486663818\n",
      "tensor([0])\n",
      "tensor([[ 1.1454,  0.4298, -0.0401]], grad_fn=<AddmmBackward0>)\n",
      "Batch 439, Loss: -1.1454386711120605\n",
      "tensor([1])\n",
      "tensor([[ 1.1339,  0.4326, -0.0554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 440, Loss: -0.4325926601886749\n",
      "tensor([2])\n",
      "tensor([[ 1.1342,  0.4327, -0.0551]], grad_fn=<AddmmBackward0>)\n",
      "Batch 441, Loss: 0.05509650707244873\n",
      "tensor([2])\n",
      "tensor([[ 1.1014,  0.4403, -0.0984]], grad_fn=<AddmmBackward0>)\n",
      "Batch 442, Loss: 0.09836012125015259\n",
      "tensor([2])\n",
      "tensor([[ 1.1333,  0.4331, -0.0562]], grad_fn=<AddmmBackward0>)\n",
      "Batch 443, Loss: 0.05619710683822632\n",
      "tensor([1])\n",
      "tensor([[ 1.1188,  0.4365, -0.0753]], grad_fn=<AddmmBackward0>)\n",
      "Batch 444, Loss: -0.4365222454071045\n",
      "tensor([0])\n",
      "tensor([[ 1.1406,  0.4316, -0.0465]], grad_fn=<AddmmBackward0>)\n",
      "Batch 445, Loss: -1.1405549049377441\n",
      "tensor([1])\n",
      "tensor([[ 1.1439,  0.4310, -0.0420]], grad_fn=<AddmmBackward0>)\n",
      "Batch 446, Loss: -0.43099138140678406\n",
      "tensor([0])\n",
      "tensor([[ 1.1144,  0.4379, -0.0809]], grad_fn=<AddmmBackward0>)\n",
      "Batch 447, Loss: -1.1144475936889648\n",
      "tensor([0])\n",
      "tensor([[ 1.1331,  0.4337, -0.0563]], grad_fn=<AddmmBackward0>)\n",
      "Batch 448, Loss: -1.1331219673156738\n",
      "tensor([0])\n",
      "tensor([[ 1.1316,  0.4342, -0.0585]], grad_fn=<AddmmBackward0>)\n",
      "Batch 449, Loss: -1.1315770149230957\n",
      "tensor([0])\n",
      "tensor([[ 1.0703,  0.4484, -0.1394]], grad_fn=<AddmmBackward0>)\n",
      "Batch 450, Loss: -1.070314884185791\n",
      "tensor([1])\n",
      "tensor([[ 1.1467,  0.4310, -0.0388]], grad_fn=<AddmmBackward0>)\n",
      "Batch 451, Loss: -0.43101510405540466\n",
      "tensor([0])\n",
      "tensor([[ 1.1463,  0.4312, -0.0395]], grad_fn=<AddmmBackward0>)\n",
      "Batch 452, Loss: -1.1462610960006714\n",
      "tensor([0])\n",
      "tensor([[ 1.1415,  0.4325, -0.0459]], grad_fn=<AddmmBackward0>)\n",
      "Batch 453, Loss: -1.1415283679962158\n",
      "tensor([1])\n",
      "tensor([[ 1.1202,  0.4375, -0.0741]], grad_fn=<AddmmBackward0>)\n",
      "Batch 454, Loss: -0.4374506175518036\n",
      "tensor([2])\n",
      "tensor([[ 1.1184,  0.4380, -0.0768]], grad_fn=<AddmmBackward0>)\n",
      "Batch 455, Loss: 0.07679307460784912\n",
      "tensor([0])\n",
      "tensor([[ 1.1326,  0.4349, -0.0582]], grad_fn=<AddmmBackward0>)\n",
      "Batch 456, Loss: -1.1325695514678955\n",
      "tensor([1])\n",
      "tensor([[ 1.1319,  0.4352, -0.0592]], grad_fn=<AddmmBackward0>)\n",
      "Batch 457, Loss: -0.4351927936077118\n",
      "tensor([0])\n",
      "tensor([[ 1.1477,  0.4317, -0.0385]], grad_fn=<AddmmBackward0>)\n",
      "Batch 458, Loss: -1.1477419137954712\n",
      "tensor([0])\n",
      "tensor([[ 1.1457,  0.4323, -0.0414]], grad_fn=<AddmmBackward0>)\n",
      "Batch 459, Loss: -1.1456866264343262\n",
      "tensor([0])\n",
      "tensor([[ 1.1480,  0.4320, -0.0385]], grad_fn=<AddmmBackward0>)\n",
      "Batch 460, Loss: -1.147979497909546\n",
      "tensor([1])\n",
      "tensor([[ 1.1418,  0.4335, -0.0469]], grad_fn=<AddmmBackward0>)\n",
      "Batch 461, Loss: -0.4335050582885742\n",
      "tensor([2])\n",
      "tensor([[ 1.1512,  0.4315, -0.0347]], grad_fn=<AddmmBackward0>)\n",
      "Batch 462, Loss: 0.034723520278930664\n",
      "tensor([2])\n",
      "tensor([[ 1.1345,  0.4355, -0.0569]], grad_fn=<AddmmBackward0>)\n",
      "Batch 463, Loss: 0.05691605806350708\n",
      "tensor([0])\n",
      "tensor([[ 1.1467,  0.4328, -0.0409]], grad_fn=<AddmmBackward0>)\n",
      "Batch 464, Loss: -1.1466729640960693\n",
      "tensor([0])\n",
      "tensor([[ 1.0550,  0.4537, -0.1618]], grad_fn=<AddmmBackward0>)\n",
      "Batch 465, Loss: -1.0549641847610474\n",
      "tensor([2])\n",
      "tensor([[ 1.1390,  0.4348, -0.0513]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 466, Loss: 0.05128687620162964\n",
      "tensor([1])\n",
      "tensor([[ 1.1118,  0.4411, -0.0873]], grad_fn=<AddmmBackward0>)\n",
      "Batch 467, Loss: -0.44105279445648193\n",
      "tensor([1])\n",
      "tensor([[ 1.0828,  0.4477, -0.1255]], grad_fn=<AddmmBackward0>)\n",
      "Batch 468, Loss: -0.4477407932281494\n",
      "tensor([1])\n",
      "tensor([[ 1.1326,  0.4366, -0.0600]], grad_fn=<AddmmBackward0>)\n",
      "Batch 469, Loss: -0.43659067153930664\n",
      "tensor([1])\n",
      "tensor([[ 1.1414,  0.4348, -0.0486]], grad_fn=<AddmmBackward0>)\n",
      "Batch 470, Loss: -0.43476927280426025\n",
      "tensor([2])\n",
      "tensor([[ 1.1432,  0.4345, -0.0462]], grad_fn=<AddmmBackward0>)\n",
      "Batch 471, Loss: 0.046201467514038086\n",
      "tensor([0])\n",
      "tensor([[ 1.1405,  0.4353, -0.0498]], grad_fn=<AddmmBackward0>)\n",
      "Batch 472, Loss: -1.1405255794525146\n",
      "tensor([1])\n",
      "tensor([[ 1.1450,  0.4344, -0.0440]], grad_fn=<AddmmBackward0>)\n",
      "Batch 473, Loss: -0.43444979190826416\n",
      "tensor([1])\n",
      "tensor([[ 1.1345,  0.4370, -0.0579]], grad_fn=<AddmmBackward0>)\n",
      "Batch 474, Loss: -0.4369949698448181\n",
      "tensor([2])\n",
      "tensor([[ 1.1382,  0.4363, -0.0531]], grad_fn=<AddmmBackward0>)\n",
      "Batch 475, Loss: 0.053073883056640625\n",
      "tensor([2])\n",
      "tensor([[ 1.1046,  0.4441, -0.0972]], grad_fn=<AddmmBackward0>)\n",
      "Batch 476, Loss: 0.09724020957946777\n",
      "tensor([1])\n",
      "tensor([[ 1.1494,  0.4341, -0.0383]], grad_fn=<AddmmBackward0>)\n",
      "Batch 477, Loss: -0.4341435730457306\n",
      "tensor([1])\n",
      "tensor([[ 1.1223,  0.4404, -0.0739]], grad_fn=<AddmmBackward0>)\n",
      "Batch 478, Loss: -0.4404054284095764\n",
      "tensor([0])\n",
      "tensor([[ 1.1507,  0.4342, -0.0365]], grad_fn=<AddmmBackward0>)\n",
      "Batch 479, Loss: -1.1507245302200317\n",
      "tensor([0])\n",
      "tensor([[ 1.1520,  0.4341, -0.0349]], grad_fn=<AddmmBackward0>)\n",
      "Batch 480, Loss: -1.1519784927368164\n",
      "tensor([1])\n",
      "tensor([[ 0.9154,  0.4875, -0.3464]], grad_fn=<AddmmBackward0>)\n",
      "Batch 481, Loss: -0.48748210072517395\n",
      "tensor([1])\n",
      "tensor([[ 1.1196,  0.4417, -0.0776]], grad_fn=<AddmmBackward0>)\n",
      "Batch 482, Loss: -0.44171059131622314\n",
      "tensor([1])\n",
      "tensor([[ 1.0885,  0.4489, -0.1187]], grad_fn=<AddmmBackward0>)\n",
      "Batch 483, Loss: -0.4488924443721771\n",
      "tensor([1])\n",
      "tensor([[ 1.1540,  0.4344, -0.0325]], grad_fn=<AddmmBackward0>)\n",
      "Batch 484, Loss: -0.43438178300857544\n",
      "tensor([2])\n",
      "tensor([[ 1.1336,  0.4392, -0.0593]], grad_fn=<AddmmBackward0>)\n",
      "Batch 485, Loss: 0.05934208631515503\n",
      "tensor([0])\n",
      "tensor([[ 1.1450,  0.4368, -0.0444]], grad_fn=<AddmmBackward0>)\n",
      "Batch 486, Loss: -1.145010232925415\n",
      "tensor([2])\n",
      "tensor([[ 1.1446,  0.4371, -0.0450]], grad_fn=<AddmmBackward0>)\n",
      "Batch 487, Loss: 0.04495251178741455\n",
      "tensor([2])\n",
      "tensor([[ 1.1176,  0.4433, -0.0805]], grad_fn=<AddmmBackward0>)\n",
      "Batch 488, Loss: 0.08049869537353516\n",
      "tensor([2])\n",
      "tensor([[ 1.1446,  0.4374, -0.0449]], grad_fn=<AddmmBackward0>)\n",
      "Batch 489, Loss: 0.044927000999450684\n",
      "tensor([2])\n",
      "tensor([[ 0.9971,  0.4705, -0.2390]], grad_fn=<AddmmBackward0>)\n",
      "Batch 490, Loss: 0.2390170693397522\n",
      "tensor([0])\n",
      "tensor([[ 1.1355,  0.4397, -0.0568]], grad_fn=<AddmmBackward0>)\n",
      "Batch 491, Loss: -1.1355087757110596\n",
      "tensor([0])\n",
      "tensor([[ 1.1124,  0.4450, -0.0871]], grad_fn=<AddmmBackward0>)\n",
      "Batch 492, Loss: -1.1124467849731445\n",
      "tensor([1])\n",
      "tensor([[ 1.1510,  0.4365, -0.0364]], grad_fn=<AddmmBackward0>)\n",
      "Batch 493, Loss: -0.43646982312202454\n",
      "tensor([1])\n",
      "tensor([[ 1.1519,  0.4364, -0.0353]], grad_fn=<AddmmBackward0>)\n",
      "Batch 494, Loss: -0.4364064633846283\n",
      "tensor([2])\n",
      "tensor([[ 1.1362,  0.4400, -0.0559]], grad_fn=<AddmmBackward0>)\n",
      "Batch 495, Loss: 0.05585545301437378\n",
      "tensor([0])\n",
      "tensor([[ 1.1287,  0.4419, -0.0658]], grad_fn=<AddmmBackward0>)\n",
      "Batch 496, Loss: -1.1286733150482178\n",
      "tensor([1])\n",
      "tensor([[ 1.1444,  0.4385, -0.0451]], grad_fn=<AddmmBackward0>)\n",
      "Batch 497, Loss: -0.4384898841381073\n",
      "tensor([2])\n",
      "tensor([[ 1.1108,  0.4461, -0.0894]], grad_fn=<AddmmBackward0>)\n",
      "Batch 498, Loss: 0.08941245079040527\n",
      "tensor([2])\n",
      "tensor([[ 1.1549,  0.4365, -0.0313]], grad_fn=<AddmmBackward0>)\n",
      "Batch 499, Loss: 0.031335651874542236\n",
      "tensor([2])\n",
      "tensor([[ 1.1443,  0.4389, -0.0452]], grad_fn=<AddmmBackward0>)\n",
      "Batch 500, Loss: 0.04522818326950073\n",
      "tensor([1])\n",
      "tensor([[ 1.1506,  0.4376, -0.0368]], grad_fn=<AddmmBackward0>)\n",
      "Batch 501, Loss: -0.4376462996006012\n",
      "tensor([1])\n",
      "tensor([[ 1.1332,  0.4416, -0.0596]], grad_fn=<AddmmBackward0>)\n",
      "Batch 502, Loss: -0.4416424036026001\n",
      "tensor([2])\n",
      "tensor([[ 1.1475,  0.4386, -0.0408]], grad_fn=<AddmmBackward0>)\n",
      "Batch 503, Loss: 0.04082006216049194\n",
      "tensor([2])\n",
      "tensor([[ 1.1557,  0.4370, -0.0300]], grad_fn=<AddmmBackward0>)\n",
      "Batch 504, Loss: 0.02997368574142456\n",
      "tensor([2])\n",
      "tensor([[ 0.9605,  0.4804, -0.2868]], grad_fn=<AddmmBackward0>)\n",
      "Batch 505, Loss: 0.2868141531944275\n",
      "tensor([1])\n",
      "tensor([[ 1.1212,  0.4448, -0.0751]], grad_fn=<AddmmBackward0>)\n",
      "Batch 506, Loss: -0.4448454976081848\n",
      "tensor([2])\n",
      "tensor([[ 1.1523,  0.4381, -0.0341]], grad_fn=<AddmmBackward0>)\n",
      "Batch 507, Loss: 0.034079015254974365\n",
      "tensor([1])\n",
      "tensor([[ 1.0343,  0.4643, -0.1893]], grad_fn=<AddmmBackward0>)\n",
      "Batch 508, Loss: -0.46434688568115234\n",
      "tensor([2])\n",
      "tensor([[ 1.0141,  0.4690, -0.2158]], grad_fn=<AddmmBackward0>)\n",
      "Batch 509, Loss: 0.21584439277648926\n",
      "tensor([2])\n",
      "tensor([[ 1.1557,  0.4377, -0.0291]], grad_fn=<AddmmBackward0>)\n",
      "Batch 510, Loss: 0.029138028621673584\n",
      "tensor([1])\n",
      "tensor([[ 1.1543,  0.4381, -0.0308]], grad_fn=<AddmmBackward0>)\n",
      "Batch 511, Loss: -0.43813419342041016\n",
      "tensor([1])\n",
      "tensor([[ 1.1074,  0.4486, -0.0924]], grad_fn=<AddmmBackward0>)\n",
      "Batch 512, Loss: -0.44863981008529663\n",
      "tensor([2])\n",
      "tensor([[ 1.0755,  0.4558, -0.1344]], grad_fn=<AddmmBackward0>)\n",
      "Batch 513, Loss: 0.1343558430671692\n",
      "tensor([2])\n",
      "tensor([[ 1.1540,  0.4386, -0.0308]], grad_fn=<AddmmBackward0>)\n",
      "Batch 514, Loss: 0.0308457612991333\n",
      "tensor([1])\n",
      "tensor([[ 1.1083,  0.4489, -0.0909]], grad_fn=<AddmmBackward0>)\n",
      "Batch 515, Loss: -0.44885918498039246\n",
      "tensor([0])\n",
      "tensor([[ 1.1552,  0.4386, -0.0289]], grad_fn=<AddmmBackward0>)\n",
      "Batch 516, Loss: -1.155164122581482\n",
      "tensor([1])\n",
      "tensor([[ 1.1311,  0.4441, -0.0605]], grad_fn=<AddmmBackward0>)\n",
      "Batch 517, Loss: -0.4440850019454956\n",
      "tensor([1])\n",
      "tensor([[ 1.1345,  0.4435, -0.0560]], grad_fn=<AddmmBackward0>)\n",
      "Batch 518, Loss: -0.44350504875183105\n",
      "tensor([1])\n",
      "tensor([[ 1.1343,  0.4437, -0.0562]], grad_fn=<AddmmBackward0>)\n",
      "Batch 519, Loss: -0.4437144100666046\n",
      "tensor([0])\n",
      "tensor([[ 1.1566,  0.4390, -0.0267]], grad_fn=<AddmmBackward0>)\n",
      "Batch 520, Loss: -1.1566135883331299\n",
      "tensor([2])\n",
      "tensor([[ 1.0947,  0.4528, -0.1083]], grad_fn=<AddmmBackward0>)\n",
      "Batch 521, Loss: 0.10829788446426392\n",
      "tensor([0])\n",
      "tensor([[ 1.1446,  0.4420, -0.0424]], grad_fn=<AddmmBackward0>)\n",
      "Batch 522, Loss: -1.1446449756622314\n",
      "tensor([0])\n",
      "tensor([[ 1.1475,  0.4415, -0.0387]], grad_fn=<AddmmBackward0>)\n",
      "Batch 523, Loss: -1.1474815607070923\n",
      "tensor([0])\n",
      "tensor([[ 1.1569,  0.4396, -0.0263]], grad_fn=<AddmmBackward0>)\n",
      "Batch 524, Loss: -1.1569230556488037\n",
      "tensor([2])\n",
      "tensor([[ 1.1491,  0.4414, -0.0366]], grad_fn=<AddmmBackward0>)\n",
      "Batch 525, Loss: 0.03664052486419678\n",
      "tensor([0])\n",
      "tensor([[ 1.1557,  0.4401, -0.0281]], grad_fn=<AddmmBackward0>)\n",
      "Batch 526, Loss: -1.1556689739227295\n",
      "tensor([0])\n",
      "tensor([[ 1.1560,  0.4401, -0.0277]], grad_fn=<AddmmBackward0>)\n",
      "Batch 527, Loss: -1.156041145324707\n",
      "tensor([0])\n",
      "tensor([[ 1.1308,  0.4458, -0.0610]], grad_fn=<AddmmBackward0>)\n",
      "Batch 528, Loss: -1.1308066844940186\n",
      "tensor([1])\n",
      "tensor([[ 1.1498,  0.4417, -0.0361]], grad_fn=<AddmmBackward0>)\n",
      "Batch 529, Loss: -0.4417293071746826\n",
      "tensor([2])\n",
      "tensor([[ 1.1454,  0.4428, -0.0420]], grad_fn=<AddmmBackward0>)\n",
      "Batch 530, Loss: 0.04204761981964111\n",
      "tensor([1])\n",
      "tensor([[ 1.1594,  0.4399, -0.0237]], grad_fn=<AddmmBackward0>)\n",
      "Batch 531, Loss: -0.43988484144210815\n",
      "tensor([0])\n",
      "tensor([[ 1.1517,  0.4417, -0.0340]], grad_fn=<AddmmBackward0>)\n",
      "Batch 532, Loss: -1.1516696214675903\n",
      "tensor([0])\n",
      "tensor([[ 1.1582,  0.4404, -0.0255]], grad_fn=<AddmmBackward0>)\n",
      "Batch 533, Loss: -1.158172607421875\n",
      "tensor([2])\n",
      "tensor([[ 1.1588,  0.4404, -0.0248]], grad_fn=<AddmmBackward0>)\n",
      "Batch 534, Loss: 0.024823546409606934\n",
      "tensor([2])\n",
      "tensor([[ 1.1586,  0.4406, -0.0252]], grad_fn=<AddmmBackward0>)\n",
      "Batch 535, Loss: 0.025242507457733154\n",
      "tensor([1])\n",
      "tensor([[ 1.1572,  0.4410, -0.0272]], grad_fn=<AddmmBackward0>)\n",
      "Batch 536, Loss: -0.44101059436798096\n",
      "tensor([0])\n",
      "tensor([[ 1.1506,  0.4426, -0.0359]], grad_fn=<AddmmBackward0>)\n",
      "Batch 537, Loss: -1.1505627632141113\n",
      "tensor([0])\n",
      "tensor([[ 1.1510,  0.4426, -0.0354]], grad_fn=<AddmmBackward0>)\n",
      "Batch 538, Loss: -1.1510412693023682\n",
      "tensor([2])\n",
      "tensor([[ 1.1196,  0.4496, -0.0769]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 539, Loss: 0.07693809270858765\n",
      "tensor([2])\n",
      "tensor([[ 1.1258,  0.4483, -0.0688]], grad_fn=<AddmmBackward0>)\n",
      "Batch 540, Loss: 0.06877946853637695\n",
      "tensor([1])\n",
      "tensor([[ 1.1593,  0.4411, -0.0247]], grad_fn=<AddmmBackward0>)\n",
      "Batch 541, Loss: -0.4411092698574066\n",
      "tensor([0])\n",
      "tensor([[ 1.1537,  0.4425, -0.0322]], grad_fn=<AddmmBackward0>)\n",
      "Batch 542, Loss: -1.153712511062622\n",
      "tensor([0])\n",
      "tensor([[ 1.1137,  0.4513, -0.0849]], grad_fn=<AddmmBackward0>)\n",
      "Batch 543, Loss: -1.1137471199035645\n",
      "tensor([0])\n",
      "tensor([[ 1.1568,  0.4420, -0.0283]], grad_fn=<AddmmBackward0>)\n",
      "Batch 544, Loss: -1.1567976474761963\n",
      "tensor([1])\n",
      "tensor([[ 1.1045,  0.4535, -0.0973]], grad_fn=<AddmmBackward0>)\n",
      "Batch 545, Loss: -0.45350581407546997\n",
      "tensor([2])\n",
      "tensor([[ 1.1494,  0.4439, -0.0383]], grad_fn=<AddmmBackward0>)\n",
      "Batch 546, Loss: 0.03833580017089844\n",
      "tensor([0])\n",
      "tensor([[ 1.1598,  0.4417, -0.0247]], grad_fn=<AddmmBackward0>)\n",
      "Batch 547, Loss: -1.1597728729248047\n",
      "tensor([1])\n",
      "tensor([[ 1.1408,  0.4460, -0.0498]], grad_fn=<AddmmBackward0>)\n",
      "Batch 548, Loss: -0.44596004486083984\n",
      "tensor([1])\n",
      "tensor([[ 1.1497,  0.4442, -0.0382]], grad_fn=<AddmmBackward0>)\n",
      "Batch 549, Loss: -0.44416099786758423\n",
      "tensor([1])\n",
      "tensor([[ 1.1374,  0.4470, -0.0545]], grad_fn=<AddmmBackward0>)\n",
      "Batch 550, Loss: -0.4469860792160034\n",
      "tensor([2])\n",
      "tensor([[ 1.1495,  0.4445, -0.0386]], grad_fn=<AddmmBackward0>)\n",
      "Batch 551, Loss: 0.038636744022369385\n",
      "tensor([1])\n",
      "tensor([[ 1.1307,  0.4488, -0.0634]], grad_fn=<AddmmBackward0>)\n",
      "Batch 552, Loss: -0.4487531781196594\n",
      "tensor([0])\n",
      "tensor([[ 1.1591,  0.4428, -0.0260]], grad_fn=<AddmmBackward0>)\n",
      "Batch 553, Loss: -1.1591486930847168\n",
      "tensor([2])\n",
      "tensor([[ 1.1573,  0.4433, -0.0286]], grad_fn=<AddmmBackward0>)\n",
      "Batch 554, Loss: 0.028556466102600098\n",
      "tensor([0])\n",
      "tensor([[ 1.0919,  0.4576, -0.1147]], grad_fn=<AddmmBackward0>)\n",
      "Batch 555, Loss: -1.0918633937835693\n",
      "tensor([0])\n",
      "tensor([[ 1.1593,  0.4432, -0.0260]], grad_fn=<AddmmBackward0>)\n",
      "Batch 556, Loss: -1.1593451499938965\n",
      "tensor([1])\n",
      "tensor([[ 1.1448,  0.4464, -0.0453]], grad_fn=<AddmmBackward0>)\n",
      "Batch 557, Loss: -0.44644999504089355\n",
      "tensor([0])\n",
      "tensor([[ 1.1607,  0.4432, -0.0244]], grad_fn=<AddmmBackward0>)\n",
      "Batch 558, Loss: -1.1607407331466675\n",
      "tensor([2])\n",
      "tensor([[ 1.1130,  0.4536, -0.0873]], grad_fn=<AddmmBackward0>)\n",
      "Batch 559, Loss: 0.08733546733856201\n",
      "tensor([1])\n",
      "tensor([[ 1.1640,  0.4427, -0.0203]], grad_fn=<AddmmBackward0>)\n",
      "Batch 560, Loss: -0.4427172541618347\n",
      "tensor([1])\n",
      "tensor([[ 1.1090,  0.4547, -0.0928]], grad_fn=<AddmmBackward0>)\n",
      "Batch 561, Loss: -0.45474863052368164\n",
      "tensor([1])\n",
      "tensor([[ 1.1635,  0.4432, -0.0212]], grad_fn=<AddmmBackward0>)\n",
      "Batch 562, Loss: -0.4431625008583069\n",
      "tensor([0])\n",
      "tensor([[ 1.1172,  0.4533, -0.0821]], grad_fn=<AddmmBackward0>)\n",
      "Batch 563, Loss: -1.117192268371582\n",
      "tensor([0])\n",
      "tensor([[ 1.1333,  0.4500, -0.0610]], grad_fn=<AddmmBackward0>)\n",
      "Batch 564, Loss: -1.1332765817642212\n",
      "tensor([2])\n",
      "tensor([[ 1.1636,  0.4437, -0.0213]], grad_fn=<AddmmBackward0>)\n",
      "Batch 565, Loss: 0.02131175994873047\n",
      "tensor([0])\n",
      "tensor([[ 1.1608,  0.4444, -0.0250]], grad_fn=<AddmmBackward0>)\n",
      "Batch 566, Loss: -1.1608316898345947\n",
      "tensor([0])\n",
      "tensor([[ 1.1620,  0.4443, -0.0236]], grad_fn=<AddmmBackward0>)\n",
      "Batch 567, Loss: -1.1620147228240967\n",
      "tensor([1])\n",
      "tensor([[ 1.1661,  0.4436, -0.0184]], grad_fn=<AddmmBackward0>)\n",
      "Batch 568, Loss: -0.4435543715953827\n",
      "tensor([2])\n",
      "tensor([[ 1.1579,  0.4455, -0.0293]], grad_fn=<AddmmBackward0>)\n",
      "Batch 569, Loss: 0.029304325580596924\n",
      "tensor([2])\n",
      "tensor([[ 1.1281,  0.4520, -0.0686]], grad_fn=<AddmmBackward0>)\n",
      "Batch 570, Loss: 0.06861323118209839\n",
      "tensor([1])\n",
      "tensor([[ 1.1538,  0.4466, -0.0349]], grad_fn=<AddmmBackward0>)\n",
      "Batch 571, Loss: -0.4466091990470886\n",
      "tensor([1])\n",
      "tensor([[ 1.1064,  0.4569, -0.0972]], grad_fn=<AddmmBackward0>)\n",
      "Batch 572, Loss: -0.4569093883037567\n",
      "tensor([0])\n",
      "tensor([[ 1.1658,  0.4443, -0.0191]], grad_fn=<AddmmBackward0>)\n",
      "Batch 573, Loss: -1.165846347808838\n",
      "tensor([0])\n",
      "tensor([[ 1.1360,  0.4509, -0.0584]], grad_fn=<AddmmBackward0>)\n",
      "Batch 574, Loss: -1.1359796524047852\n",
      "tensor([2])\n",
      "tensor([[ 1.1489,  0.4483, -0.0416]], grad_fn=<AddmmBackward0>)\n",
      "Batch 575, Loss: 0.04157829284667969\n",
      "tensor([2])\n",
      "tensor([[ 1.1484,  0.4485, -0.0422]], grad_fn=<AddmmBackward0>)\n",
      "Batch 576, Loss: 0.04223507642745972\n",
      "tensor([2])\n",
      "tensor([[ 1.1630,  0.4455, -0.0232]], grad_fn=<AddmmBackward0>)\n",
      "Batch 577, Loss: 0.023161590099334717\n",
      "tensor([2])\n",
      "tensor([[ 1.1335,  0.4519, -0.0619]], grad_fn=<AddmmBackward0>)\n",
      "Batch 578, Loss: 0.06190890073776245\n",
      "tensor([0])\n",
      "tensor([[ 1.1603,  0.4463, -0.0265]], grad_fn=<AddmmBackward0>)\n",
      "Batch 579, Loss: -1.160342812538147\n",
      "tensor([0])\n",
      "tensor([[ 1.1464,  0.4493, -0.0449]], grad_fn=<AddmmBackward0>)\n",
      "Batch 580, Loss: -1.1464171409606934\n",
      "tensor([1])\n",
      "tensor([[ 1.1482,  0.4490, -0.0426]], grad_fn=<AddmmBackward0>)\n",
      "Batch 581, Loss: -0.44904252886772156\n",
      "tensor([0])\n",
      "tensor([[ 1.1169,  0.4558, -0.0838]], grad_fn=<AddmmBackward0>)\n",
      "Batch 582, Loss: -1.1168636083602905\n",
      "tensor([0])\n",
      "tensor([[ 1.1656,  0.4456, -0.0198]], grad_fn=<AddmmBackward0>)\n",
      "Batch 583, Loss: -1.1656010150909424\n",
      "tensor([2])\n",
      "tensor([[ 1.1614,  0.4466, -0.0254]], grad_fn=<AddmmBackward0>)\n",
      "Batch 584, Loss: 0.02542901039123535\n",
      "tensor([0])\n",
      "tensor([[ 1.1509,  0.4489, -0.0393]], grad_fn=<AddmmBackward0>)\n",
      "Batch 585, Loss: -1.1509437561035156\n",
      "tensor([1])\n",
      "tensor([[ 1.1621,  0.4466, -0.0247]], grad_fn=<AddmmBackward0>)\n",
      "Batch 586, Loss: -0.44659772515296936\n",
      "tensor([2])\n",
      "tensor([[ 1.1333,  0.4529, -0.0627]], grad_fn=<AddmmBackward0>)\n",
      "Batch 587, Loss: 0.06270521879196167\n",
      "tensor([2])\n",
      "tensor([[ 1.1171,  0.4564, -0.0841]], grad_fn=<AddmmBackward0>)\n",
      "Batch 588, Loss: 0.08405959606170654\n",
      "tensor([2])\n",
      "tensor([[ 1.1549,  0.4485, -0.0344]], grad_fn=<AddmmBackward0>)\n",
      "Batch 589, Loss: 0.03436070680618286\n",
      "tensor([1])\n",
      "tensor([[ 1.1668,  0.4460, -0.0187]], grad_fn=<AddmmBackward0>)\n",
      "Batch 590, Loss: -0.44600367546081543\n",
      "tensor([0])\n",
      "tensor([[ 1.1634,  0.4468, -0.0231]], grad_fn=<AddmmBackward0>)\n",
      "Batch 591, Loss: -1.1634342670440674\n",
      "tensor([0])\n",
      "tensor([[ 1.1627,  0.4471, -0.0241]], grad_fn=<AddmmBackward0>)\n",
      "Batch 592, Loss: -1.1627062559127808\n",
      "tensor([2])\n",
      "tensor([[ 1.1346,  0.4532, -0.0611]], grad_fn=<AddmmBackward0>)\n",
      "Batch 593, Loss: 0.0611499547958374\n",
      "tensor([2])\n",
      "tensor([[ 1.1651,  0.4468, -0.0211]], grad_fn=<AddmmBackward0>)\n",
      "Batch 594, Loss: 0.02107471227645874\n",
      "tensor([2])\n",
      "tensor([[ 1.1565,  0.4487, -0.0323]], grad_fn=<AddmmBackward0>)\n",
      "Batch 595, Loss: 0.03231579065322876\n",
      "tensor([1])\n",
      "tensor([[ 1.1700,  0.4459, -0.0146]], grad_fn=<AddmmBackward0>)\n",
      "Batch 596, Loss: -0.44591689109802246\n",
      "tensor([1])\n",
      "tensor([[ 1.1643,  0.4472, -0.0220]], grad_fn=<AddmmBackward0>)\n",
      "Batch 597, Loss: -0.4472185969352722\n",
      "tensor([0])\n",
      "tensor([[ 1.1529,  0.4498, -0.0369]], grad_fn=<AddmmBackward0>)\n",
      "Batch 598, Loss: -1.1529362201690674\n",
      "tensor([2])\n",
      "tensor([[ 1.1687,  0.4465, -0.0162]], grad_fn=<AddmmBackward0>)\n",
      "Batch 599, Loss: 0.016218185424804688\n",
      "Training [80%]\tLoss: -0.4736\n",
      "tensor([2])\n",
      "tensor([[ 1.1637,  0.4477, -0.0228]], grad_fn=<AddmmBackward0>)\n",
      "Batch 0, Loss: 0.022757530212402344\n",
      "tensor([1])\n",
      "tensor([[ 1.1672,  0.4471, -0.0181]], grad_fn=<AddmmBackward0>)\n",
      "Batch 1, Loss: -0.4470617175102234\n",
      "tensor([0])\n",
      "tensor([[ 1.1547,  0.4498, -0.0345]], grad_fn=<AddmmBackward0>)\n",
      "Batch 2, Loss: -1.1547094583511353\n",
      "tensor([1])\n",
      "tensor([[ 1.1675,  0.4472, -0.0177]], grad_fn=<AddmmBackward0>)\n",
      "Batch 3, Loss: -0.4472407400608063\n",
      "tensor([1])\n",
      "tensor([[ 1.1386,  0.4535, -0.0556]], grad_fn=<AddmmBackward0>)\n",
      "Batch 4, Loss: -0.45349225401878357\n",
      "tensor([2])\n",
      "tensor([[ 1.0632,  0.4696, -0.1547]], grad_fn=<AddmmBackward0>)\n",
      "Batch 5, Loss: 0.15474969148635864\n",
      "tensor([2])\n",
      "tensor([[ 1.1624,  0.4488, -0.0244]], grad_fn=<AddmmBackward0>)\n",
      "Batch 6, Loss: 0.024353623390197754\n",
      "tensor([2])\n",
      "tensor([[ 1.1615,  0.4491, -0.0254]], grad_fn=<AddmmBackward0>)\n",
      "Batch 7, Loss: 0.025433361530303955\n",
      "tensor([1])\n",
      "tensor([[ 1.1681,  0.4478, -0.0167]], grad_fn=<AddmmBackward0>)\n",
      "Batch 8, Loss: -0.4478111267089844\n",
      "tensor([0])\n",
      "tensor([[ 1.1203,  0.4580, -0.0795]], grad_fn=<AddmmBackward0>)\n",
      "Batch 9, Loss: -1.1202603578567505\n",
      "tensor([0])\n",
      "tensor([[ 1.0928,  0.4639, -0.1155]], grad_fn=<AddmmBackward0>)\n",
      "Batch 10, Loss: -1.0927996635437012\n",
      "tensor([0])\n",
      "tensor([[ 1.1613,  0.4496, -0.0256]], grad_fn=<AddmmBackward0>)\n",
      "Batch 11, Loss: -1.1612520217895508\n",
      "tensor([2])\n",
      "tensor([[ 1.1709,  0.4477, -0.0129]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12, Loss: 0.012943029403686523\n",
      "tensor([2])\n",
      "tensor([[ 1.1446,  0.4534, -0.0476]], grad_fn=<AddmmBackward0>)\n",
      "Batch 13, Loss: 0.04761457443237305\n",
      "tensor([1])\n",
      "tensor([[ 1.1664,  0.4489, -0.0189]], grad_fn=<AddmmBackward0>)\n",
      "Batch 14, Loss: -0.4488826096057892\n",
      "tensor([2])\n",
      "tensor([[ 1.1623,  0.4499, -0.0242]], grad_fn=<AddmmBackward0>)\n",
      "Batch 15, Loss: 0.024241864681243896\n",
      "tensor([1])\n",
      "tensor([[ 1.1731,  0.4477, -0.0100]], grad_fn=<AddmmBackward0>)\n",
      "Batch 16, Loss: -0.44770580530166626\n",
      "tensor([1])\n",
      "tensor([[ 1.1705,  0.4484, -0.0134]], grad_fn=<AddmmBackward0>)\n",
      "Batch 17, Loss: -0.44838690757751465\n",
      "tensor([2])\n",
      "tensor([[ 1.1634,  0.4500, -0.0228]], grad_fn=<AddmmBackward0>)\n",
      "Batch 18, Loss: 0.02275747060775757\n",
      "tensor([1])\n",
      "tensor([[ 1.1707,  0.4486, -0.0130]], grad_fn=<AddmmBackward0>)\n",
      "Batch 19, Loss: -0.44863903522491455\n",
      "tensor([0])\n",
      "tensor([[ 1.1406,  0.4551, -0.0525]], grad_fn=<AddmmBackward0>)\n",
      "Batch 20, Loss: -1.1406402587890625\n",
      "tensor([1])\n",
      "tensor([[ 1.1561,  0.4520, -0.0322]], grad_fn=<AddmmBackward0>)\n",
      "Batch 21, Loss: -0.4520108103752136\n",
      "tensor([0])\n",
      "tensor([[ 1.1625,  0.4508, -0.0238]], grad_fn=<AddmmBackward0>)\n",
      "Batch 22, Loss: -1.162465214729309\n",
      "tensor([1])\n",
      "tensor([[ 1.1483,  0.4540, -0.0425]], grad_fn=<AddmmBackward0>)\n",
      "Batch 23, Loss: -0.4539709687232971\n",
      "tensor([2])\n",
      "tensor([[ 1.1725,  0.4491, -0.0107]], grad_fn=<AddmmBackward0>)\n",
      "Batch 24, Loss: 0.010690093040466309\n",
      "tensor([1])\n",
      "tensor([[ 1.1405,  0.4559, -0.0526]], grad_fn=<AddmmBackward0>)\n",
      "Batch 25, Loss: -0.45591336488723755\n",
      "tensor([1])\n",
      "tensor([[ 1.1705,  0.4498, -0.0133]], grad_fn=<AddmmBackward0>)\n",
      "Batch 26, Loss: -0.4498317241668701\n",
      "tensor([1])\n",
      "tensor([[ 1.1135,  0.4619, -0.0882]], grad_fn=<AddmmBackward0>)\n",
      "Batch 27, Loss: -0.461919903755188\n",
      "tensor([1])\n",
      "tensor([[ 1.1561,  0.4532, -0.0322]], grad_fn=<AddmmBackward0>)\n",
      "Batch 28, Loss: -0.45322340726852417\n",
      "tensor([2])\n",
      "tensor([[ 1.1679,  0.4510, -0.0167]], grad_fn=<AddmmBackward0>)\n",
      "Batch 29, Loss: 0.01670372486114502\n",
      "tensor([0])\n",
      "tensor([[ 1.1525,  0.4544, -0.0369]], grad_fn=<AddmmBackward0>)\n",
      "Batch 30, Loss: -1.1525049209594727\n",
      "tensor([2])\n",
      "tensor([[ 1.1653,  0.4519, -0.0201]], grad_fn=<AddmmBackward0>)\n",
      "Batch 31, Loss: 0.020093083381652832\n",
      "tensor([0])\n",
      "tensor([[ 1.1727,  0.4505, -0.0103]], grad_fn=<AddmmBackward0>)\n",
      "Batch 32, Loss: -1.1727350950241089\n",
      "tensor([1])\n",
      "tensor([[ 1.1672,  0.4518, -0.0176]], grad_fn=<AddmmBackward0>)\n",
      "Batch 33, Loss: -0.45183244347572327\n",
      "tensor([2])\n",
      "tensor([[ 1.1703,  0.4514, -0.0135]], grad_fn=<AddmmBackward0>)\n",
      "Batch 34, Loss: 0.013522982597351074\n",
      "tensor([1])\n",
      "tensor([[ 1.1352,  0.4588, -0.0596]], grad_fn=<AddmmBackward0>)\n",
      "Batch 35, Loss: -0.45879897475242615\n",
      "tensor([1])\n",
      "tensor([[ 0.9953,  0.4880, -0.2435]], grad_fn=<AddmmBackward0>)\n",
      "Batch 36, Loss: -0.48798319697380066\n",
      "tensor([1])\n",
      "tensor([[ 1.1345,  0.4593, -0.0605]], grad_fn=<AddmmBackward0>)\n",
      "Batch 37, Loss: -0.45929545164108276\n",
      "tensor([1])\n",
      "tensor([[ 1.1679,  0.4526, -0.0167]], grad_fn=<AddmmBackward0>)\n",
      "Batch 38, Loss: -0.452587366104126\n",
      "tensor([0])\n",
      "tensor([[ 1.1679,  0.4528, -0.0165]], grad_fn=<AddmmBackward0>)\n",
      "Batch 39, Loss: -1.1679487228393555\n",
      "tensor([0])\n",
      "tensor([[ 1.1474,  0.4572, -0.0436]], grad_fn=<AddmmBackward0>)\n",
      "Batch 40, Loss: -1.1474199295043945\n",
      "tensor([2])\n",
      "tensor([[ 1.1688,  0.4530, -0.0155]], grad_fn=<AddmmBackward0>)\n",
      "Batch 41, Loss: 0.015496015548706055\n",
      "tensor([1])\n",
      "tensor([[ 1.1456,  0.4579, -0.0460]], grad_fn=<AddmmBackward0>)\n",
      "Batch 42, Loss: -0.4579431712627411\n",
      "tensor([1])\n",
      "tensor([[ 1.1746,  0.4521, -0.0080]], grad_fn=<AddmmBackward0>)\n",
      "Batch 43, Loss: -0.45214492082595825\n",
      "tensor([2])\n",
      "tensor([[ 1.1688,  0.4535, -0.0156]], grad_fn=<AddmmBackward0>)\n",
      "Batch 44, Loss: 0.015611827373504639\n",
      "tensor([0])\n",
      "tensor([[ 1.1490,  0.4578, -0.0416]], grad_fn=<AddmmBackward0>)\n",
      "Batch 45, Loss: -1.148988962173462\n",
      "tensor([1])\n",
      "tensor([[ 1.1472,  0.4583, -0.0441]], grad_fn=<AddmmBackward0>)\n",
      "Batch 46, Loss: -0.4583432376384735\n",
      "tensor([1])\n",
      "tensor([[ 1.1728,  0.4532, -0.0104]], grad_fn=<AddmmBackward0>)\n",
      "Batch 47, Loss: -0.4532407522201538\n",
      "tensor([0])\n",
      "tensor([[ 1.1764,  0.4527, -0.0057]], grad_fn=<AddmmBackward0>)\n",
      "Batch 48, Loss: -1.176405429840088\n",
      "tensor([0])\n",
      "tensor([[ 1.1650,  0.4552, -0.0207]], grad_fn=<AddmmBackward0>)\n",
      "Batch 49, Loss: -1.1650358438491821\n",
      "tensor([1])\n",
      "tensor([[ 1.1748,  0.4534, -0.0080]], grad_fn=<AddmmBackward0>)\n",
      "Batch 50, Loss: -0.45340514183044434\n",
      "tensor([2])\n",
      "tensor([[ 1.1723,  0.4541, -0.0114]], grad_fn=<AddmmBackward0>)\n",
      "Batch 51, Loss: 0.01138371229171753\n",
      "tensor([1])\n",
      "tensor([[ 1.1584,  0.4571, -0.0298]], grad_fn=<AddmmBackward0>)\n",
      "Batch 52, Loss: -0.4571366310119629\n",
      "tensor([2])\n",
      "tensor([[ 1.1439,  0.4603, -0.0489]], grad_fn=<AddmmBackward0>)\n",
      "Batch 53, Loss: 0.04885399341583252\n",
      "tensor([1])\n",
      "tensor([[ 1.1757,  0.4539, -0.0070]], grad_fn=<AddmmBackward0>)\n",
      "Batch 54, Loss: -0.45393580198287964\n",
      "tensor([1])\n",
      "tensor([[ 1.1737,  0.4545, -0.0097]], grad_fn=<AddmmBackward0>)\n",
      "Batch 55, Loss: -0.45453646779060364\n",
      "tensor([2])\n",
      "tensor([[ 1.1236,  0.4650, -0.0756]], grad_fn=<AddmmBackward0>)\n",
      "Batch 56, Loss: 0.07555747032165527\n",
      "tensor([1])\n",
      "tensor([[ 1.1334,  0.4631, -0.0625]], grad_fn=<AddmmBackward0>)\n",
      "Batch 57, Loss: -0.46314072608947754\n",
      "tensor([2])\n",
      "tensor([[ 1.1720,  0.4555, -0.0118]], grad_fn=<AddmmBackward0>)\n",
      "Batch 58, Loss: 0.011832594871520996\n",
      "tensor([0])\n",
      "tensor([[ 1.1766,  0.4547, -0.0058]], grad_fn=<AddmmBackward0>)\n",
      "Batch 59, Loss: -1.176579475402832\n",
      "tensor([2])\n",
      "tensor([[ 1.1771,  0.4548, -0.0051]], grad_fn=<AddmmBackward0>)\n",
      "Batch 60, Loss: 0.0051138997077941895\n",
      "tensor([2])\n",
      "tensor([[ 1.1667,  0.4570, -0.0187]], grad_fn=<AddmmBackward0>)\n",
      "Batch 61, Loss: 0.01868218183517456\n",
      "tensor([1])\n",
      "tensor([[ 1.1767,  0.4551, -0.0055]], grad_fn=<AddmmBackward0>)\n",
      "Batch 62, Loss: -0.4551349878311157\n",
      "tensor([2])\n",
      "tensor([[ 1.1730,  0.4560, -0.0104]], grad_fn=<AddmmBackward0>)\n",
      "Batch 63, Loss: 0.010355889797210693\n",
      "tensor([1])\n",
      "tensor([[ 1.1723,  0.4563, -0.0111]], grad_fn=<AddmmBackward0>)\n",
      "Batch 64, Loss: -0.45631641149520874\n",
      "tensor([1])\n",
      "tensor([[ 1.1457,  0.4619, -0.0461]], grad_fn=<AddmmBackward0>)\n",
      "Batch 65, Loss: -0.46188807487487793\n",
      "tensor([1])\n",
      "tensor([[ 1.1742,  0.4563, -0.0085]], grad_fn=<AddmmBackward0>)\n",
      "Batch 66, Loss: -0.456268310546875\n",
      "tensor([1])\n",
      "tensor([[ 1.1774,  0.4558, -0.0043]], grad_fn=<AddmmBackward0>)\n",
      "Batch 67, Loss: -0.4558219909667969\n",
      "tensor([1])\n",
      "tensor([[ 1.1754,  0.4564, -0.0068]], grad_fn=<AddmmBackward0>)\n",
      "Batch 68, Loss: -0.45642518997192383\n",
      "tensor([2])\n",
      "tensor([[ 1.1781,  0.4561, -0.0032]], grad_fn=<AddmmBackward0>)\n",
      "Batch 69, Loss: 0.0032276511192321777\n",
      "tensor([2])\n",
      "tensor([[ 1.1404,  0.4639, -0.0527]], grad_fn=<AddmmBackward0>)\n",
      "Batch 70, Loss: 0.052687764167785645\n",
      "tensor([2])\n",
      "tensor([[ 1.1685,  0.4584, -0.0156]], grad_fn=<AddmmBackward0>)\n",
      "Batch 71, Loss: 0.01559382677078247\n",
      "tensor([0])\n",
      "tensor([[ 1.1734,  0.4576, -0.0090]], grad_fn=<AddmmBackward0>)\n",
      "Batch 72, Loss: -1.1734129190444946\n",
      "tensor([0])\n",
      "tensor([[ 1.1725,  0.4579, -0.0101]], grad_fn=<AddmmBackward0>)\n",
      "Batch 73, Loss: -1.1725261211395264\n",
      "tensor([0])\n",
      "tensor([[ 1.1752,  0.4575, -0.0066]], grad_fn=<AddmmBackward0>)\n",
      "Batch 74, Loss: -1.175201177597046\n",
      "tensor([0])\n",
      "tensor([[ 1.1750,  0.4577, -0.0069]], grad_fn=<AddmmBackward0>)\n",
      "Batch 75, Loss: -1.1749521493911743\n",
      "tensor([2])\n",
      "tensor([[ 1.1400,  0.4649, -0.0530]], grad_fn=<AddmmBackward0>)\n",
      "Batch 76, Loss: 0.05296957492828369\n",
      "tensor([0])\n",
      "tensor([[ 1.1729,  0.4583, -0.0097]], grad_fn=<AddmmBackward0>)\n",
      "Batch 77, Loss: -1.1729495525360107\n",
      "tensor([2])\n",
      "tensor([[ 1.1510,  0.4629, -0.0386]], grad_fn=<AddmmBackward0>)\n",
      "Batch 78, Loss: 0.038599252700805664\n",
      "tensor([2])\n",
      "tensor([[ 1.1478,  0.4636, -0.0428]], grad_fn=<AddmmBackward0>)\n",
      "Batch 79, Loss: 0.04279589653015137\n",
      "tensor([2])\n",
      "tensor([[ 1.1761,  0.4580, -0.0056]], grad_fn=<AddmmBackward0>)\n",
      "Batch 80, Loss: 0.005607187747955322\n",
      "tensor([2])\n",
      "tensor([[ 1.1690,  0.4595, -0.0149]], grad_fn=<AddmmBackward0>)\n",
      "Batch 81, Loss: 0.014881372451782227\n",
      "tensor([1])\n",
      "tensor([[ 1.1649,  0.4604, -0.0203]], grad_fn=<AddmmBackward0>)\n",
      "Batch 82, Loss: -0.46043020486831665\n",
      "tensor([0])\n",
      "tensor([[ 1.1686,  0.4598, -0.0153]], grad_fn=<AddmmBackward0>)\n",
      "Batch 83, Loss: -1.1685841083526611\n",
      "tensor([1])\n",
      "tensor([[ 1.1734,  0.4589, -0.0090]], grad_fn=<AddmmBackward0>)\n",
      "Batch 84, Loss: -0.45890673995018005\n",
      "tensor([1])\n",
      "tensor([[ 1.1762,  0.4585, -0.0053]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 85, Loss: -0.4584684371948242\n",
      "tensor([2])\n",
      "tensor([[ 1.1750,  0.4588, -0.0069]], grad_fn=<AddmmBackward0>)\n",
      "Batch 86, Loss: 0.0068625807762146\n",
      "tensor([1])\n",
      "tensor([[ 1.1780,  0.4584, -0.0028]], grad_fn=<AddmmBackward0>)\n",
      "Batch 87, Loss: -0.4583654999732971\n",
      "tensor([1])\n",
      "tensor([[ 1.1519,  0.4638, -0.0372]], grad_fn=<AddmmBackward0>)\n",
      "Batch 88, Loss: -0.46376535296440125\n",
      "tensor([0])\n",
      "tensor([[ 1.1798e+00,  4.5833e-01, -4.2909e-04]], grad_fn=<AddmmBackward0>)\n",
      "Batch 89, Loss: -1.1798195838928223\n",
      "tensor([2])\n",
      "tensor([[ 1.1801e+00,  4.5842e-01, -2.6166e-05]], grad_fn=<AddmmBackward0>)\n",
      "Batch 90, Loss: 2.6166439056396484e-05\n",
      "tensor([2])\n",
      "tensor([[ 1.1775,  0.4591, -0.0035]], grad_fn=<AddmmBackward0>)\n",
      "Batch 91, Loss: 0.0035319924354553223\n",
      "tensor([0])\n",
      "tensor([[ 1.1566,  0.4634, -0.0308]], grad_fn=<AddmmBackward0>)\n",
      "Batch 92, Loss: -1.15664803981781\n",
      "tensor([1])\n",
      "tensor([[ 1.1781,  0.4592, -0.0027]], grad_fn=<AddmmBackward0>)\n",
      "Batch 93, Loss: -0.45922210812568665\n",
      "tensor([1])\n",
      "tensor([[ 1.1427,  0.4664, -0.0491]], grad_fn=<AddmmBackward0>)\n",
      "Batch 94, Loss: -0.46642839908599854\n",
      "tensor([2])\n",
      "tensor([[1.1814, 0.4589, 0.0017]], grad_fn=<AddmmBackward0>)\n",
      "Batch 95, Loss: -0.0016533732414245605\n",
      "tensor([0])\n",
      "tensor([[ 1.1740,  0.4605, -0.0080]], grad_fn=<AddmmBackward0>)\n",
      "Batch 96, Loss: -1.1739766597747803\n",
      "tensor([2])\n",
      "tensor([[ 1.1614,  0.4631, -0.0245]], grad_fn=<AddmmBackward0>)\n",
      "Batch 97, Loss: 0.02452188730239868\n",
      "tensor([2])\n",
      "tensor([[ 1.1692,  0.4617, -0.0143]], grad_fn=<AddmmBackward0>)\n",
      "Batch 98, Loss: 0.014253020286560059\n",
      "tensor([0])\n",
      "tensor([[1.1823, 0.4592, 0.0029]], grad_fn=<AddmmBackward0>)\n",
      "Batch 99, Loss: -1.1822538375854492\n",
      "tensor([0])\n",
      "tensor([[ 1.1723,  0.4613, -0.0101]], grad_fn=<AddmmBackward0>)\n",
      "Batch 100, Loss: -1.1723333597183228\n",
      "tensor([2])\n",
      "tensor([[ 1.1509,  0.4657, -0.0383]], grad_fn=<AddmmBackward0>)\n",
      "Batch 101, Loss: 0.038274168968200684\n",
      "tensor([1])\n",
      "tensor([[ 1.1517,  0.4656, -0.0373]], grad_fn=<AddmmBackward0>)\n",
      "Batch 102, Loss: -0.4656035304069519\n",
      "tensor([0])\n",
      "tensor([[ 1.0618,  0.4836, -0.1554]], grad_fn=<AddmmBackward0>)\n",
      "Batch 103, Loss: -1.061846375465393\n",
      "tensor([0])\n",
      "tensor([[1.1802e+00, 4.6015e-01, 1.6189e-04]], grad_fn=<AddmmBackward0>)\n",
      "Batch 104, Loss: -1.1802098751068115\n",
      "tensor([0])\n",
      "tensor([[ 1.0267,  0.4908, -0.2017]], grad_fn=<AddmmBackward0>)\n",
      "Batch 105, Loss: -1.026707410812378\n",
      "tensor([2])\n",
      "tensor([[ 1.1481,  0.4668, -0.0423]], grad_fn=<AddmmBackward0>)\n",
      "Batch 106, Loss: 0.04231429100036621\n",
      "tensor([2])\n",
      "tensor([[ 1.1795e+00,  4.6061e-01, -1.0968e-03]], grad_fn=<AddmmBackward0>)\n",
      "Batch 107, Loss: 0.0010967850685119629\n",
      "tensor([1])\n",
      "tensor([[ 1.1693,  0.4627, -0.0145]], grad_fn=<AddmmBackward0>)\n",
      "Batch 108, Loss: -0.4627014994621277\n",
      "tensor([1])\n",
      "tensor([[1.1821, 0.4603, 0.0023]], grad_fn=<AddmmBackward0>)\n",
      "Batch 109, Loss: -0.4602685570716858\n",
      "tensor([0])\n",
      "tensor([[ 1.1706,  0.4627, -0.0128]], grad_fn=<AddmmBackward0>)\n",
      "Batch 110, Loss: -1.1705986261367798\n",
      "tensor([0])\n",
      "tensor([[ 1.1610,  0.4647, -0.0255]], grad_fn=<AddmmBackward0>)\n",
      "Batch 111, Loss: -1.161003589630127\n",
      "tensor([2])\n",
      "tensor([[ 1.1704,  0.4630, -0.0133]], grad_fn=<AddmmBackward0>)\n",
      "Batch 112, Loss: 0.01328808069229126\n",
      "tensor([0])\n",
      "tensor([[1.1821, 0.4608, 0.0020]], grad_fn=<AddmmBackward0>)\n",
      "Batch 113, Loss: -1.1820857524871826\n",
      "tensor([1])\n",
      "tensor([[ 1.1735,  0.4626, -0.0093]], grad_fn=<AddmmBackward0>)\n",
      "Batch 114, Loss: -0.46255528926849365\n",
      "tensor([2])\n",
      "tensor([[ 1.1179,  0.4737, -0.0825]], grad_fn=<AddmmBackward0>)\n",
      "Batch 115, Loss: 0.08254319429397583\n",
      "tensor([0])\n",
      "tensor([[ 1.1743,  0.4626, -0.0084]], grad_fn=<AddmmBackward0>)\n",
      "Batch 116, Loss: -1.1743388175964355\n",
      "tensor([2])\n",
      "tensor([[ 1.1698,  0.4636, -0.0144]], grad_fn=<AddmmBackward0>)\n",
      "Batch 117, Loss: 0.014412939548492432\n",
      "tensor([1])\n",
      "tensor([[1.1846, 0.4608, 0.0050]], grad_fn=<AddmmBackward0>)\n",
      "Batch 118, Loss: -0.4608149528503418\n",
      "tensor([0])\n",
      "tensor([[ 1.1792,  0.4620, -0.0022]], grad_fn=<AddmmBackward0>)\n",
      "Batch 119, Loss: -1.1791934967041016\n",
      "tensor([1])\n",
      "tensor([[ 1.1742,  0.4631, -0.0088]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            print(target)\n",
    "            print(output)\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(qi)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            print(target)\n",
    "            print(output)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
