{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fish', 'apple']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"hybridqnn_shallow\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "LR = 0.001\n",
    "n_samples_train = 200 #128\n",
    "n_samples_test = 50 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"CIFAR100\" # MNIST / CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = ['fish','apple'] # ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "if network == 'hybridqnn_shallow' or network == 'QSVM':\n",
    "    # Set preprocessing transforms\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "else:\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    \n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: 0.00023650005459785461 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZ),\n",
      "  CircuitStateFn(\n",
      "       ┌──────────────────────────┐┌──────────────────────────────────────┐\n",
      "  q_0: ┤0                         ├┤0                                     ├\n",
      "       │  ZZFeatureMap(x[0],x[1]) ││  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
      "  q_1: ┤1                         ├┤1                                     ├\n",
      "       └──────────────────────────┘└──────────────────────────────────────┘\n",
      "  )\n",
      "])\n",
      "HybridQNN_Shallow(\n",
      "  (conv1): Conv2d(3, 2, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (qnn): TorchConnector()\n",
      "  (fc3): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Network init elapsed time: -0.00039108656346797943 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        #n_filts_fc1 = int(((((input_resolution[0])/2)-4)/2)**2)*16 # +7\n",
    "        n_filts_fc1 = 256\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 0.2588122487068176\n",
      "Batch 1, Loss: -1.3727190494537354\n",
      "Batch 2, Loss: 0.30189457535743713\n",
      "Batch 3, Loss: -1.2926857471466064\n",
      "Batch 4, Loss: -1.2980340719223022\n",
      "Batch 5, Loss: -1.4221248626708984\n",
      "Batch 6, Loss: -1.440446376800537\n",
      "Batch 7, Loss: -1.1996232271194458\n",
      "Batch 8, Loss: 0.5124842524528503\n",
      "Batch 9, Loss: -1.2448395490646362\n",
      "Batch 10, Loss: -1.611942172050476\n",
      "Batch 11, Loss: -1.3116666078567505\n",
      "Batch 12, Loss: -1.3752849102020264\n",
      "Batch 13, Loss: 0.30929380655288696\n",
      "Batch 14, Loss: -1.497912883758545\n",
      "Batch 15, Loss: 0.5144872069358826\n",
      "Batch 16, Loss: 0.6283481121063232\n",
      "Batch 17, Loss: -1.4871702194213867\n",
      "Batch 18, Loss: -1.6485179662704468\n",
      "Batch 19, Loss: -1.414034366607666\n",
      "Batch 20, Loss: 0.46465522050857544\n",
      "Batch 21, Loss: 0.5277193784713745\n",
      "Batch 22, Loss: -1.5836567878723145\n",
      "Batch 23, Loss: -1.6559722423553467\n",
      "Batch 24, Loss: -1.5888381004333496\n",
      "Batch 25, Loss: -1.533888816833496\n",
      "Batch 26, Loss: 0.471618115901947\n",
      "Batch 27, Loss: -1.393414855003357\n",
      "Batch 28, Loss: -1.4362947940826416\n",
      "Batch 29, Loss: 0.4739060699939728\n",
      "Batch 30, Loss: -1.4946506023406982\n",
      "Batch 31, Loss: 0.3563687205314636\n",
      "Batch 32, Loss: 0.3864487409591675\n",
      "Batch 33, Loss: 0.3973592221736908\n",
      "Batch 34, Loss: 0.37080296874046326\n",
      "Batch 35, Loss: 0.461063414812088\n",
      "Batch 36, Loss: -1.5098861455917358\n",
      "Batch 37, Loss: -1.3202295303344727\n",
      "Batch 38, Loss: 0.3439575433731079\n",
      "Batch 39, Loss: -1.5846753120422363\n",
      "Batch 40, Loss: -1.4112203121185303\n",
      "Batch 41, Loss: -1.4870364665985107\n",
      "Batch 42, Loss: 0.504123330116272\n",
      "Batch 43, Loss: 0.31534770131111145\n",
      "Batch 44, Loss: 0.3929320275783539\n",
      "Batch 45, Loss: 0.4814249277114868\n",
      "Batch 46, Loss: -1.5160512924194336\n",
      "Batch 47, Loss: -1.297437071800232\n",
      "Batch 48, Loss: 0.36723023653030396\n",
      "Batch 49, Loss: -1.4250332117080688\n",
      "Batch 50, Loss: -1.4056276082992554\n",
      "Batch 51, Loss: -1.366335153579712\n",
      "Batch 52, Loss: -1.4293572902679443\n",
      "Batch 53, Loss: -1.383959174156189\n",
      "Batch 54, Loss: 0.5369350910186768\n",
      "Batch 55, Loss: 0.5964037179946899\n",
      "Batch 56, Loss: 0.3041282296180725\n",
      "Batch 57, Loss: -1.429189920425415\n",
      "Batch 58, Loss: 0.3397708535194397\n",
      "Batch 59, Loss: -1.4890824556350708\n",
      "Batch 60, Loss: -1.4456210136413574\n",
      "Batch 61, Loss: -1.3965755701065063\n",
      "Batch 62, Loss: 0.572182297706604\n",
      "Batch 63, Loss: 0.4704451560974121\n",
      "Batch 64, Loss: -1.6795759201049805\n",
      "Batch 65, Loss: 0.5857920050621033\n",
      "Batch 66, Loss: -1.6868183612823486\n",
      "Batch 67, Loss: -1.4650275707244873\n",
      "Batch 68, Loss: 0.5184451341629028\n",
      "Batch 69, Loss: 0.46996253728866577\n",
      "Batch 70, Loss: -1.51033616065979\n",
      "Batch 71, Loss: -1.470012903213501\n",
      "Batch 72, Loss: 0.4945336580276489\n",
      "Batch 73, Loss: -1.5730514526367188\n",
      "Batch 74, Loss: 0.47293007373809814\n",
      "Batch 75, Loss: -1.2944719791412354\n",
      "Batch 76, Loss: 0.4060336947441101\n",
      "Batch 77, Loss: -1.491885781288147\n",
      "Batch 78, Loss: -1.4710291624069214\n",
      "Batch 79, Loss: -1.3556725978851318\n",
      "Batch 80, Loss: 0.34458765387535095\n",
      "Batch 81, Loss: -1.5060093402862549\n",
      "Batch 82, Loss: -1.3383867740631104\n",
      "Batch 83, Loss: -1.448483943939209\n",
      "Batch 84, Loss: -1.435740351676941\n",
      "Batch 85, Loss: 0.5551992058753967\n",
      "Batch 86, Loss: 0.6275136470794678\n",
      "Batch 87, Loss: -1.534616470336914\n",
      "Batch 88, Loss: 0.49203914403915405\n",
      "Batch 89, Loss: -1.648136019706726\n",
      "Batch 90, Loss: -1.5368510484695435\n",
      "Batch 91, Loss: -1.6964762210845947\n",
      "Batch 92, Loss: -1.3629589080810547\n",
      "Batch 93, Loss: 0.6584101915359497\n",
      "Batch 94, Loss: 0.4734542667865753\n",
      "Batch 95, Loss: 0.6800268888473511\n",
      "Batch 96, Loss: 0.5561178922653198\n",
      "Batch 97, Loss: 0.6090233325958252\n",
      "Batch 98, Loss: 0.4746328890323639\n",
      "Batch 99, Loss: 0.5971429944038391\n",
      "Batch 100, Loss: -1.5931020975112915\n",
      "Batch 101, Loss: -1.4774903059005737\n",
      "Batch 102, Loss: -1.3975213766098022\n",
      "Batch 103, Loss: -1.4483287334442139\n",
      "Batch 104, Loss: -1.432322382926941\n",
      "Batch 105, Loss: -1.657450795173645\n",
      "Batch 106, Loss: 0.4110703766345978\n",
      "Batch 107, Loss: 0.4928012490272522\n",
      "Batch 108, Loss: 0.3897010087966919\n",
      "Batch 109, Loss: -1.5972144603729248\n",
      "Batch 110, Loss: -1.6122708320617676\n",
      "Batch 111, Loss: 0.36410871148109436\n",
      "Batch 112, Loss: 0.5067577362060547\n",
      "Batch 113, Loss: -1.5720410346984863\n",
      "Batch 114, Loss: -1.5245145559310913\n",
      "Batch 115, Loss: -1.6192800998687744\n",
      "Batch 116, Loss: -1.600783109664917\n",
      "Batch 117, Loss: -1.6323230266571045\n",
      "Batch 118, Loss: 0.5616438984870911\n",
      "Batch 119, Loss: 0.3356584310531616\n",
      "Batch 120, Loss: -1.4048781394958496\n",
      "Batch 121, Loss: 0.47493064403533936\n",
      "Batch 122, Loss: -1.705312967300415\n",
      "Batch 123, Loss: 0.5119296908378601\n",
      "Batch 124, Loss: 0.4719542860984802\n",
      "Batch 125, Loss: -1.6282827854156494\n",
      "Batch 126, Loss: -1.416459321975708\n",
      "Batch 127, Loss: 0.4529658257961273\n",
      "Batch 128, Loss: 0.4880307912826538\n",
      "Batch 129, Loss: -1.6061556339263916\n",
      "Batch 130, Loss: 0.5194762945175171\n",
      "Batch 131, Loss: 0.2890700399875641\n",
      "Batch 132, Loss: 0.6444937586784363\n",
      "Batch 133, Loss: -1.677325963973999\n",
      "Batch 134, Loss: -1.6400301456451416\n",
      "Batch 135, Loss: 0.47698989510536194\n",
      "Batch 136, Loss: 0.6700850129127502\n",
      "Batch 137, Loss: 0.43250522017478943\n",
      "Batch 138, Loss: 0.570959210395813\n",
      "Batch 139, Loss: 0.27363184094429016\n",
      "Batch 140, Loss: 0.27337202429771423\n",
      "Batch 141, Loss: -1.4349333047866821\n",
      "Batch 142, Loss: 0.6387228965759277\n",
      "Batch 143, Loss: 0.4431077837944031\n",
      "Batch 144, Loss: -1.4908678531646729\n",
      "Batch 145, Loss: -1.681315541267395\n",
      "Batch 146, Loss: 0.6621264219284058\n",
      "Batch 147, Loss: 0.37202537059783936\n",
      "Batch 148, Loss: 0.5736942291259766\n",
      "Batch 149, Loss: -1.5155116319656372\n",
      "Batch 150, Loss: -1.2076910734176636\n",
      "Batch 151, Loss: -1.441343069076538\n",
      "Batch 152, Loss: 0.34913888573646545\n",
      "Batch 153, Loss: -1.6395453214645386\n",
      "Batch 154, Loss: 0.5211924314498901\n",
      "Batch 155, Loss: -1.4001319408416748\n",
      "Batch 156, Loss: 0.26082322001457214\n",
      "Batch 157, Loss: 0.4433467984199524\n",
      "Batch 158, Loss: -1.4264349937438965\n",
      "Batch 159, Loss: 0.4080992341041565\n",
      "Batch 160, Loss: 0.5690127015113831\n",
      "Batch 161, Loss: -1.2513971328735352\n",
      "Batch 162, Loss: -1.2840864658355713\n",
      "Batch 163, Loss: 0.3525128960609436\n",
      "Batch 164, Loss: -1.4156337976455688\n",
      "Batch 165, Loss: -1.3518924713134766\n",
      "Batch 166, Loss: -1.3787869215011597\n",
      "Batch 167, Loss: -1.5153923034667969\n",
      "Batch 168, Loss: 0.46850165724754333\n",
      "Batch 169, Loss: 0.36547794938087463\n",
      "Batch 170, Loss: 0.5264062285423279\n",
      "Batch 171, Loss: -1.5112192630767822\n",
      "Batch 172, Loss: -1.501660943031311\n",
      "Batch 173, Loss: -1.4444670677185059\n",
      "Batch 174, Loss: 0.578204870223999\n",
      "Batch 175, Loss: -1.3629193305969238\n",
      "Batch 176, Loss: -1.6381086111068726\n",
      "Batch 177, Loss: 0.4623074531555176\n",
      "Batch 178, Loss: 0.6203047633171082\n",
      "Batch 179, Loss: 0.3859674334526062\n",
      "Batch 180, Loss: 0.3138056695461273\n",
      "Batch 181, Loss: -1.3591736555099487\n",
      "Batch 182, Loss: -1.3788049221038818\n",
      "Batch 183, Loss: 0.5038779377937317\n",
      "Batch 184, Loss: -1.603062391281128\n",
      "Batch 185, Loss: 0.2988801896572113\n",
      "Batch 186, Loss: -1.491982102394104\n",
      "Batch 187, Loss: 0.24807250499725342\n",
      "Batch 188, Loss: 0.5667252540588379\n",
      "Batch 189, Loss: -1.5970327854156494\n",
      "Batch 190, Loss: 0.6861413717269897\n",
      "Batch 191, Loss: 0.3453795909881592\n",
      "Batch 192, Loss: 0.5155661106109619\n",
      "Batch 193, Loss: -1.6879098415374756\n",
      "Batch 194, Loss: -1.3658740520477295\n",
      "Batch 195, Loss: 0.3558725416660309\n",
      "Batch 196, Loss: -1.2909646034240723\n",
      "Batch 197, Loss: 0.687638521194458\n",
      "Batch 198, Loss: 0.4640103280544281\n",
      "Batch 199, Loss: 0.3056219816207886\n",
      "Batch 200, Loss: -1.5759351253509521\n",
      "Batch 201, Loss: -1.488183856010437\n",
      "Batch 202, Loss: -1.5914218425750732\n",
      "Batch 203, Loss: -1.47526216506958\n",
      "Batch 204, Loss: 0.42909303307533264\n",
      "Batch 205, Loss: -1.3256744146347046\n",
      "Batch 206, Loss: 0.5294645428657532\n",
      "Batch 207, Loss: 0.40253886580467224\n",
      "Batch 208, Loss: 0.5122387409210205\n",
      "Batch 209, Loss: -1.3388947248458862\n",
      "Batch 210, Loss: 0.5992646217346191\n",
      "Batch 211, Loss: -1.3112916946411133\n",
      "Batch 212, Loss: 0.3968943953514099\n",
      "Batch 213, Loss: -1.4924583435058594\n",
      "Batch 214, Loss: -1.5396521091461182\n",
      "Batch 215, Loss: 0.393856942653656\n",
      "Batch 216, Loss: -1.5195995569229126\n",
      "Batch 217, Loss: -1.4218535423278809\n",
      "Batch 218, Loss: -1.3937327861785889\n",
      "Batch 219, Loss: 0.4076729714870453\n",
      "Batch 220, Loss: 0.32993465662002563\n",
      "Batch 221, Loss: 0.3783789873123169\n",
      "Batch 222, Loss: 0.23949405550956726\n",
      "Batch 223, Loss: -1.3384196758270264\n",
      "Batch 224, Loss: 0.4404892027378082\n",
      "Batch 225, Loss: -1.3617619276046753\n",
      "Batch 226, Loss: 0.2015298455953598\n",
      "Batch 227, Loss: 0.18829260766506195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 228, Loss: 0.3454056680202484\n",
      "Batch 229, Loss: 0.41423165798187256\n",
      "Batch 230, Loss: 0.15803436934947968\n",
      "Batch 231, Loss: 0.1584128439426422\n",
      "Batch 232, Loss: 0.16020505130290985\n",
      "Batch 233, Loss: -1.1760333776474\n",
      "Batch 234, Loss: 0.2731703221797943\n",
      "Batch 235, Loss: -1.4008419513702393\n",
      "Batch 236, Loss: 0.299980491399765\n",
      "Batch 237, Loss: 0.3574437201023102\n",
      "Batch 238, Loss: 0.17037436366081238\n",
      "Batch 239, Loss: 0.1506514847278595\n",
      "Batch 240, Loss: 0.36612844467163086\n",
      "Batch 241, Loss: 0.2791845500469208\n",
      "Batch 242, Loss: 0.10401096940040588\n",
      "Batch 243, Loss: 0.08870571851730347\n",
      "Batch 244, Loss: 0.08814570307731628\n",
      "Batch 245, Loss: 0.1609172374010086\n",
      "Batch 246, Loss: -1.098236322402954\n",
      "Batch 247, Loss: -1.2929198741912842\n",
      "Batch 248, Loss: 0.3311697542667389\n",
      "Batch 249, Loss: -1.127856969833374\n",
      "Batch 250, Loss: 0.06930160522460938\n",
      "Batch 251, Loss: 0.14442814886569977\n",
      "Batch 252, Loss: -1.0612977743148804\n",
      "Batch 253, Loss: 0.2530902922153473\n",
      "Batch 254, Loss: 0.08640927076339722\n",
      "Batch 255, Loss: -1.140946626663208\n",
      "Batch 256, Loss: -1.2142393589019775\n",
      "Batch 257, Loss: 0.12891702353954315\n",
      "Batch 258, Loss: -1.0724818706512451\n",
      "Batch 259, Loss: -1.0907362699508667\n",
      "Batch 260, Loss: -1.0661907196044922\n",
      "Batch 261, Loss: -1.0802768468856812\n",
      "Batch 262, Loss: 0.1371970921754837\n",
      "Batch 263, Loss: -1.0616730451583862\n",
      "Batch 264, Loss: 0.400477796792984\n",
      "Batch 265, Loss: -1.0721726417541504\n",
      "Batch 266, Loss: -1.1001012325286865\n",
      "Batch 267, Loss: 0.06301721930503845\n",
      "Batch 268, Loss: 0.10585752129554749\n",
      "Batch 269, Loss: -1.0932347774505615\n",
      "Batch 270, Loss: 0.17354927957057953\n",
      "Batch 271, Loss: 0.07025265693664551\n",
      "Batch 272, Loss: 0.15833830833435059\n",
      "Batch 273, Loss: 0.18567733466625214\n",
      "Batch 274, Loss: -1.2265909910202026\n",
      "Batch 275, Loss: 0.25287455320358276\n",
      "Batch 276, Loss: -1.062822937965393\n",
      "Batch 277, Loss: -1.1526821851730347\n",
      "Batch 278, Loss: -1.1564445495605469\n",
      "Batch 279, Loss: 0.13220345973968506\n",
      "Batch 280, Loss: 0.06535682082176208\n",
      "Batch 281, Loss: -1.2203421592712402\n",
      "Batch 282, Loss: 0.15332745015621185\n",
      "Batch 283, Loss: 0.06145915389060974\n",
      "Batch 284, Loss: -1.064050555229187\n",
      "Batch 285, Loss: 0.08659031987190247\n",
      "Batch 286, Loss: -1.0683040618896484\n",
      "Batch 287, Loss: 0.07003316283226013\n",
      "Batch 288, Loss: -1.1679127216339111\n",
      "Batch 289, Loss: -1.062821626663208\n",
      "Batch 290, Loss: -1.0715134143829346\n",
      "Batch 291, Loss: -1.0648828744888306\n",
      "Batch 292, Loss: -1.0941760540008545\n",
      "Batch 293, Loss: 0.09179860353469849\n",
      "Batch 294, Loss: 0.11549851298332214\n",
      "Batch 295, Loss: 0.07471460103988647\n",
      "Batch 296, Loss: -1.110866904258728\n",
      "Batch 297, Loss: 0.08303394913673401\n",
      "Batch 298, Loss: 0.0655343234539032\n",
      "Batch 299, Loss: -1.110893726348877\n",
      "Batch 300, Loss: -1.1605944633483887\n",
      "Batch 301, Loss: 0.06690821051597595\n",
      "Batch 302, Loss: -1.167094349861145\n",
      "Batch 303, Loss: 0.19577325880527496\n",
      "Batch 304, Loss: 0.09460130333900452\n",
      "Batch 305, Loss: -1.181395411491394\n",
      "Batch 306, Loss: -1.0943442583084106\n",
      "Batch 307, Loss: -1.1388598680496216\n",
      "Batch 308, Loss: 0.09904399514198303\n",
      "Batch 309, Loss: -1.062393069267273\n",
      "Batch 310, Loss: 0.24451133608818054\n",
      "Batch 311, Loss: 0.2685990035533905\n",
      "Batch 312, Loss: 0.06265726685523987\n",
      "Batch 313, Loss: -1.2024028301239014\n",
      "Batch 314, Loss: 0.06426069140434265\n",
      "Batch 315, Loss: -1.3332693576812744\n",
      "Batch 316, Loss: 0.17588894069194794\n",
      "Batch 317, Loss: 0.12925484776496887\n",
      "Batch 318, Loss: -1.1909607648849487\n",
      "Batch 319, Loss: -1.2521624565124512\n",
      "Batch 320, Loss: 0.07916423678398132\n",
      "Batch 321, Loss: -1.119031310081482\n",
      "Batch 322, Loss: 0.36188092827796936\n",
      "Batch 323, Loss: 0.05962905287742615\n",
      "Batch 324, Loss: -1.1615628004074097\n",
      "Batch 325, Loss: 0.16491536796092987\n",
      "Batch 326, Loss: -1.0596128702163696\n",
      "Batch 327, Loss: 0.1600775420665741\n",
      "Batch 328, Loss: -1.4247808456420898\n",
      "Batch 329, Loss: 0.21734145283699036\n",
      "Batch 330, Loss: -1.1552717685699463\n",
      "Batch 331, Loss: -1.2007529735565186\n",
      "Batch 332, Loss: 0.30608946084976196\n",
      "Batch 333, Loss: -1.08488130569458\n",
      "Batch 334, Loss: 0.10885807871818542\n",
      "Batch 335, Loss: 0.1715383529663086\n",
      "Batch 336, Loss: -1.2752656936645508\n",
      "Batch 337, Loss: -1.1997874975204468\n",
      "Batch 338, Loss: 0.12189623713493347\n",
      "Batch 339, Loss: -1.4044386148452759\n",
      "Batch 340, Loss: 0.14095158874988556\n",
      "Batch 341, Loss: -1.2013757228851318\n",
      "Batch 342, Loss: -1.3807891607284546\n",
      "Batch 343, Loss: -1.3794922828674316\n",
      "Batch 344, Loss: -1.175451636314392\n",
      "Batch 345, Loss: 0.677943229675293\n",
      "Batch 346, Loss: -1.103137731552124\n",
      "Batch 347, Loss: 0.6547436714172363\n",
      "Batch 348, Loss: -1.6339912414550781\n",
      "Batch 349, Loss: 0.4662247896194458\n",
      "Batch 350, Loss: -1.6356785297393799\n",
      "Batch 351, Loss: -1.5073413848876953\n",
      "Batch 352, Loss: -1.6135836839675903\n",
      "Batch 353, Loss: -1.1136088371276855\n",
      "Batch 354, Loss: -1.0413062572479248\n",
      "Batch 355, Loss: -1.3071939945220947\n",
      "Batch 356, Loss: -1.5075337886810303\n",
      "Batch 357, Loss: 0.24352706968784332\n",
      "Batch 358, Loss: 0.40046221017837524\n",
      "Batch 359, Loss: -1.157564640045166\n",
      "Batch 360, Loss: -1.2884118556976318\n",
      "Batch 361, Loss: 0.34109342098236084\n",
      "Batch 362, Loss: 0.6598858833312988\n",
      "Batch 363, Loss: -1.1855264902114868\n",
      "Batch 364, Loss: 0.2846328318119049\n",
      "Batch 365, Loss: 0.416020929813385\n",
      "Batch 366, Loss: 0.3606157600879669\n",
      "Batch 367, Loss: 0.27625131607055664\n",
      "Batch 368, Loss: -1.5399264097213745\n",
      "Batch 369, Loss: -1.4125968217849731\n",
      "Batch 370, Loss: 0.4528980255126953\n",
      "Batch 371, Loss: 0.35450732707977295\n",
      "Batch 372, Loss: -1.1366719007492065\n",
      "Batch 373, Loss: -1.1083821058273315\n",
      "Batch 374, Loss: -1.215084195137024\n",
      "Batch 375, Loss: -1.2873822450637817\n",
      "Batch 376, Loss: 0.256529301404953\n",
      "Batch 377, Loss: 0.4487512707710266\n",
      "Batch 378, Loss: -1.0567114353179932\n",
      "Batch 379, Loss: 0.6705087423324585\n",
      "Batch 380, Loss: -1.2251595258712769\n",
      "Batch 381, Loss: 0.11196917295455933\n",
      "Batch 382, Loss: 0.41659078001976013\n",
      "Batch 383, Loss: -1.443321943283081\n",
      "Batch 384, Loss: -1.2577383518218994\n",
      "Batch 385, Loss: 0.6146244406700134\n",
      "Batch 386, Loss: -1.3931677341461182\n",
      "Batch 387, Loss: 0.45291662216186523\n",
      "Batch 388, Loss: -1.1889616250991821\n",
      "Batch 389, Loss: -1.303539752960205\n",
      "Batch 390, Loss: 0.2505302131175995\n",
      "Batch 391, Loss: 0.05713522434234619\n",
      "Batch 392, Loss: 0.4567091166973114\n",
      "Batch 393, Loss: -1.2166814804077148\n",
      "Batch 394, Loss: 0.32199594378471375\n",
      "Batch 395, Loss: -1.1851627826690674\n",
      "Batch 396, Loss: -1.5806047916412354\n",
      "Batch 397, Loss: 0.31425198912620544\n",
      "Batch 398, Loss: 0.4819156229496002\n",
      "Batch 399, Loss: 0.2512562572956085\n",
      "Training [10%]\tLoss: -0.5052\n",
      "Batch 0, Loss: 0.3585764169692993\n",
      "Batch 1, Loss: -1.3010492324829102\n",
      "Batch 2, Loss: 0.5252666473388672\n",
      "Batch 3, Loss: -1.1841119527816772\n",
      "Batch 4, Loss: 0.4425283670425415\n",
      "Batch 5, Loss: -1.1138277053833008\n",
      "Batch 6, Loss: -1.2848867177963257\n",
      "Batch 7, Loss: -1.166591763496399\n",
      "Batch 8, Loss: 0.25501030683517456\n",
      "Batch 9, Loss: 0.45026862621307373\n",
      "Batch 10, Loss: -1.2385703325271606\n",
      "Batch 11, Loss: -1.4696755409240723\n",
      "Batch 12, Loss: -1.407055139541626\n",
      "Batch 13, Loss: 0.5442753434181213\n",
      "Batch 14, Loss: -1.1469076871871948\n",
      "Batch 15, Loss: -1.6116676330566406\n",
      "Batch 16, Loss: -1.488741159439087\n",
      "Batch 17, Loss: -1.5258209705352783\n",
      "Batch 18, Loss: 0.2921792268753052\n",
      "Batch 19, Loss: -1.3522329330444336\n",
      "Batch 20, Loss: -1.5912364721298218\n",
      "Batch 21, Loss: 0.2618944048881531\n",
      "Batch 22, Loss: -1.52041494846344\n",
      "Batch 23, Loss: 0.6360453367233276\n",
      "Batch 24, Loss: -1.5390357971191406\n",
      "Batch 25, Loss: 0.3312367796897888\n",
      "Batch 26, Loss: 0.6555352210998535\n",
      "Batch 27, Loss: -1.4837380647659302\n",
      "Batch 28, Loss: -1.4248168468475342\n",
      "Batch 29, Loss: -1.3445855379104614\n",
      "Batch 30, Loss: 0.0903271734714508\n",
      "Batch 31, Loss: -1.3508626222610474\n",
      "Batch 32, Loss: 0.35843726992607117\n",
      "Batch 33, Loss: 0.13833396136760712\n",
      "Batch 34, Loss: -1.4001874923706055\n",
      "Batch 35, Loss: -1.305461049079895\n",
      "Batch 36, Loss: 0.6169842481613159\n",
      "Batch 37, Loss: -1.6125410795211792\n",
      "Batch 38, Loss: -1.570923089981079\n",
      "Batch 39, Loss: 0.3098475933074951\n",
      "Batch 40, Loss: -1.2428476810455322\n",
      "Batch 41, Loss: -1.5524814128875732\n",
      "Batch 42, Loss: -1.3916047811508179\n",
      "Batch 43, Loss: 0.20903804898262024\n",
      "Batch 44, Loss: 0.11236938834190369\n",
      "Batch 45, Loss: 0.2117086797952652\n",
      "Batch 46, Loss: -1.1156247854232788\n",
      "Batch 47, Loss: 0.32245391607284546\n",
      "Batch 48, Loss: -1.560522437095642\n",
      "Batch 49, Loss: 0.30917632579803467\n",
      "Batch 50, Loss: -1.4879182577133179\n",
      "Batch 51, Loss: 0.37187910079956055\n",
      "Batch 52, Loss: 0.5755680799484253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 53, Loss: -1.4168353080749512\n",
      "Batch 54, Loss: -1.6560497283935547\n",
      "Batch 55, Loss: -1.4430493116378784\n",
      "Batch 56, Loss: 0.3670339584350586\n",
      "Batch 57, Loss: 0.35118016600608826\n",
      "Batch 58, Loss: 0.471291720867157\n",
      "Batch 59, Loss: 0.352267324924469\n",
      "Batch 60, Loss: -1.4390807151794434\n",
      "Batch 61, Loss: -1.5953460931777954\n",
      "Batch 62, Loss: -1.3795193433761597\n",
      "Batch 63, Loss: 0.27252915501594543\n",
      "Batch 64, Loss: -1.5892919301986694\n",
      "Batch 65, Loss: -1.2681361436843872\n",
      "Batch 66, Loss: 0.20084138214588165\n",
      "Batch 67, Loss: -1.489824652671814\n",
      "Batch 68, Loss: 0.2300788164138794\n",
      "Batch 69, Loss: 0.17786909639835358\n",
      "Batch 70, Loss: -1.673468828201294\n",
      "Batch 71, Loss: -1.109921932220459\n",
      "Batch 72, Loss: 0.3143109679222107\n",
      "Batch 73, Loss: -1.286578893661499\n",
      "Batch 74, Loss: 0.2783259153366089\n",
      "Batch 75, Loss: 0.3628549575805664\n",
      "Batch 76, Loss: 0.49949848651885986\n",
      "Batch 77, Loss: -1.194149374961853\n",
      "Batch 78, Loss: -1.0719044208526611\n",
      "Batch 79, Loss: -1.054772138595581\n",
      "Batch 80, Loss: 0.13855063915252686\n",
      "Batch 81, Loss: -1.1733037233352661\n",
      "Batch 82, Loss: 0.6365196704864502\n",
      "Batch 83, Loss: 0.41670525074005127\n",
      "Batch 84, Loss: -1.558490514755249\n",
      "Batch 85, Loss: 0.20754919946193695\n",
      "Batch 86, Loss: 0.10269138216972351\n",
      "Batch 87, Loss: 0.4771803319454193\n",
      "Batch 88, Loss: 0.5070092082023621\n",
      "Batch 89, Loss: 0.06551152467727661\n",
      "Batch 90, Loss: -1.0520246028900146\n",
      "Batch 91, Loss: 0.15026457607746124\n",
      "Batch 92, Loss: -1.052657127380371\n",
      "Batch 93, Loss: -1.134041666984558\n",
      "Batch 94, Loss: -1.1525269746780396\n",
      "Batch 95, Loss: -1.474900722503662\n",
      "Batch 96, Loss: 0.5091118812561035\n",
      "Batch 97, Loss: -1.41066575050354\n",
      "Batch 98, Loss: 0.4443860650062561\n",
      "Batch 99, Loss: 0.31262651085853577\n",
      "Batch 100, Loss: 0.3290044367313385\n",
      "Batch 101, Loss: -1.0865689516067505\n",
      "Batch 102, Loss: 0.23530447483062744\n",
      "Batch 103, Loss: 0.5341141819953918\n",
      "Batch 104, Loss: -1.2251449823379517\n",
      "Batch 105, Loss: -1.328315019607544\n",
      "Batch 106, Loss: -1.4400714635849\n",
      "Batch 107, Loss: -1.290285587310791\n",
      "Batch 108, Loss: -1.4095677137374878\n",
      "Batch 109, Loss: -1.2404406070709229\n",
      "Batch 110, Loss: 0.2025996893644333\n",
      "Batch 111, Loss: 0.6218778491020203\n",
      "Batch 112, Loss: 0.2375442534685135\n",
      "Batch 113, Loss: -1.5698131322860718\n",
      "Batch 114, Loss: -1.3590521812438965\n",
      "Batch 115, Loss: -1.3168144226074219\n",
      "Batch 116, Loss: 0.5360898971557617\n",
      "Batch 117, Loss: 0.36767256259918213\n",
      "Batch 118, Loss: -1.300835371017456\n",
      "Batch 119, Loss: -1.1679117679595947\n",
      "Batch 120, Loss: -1.3204026222229004\n",
      "Batch 121, Loss: 0.5372539758682251\n",
      "Batch 122, Loss: 0.21536852419376373\n",
      "Batch 123, Loss: 0.36754468083381653\n",
      "Batch 124, Loss: -1.4180643558502197\n",
      "Batch 125, Loss: -1.1922659873962402\n",
      "Batch 126, Loss: 0.6922032833099365\n",
      "Batch 127, Loss: 0.36347219347953796\n",
      "Batch 128, Loss: -1.468281626701355\n",
      "Batch 129, Loss: 0.25177454948425293\n",
      "Batch 130, Loss: -1.5951812267303467\n",
      "Batch 131, Loss: -1.718705415725708\n",
      "Batch 132, Loss: -1.2770944833755493\n",
      "Batch 133, Loss: 0.573732316493988\n",
      "Batch 134, Loss: 0.6872025728225708\n",
      "Batch 135, Loss: -1.4759937524795532\n",
      "Batch 136, Loss: 0.46972113847732544\n",
      "Batch 137, Loss: 0.04630166292190552\n",
      "Batch 138, Loss: -1.521415114402771\n",
      "Batch 139, Loss: -1.3518874645233154\n",
      "Batch 140, Loss: -1.3790295124053955\n",
      "Batch 141, Loss: 0.24434618651866913\n",
      "Batch 142, Loss: 0.25091636180877686\n",
      "Batch 143, Loss: -1.1319133043289185\n",
      "Batch 144, Loss: -1.1372939348220825\n",
      "Batch 145, Loss: -1.4865400791168213\n",
      "Batch 146, Loss: 0.16689090430736542\n",
      "Batch 147, Loss: 0.39353057742118835\n",
      "Batch 148, Loss: 0.1582096517086029\n",
      "Batch 149, Loss: -1.2991976737976074\n",
      "Batch 150, Loss: -1.289143443107605\n",
      "Batch 151, Loss: -1.3789100646972656\n",
      "Batch 152, Loss: -1.3840759992599487\n",
      "Batch 153, Loss: 0.4921824336051941\n",
      "Batch 154, Loss: -1.1880959272384644\n",
      "Batch 155, Loss: 0.23425191640853882\n",
      "Batch 156, Loss: -1.1627840995788574\n",
      "Batch 157, Loss: -1.2569499015808105\n",
      "Batch 158, Loss: 0.19798725843429565\n",
      "Batch 159, Loss: -1.2870912551879883\n",
      "Batch 160, Loss: -1.6631157398223877\n",
      "Batch 161, Loss: 0.6721872091293335\n",
      "Batch 162, Loss: 0.21027667820453644\n",
      "Batch 163, Loss: 0.48781490325927734\n",
      "Batch 164, Loss: -1.6560566425323486\n",
      "Batch 165, Loss: -1.3079451322555542\n",
      "Batch 166, Loss: 0.7075417041778564\n",
      "Batch 167, Loss: 0.36205047369003296\n",
      "Batch 168, Loss: -1.2691318988800049\n",
      "Batch 169, Loss: -1.3743153810501099\n",
      "Batch 170, Loss: 0.6793811321258545\n",
      "Batch 171, Loss: -1.1982316970825195\n",
      "Batch 172, Loss: 0.2963067889213562\n",
      "Batch 173, Loss: 0.2996799647808075\n",
      "Batch 174, Loss: -1.6727814674377441\n",
      "Batch 175, Loss: 0.1414879411458969\n",
      "Batch 176, Loss: -1.6445260047912598\n",
      "Batch 177, Loss: -1.5685863494873047\n",
      "Batch 178, Loss: 0.43071654438972473\n",
      "Batch 179, Loss: -1.4533531665802002\n",
      "Batch 180, Loss: 0.293016254901886\n",
      "Batch 181, Loss: -1.1096398830413818\n",
      "Batch 182, Loss: -1.0580780506134033\n",
      "Batch 183, Loss: -1.482969880104065\n",
      "Batch 184, Loss: -1.4303582906723022\n",
      "Batch 185, Loss: 0.11558672785758972\n",
      "Batch 186, Loss: 0.23102043569087982\n",
      "Batch 187, Loss: 0.6512583494186401\n",
      "Batch 188, Loss: 0.38680028915405273\n",
      "Batch 189, Loss: 0.2288103550672531\n",
      "Batch 190, Loss: 0.22686989605426788\n",
      "Batch 191, Loss: 0.12121400237083435\n",
      "Batch 192, Loss: -1.4359098672866821\n",
      "Batch 193, Loss: 0.44662588834762573\n",
      "Batch 194, Loss: 0.5840467810630798\n",
      "Batch 195, Loss: 0.4435991048812866\n",
      "Batch 196, Loss: -1.1933718919754028\n",
      "Batch 197, Loss: -1.502160906791687\n",
      "Batch 198, Loss: 0.47018277645111084\n",
      "Batch 199, Loss: 0.33013859391212463\n",
      "Batch 200, Loss: -1.4562658071517944\n",
      "Batch 201, Loss: 0.3894825577735901\n",
      "Batch 202, Loss: 0.3785381615161896\n",
      "Batch 203, Loss: -1.352697730064392\n",
      "Batch 204, Loss: 0.4545735716819763\n",
      "Batch 205, Loss: 0.19484521448612213\n",
      "Batch 206, Loss: -1.2251970767974854\n",
      "Batch 207, Loss: -1.5913846492767334\n",
      "Batch 208, Loss: 0.4441003203392029\n",
      "Batch 209, Loss: -1.4179182052612305\n",
      "Batch 210, Loss: 0.31896114349365234\n",
      "Batch 211, Loss: 0.4459139406681061\n",
      "Batch 212, Loss: -1.1186184883117676\n",
      "Batch 213, Loss: 0.2972456216812134\n",
      "Batch 214, Loss: 0.47405582666397095\n",
      "Batch 215, Loss: 0.2501870393753052\n",
      "Batch 216, Loss: -1.377403736114502\n",
      "Batch 217, Loss: 0.5398640632629395\n",
      "Batch 218, Loss: -1.134824514389038\n",
      "Batch 219, Loss: 0.5593013167381287\n",
      "Batch 220, Loss: 0.5779809951782227\n",
      "Batch 221, Loss: 0.5462285280227661\n",
      "Batch 222, Loss: -1.5092023611068726\n",
      "Batch 223, Loss: -1.2062759399414062\n",
      "Batch 224, Loss: -1.5819644927978516\n",
      "Batch 225, Loss: 0.2543482780456543\n",
      "Batch 226, Loss: 0.45199888944625854\n",
      "Batch 227, Loss: 0.41425976157188416\n",
      "Batch 228, Loss: 0.11323314905166626\n",
      "Batch 229, Loss: -1.2367678880691528\n",
      "Batch 230, Loss: 0.27198222279548645\n",
      "Batch 231, Loss: 0.05757305026054382\n",
      "Batch 232, Loss: 0.5337128639221191\n",
      "Batch 233, Loss: 0.6052345633506775\n",
      "Batch 234, Loss: 0.3940073549747467\n",
      "Batch 235, Loss: 0.2013273388147354\n",
      "Batch 236, Loss: -1.5429775714874268\n",
      "Batch 237, Loss: 0.6224254369735718\n",
      "Batch 238, Loss: 0.22397062182426453\n",
      "Batch 239, Loss: 0.5409508347511292\n",
      "Batch 240, Loss: 0.23407690227031708\n",
      "Batch 241, Loss: -1.3617664575576782\n",
      "Batch 242, Loss: -1.3704535961151123\n",
      "Batch 243, Loss: -1.4192886352539062\n",
      "Batch 244, Loss: -1.148734211921692\n",
      "Batch 245, Loss: 0.09893614053726196\n",
      "Batch 246, Loss: -1.2465150356292725\n",
      "Batch 247, Loss: -1.5877516269683838\n",
      "Batch 248, Loss: -1.3812123537063599\n",
      "Batch 249, Loss: -1.2333619594573975\n",
      "Batch 250, Loss: 0.3394840359687805\n",
      "Batch 251, Loss: -1.322152018547058\n",
      "Batch 252, Loss: -1.2490577697753906\n",
      "Batch 253, Loss: 0.382782518863678\n",
      "Batch 254, Loss: 0.6038621664047241\n",
      "Batch 255, Loss: 0.4582051932811737\n",
      "Batch 256, Loss: -1.4414584636688232\n",
      "Batch 257, Loss: 0.2957196831703186\n",
      "Batch 258, Loss: 0.5166336297988892\n",
      "Batch 259, Loss: -1.5554739236831665\n",
      "Batch 260, Loss: 0.2609591484069824\n",
      "Batch 261, Loss: 0.6226251721382141\n",
      "Batch 262, Loss: -1.657912015914917\n",
      "Batch 263, Loss: -1.4353370666503906\n",
      "Batch 264, Loss: 0.324281245470047\n",
      "Batch 265, Loss: 0.3274518847465515\n",
      "Batch 266, Loss: 0.28628116846084595\n",
      "Batch 267, Loss: -1.318981409072876\n",
      "Batch 268, Loss: 0.38088440895080566\n",
      "Batch 269, Loss: 0.19048553705215454\n",
      "Batch 270, Loss: 0.2121366709470749\n",
      "Batch 271, Loss: -1.3301985263824463\n",
      "Batch 272, Loss: 0.22290192544460297\n",
      "Batch 273, Loss: -1.3367629051208496\n",
      "Batch 274, Loss: -1.4901397228240967\n",
      "Batch 275, Loss: -1.476928949356079\n",
      "Batch 276, Loss: -1.2935553789138794\n",
      "Batch 277, Loss: 0.22072629630565643\n",
      "Batch 278, Loss: -1.388017177581787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 279, Loss: -1.2467045783996582\n",
      "Batch 280, Loss: -1.6108615398406982\n",
      "Batch 281, Loss: -1.3706419467926025\n",
      "Batch 282, Loss: -1.277622938156128\n",
      "Batch 283, Loss: 0.195111483335495\n",
      "Batch 284, Loss: -1.15692138671875\n",
      "Batch 285, Loss: -1.6708643436431885\n",
      "Batch 286, Loss: -1.3210119009017944\n",
      "Batch 287, Loss: 0.658233642578125\n",
      "Batch 288, Loss: -1.1908576488494873\n",
      "Batch 289, Loss: 0.2124248743057251\n",
      "Batch 290, Loss: -1.4048457145690918\n",
      "Batch 291, Loss: -1.2403197288513184\n",
      "Batch 292, Loss: 0.3141762316226959\n",
      "Batch 293, Loss: -1.588602900505066\n",
      "Batch 294, Loss: -1.2151033878326416\n",
      "Batch 295, Loss: 0.2232406586408615\n",
      "Batch 296, Loss: -1.2182928323745728\n",
      "Batch 297, Loss: 0.4581795930862427\n",
      "Batch 298, Loss: -1.2924593687057495\n",
      "Batch 299, Loss: 0.5228549838066101\n",
      "Batch 300, Loss: 0.228739932179451\n",
      "Batch 301, Loss: -1.712639570236206\n",
      "Batch 302, Loss: -1.4362568855285645\n",
      "Batch 303, Loss: 0.4203987717628479\n",
      "Batch 304, Loss: 0.5401278734207153\n",
      "Batch 305, Loss: 0.45541754364967346\n",
      "Batch 306, Loss: 0.6771443486213684\n",
      "Batch 307, Loss: -1.3520556688308716\n",
      "Batch 308, Loss: -1.6062294244766235\n",
      "Batch 309, Loss: -1.6942683458328247\n",
      "Batch 310, Loss: -1.1125133037567139\n",
      "Batch 311, Loss: 0.3930571973323822\n",
      "Batch 312, Loss: 0.3507710099220276\n",
      "Batch 313, Loss: 0.5445836782455444\n",
      "Batch 314, Loss: -1.6188569068908691\n",
      "Batch 315, Loss: -1.1073919534683228\n",
      "Batch 316, Loss: -1.2577030658721924\n",
      "Batch 317, Loss: 0.5492395162582397\n",
      "Batch 318, Loss: -1.055350661277771\n",
      "Batch 319, Loss: -1.5441350936889648\n",
      "Batch 320, Loss: 0.3537009358406067\n",
      "Batch 321, Loss: 0.35959240794181824\n",
      "Batch 322, Loss: -1.4742093086242676\n",
      "Batch 323, Loss: 0.5585793256759644\n",
      "Batch 324, Loss: -1.425976276397705\n",
      "Batch 325, Loss: -1.1588197946548462\n",
      "Batch 326, Loss: 0.4222494065761566\n",
      "Batch 327, Loss: 0.437193363904953\n",
      "Batch 328, Loss: -1.4196422100067139\n",
      "Batch 329, Loss: -1.219260334968567\n",
      "Batch 330, Loss: 0.38360628485679626\n",
      "Batch 331, Loss: 0.07586869597434998\n",
      "Batch 332, Loss: 0.3555014729499817\n",
      "Batch 333, Loss: -1.691239356994629\n",
      "Batch 334, Loss: -1.0473750829696655\n",
      "Batch 335, Loss: -1.1895090341567993\n",
      "Batch 336, Loss: 0.2577859163284302\n",
      "Batch 337, Loss: 0.6860564947128296\n",
      "Batch 338, Loss: 0.4878857135772705\n",
      "Batch 339, Loss: 0.6549295783042908\n",
      "Batch 340, Loss: 0.2225932627916336\n",
      "Batch 341, Loss: 0.4590793251991272\n",
      "Batch 342, Loss: -1.6388063430786133\n",
      "Batch 343, Loss: -1.447961688041687\n",
      "Batch 344, Loss: -1.494783639907837\n",
      "Batch 345, Loss: 0.3092384338378906\n",
      "Batch 346, Loss: 0.5043315291404724\n",
      "Batch 347, Loss: -1.4618175029754639\n",
      "Batch 348, Loss: -1.2571423053741455\n",
      "Batch 349, Loss: 0.5829816460609436\n",
      "Batch 350, Loss: 0.19356606900691986\n",
      "Batch 351, Loss: -1.6731189489364624\n",
      "Batch 352, Loss: 0.208909809589386\n",
      "Batch 353, Loss: -1.5921223163604736\n",
      "Batch 354, Loss: -1.4851361513137817\n",
      "Batch 355, Loss: -1.4600027799606323\n",
      "Batch 356, Loss: -1.289456844329834\n",
      "Batch 357, Loss: -1.5620321035385132\n",
      "Batch 358, Loss: -1.3567143678665161\n",
      "Batch 359, Loss: 0.24793976545333862\n",
      "Batch 360, Loss: -1.3262596130371094\n",
      "Batch 361, Loss: -1.3279073238372803\n",
      "Batch 362, Loss: 0.5525433421134949\n",
      "Batch 363, Loss: 0.5155000686645508\n",
      "Batch 364, Loss: -1.475865364074707\n",
      "Batch 365, Loss: 0.44021910429000854\n",
      "Batch 366, Loss: 0.6906810402870178\n",
      "Batch 367, Loss: 0.09815475344657898\n",
      "Batch 368, Loss: -1.2680766582489014\n",
      "Batch 369, Loss: 0.5653171539306641\n",
      "Batch 370, Loss: 0.5285118818283081\n",
      "Batch 371, Loss: 0.3569273352622986\n",
      "Batch 372, Loss: 0.670488178730011\n",
      "Batch 373, Loss: 0.5347294211387634\n",
      "Batch 374, Loss: 0.15726755559444427\n",
      "Batch 375, Loss: -1.467708945274353\n",
      "Batch 376, Loss: -1.5609545707702637\n",
      "Batch 377, Loss: 0.35473304986953735\n",
      "Batch 378, Loss: -1.5624042749404907\n",
      "Batch 379, Loss: 0.35433661937713623\n",
      "Batch 380, Loss: 0.16629312932491302\n",
      "Batch 381, Loss: -1.619570255279541\n",
      "Batch 382, Loss: -1.3826115131378174\n",
      "Batch 383, Loss: 0.33105790615081787\n",
      "Batch 384, Loss: 0.5532739758491516\n",
      "Batch 385, Loss: -1.1712088584899902\n",
      "Batch 386, Loss: 0.38970062136650085\n",
      "Batch 387, Loss: -1.1284351348876953\n",
      "Batch 388, Loss: -1.194117784500122\n",
      "Batch 389, Loss: 0.39195820689201355\n",
      "Batch 390, Loss: 0.46910351514816284\n",
      "Batch 391, Loss: 0.3094649016857147\n",
      "Batch 392, Loss: -1.518150806427002\n",
      "Batch 393, Loss: 0.6224943399429321\n",
      "Batch 394, Loss: -1.4090555906295776\n",
      "Batch 395, Loss: -1.3182708024978638\n",
      "Batch 396, Loss: 0.03194063901901245\n",
      "Batch 397, Loss: -1.339684009552002\n",
      "Batch 398, Loss: 0.23822660744190216\n",
      "Batch 399, Loss: -1.1455883979797363\n",
      "Training [20%]\tLoss: -0.4995\n",
      "Batch 0, Loss: -1.6602636575698853\n",
      "Batch 1, Loss: 0.5234335660934448\n",
      "Batch 2, Loss: 0.5502864122390747\n",
      "Batch 3, Loss: 0.28227299451828003\n",
      "Batch 4, Loss: -1.2434791326522827\n",
      "Batch 5, Loss: 0.6117774844169617\n",
      "Batch 6, Loss: -1.2558003664016724\n",
      "Batch 7, Loss: -1.3408093452453613\n",
      "Batch 8, Loss: 0.6687202453613281\n",
      "Batch 9, Loss: -1.1682332754135132\n",
      "Batch 10, Loss: -1.2770872116088867\n",
      "Batch 11, Loss: 0.5901656150817871\n",
      "Batch 12, Loss: -1.1291877031326294\n",
      "Batch 13, Loss: -1.4331178665161133\n",
      "Batch 14, Loss: -1.261451244354248\n",
      "Batch 15, Loss: 0.2906568646430969\n",
      "Batch 16, Loss: 0.6757288575172424\n",
      "Batch 17, Loss: -1.3722498416900635\n",
      "Batch 18, Loss: 0.31426098942756653\n",
      "Batch 19, Loss: 0.09958288073539734\n",
      "Batch 20, Loss: -1.066257357597351\n",
      "Batch 21, Loss: -1.6079990863800049\n",
      "Batch 22, Loss: -1.3531582355499268\n",
      "Batch 23, Loss: -1.4801788330078125\n",
      "Batch 24, Loss: 0.4313535690307617\n",
      "Batch 25, Loss: -1.4981629848480225\n",
      "Batch 26, Loss: 0.05729362368583679\n",
      "Batch 27, Loss: -1.4708118438720703\n",
      "Batch 28, Loss: 0.26959389448165894\n",
      "Batch 29, Loss: -1.288392186164856\n",
      "Batch 30, Loss: 0.48901087045669556\n",
      "Batch 31, Loss: -1.1810286045074463\n",
      "Batch 32, Loss: 0.6092627048492432\n",
      "Batch 33, Loss: -1.521414041519165\n",
      "Batch 34, Loss: 0.41704586148262024\n",
      "Batch 35, Loss: 0.5250221490859985\n",
      "Batch 36, Loss: -1.5792264938354492\n",
      "Batch 37, Loss: -1.327799916267395\n",
      "Batch 38, Loss: 0.539993166923523\n",
      "Batch 39, Loss: -1.4805494546890259\n",
      "Batch 40, Loss: -1.121817946434021\n",
      "Batch 41, Loss: -1.4844938516616821\n",
      "Batch 42, Loss: 0.3623077869415283\n",
      "Batch 43, Loss: 0.41017764806747437\n",
      "Batch 44, Loss: 0.2134963721036911\n",
      "Batch 45, Loss: -1.405778408050537\n",
      "Batch 46, Loss: 0.34620946645736694\n",
      "Batch 47, Loss: -1.3634915351867676\n",
      "Batch 48, Loss: -1.1306970119476318\n",
      "Batch 49, Loss: -1.05897057056427\n",
      "Batch 50, Loss: 0.4708837866783142\n",
      "Batch 51, Loss: -1.5238622426986694\n",
      "Batch 52, Loss: 0.24163924157619476\n",
      "Batch 53, Loss: -1.1101289987564087\n",
      "Batch 54, Loss: 0.3152766525745392\n",
      "Batch 55, Loss: -1.146693468093872\n",
      "Batch 56, Loss: -1.303873062133789\n",
      "Batch 57, Loss: 0.20919448137283325\n",
      "Batch 58, Loss: 0.1431550532579422\n",
      "Batch 59, Loss: 0.4219928979873657\n",
      "Batch 60, Loss: -1.3426005840301514\n",
      "Batch 61, Loss: 0.5075188875198364\n",
      "Batch 62, Loss: -1.1259703636169434\n",
      "Batch 63, Loss: 0.443433552980423\n",
      "Batch 64, Loss: 0.5554959177970886\n",
      "Batch 65, Loss: -1.5939671993255615\n",
      "Batch 66, Loss: 0.6083990335464478\n",
      "Batch 67, Loss: -1.4017066955566406\n",
      "Batch 68, Loss: -1.5795034170150757\n",
      "Batch 69, Loss: -1.2392792701721191\n",
      "Batch 70, Loss: 0.2238500565290451\n",
      "Batch 71, Loss: -1.363761067390442\n",
      "Batch 72, Loss: -1.3607120513916016\n",
      "Batch 73, Loss: -1.0697746276855469\n",
      "Batch 74, Loss: 0.2539365887641907\n",
      "Batch 75, Loss: -1.5666781663894653\n",
      "Batch 76, Loss: -1.0577086210250854\n",
      "Batch 77, Loss: -1.2249454259872437\n",
      "Batch 78, Loss: -1.3323112726211548\n",
      "Batch 79, Loss: 0.21308116614818573\n",
      "Batch 80, Loss: -1.6074228286743164\n",
      "Batch 81, Loss: -1.3187819719314575\n",
      "Batch 82, Loss: -1.6173105239868164\n",
      "Batch 83, Loss: -1.5494399070739746\n",
      "Batch 84, Loss: 0.3689773976802826\n",
      "Batch 85, Loss: -1.5774885416030884\n",
      "Batch 86, Loss: -1.1856203079223633\n",
      "Batch 87, Loss: 0.5070369243621826\n",
      "Batch 88, Loss: -1.178040862083435\n",
      "Batch 89, Loss: -1.3538949489593506\n",
      "Batch 90, Loss: 0.5019378662109375\n",
      "Batch 91, Loss: 0.0750262439250946\n",
      "Batch 92, Loss: 0.4599847197532654\n",
      "Batch 93, Loss: -1.0762338638305664\n",
      "Batch 94, Loss: 0.25057753920555115\n",
      "Batch 95, Loss: -1.6609437465667725\n",
      "Batch 96, Loss: -1.2573034763336182\n",
      "Batch 97, Loss: 0.20172294974327087\n",
      "Batch 98, Loss: 0.39415112137794495\n",
      "Batch 99, Loss: 0.3024041950702667\n",
      "Batch 100, Loss: 0.39523470401763916\n",
      "Batch 101, Loss: 0.2018902599811554\n",
      "Batch 102, Loss: -1.2361230850219727\n",
      "Batch 103, Loss: -1.590903401374817\n",
      "Batch 104, Loss: 0.24337704479694366\n",
      "Batch 105, Loss: 0.6489280462265015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 106, Loss: -1.6136283874511719\n",
      "Batch 107, Loss: -1.3827500343322754\n",
      "Batch 108, Loss: -1.4232182502746582\n",
      "Batch 109, Loss: 0.6318864822387695\n",
      "Batch 110, Loss: -1.498328447341919\n",
      "Batch 111, Loss: -1.3855575323104858\n",
      "Batch 112, Loss: -1.0723003149032593\n",
      "Batch 113, Loss: 0.5905336737632751\n",
      "Batch 114, Loss: -1.4528207778930664\n",
      "Batch 115, Loss: 0.13158999383449554\n",
      "Batch 116, Loss: 0.45593249797821045\n",
      "Batch 117, Loss: 0.6117305159568787\n",
      "Batch 118, Loss: 0.40122082829475403\n",
      "Batch 119, Loss: -1.5145853757858276\n",
      "Batch 120, Loss: 0.23602376878261566\n",
      "Batch 121, Loss: -1.1895943880081177\n",
      "Batch 122, Loss: 0.40051668882369995\n",
      "Batch 123, Loss: 0.4476710855960846\n",
      "Batch 124, Loss: -1.438241958618164\n",
      "Batch 125, Loss: -1.1140351295471191\n",
      "Batch 126, Loss: -1.3473994731903076\n",
      "Batch 127, Loss: 0.2910236120223999\n",
      "Batch 128, Loss: 0.5113310813903809\n",
      "Batch 129, Loss: -1.4930821657180786\n",
      "Batch 130, Loss: 0.6289783120155334\n",
      "Batch 131, Loss: -1.3289427757263184\n",
      "Batch 132, Loss: -1.364324688911438\n",
      "Batch 133, Loss: 0.6741172075271606\n",
      "Batch 134, Loss: 0.19567200541496277\n",
      "Batch 135, Loss: 0.3749048411846161\n",
      "Batch 136, Loss: 0.5202752947807312\n",
      "Batch 137, Loss: 0.23798389732837677\n",
      "Batch 138, Loss: -1.1239855289459229\n",
      "Batch 139, Loss: 0.27879396080970764\n",
      "Batch 140, Loss: -1.2462525367736816\n",
      "Batch 141, Loss: -1.161616563796997\n",
      "Batch 142, Loss: -1.3937939405441284\n",
      "Batch 143, Loss: 0.12493672966957092\n",
      "Batch 144, Loss: 0.4594622552394867\n",
      "Batch 145, Loss: 0.13964800536632538\n",
      "Batch 146, Loss: -1.111067771911621\n",
      "Batch 147, Loss: -1.347419261932373\n",
      "Batch 148, Loss: -1.2934478521347046\n",
      "Batch 149, Loss: -1.4014308452606201\n",
      "Batch 150, Loss: 0.36748045682907104\n",
      "Batch 151, Loss: 0.4075106382369995\n",
      "Batch 152, Loss: 0.2027294635772705\n",
      "Batch 153, Loss: 0.15234558284282684\n",
      "Batch 154, Loss: 0.5959030985832214\n",
      "Batch 155, Loss: -1.1869592666625977\n",
      "Batch 156, Loss: 0.17433831095695496\n",
      "Batch 157, Loss: 0.26151713728904724\n",
      "Batch 158, Loss: 0.047892242670059204\n",
      "Batch 159, Loss: 0.47808146476745605\n",
      "Batch 160, Loss: 0.226604625582695\n",
      "Batch 161, Loss: 0.33492472767829895\n",
      "Batch 162, Loss: -1.4825091361999512\n",
      "Batch 163, Loss: -1.500084638595581\n",
      "Batch 164, Loss: 0.32841774821281433\n",
      "Batch 165, Loss: 0.19185544550418854\n",
      "Batch 166, Loss: 0.5599404573440552\n",
      "Batch 167, Loss: 0.20786595344543457\n",
      "Batch 168, Loss: 0.10049387812614441\n",
      "Batch 169, Loss: 0.4800419211387634\n",
      "Batch 170, Loss: 0.21784567832946777\n",
      "Batch 171, Loss: -1.2315592765808105\n",
      "Batch 172, Loss: 0.3741549849510193\n",
      "Batch 173, Loss: 0.49225252866744995\n",
      "Batch 174, Loss: -1.46392822265625\n",
      "Batch 175, Loss: -1.4121094942092896\n",
      "Batch 176, Loss: -1.3262603282928467\n",
      "Batch 177, Loss: -1.6687006950378418\n",
      "Batch 178, Loss: -1.2645153999328613\n",
      "Batch 179, Loss: -1.4487193822860718\n",
      "Batch 180, Loss: 0.36397188901901245\n",
      "Batch 181, Loss: 0.5434623956680298\n",
      "Batch 182, Loss: -1.1411759853363037\n",
      "Batch 183, Loss: -1.313582181930542\n",
      "Batch 184, Loss: 0.27893298864364624\n",
      "Batch 185, Loss: 0.5617457032203674\n",
      "Batch 186, Loss: 0.4551645815372467\n",
      "Batch 187, Loss: -1.1801434755325317\n",
      "Batch 188, Loss: 0.5078197717666626\n",
      "Batch 189, Loss: 0.5089028477668762\n",
      "Batch 190, Loss: -1.4311400651931763\n",
      "Batch 191, Loss: 0.3471300005912781\n",
      "Batch 192, Loss: 0.44921329617500305\n",
      "Batch 193, Loss: -1.4266339540481567\n",
      "Batch 194, Loss: 0.35252341628074646\n",
      "Batch 195, Loss: 0.14573481678962708\n",
      "Batch 196, Loss: -1.57063889503479\n",
      "Batch 197, Loss: -1.362269639968872\n",
      "Batch 198, Loss: -1.1094739437103271\n",
      "Batch 199, Loss: 0.30756676197052\n",
      "Batch 200, Loss: 0.4062650799751282\n",
      "Batch 201, Loss: -1.5303504467010498\n",
      "Batch 202, Loss: 0.266284704208374\n",
      "Batch 203, Loss: 0.44034796953201294\n",
      "Batch 204, Loss: -1.219843864440918\n",
      "Batch 205, Loss: -1.302536129951477\n",
      "Batch 206, Loss: -1.2289276123046875\n",
      "Batch 207, Loss: 0.14607632160186768\n",
      "Batch 208, Loss: -1.292978286743164\n",
      "Batch 209, Loss: -1.3088185787200928\n",
      "Batch 210, Loss: -1.1522949934005737\n",
      "Batch 211, Loss: 0.13945834338665009\n",
      "Batch 212, Loss: 0.4951412081718445\n",
      "Batch 213, Loss: 0.2182413637638092\n",
      "Batch 214, Loss: 0.4873557686805725\n",
      "Batch 215, Loss: 0.6144692301750183\n",
      "Batch 216, Loss: 0.38523662090301514\n",
      "Batch 217, Loss: 0.43512094020843506\n",
      "Batch 218, Loss: 0.2719711661338806\n",
      "Batch 219, Loss: -1.2060081958770752\n",
      "Batch 220, Loss: -1.5847606658935547\n",
      "Batch 221, Loss: -1.601656198501587\n",
      "Batch 222, Loss: -1.2280116081237793\n",
      "Batch 223, Loss: -1.4082057476043701\n",
      "Batch 224, Loss: -1.1129167079925537\n",
      "Batch 225, Loss: 0.5000849962234497\n",
      "Batch 226, Loss: -1.444221019744873\n",
      "Batch 227, Loss: -1.314927577972412\n",
      "Batch 228, Loss: 0.2590557932853699\n",
      "Batch 229, Loss: -1.2094677686691284\n",
      "Batch 230, Loss: -1.434929609298706\n",
      "Batch 231, Loss: -1.5834659337997437\n",
      "Batch 232, Loss: -1.5101045370101929\n",
      "Batch 233, Loss: -1.5058238506317139\n",
      "Batch 234, Loss: 0.26853835582733154\n",
      "Batch 235, Loss: 0.5179612040519714\n",
      "Batch 236, Loss: 0.5999692678451538\n",
      "Batch 237, Loss: 0.1918361634016037\n",
      "Batch 238, Loss: -1.3302068710327148\n",
      "Batch 239, Loss: 0.1970040649175644\n",
      "Batch 240, Loss: 0.6081094145774841\n",
      "Batch 241, Loss: 0.4728334844112396\n",
      "Batch 242, Loss: 0.25558140873908997\n",
      "Batch 243, Loss: -1.369158387184143\n",
      "Batch 244, Loss: -1.6398987770080566\n",
      "Batch 245, Loss: 0.3839968144893646\n",
      "Batch 246, Loss: -1.2608873844146729\n",
      "Batch 247, Loss: 0.48400020599365234\n",
      "Batch 248, Loss: 0.2577882707118988\n",
      "Batch 249, Loss: -1.536728024482727\n",
      "Batch 250, Loss: 0.3330603837966919\n",
      "Batch 251, Loss: 0.5783964395523071\n",
      "Batch 252, Loss: 0.5756930708885193\n",
      "Batch 253, Loss: -1.286744236946106\n",
      "Batch 254, Loss: -1.1101932525634766\n",
      "Batch 255, Loss: -1.4758647680282593\n",
      "Batch 256, Loss: -1.5495046377182007\n",
      "Batch 257, Loss: -1.2370693683624268\n",
      "Batch 258, Loss: 0.6805880069732666\n",
      "Batch 259, Loss: 0.2943062484264374\n",
      "Batch 260, Loss: 0.3091255724430084\n",
      "Batch 261, Loss: -1.3821277618408203\n",
      "Batch 262, Loss: 0.3241059184074402\n",
      "Batch 263, Loss: 0.21413230895996094\n",
      "Batch 264, Loss: -1.174619197845459\n",
      "Batch 265, Loss: -1.5713801383972168\n",
      "Batch 266, Loss: -1.1118558645248413\n",
      "Batch 267, Loss: -1.3059900999069214\n",
      "Batch 268, Loss: 0.262077271938324\n",
      "Batch 269, Loss: 0.28392547369003296\n",
      "Batch 270, Loss: -1.440995216369629\n",
      "Batch 271, Loss: -1.2771333456039429\n",
      "Batch 272, Loss: 0.46783286333084106\n",
      "Batch 273, Loss: 0.29655319452285767\n",
      "Batch 274, Loss: 0.28682005405426025\n",
      "Batch 275, Loss: 0.39763861894607544\n",
      "Batch 276, Loss: 0.45484915375709534\n",
      "Batch 277, Loss: 0.618437647819519\n",
      "Batch 278, Loss: 0.3812231421470642\n",
      "Batch 279, Loss: -1.4246785640716553\n",
      "Batch 280, Loss: -1.4962806701660156\n",
      "Batch 281, Loss: 0.4238692820072174\n",
      "Batch 282, Loss: 0.24941621720790863\n",
      "Batch 283, Loss: -1.5326811075210571\n",
      "Batch 284, Loss: 0.19126683473587036\n",
      "Batch 285, Loss: -1.2811120748519897\n",
      "Batch 286, Loss: -1.2748711109161377\n",
      "Batch 287, Loss: 0.2461819350719452\n",
      "Batch 288, Loss: -1.652174949645996\n",
      "Batch 289, Loss: -1.3608741760253906\n",
      "Batch 290, Loss: 0.47675156593322754\n",
      "Batch 291, Loss: 0.49274545907974243\n",
      "Batch 292, Loss: 0.49004578590393066\n",
      "Batch 293, Loss: 0.22954921424388885\n",
      "Batch 294, Loss: -1.1235196590423584\n",
      "Batch 295, Loss: -1.4846488237380981\n",
      "Batch 296, Loss: -1.2905609607696533\n",
      "Batch 297, Loss: 0.11517579853534698\n",
      "Batch 298, Loss: 0.4649735689163208\n",
      "Batch 299, Loss: -1.6273388862609863\n",
      "Batch 300, Loss: 0.31965193152427673\n",
      "Batch 301, Loss: 0.3339069187641144\n",
      "Batch 302, Loss: 0.06660789251327515\n",
      "Batch 303, Loss: -1.3711565732955933\n",
      "Batch 304, Loss: -1.5067625045776367\n",
      "Batch 305, Loss: -1.6486364603042603\n",
      "Batch 306, Loss: -1.4231576919555664\n",
      "Batch 307, Loss: 0.5266520977020264\n",
      "Batch 308, Loss: 0.5449058413505554\n",
      "Batch 309, Loss: 0.15383975207805634\n",
      "Batch 310, Loss: -1.6935248374938965\n",
      "Batch 311, Loss: -1.4145300388336182\n",
      "Batch 312, Loss: -1.6349070072174072\n",
      "Batch 313, Loss: 0.13408882915973663\n",
      "Batch 314, Loss: 0.28529220819473267\n",
      "Batch 315, Loss: 0.5491634011268616\n",
      "Batch 316, Loss: -1.2766027450561523\n",
      "Batch 317, Loss: -1.3754603862762451\n",
      "Batch 318, Loss: -1.0392767190933228\n",
      "Batch 319, Loss: 0.42475324869155884\n",
      "Batch 320, Loss: -1.451521635055542\n",
      "Batch 321, Loss: 0.5347057580947876\n",
      "Batch 322, Loss: 0.44662296772003174\n",
      "Batch 323, Loss: -1.2249395847320557\n",
      "Batch 324, Loss: 0.611407995223999\n",
      "Batch 325, Loss: -1.6138724088668823\n",
      "Batch 326, Loss: -1.1439862251281738\n",
      "Batch 327, Loss: -1.6203500032424927\n",
      "Batch 328, Loss: 0.30223965644836426\n",
      "Batch 329, Loss: 0.41264843940734863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 330, Loss: -1.2323687076568604\n",
      "Batch 331, Loss: 0.47480446100234985\n",
      "Batch 332, Loss: -1.5801918506622314\n",
      "Batch 333, Loss: -1.3340567350387573\n",
      "Batch 334, Loss: 0.5144633054733276\n",
      "Batch 335, Loss: 0.31732866168022156\n",
      "Batch 336, Loss: 0.2932642102241516\n",
      "Batch 337, Loss: 0.30157768726348877\n",
      "Batch 338, Loss: -1.3137593269348145\n",
      "Batch 339, Loss: 0.3680621087551117\n",
      "Batch 340, Loss: -1.404802680015564\n",
      "Batch 341, Loss: -1.3600565195083618\n",
      "Batch 342, Loss: -1.0791716575622559\n",
      "Batch 343, Loss: 0.656740665435791\n",
      "Batch 344, Loss: 0.40300509333610535\n",
      "Batch 345, Loss: -1.255695104598999\n",
      "Batch 346, Loss: 0.5551638603210449\n",
      "Batch 347, Loss: 0.3692520260810852\n",
      "Batch 348, Loss: -1.467710256576538\n",
      "Batch 349, Loss: 0.44524383544921875\n",
      "Batch 350, Loss: -1.435485601425171\n",
      "Batch 351, Loss: 0.1622541844844818\n",
      "Batch 352, Loss: 0.3776837885379791\n",
      "Batch 353, Loss: -1.5653867721557617\n",
      "Batch 354, Loss: 0.19450239837169647\n",
      "Batch 355, Loss: -1.1226500272750854\n",
      "Batch 356, Loss: -1.5581843852996826\n",
      "Batch 357, Loss: -1.487276554107666\n",
      "Batch 358, Loss: -1.3322068452835083\n",
      "Batch 359, Loss: 0.3136408030986786\n",
      "Batch 360, Loss: -1.184332251548767\n",
      "Batch 361, Loss: -1.2555304765701294\n",
      "Batch 362, Loss: -1.4909889698028564\n",
      "Batch 363, Loss: -1.5183053016662598\n",
      "Batch 364, Loss: -1.6599986553192139\n",
      "Batch 365, Loss: 0.10417616367340088\n",
      "Batch 366, Loss: -1.1874895095825195\n",
      "Batch 367, Loss: 0.3450412154197693\n",
      "Batch 368, Loss: 0.5531367063522339\n",
      "Batch 369, Loss: -1.5839629173278809\n",
      "Batch 370, Loss: -1.1245970726013184\n",
      "Batch 371, Loss: -1.6270679235458374\n",
      "Batch 372, Loss: 0.19400358200073242\n",
      "Batch 373, Loss: 0.6298017501831055\n",
      "Batch 374, Loss: 0.3995826542377472\n",
      "Batch 375, Loss: -1.248548984527588\n",
      "Batch 376, Loss: 0.29347750544548035\n",
      "Batch 377, Loss: 0.2172175943851471\n",
      "Batch 378, Loss: 0.2934873402118683\n",
      "Batch 379, Loss: 0.13337527215480804\n",
      "Batch 380, Loss: 0.10820165276527405\n",
      "Batch 381, Loss: -1.5862600803375244\n",
      "Batch 382, Loss: -1.5346901416778564\n",
      "Batch 383, Loss: -1.400264859199524\n",
      "Batch 384, Loss: -1.4130113124847412\n",
      "Batch 385, Loss: -1.4682769775390625\n",
      "Batch 386, Loss: 0.5683303475379944\n",
      "Batch 387, Loss: -1.6559019088745117\n",
      "Batch 388, Loss: -1.1755549907684326\n",
      "Batch 389, Loss: -1.4269238710403442\n",
      "Batch 390, Loss: -1.2041646242141724\n",
      "Batch 391, Loss: -1.5608115196228027\n",
      "Batch 392, Loss: 0.3437633216381073\n",
      "Batch 393, Loss: -1.3288588523864746\n",
      "Batch 394, Loss: 0.082232266664505\n",
      "Batch 395, Loss: -1.1738582849502563\n",
      "Batch 396, Loss: 0.6422569155693054\n",
      "Batch 397, Loss: -1.6600409746170044\n",
      "Batch 398, Loss: 0.12179985642433167\n",
      "Batch 399, Loss: 0.6338887810707092\n",
      "Training [30%]\tLoss: -0.4981\n",
      "Batch 0, Loss: 0.6895800828933716\n",
      "Batch 1, Loss: 0.15187372267246246\n",
      "Batch 2, Loss: -1.1680233478546143\n",
      "Batch 3, Loss: 0.44928234815597534\n",
      "Batch 4, Loss: 0.601354718208313\n",
      "Batch 5, Loss: -1.684784173965454\n",
      "Batch 6, Loss: 0.4108123779296875\n",
      "Batch 7, Loss: -1.2650854587554932\n",
      "Batch 8, Loss: 0.6608620882034302\n",
      "Batch 9, Loss: 0.18518704175949097\n",
      "Batch 10, Loss: 0.30933576822280884\n",
      "Batch 11, Loss: 0.08872953057289124\n",
      "Batch 12, Loss: 0.3109280467033386\n",
      "Batch 13, Loss: -1.2067164182662964\n",
      "Batch 14, Loss: -1.1396043300628662\n",
      "Batch 15, Loss: 0.3666582405567169\n",
      "Batch 16, Loss: 0.4552331268787384\n",
      "Batch 17, Loss: -1.6072187423706055\n",
      "Batch 18, Loss: 0.10372573137283325\n",
      "Batch 19, Loss: 0.2560884952545166\n",
      "Batch 20, Loss: 0.37024930119514465\n",
      "Batch 21, Loss: -1.2501745223999023\n",
      "Batch 22, Loss: 0.3010295331478119\n",
      "Batch 23, Loss: -1.5905981063842773\n",
      "Batch 24, Loss: -1.3674941062927246\n",
      "Batch 25, Loss: 0.5816860795021057\n",
      "Batch 26, Loss: 0.14082562923431396\n",
      "Batch 27, Loss: 0.3394710123538971\n",
      "Batch 28, Loss: -1.6285667419433594\n",
      "Batch 29, Loss: 0.41949763894081116\n",
      "Batch 30, Loss: 0.3623664081096649\n",
      "Batch 31, Loss: -1.6327166557312012\n",
      "Batch 32, Loss: 0.36001819372177124\n",
      "Batch 33, Loss: 0.2259249985218048\n",
      "Batch 34, Loss: 0.31891027092933655\n",
      "Batch 35, Loss: -1.4627877473831177\n",
      "Batch 36, Loss: -1.396932601928711\n",
      "Batch 37, Loss: 0.2726393938064575\n",
      "Batch 38, Loss: -1.1996036767959595\n",
      "Batch 39, Loss: -1.3054966926574707\n",
      "Batch 40, Loss: 0.18353420495986938\n",
      "Batch 41, Loss: 0.5572517514228821\n",
      "Batch 42, Loss: -1.3418455123901367\n",
      "Batch 43, Loss: 0.4141184091567993\n",
      "Batch 44, Loss: 0.6208324432373047\n",
      "Batch 45, Loss: 0.27994853258132935\n",
      "Batch 46, Loss: -1.4109768867492676\n",
      "Batch 47, Loss: -1.2613072395324707\n",
      "Batch 48, Loss: 0.0672181248664856\n",
      "Batch 49, Loss: 0.04923287034034729\n",
      "Batch 50, Loss: -1.3262548446655273\n",
      "Batch 51, Loss: -1.3942097425460815\n",
      "Batch 52, Loss: 0.3588153123855591\n",
      "Batch 53, Loss: 0.7053024768829346\n",
      "Batch 54, Loss: 0.113019198179245\n",
      "Batch 55, Loss: -1.0893528461456299\n",
      "Batch 56, Loss: -1.0899999141693115\n",
      "Batch 57, Loss: 0.04284033179283142\n",
      "Batch 58, Loss: 0.03734481334686279\n",
      "Batch 59, Loss: -1.0273921489715576\n",
      "Batch 60, Loss: 0.14066456258296967\n",
      "Batch 61, Loss: -1.3054735660552979\n",
      "Batch 62, Loss: 0.2501031160354614\n",
      "Batch 63, Loss: 0.34532201290130615\n",
      "Batch 64, Loss: 0.4091228246688843\n",
      "Batch 65, Loss: 0.07896178960800171\n",
      "Batch 66, Loss: 0.3000190258026123\n",
      "Batch 67, Loss: 0.25474146008491516\n",
      "Batch 68, Loss: -1.5555386543273926\n",
      "Batch 69, Loss: 0.16987425088882446\n",
      "Batch 70, Loss: 0.3481297492980957\n",
      "Batch 71, Loss: -1.53804612159729\n",
      "Batch 72, Loss: 0.245082825422287\n",
      "Batch 73, Loss: 0.17002741992473602\n",
      "Batch 74, Loss: 0.24314552545547485\n",
      "Batch 75, Loss: 0.39910000562667847\n",
      "Batch 76, Loss: 0.6389046907424927\n",
      "Batch 77, Loss: 0.3979472517967224\n",
      "Batch 78, Loss: 0.03946036100387573\n",
      "Batch 79, Loss: -1.168949842453003\n",
      "Batch 80, Loss: -1.1521167755126953\n",
      "Batch 81, Loss: -1.2630103826522827\n",
      "Batch 82, Loss: 0.5093790888786316\n",
      "Batch 83, Loss: 0.47586560249328613\n",
      "Batch 84, Loss: -1.543735384941101\n",
      "Batch 85, Loss: -1.279189109802246\n",
      "Batch 86, Loss: 0.06945246458053589\n",
      "Batch 87, Loss: -1.252988576889038\n",
      "Batch 88, Loss: -1.3492801189422607\n",
      "Batch 89, Loss: -1.2625408172607422\n",
      "Batch 90, Loss: -1.3309836387634277\n",
      "Batch 91, Loss: 0.20338161289691925\n",
      "Batch 92, Loss: -1.364755392074585\n",
      "Batch 93, Loss: 0.12850815057754517\n",
      "Batch 94, Loss: 0.6383764743804932\n",
      "Batch 95, Loss: 0.14858078956604004\n",
      "Batch 96, Loss: -1.1737538576126099\n",
      "Batch 97, Loss: -1.1948555707931519\n",
      "Batch 98, Loss: -1.3226532936096191\n",
      "Batch 99, Loss: 0.17889943718910217\n",
      "Batch 100, Loss: 0.034898996353149414\n",
      "Batch 101, Loss: -1.2491657733917236\n",
      "Batch 102, Loss: -1.1120686531066895\n",
      "Batch 103, Loss: 0.05323675274848938\n",
      "Batch 104, Loss: -1.2374186515808105\n",
      "Batch 105, Loss: -1.090670108795166\n",
      "Batch 106, Loss: -1.255176305770874\n",
      "Batch 107, Loss: 0.5603095293045044\n",
      "Batch 108, Loss: 0.5065628290176392\n",
      "Batch 109, Loss: -1.2461318969726562\n",
      "Batch 110, Loss: -1.6924012899398804\n",
      "Batch 111, Loss: -1.349601149559021\n",
      "Batch 112, Loss: 0.29362091422080994\n",
      "Batch 113, Loss: -1.3408734798431396\n",
      "Batch 114, Loss: -1.5959694385528564\n",
      "Batch 115, Loss: 0.1204136312007904\n",
      "Batch 116, Loss: -1.431137204170227\n",
      "Batch 117, Loss: -1.291129469871521\n",
      "Batch 118, Loss: 0.29551053047180176\n",
      "Batch 119, Loss: -1.0305758714675903\n",
      "Batch 120, Loss: -1.5230028629302979\n",
      "Batch 121, Loss: -1.5873346328735352\n",
      "Batch 122, Loss: 0.5805402398109436\n",
      "Batch 123, Loss: -1.194696307182312\n",
      "Batch 124, Loss: -1.3330475091934204\n",
      "Batch 125, Loss: -1.3551439046859741\n",
      "Batch 126, Loss: -1.3935433626174927\n",
      "Batch 127, Loss: -1.0760083198547363\n",
      "Batch 128, Loss: 0.6149777173995972\n",
      "Batch 129, Loss: -1.2084295749664307\n",
      "Batch 130, Loss: -1.200727105140686\n",
      "Batch 131, Loss: 0.3787887692451477\n",
      "Batch 132, Loss: -1.3066014051437378\n",
      "Batch 133, Loss: 0.13323114812374115\n",
      "Batch 134, Loss: 0.15509094297885895\n",
      "Batch 135, Loss: 0.4531483054161072\n",
      "Batch 136, Loss: 0.24830572307109833\n",
      "Batch 137, Loss: -1.3353819847106934\n",
      "Batch 138, Loss: -1.6027562618255615\n",
      "Batch 139, Loss: 0.36743301153182983\n",
      "Batch 140, Loss: -1.2824381589889526\n",
      "Batch 141, Loss: -1.1474188566207886\n",
      "Batch 142, Loss: 0.0446837842464447\n",
      "Batch 143, Loss: 0.09627655148506165\n",
      "Batch 144, Loss: 0.04653409123420715\n",
      "Batch 145, Loss: -1.280125379562378\n",
      "Batch 146, Loss: 0.3729189932346344\n",
      "Batch 147, Loss: -1.5744121074676514\n",
      "Batch 148, Loss: -1.1790724992752075\n",
      "Batch 149, Loss: 0.3234109878540039\n",
      "Batch 150, Loss: 0.47833260893821716\n",
      "Batch 151, Loss: 0.04826229810714722\n",
      "Batch 152, Loss: 0.22710373997688293\n",
      "Batch 153, Loss: -1.5412455797195435\n",
      "Batch 154, Loss: 0.08449265360832214\n",
      "Batch 155, Loss: 0.3545871078968048\n",
      "Batch 156, Loss: -1.082672357559204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 157, Loss: -1.4513461589813232\n",
      "Batch 158, Loss: -1.235317349433899\n",
      "Batch 159, Loss: -1.499528408050537\n",
      "Batch 160, Loss: 0.4630318284034729\n",
      "Batch 161, Loss: -1.2304725646972656\n",
      "Batch 162, Loss: -1.5915052890777588\n",
      "Batch 163, Loss: 0.31274083256721497\n",
      "Batch 164, Loss: -1.2569555044174194\n",
      "Batch 165, Loss: 0.5543427467346191\n",
      "Batch 166, Loss: 0.1395508050918579\n",
      "Batch 167, Loss: 0.04050740599632263\n",
      "Batch 168, Loss: 0.3663313388824463\n",
      "Batch 169, Loss: -1.3863213062286377\n",
      "Batch 170, Loss: -1.0106889009475708\n",
      "Batch 171, Loss: -1.3815828561782837\n",
      "Batch 172, Loss: 0.10962027311325073\n",
      "Batch 173, Loss: -1.6175230741500854\n",
      "Batch 174, Loss: 0.3060075640678406\n",
      "Batch 175, Loss: 0.2213314026594162\n",
      "Batch 176, Loss: 0.3261551260948181\n",
      "Batch 177, Loss: -1.5221915245056152\n",
      "Batch 178, Loss: 0.09964975714683533\n",
      "Batch 179, Loss: -1.0796324014663696\n",
      "Batch 180, Loss: -1.6715539693832397\n",
      "Batch 181, Loss: 0.2604946196079254\n",
      "Batch 182, Loss: -1.3510339260101318\n",
      "Batch 183, Loss: 0.6466683149337769\n",
      "Batch 184, Loss: 0.4498717188835144\n",
      "Batch 185, Loss: -1.1419473886489868\n",
      "Batch 186, Loss: 0.0878940224647522\n",
      "Batch 187, Loss: 0.6774940490722656\n",
      "Batch 188, Loss: -1.222346305847168\n",
      "Batch 189, Loss: 0.08743375539779663\n",
      "Batch 190, Loss: 0.0674360990524292\n",
      "Batch 191, Loss: -1.6757937669754028\n",
      "Batch 192, Loss: -1.31107759475708\n",
      "Batch 193, Loss: -1.6893177032470703\n",
      "Batch 194, Loss: 0.02611488103866577\n",
      "Batch 195, Loss: -1.0528085231781006\n",
      "Batch 196, Loss: -1.2192450761795044\n",
      "Batch 197, Loss: -1.4219239950180054\n",
      "Batch 198, Loss: 0.27854257822036743\n",
      "Batch 199, Loss: 0.1905546337366104\n",
      "Batch 200, Loss: -1.5630476474761963\n",
      "Batch 201, Loss: 0.6168912649154663\n",
      "Batch 202, Loss: -1.2464786767959595\n",
      "Batch 203, Loss: 0.09546640515327454\n",
      "Batch 204, Loss: -1.3273944854736328\n",
      "Batch 205, Loss: 0.6251275539398193\n",
      "Batch 206, Loss: 0.19904062151908875\n",
      "Batch 207, Loss: -1.4216885566711426\n",
      "Batch 208, Loss: 0.36993148922920227\n",
      "Batch 209, Loss: 0.24256309866905212\n",
      "Batch 210, Loss: 0.03898531198501587\n",
      "Batch 211, Loss: 0.5101245641708374\n",
      "Batch 212, Loss: 0.6461514234542847\n",
      "Batch 213, Loss: -1.0573904514312744\n",
      "Batch 214, Loss: 0.17369115352630615\n",
      "Batch 215, Loss: -1.400627851486206\n",
      "Batch 216, Loss: -1.4951722621917725\n",
      "Batch 217, Loss: -1.0565475225448608\n",
      "Batch 218, Loss: -1.0532875061035156\n",
      "Batch 219, Loss: 0.4351872205734253\n",
      "Batch 220, Loss: -1.0952714681625366\n",
      "Batch 221, Loss: -1.3493881225585938\n",
      "Batch 222, Loss: 0.49112504720687866\n",
      "Batch 223, Loss: 0.05423840880393982\n",
      "Batch 224, Loss: -1.2798408269882202\n",
      "Batch 225, Loss: 0.44043073058128357\n",
      "Batch 226, Loss: 0.6099115610122681\n",
      "Batch 227, Loss: -1.1936354637145996\n",
      "Batch 228, Loss: 0.22886055707931519\n",
      "Batch 229, Loss: -1.3599528074264526\n",
      "Batch 230, Loss: 0.26404982805252075\n",
      "Batch 231, Loss: -1.0361899137496948\n",
      "Batch 232, Loss: -1.0973114967346191\n",
      "Batch 233, Loss: 0.7164064645767212\n",
      "Batch 234, Loss: 0.12785114347934723\n",
      "Batch 235, Loss: -1.1142250299453735\n",
      "Batch 236, Loss: -1.3877840042114258\n",
      "Batch 237, Loss: 0.47534477710723877\n",
      "Batch 238, Loss: 0.06918773055076599\n",
      "Batch 239, Loss: 0.06907039880752563\n",
      "Batch 240, Loss: 0.6891697645187378\n",
      "Batch 241, Loss: -1.2208040952682495\n",
      "Batch 242, Loss: -1.372077465057373\n",
      "Batch 243, Loss: -1.224216341972351\n",
      "Batch 244, Loss: -1.6969748735427856\n",
      "Batch 245, Loss: -1.1234896183013916\n",
      "Batch 246, Loss: 0.35864415764808655\n",
      "Batch 247, Loss: -1.5229017734527588\n",
      "Batch 248, Loss: -1.4716038703918457\n",
      "Batch 249, Loss: -1.09743070602417\n",
      "Batch 250, Loss: -1.2219505310058594\n",
      "Batch 251, Loss: -1.1679282188415527\n",
      "Batch 252, Loss: 0.16047966480255127\n",
      "Batch 253, Loss: -1.1182363033294678\n",
      "Batch 254, Loss: -1.2346051931381226\n",
      "Batch 255, Loss: -1.5114960670471191\n",
      "Batch 256, Loss: -1.108577847480774\n",
      "Batch 257, Loss: -1.6173317432403564\n",
      "Batch 258, Loss: 0.11148682236671448\n",
      "Batch 259, Loss: -1.3204796314239502\n",
      "Batch 260, Loss: -1.2744771242141724\n",
      "Batch 261, Loss: -1.063685655593872\n",
      "Batch 262, Loss: 0.15233832597732544\n",
      "Batch 263, Loss: 0.2547937035560608\n",
      "Batch 264, Loss: -1.2126524448394775\n",
      "Batch 265, Loss: -1.124085783958435\n",
      "Batch 266, Loss: 0.11688655614852905\n",
      "Batch 267, Loss: 0.5362476706504822\n",
      "Batch 268, Loss: -1.3251097202301025\n",
      "Batch 269, Loss: -1.3597902059555054\n",
      "Batch 270, Loss: 0.2356034517288208\n",
      "Batch 271, Loss: -1.1551505327224731\n",
      "Batch 272, Loss: -1.2864551544189453\n",
      "Batch 273, Loss: 0.31989187002182007\n",
      "Batch 274, Loss: 0.6356208324432373\n",
      "Batch 275, Loss: -1.295019507408142\n",
      "Batch 276, Loss: 0.3038303256034851\n",
      "Batch 277, Loss: 0.34361597895622253\n",
      "Batch 278, Loss: 0.4497752785682678\n",
      "Batch 279, Loss: -1.6136908531188965\n",
      "Batch 280, Loss: 0.05446693301200867\n",
      "Batch 281, Loss: 0.6385046243667603\n",
      "Batch 282, Loss: -1.5611824989318848\n",
      "Batch 283, Loss: 0.2264065444469452\n",
      "Batch 284, Loss: 0.08641132712364197\n",
      "Batch 285, Loss: -1.350783348083496\n",
      "Batch 286, Loss: -1.094833254814148\n",
      "Batch 287, Loss: 0.08574619889259338\n",
      "Batch 288, Loss: -1.5459840297698975\n",
      "Batch 289, Loss: 0.6411494016647339\n",
      "Batch 290, Loss: -1.307372808456421\n",
      "Batch 291, Loss: -1.5079340934753418\n",
      "Batch 292, Loss: 0.16501189768314362\n",
      "Batch 293, Loss: 0.08665761351585388\n",
      "Batch 294, Loss: 0.44810524582862854\n",
      "Batch 295, Loss: -1.161180853843689\n",
      "Batch 296, Loss: -1.142440676689148\n",
      "Batch 297, Loss: 0.09896871447563171\n",
      "Batch 298, Loss: -1.6360923051834106\n",
      "Batch 299, Loss: 0.39083170890808105\n",
      "Batch 300, Loss: -1.5073610544204712\n",
      "Batch 301, Loss: 0.40994611382484436\n",
      "Batch 302, Loss: -1.2263100147247314\n",
      "Batch 303, Loss: 0.6374887228012085\n",
      "Batch 304, Loss: 0.10568633675575256\n",
      "Batch 305, Loss: -1.1028714179992676\n",
      "Batch 306, Loss: -1.11680269241333\n",
      "Batch 307, Loss: -1.3437498807907104\n",
      "Batch 308, Loss: 0.1716616451740265\n",
      "Batch 309, Loss: 0.08967787027359009\n",
      "Batch 310, Loss: 0.6413710117340088\n",
      "Batch 311, Loss: -1.0295374393463135\n",
      "Batch 312, Loss: -1.5012073516845703\n",
      "Batch 313, Loss: -1.6748864650726318\n",
      "Batch 314, Loss: -1.2006522417068481\n",
      "Batch 315, Loss: 0.19843538105487823\n",
      "Batch 316, Loss: -1.2731354236602783\n",
      "Batch 317, Loss: 0.4191873371601105\n",
      "Batch 318, Loss: -1.6642082929611206\n",
      "Batch 319, Loss: 0.5639792084693909\n",
      "Batch 320, Loss: -1.1454354524612427\n",
      "Batch 321, Loss: -1.1193650960922241\n",
      "Batch 322, Loss: -1.0861049890518188\n",
      "Batch 323, Loss: 0.2836766242980957\n",
      "Batch 324, Loss: 0.4022274911403656\n",
      "Batch 325, Loss: 0.5412530303001404\n",
      "Batch 326, Loss: -1.5337764024734497\n",
      "Batch 327, Loss: -1.1388802528381348\n",
      "Batch 328, Loss: -1.5605006217956543\n",
      "Batch 329, Loss: -1.2058587074279785\n",
      "Batch 330, Loss: 0.5686647295951843\n",
      "Batch 331, Loss: 0.2790766954421997\n",
      "Batch 332, Loss: 0.6088776588439941\n",
      "Batch 333, Loss: 0.3411158621311188\n",
      "Batch 334, Loss: 0.0824299156665802\n",
      "Batch 335, Loss: 0.23946499824523926\n",
      "Batch 336, Loss: 0.33747875690460205\n",
      "Batch 337, Loss: -1.6061642169952393\n",
      "Batch 338, Loss: -1.2681076526641846\n",
      "Batch 339, Loss: 0.3503980040550232\n",
      "Batch 340, Loss: -1.2260185480117798\n",
      "Batch 341, Loss: -1.101302981376648\n",
      "Batch 342, Loss: 0.49864447116851807\n",
      "Batch 343, Loss: 0.22001825273036957\n",
      "Batch 344, Loss: -1.3649234771728516\n",
      "Batch 345, Loss: -1.508437156677246\n",
      "Batch 346, Loss: 0.13126595318317413\n",
      "Batch 347, Loss: -1.128438949584961\n",
      "Batch 348, Loss: 0.278780996799469\n",
      "Batch 349, Loss: -1.268564224243164\n",
      "Batch 350, Loss: -1.2132681608200073\n",
      "Batch 351, Loss: -1.2852118015289307\n",
      "Batch 352, Loss: -1.374528169631958\n",
      "Batch 353, Loss: -1.3964778184890747\n",
      "Batch 354, Loss: 0.5105547904968262\n",
      "Batch 355, Loss: 0.24303871393203735\n",
      "Batch 356, Loss: 0.12316150963306427\n",
      "Batch 357, Loss: -1.4419598579406738\n",
      "Batch 358, Loss: -1.2566401958465576\n",
      "Batch 359, Loss: 0.5088489055633545\n",
      "Batch 360, Loss: -1.2938694953918457\n",
      "Batch 361, Loss: 0.34370365738868713\n",
      "Batch 362, Loss: 0.23094777762889862\n",
      "Batch 363, Loss: -1.2904024124145508\n",
      "Batch 364, Loss: -1.4956727027893066\n",
      "Batch 365, Loss: 0.20361067354679108\n",
      "Batch 366, Loss: 0.4776647388935089\n",
      "Batch 367, Loss: -1.6639339923858643\n",
      "Batch 368, Loss: 0.6567273139953613\n",
      "Batch 369, Loss: -1.3244688510894775\n",
      "Batch 370, Loss: -1.0793354511260986\n",
      "Batch 371, Loss: 0.17718356847763062\n",
      "Batch 372, Loss: 0.3062426745891571\n",
      "Batch 373, Loss: 0.45067065954208374\n",
      "Batch 374, Loss: -1.113599419593811\n",
      "Batch 375, Loss: 0.45799508690834045\n",
      "Batch 376, Loss: 0.6400655508041382\n",
      "Batch 377, Loss: 0.20610029995441437\n",
      "Batch 378, Loss: -1.3000614643096924\n",
      "Batch 379, Loss: 0.07208055257797241\n",
      "Batch 380, Loss: -1.4576432704925537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 381, Loss: -1.5506787300109863\n",
      "Batch 382, Loss: -1.6177778244018555\n",
      "Batch 383, Loss: -1.1059496402740479\n",
      "Batch 384, Loss: -1.3368494510650635\n",
      "Batch 385, Loss: 0.3298299312591553\n",
      "Batch 386, Loss: -1.3502638339996338\n",
      "Batch 387, Loss: -1.2024937868118286\n",
      "Batch 388, Loss: -1.5865665674209595\n",
      "Batch 389, Loss: 0.4538499414920807\n",
      "Batch 390, Loss: -1.118939995765686\n",
      "Batch 391, Loss: -1.0597124099731445\n",
      "Batch 392, Loss: 0.6360158920288086\n",
      "Batch 393, Loss: 0.6353909373283386\n",
      "Batch 394, Loss: 0.21465161442756653\n",
      "Batch 395, Loss: -1.1951541900634766\n",
      "Batch 396, Loss: 0.11137190461158752\n",
      "Batch 397, Loss: -1.4365506172180176\n",
      "Batch 398, Loss: -1.1788101196289062\n",
      "Batch 399, Loss: -1.1113449335098267\n",
      "Training [40%]\tLoss: -0.5009\n",
      "Batch 0, Loss: 0.26366880536079407\n",
      "Batch 1, Loss: -1.2168192863464355\n",
      "Batch 2, Loss: 0.08636260032653809\n",
      "Batch 3, Loss: 0.4982450604438782\n",
      "Batch 4, Loss: -1.6540324687957764\n",
      "Batch 5, Loss: -1.6831324100494385\n",
      "Batch 6, Loss: 0.32518625259399414\n",
      "Batch 7, Loss: 0.07532018423080444\n",
      "Batch 8, Loss: 0.4001947343349457\n",
      "Batch 9, Loss: -1.1761538982391357\n",
      "Batch 10, Loss: 0.5011513233184814\n",
      "Batch 11, Loss: -1.2524428367614746\n",
      "Batch 12, Loss: -1.2147486209869385\n",
      "Batch 13, Loss: 0.19654099643230438\n",
      "Batch 14, Loss: -1.2415499687194824\n",
      "Batch 15, Loss: 0.5997979044914246\n",
      "Batch 16, Loss: 0.05926382541656494\n",
      "Batch 17, Loss: -1.6549370288848877\n",
      "Batch 18, Loss: 0.6240580081939697\n",
      "Batch 19, Loss: -1.044740915298462\n",
      "Batch 20, Loss: 0.3246820271015167\n",
      "Batch 21, Loss: 0.19640511274337769\n",
      "Batch 22, Loss: -1.2479068040847778\n",
      "Batch 23, Loss: -1.3513044118881226\n",
      "Batch 24, Loss: -1.349066972732544\n",
      "Batch 25, Loss: 0.48018646240234375\n",
      "Batch 26, Loss: 0.2951733469963074\n",
      "Batch 27, Loss: 0.07769197225570679\n",
      "Batch 28, Loss: -1.4464596509933472\n",
      "Batch 29, Loss: 0.30925291776657104\n",
      "Batch 30, Loss: 0.578366756439209\n",
      "Batch 31, Loss: -1.1525521278381348\n",
      "Batch 32, Loss: -1.3534393310546875\n",
      "Batch 33, Loss: 0.4319922626018524\n",
      "Batch 34, Loss: -1.3683676719665527\n",
      "Batch 35, Loss: -1.0133084058761597\n",
      "Batch 36, Loss: 0.6318654417991638\n",
      "Batch 37, Loss: -1.6206930875778198\n",
      "Batch 38, Loss: 0.6882284879684448\n",
      "Batch 39, Loss: -1.3326036930084229\n",
      "Batch 40, Loss: 0.39729344844818115\n",
      "Batch 41, Loss: 0.2118254452943802\n",
      "Batch 42, Loss: -1.5092225074768066\n",
      "Batch 43, Loss: 0.1610826551914215\n",
      "Batch 44, Loss: -1.348002552986145\n",
      "Batch 45, Loss: -1.2197902202606201\n",
      "Batch 46, Loss: 0.26412057876586914\n",
      "Batch 47, Loss: 0.30110257863998413\n",
      "Batch 48, Loss: 0.24583233892917633\n",
      "Batch 49, Loss: 0.4838692843914032\n",
      "Batch 50, Loss: -1.1616342067718506\n",
      "Batch 51, Loss: -1.0861706733703613\n",
      "Batch 52, Loss: -1.6677870750427246\n",
      "Batch 53, Loss: 0.1315167099237442\n",
      "Batch 54, Loss: -1.2710602283477783\n",
      "Batch 55, Loss: 0.3284797668457031\n",
      "Batch 56, Loss: -1.7033770084381104\n",
      "Batch 57, Loss: -1.376983880996704\n",
      "Batch 58, Loss: -1.3353955745697021\n",
      "Batch 59, Loss: 0.30861392617225647\n",
      "Batch 60, Loss: 0.5298272967338562\n",
      "Batch 61, Loss: -1.5248634815216064\n",
      "Batch 62, Loss: 0.496753990650177\n",
      "Batch 63, Loss: 0.023903369903564453\n",
      "Batch 64, Loss: -1.2229740619659424\n",
      "Batch 65, Loss: -1.3151061534881592\n",
      "Batch 66, Loss: 0.14542274177074432\n",
      "Batch 67, Loss: 0.5023425817489624\n",
      "Batch 68, Loss: -1.3258802890777588\n",
      "Batch 69, Loss: 0.4339998662471771\n",
      "Batch 70, Loss: 0.10213172435760498\n",
      "Batch 71, Loss: 0.32328829169273376\n",
      "Batch 72, Loss: -1.371755838394165\n",
      "Batch 73, Loss: -1.2704747915267944\n",
      "Batch 74, Loss: -1.3701534271240234\n",
      "Batch 75, Loss: -1.2220032215118408\n",
      "Batch 76, Loss: -1.2519831657409668\n",
      "Batch 77, Loss: 0.43496647477149963\n",
      "Batch 78, Loss: -1.070128321647644\n",
      "Batch 79, Loss: 0.2480284869670868\n",
      "Batch 80, Loss: 0.2459387630224228\n",
      "Batch 81, Loss: -1.2087855339050293\n",
      "Batch 82, Loss: -1.4160287380218506\n",
      "Batch 83, Loss: 0.471229612827301\n",
      "Batch 84, Loss: 0.2944950461387634\n",
      "Batch 85, Loss: 0.31652092933654785\n",
      "Batch 86, Loss: -1.6903434991836548\n",
      "Batch 87, Loss: 0.08512493968009949\n",
      "Batch 88, Loss: 0.6748552322387695\n",
      "Batch 89, Loss: -1.1558830738067627\n",
      "Batch 90, Loss: 0.35632678866386414\n",
      "Batch 91, Loss: -1.2558236122131348\n",
      "Batch 92, Loss: 0.18846765160560608\n",
      "Batch 93, Loss: 0.3372284770011902\n",
      "Batch 94, Loss: 0.713912308216095\n",
      "Batch 95, Loss: 0.030698418617248535\n",
      "Batch 96, Loss: 0.22495412826538086\n",
      "Batch 97, Loss: -1.3406628370285034\n",
      "Batch 98, Loss: 0.3666751980781555\n",
      "Batch 99, Loss: -1.2834912538528442\n",
      "Batch 100, Loss: -1.3575019836425781\n",
      "Batch 101, Loss: 0.47614455223083496\n",
      "Batch 102, Loss: 0.12974783778190613\n",
      "Batch 103, Loss: 0.2372196912765503\n",
      "Batch 104, Loss: -1.5270814895629883\n",
      "Batch 105, Loss: -1.4519120454788208\n",
      "Batch 106, Loss: -1.1317726373672485\n",
      "Batch 107, Loss: -1.0821954011917114\n",
      "Batch 108, Loss: -1.5974442958831787\n",
      "Batch 109, Loss: -1.2972999811172485\n",
      "Batch 110, Loss: 0.07155153155326843\n",
      "Batch 111, Loss: -1.5365989208221436\n",
      "Batch 112, Loss: -1.3426568508148193\n",
      "Batch 113, Loss: 0.24769851565361023\n",
      "Batch 114, Loss: -1.5183659791946411\n",
      "Batch 115, Loss: 0.19022703170776367\n",
      "Batch 116, Loss: -1.3214213848114014\n",
      "Batch 117, Loss: 0.6647940874099731\n",
      "Batch 118, Loss: -1.702725887298584\n",
      "Batch 119, Loss: -1.4289941787719727\n",
      "Batch 120, Loss: -1.2248858213424683\n",
      "Batch 121, Loss: -1.5431549549102783\n",
      "Batch 122, Loss: 0.37024015188217163\n",
      "Batch 123, Loss: -1.6201863288879395\n",
      "Batch 124, Loss: -1.2825052738189697\n",
      "Batch 125, Loss: -1.3222386837005615\n",
      "Batch 126, Loss: -1.4707653522491455\n",
      "Batch 127, Loss: -1.0928702354431152\n",
      "Batch 128, Loss: 0.23902416229248047\n",
      "Batch 129, Loss: 0.24756106734275818\n",
      "Batch 130, Loss: -1.2432571649551392\n",
      "Batch 131, Loss: -1.5669752359390259\n",
      "Batch 132, Loss: 0.3273617625236511\n",
      "Batch 133, Loss: 0.6170336604118347\n",
      "Batch 134, Loss: 0.27166229486465454\n",
      "Batch 135, Loss: -1.2143572568893433\n",
      "Batch 136, Loss: -1.1619479656219482\n",
      "Batch 137, Loss: -1.3541607856750488\n",
      "Batch 138, Loss: 0.09517219662666321\n",
      "Batch 139, Loss: 0.25999903678894043\n",
      "Batch 140, Loss: -1.3500738143920898\n",
      "Batch 141, Loss: 0.28876256942749023\n",
      "Batch 142, Loss: -1.4228003025054932\n",
      "Batch 143, Loss: 0.46551838517189026\n",
      "Batch 144, Loss: 0.18983511626720428\n",
      "Batch 145, Loss: -1.177468180656433\n",
      "Batch 146, Loss: 0.35366716980934143\n",
      "Batch 147, Loss: -1.6474521160125732\n",
      "Batch 148, Loss: 0.4459925889968872\n",
      "Batch 149, Loss: 0.6328509449958801\n",
      "Batch 150, Loss: -1.7225103378295898\n",
      "Batch 151, Loss: 0.49892282485961914\n",
      "Batch 152, Loss: -1.2997597455978394\n",
      "Batch 153, Loss: -1.3534256219863892\n",
      "Batch 154, Loss: -1.6802685260772705\n",
      "Batch 155, Loss: -1.2554230690002441\n",
      "Batch 156, Loss: 0.2804810702800751\n",
      "Batch 157, Loss: -1.431855320930481\n",
      "Batch 158, Loss: 0.1794152408838272\n",
      "Batch 159, Loss: -1.6630786657333374\n",
      "Batch 160, Loss: 0.1638234406709671\n",
      "Batch 161, Loss: -1.4522755146026611\n",
      "Batch 162, Loss: -1.6690737009048462\n",
      "Batch 163, Loss: -1.4896634817123413\n",
      "Batch 164, Loss: 0.5128507614135742\n",
      "Batch 165, Loss: 0.20866107940673828\n",
      "Batch 166, Loss: 0.5872587561607361\n",
      "Batch 167, Loss: 0.5697354674339294\n",
      "Batch 168, Loss: 0.23177459836006165\n",
      "Batch 169, Loss: -1.28450608253479\n",
      "Batch 170, Loss: 0.5522770881652832\n",
      "Batch 171, Loss: 0.1768399029970169\n",
      "Batch 172, Loss: -1.3369433879852295\n",
      "Batch 173, Loss: 0.496862530708313\n",
      "Batch 174, Loss: 0.332916796207428\n",
      "Batch 175, Loss: 0.367440402507782\n",
      "Batch 176, Loss: -1.2436504364013672\n",
      "Batch 177, Loss: -1.2002681493759155\n",
      "Batch 178, Loss: -1.3198026418685913\n",
      "Batch 179, Loss: 0.06122082471847534\n",
      "Batch 180, Loss: 0.3650096356868744\n",
      "Batch 181, Loss: 0.0902785062789917\n",
      "Batch 182, Loss: 0.2404806762933731\n",
      "Batch 183, Loss: -1.392688512802124\n",
      "Batch 184, Loss: -1.3333752155303955\n",
      "Batch 185, Loss: 0.520491898059845\n",
      "Batch 186, Loss: -1.229595422744751\n",
      "Batch 187, Loss: 0.684556245803833\n",
      "Batch 188, Loss: -1.0783319473266602\n",
      "Batch 189, Loss: 0.51311856508255\n",
      "Batch 190, Loss: 0.1259995698928833\n",
      "Batch 191, Loss: 0.4589185118675232\n",
      "Batch 192, Loss: -1.5482940673828125\n",
      "Batch 193, Loss: 0.033889591693878174\n",
      "Batch 194, Loss: -1.330998182296753\n",
      "Batch 195, Loss: -1.6571826934814453\n",
      "Batch 196, Loss: -1.0849802494049072\n",
      "Batch 197, Loss: 0.5154742002487183\n",
      "Batch 198, Loss: -1.3885841369628906\n",
      "Batch 199, Loss: 0.41681981086730957\n",
      "Batch 200, Loss: -1.1409587860107422\n",
      "Batch 201, Loss: 0.03083866834640503\n",
      "Batch 202, Loss: -1.317980408668518\n",
      "Batch 203, Loss: -1.5233888626098633\n",
      "Batch 204, Loss: -1.5691993236541748\n",
      "Batch 205, Loss: 0.1559293419122696\n",
      "Batch 206, Loss: 0.5796337723731995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 207, Loss: 0.09562185406684875\n",
      "Batch 208, Loss: -1.463837742805481\n",
      "Batch 209, Loss: 0.4788205027580261\n",
      "Batch 210, Loss: 0.2750921845436096\n",
      "Batch 211, Loss: -1.4000215530395508\n",
      "Batch 212, Loss: 0.29148751497268677\n",
      "Batch 213, Loss: -1.408435344696045\n",
      "Batch 214, Loss: -1.5109925270080566\n",
      "Batch 215, Loss: -1.376131296157837\n",
      "Batch 216, Loss: -1.4154160022735596\n",
      "Batch 217, Loss: -1.1516752243041992\n",
      "Batch 218, Loss: 0.7276430130004883\n",
      "Batch 219, Loss: 0.20685167610645294\n",
      "Batch 220, Loss: -1.5506781339645386\n",
      "Batch 221, Loss: -1.7183738946914673\n",
      "Batch 222, Loss: -1.2415406703948975\n",
      "Batch 223, Loss: 0.230751171708107\n",
      "Batch 224, Loss: -1.0777949094772339\n",
      "Batch 225, Loss: 0.36438798904418945\n",
      "Batch 226, Loss: 0.46176040172576904\n",
      "Batch 227, Loss: -1.397043228149414\n",
      "Batch 228, Loss: -1.3085334300994873\n",
      "Batch 229, Loss: 0.3599131405353546\n",
      "Batch 230, Loss: -1.3197691440582275\n",
      "Batch 231, Loss: -1.2173819541931152\n",
      "Batch 232, Loss: -1.463240623474121\n",
      "Batch 233, Loss: -1.271154761314392\n",
      "Batch 234, Loss: 0.0521964430809021\n",
      "Batch 235, Loss: 0.5470412373542786\n",
      "Batch 236, Loss: -1.6320899724960327\n",
      "Batch 237, Loss: 0.6367850303649902\n",
      "Batch 238, Loss: -1.7111605405807495\n",
      "Batch 239, Loss: -1.5244497060775757\n",
      "Batch 240, Loss: -1.4015958309173584\n",
      "Batch 241, Loss: -1.3793615102767944\n",
      "Batch 242, Loss: -1.4029161930084229\n",
      "Batch 243, Loss: 0.5086687803268433\n",
      "Batch 244, Loss: 0.49404075741767883\n",
      "Batch 245, Loss: 0.3909705877304077\n",
      "Batch 246, Loss: -1.4957196712493896\n",
      "Batch 247, Loss: -1.377807855606079\n",
      "Batch 248, Loss: -1.3556663990020752\n",
      "Batch 249, Loss: -1.2596607208251953\n",
      "Batch 250, Loss: -1.3551702499389648\n",
      "Batch 251, Loss: -1.5481572151184082\n",
      "Batch 252, Loss: 0.5175814628601074\n",
      "Batch 253, Loss: -1.2762160301208496\n",
      "Batch 254, Loss: -1.295698642730713\n",
      "Batch 255, Loss: 0.6072423458099365\n",
      "Batch 256, Loss: 0.37425491213798523\n",
      "Batch 257, Loss: -1.69881272315979\n",
      "Batch 258, Loss: 0.6350026726722717\n",
      "Batch 259, Loss: -1.5722987651824951\n",
      "Batch 260, Loss: -1.3746707439422607\n",
      "Batch 261, Loss: 0.06781807541847229\n",
      "Batch 262, Loss: -1.2030599117279053\n",
      "Batch 263, Loss: 0.3768704831600189\n",
      "Batch 264, Loss: -1.6094911098480225\n",
      "Batch 265, Loss: 0.7073099613189697\n",
      "Batch 266, Loss: -1.218544363975525\n",
      "Batch 267, Loss: -1.3345954418182373\n",
      "Batch 268, Loss: 0.6947146654129028\n",
      "Batch 269, Loss: 0.3701370656490326\n",
      "Batch 270, Loss: -1.6302862167358398\n",
      "Batch 271, Loss: 0.06490200757980347\n",
      "Batch 272, Loss: 0.6755831241607666\n",
      "Batch 273, Loss: 0.07013896107673645\n",
      "Batch 274, Loss: 0.284504771232605\n",
      "Batch 275, Loss: -1.633070468902588\n",
      "Batch 276, Loss: 0.3421310484409332\n",
      "Batch 277, Loss: -1.337817907333374\n",
      "Batch 278, Loss: 0.45300254225730896\n",
      "Batch 279, Loss: 0.35390910506248474\n",
      "Batch 280, Loss: -1.2146793603897095\n",
      "Batch 281, Loss: -1.4832782745361328\n",
      "Batch 282, Loss: -1.2244874238967896\n",
      "Batch 283, Loss: -1.7085565328598022\n",
      "Batch 284, Loss: -1.0146279335021973\n",
      "Batch 285, Loss: 0.3020703196525574\n",
      "Batch 286, Loss: 0.08907637000083923\n",
      "Batch 287, Loss: -1.5019452571868896\n",
      "Batch 288, Loss: 0.3544101417064667\n",
      "Batch 289, Loss: 0.07257980108261108\n",
      "Batch 290, Loss: -1.4888931512832642\n",
      "Batch 291, Loss: -1.4548137187957764\n",
      "Batch 292, Loss: 0.4964871406555176\n",
      "Batch 293, Loss: 0.4314044713973999\n",
      "Batch 294, Loss: -1.4551770687103271\n",
      "Batch 295, Loss: 0.5354158878326416\n",
      "Batch 296, Loss: -1.4087915420532227\n",
      "Batch 297, Loss: -1.2121936082839966\n",
      "Batch 298, Loss: -1.2589342594146729\n",
      "Batch 299, Loss: 0.6058220267295837\n",
      "Batch 300, Loss: -1.1406335830688477\n",
      "Batch 301, Loss: -1.6014108657836914\n",
      "Batch 302, Loss: 0.017635315656661987\n",
      "Batch 303, Loss: -1.4111039638519287\n",
      "Batch 304, Loss: -1.0294458866119385\n",
      "Batch 305, Loss: 0.0969703197479248\n",
      "Batch 306, Loss: -1.702709436416626\n",
      "Batch 307, Loss: 0.35674887895584106\n",
      "Batch 308, Loss: -1.226340889930725\n",
      "Batch 309, Loss: -1.0843682289123535\n",
      "Batch 310, Loss: 0.05105575919151306\n",
      "Batch 311, Loss: -1.4152947664260864\n",
      "Batch 312, Loss: 0.4955385625362396\n",
      "Batch 313, Loss: 0.5200409293174744\n",
      "Batch 314, Loss: 0.30595311522483826\n",
      "Batch 315, Loss: 0.16491854190826416\n",
      "Batch 316, Loss: 0.3566895127296448\n",
      "Batch 317, Loss: -1.4035120010375977\n",
      "Batch 318, Loss: -1.5265698432922363\n",
      "Batch 319, Loss: 0.721318244934082\n",
      "Batch 320, Loss: -1.6447703838348389\n",
      "Batch 321, Loss: 0.6986922025680542\n",
      "Batch 322, Loss: 0.7318812012672424\n",
      "Batch 323, Loss: -1.3773345947265625\n",
      "Batch 324, Loss: 0.14732417464256287\n",
      "Batch 325, Loss: 0.3813657760620117\n",
      "Batch 326, Loss: 0.744726300239563\n",
      "Batch 327, Loss: 0.2861637473106384\n",
      "Batch 328, Loss: -1.2058355808258057\n",
      "Batch 329, Loss: 0.6217883229255676\n",
      "Batch 330, Loss: -1.577492117881775\n",
      "Batch 331, Loss: 0.5831388235092163\n",
      "Batch 332, Loss: 0.2654731571674347\n",
      "Batch 333, Loss: 0.06131264567375183\n",
      "Batch 334, Loss: 0.3499884009361267\n",
      "Batch 335, Loss: 0.7275439500808716\n",
      "Batch 336, Loss: -1.5683789253234863\n",
      "Batch 337, Loss: -1.4474241733551025\n",
      "Batch 338, Loss: 0.30434465408325195\n",
      "Batch 339, Loss: 0.4616971015930176\n",
      "Batch 340, Loss: -1.4340087175369263\n",
      "Batch 341, Loss: 0.5976476073265076\n",
      "Batch 342, Loss: -1.6407579183578491\n",
      "Batch 343, Loss: 0.4357999563217163\n",
      "Batch 344, Loss: 0.3846767544746399\n",
      "Batch 345, Loss: 0.027903586626052856\n",
      "Batch 346, Loss: -1.2770624160766602\n",
      "Batch 347, Loss: 0.3361724317073822\n",
      "Batch 348, Loss: -1.6461806297302246\n",
      "Batch 349, Loss: 0.2651195526123047\n",
      "Batch 350, Loss: 0.20070092380046844\n",
      "Batch 351, Loss: -1.570655345916748\n",
      "Batch 352, Loss: -1.0987879037857056\n",
      "Batch 353, Loss: 0.5313441753387451\n",
      "Batch 354, Loss: 0.2530324161052704\n",
      "Batch 355, Loss: 0.2471005916595459\n",
      "Batch 356, Loss: 0.6534698009490967\n",
      "Batch 357, Loss: 0.2860783636569977\n",
      "Batch 358, Loss: -1.6399245262145996\n",
      "Batch 359, Loss: 0.5546534061431885\n",
      "Batch 360, Loss: -1.322424292564392\n",
      "Batch 361, Loss: 0.6590593457221985\n",
      "Batch 362, Loss: -1.055053949356079\n",
      "Batch 363, Loss: -1.2818058729171753\n",
      "Batch 364, Loss: 0.3984917998313904\n",
      "Batch 365, Loss: -1.283207893371582\n",
      "Batch 366, Loss: -1.4089558124542236\n",
      "Batch 367, Loss: -1.376644492149353\n",
      "Batch 368, Loss: 0.06173831224441528\n",
      "Batch 369, Loss: 0.3248932659626007\n",
      "Batch 370, Loss: 0.16533346474170685\n",
      "Batch 371, Loss: 0.575522780418396\n",
      "Batch 372, Loss: -1.2767856121063232\n",
      "Batch 373, Loss: 0.5494118332862854\n",
      "Batch 374, Loss: -1.5587880611419678\n",
      "Batch 375, Loss: 0.494766503572464\n",
      "Batch 376, Loss: -1.4311167001724243\n",
      "Batch 377, Loss: -1.1104464530944824\n",
      "Batch 378, Loss: 0.054163068532943726\n",
      "Batch 379, Loss: -1.5448870658874512\n",
      "Batch 380, Loss: 0.28933241963386536\n",
      "Batch 381, Loss: -1.3352773189544678\n",
      "Batch 382, Loss: -1.306125283241272\n",
      "Batch 383, Loss: -1.424383282661438\n",
      "Batch 384, Loss: -1.486319899559021\n",
      "Batch 385, Loss: -1.3597471714019775\n",
      "Batch 386, Loss: -1.5722990036010742\n",
      "Batch 387, Loss: 0.3024738132953644\n",
      "Batch 388, Loss: 0.11204549670219421\n",
      "Batch 389, Loss: 0.3810165822505951\n",
      "Batch 390, Loss: -1.1367160081863403\n",
      "Batch 391, Loss: 0.3403363525867462\n",
      "Batch 392, Loss: 0.19303728640079498\n",
      "Batch 393, Loss: -1.0321646928787231\n",
      "Batch 394, Loss: -1.3192715644836426\n",
      "Batch 395, Loss: 0.6678285598754883\n",
      "Batch 396, Loss: 0.43685171008110046\n",
      "Batch 397, Loss: 0.177336186170578\n",
      "Batch 398, Loss: 0.19187772274017334\n",
      "Batch 399, Loss: -1.4645702838897705\n",
      "Training [50%]\tLoss: -0.5111\n",
      "Batch 0, Loss: 0.37389475107192993\n",
      "Batch 1, Loss: 0.583855390548706\n",
      "Batch 2, Loss: -1.3689675331115723\n",
      "Batch 3, Loss: 0.41078299283981323\n",
      "Batch 4, Loss: 0.6927143335342407\n",
      "Batch 5, Loss: 0.22687608003616333\n",
      "Batch 6, Loss: -1.5338537693023682\n",
      "Batch 7, Loss: 0.5455231070518494\n",
      "Batch 8, Loss: 0.538938045501709\n",
      "Batch 9, Loss: 0.06247177720069885\n",
      "Batch 10, Loss: 0.5920374393463135\n",
      "Batch 11, Loss: 0.17465001344680786\n",
      "Batch 12, Loss: -1.707147240638733\n",
      "Batch 13, Loss: -1.2292956113815308\n",
      "Batch 14, Loss: -1.6687748432159424\n",
      "Batch 15, Loss: -1.06749427318573\n",
      "Batch 16, Loss: -1.2090061902999878\n",
      "Batch 17, Loss: -1.3576557636260986\n",
      "Batch 18, Loss: -1.510562539100647\n",
      "Batch 19, Loss: 0.22991029918193817\n",
      "Batch 20, Loss: 0.4379732012748718\n",
      "Batch 21, Loss: 0.22765858471393585\n",
      "Batch 22, Loss: -1.0982551574707031\n",
      "Batch 23, Loss: -1.479797124862671\n",
      "Batch 24, Loss: -1.2117220163345337\n",
      "Batch 25, Loss: 0.6864434480667114\n",
      "Batch 26, Loss: -1.5896501541137695\n",
      "Batch 27, Loss: -1.3317559957504272\n",
      "Batch 28, Loss: 0.29904723167419434\n",
      "Batch 29, Loss: 0.34374064207077026\n",
      "Batch 30, Loss: -1.2944726943969727\n",
      "Batch 31, Loss: 0.04974287748336792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32, Loss: 0.15283671021461487\n",
      "Batch 33, Loss: 0.35680168867111206\n",
      "Batch 34, Loss: 0.6544300317764282\n",
      "Batch 35, Loss: 0.09079119563102722\n",
      "Batch 36, Loss: -1.5392957925796509\n",
      "Batch 37, Loss: 0.6585489511489868\n",
      "Batch 38, Loss: 0.7064722776412964\n",
      "Batch 39, Loss: -1.3951389789581299\n",
      "Batch 40, Loss: 0.3538171350955963\n",
      "Batch 41, Loss: 0.3131207227706909\n",
      "Batch 42, Loss: 0.284089595079422\n",
      "Batch 43, Loss: -1.6428414583206177\n",
      "Batch 44, Loss: -1.0886439085006714\n",
      "Batch 45, Loss: 0.44318056106567383\n",
      "Batch 46, Loss: 0.3293965756893158\n",
      "Batch 47, Loss: 0.11929696798324585\n",
      "Batch 48, Loss: 0.10393577814102173\n",
      "Batch 49, Loss: 0.08986029028892517\n",
      "Batch 50, Loss: 0.3041272759437561\n",
      "Batch 51, Loss: -1.118707299232483\n",
      "Batch 52, Loss: 0.3974805474281311\n",
      "Batch 53, Loss: -1.2818316221237183\n",
      "Batch 54, Loss: 0.7204943895339966\n",
      "Batch 55, Loss: 0.6324183940887451\n",
      "Batch 56, Loss: -1.6162165403366089\n",
      "Batch 57, Loss: -1.0327670574188232\n",
      "Batch 58, Loss: 0.619875431060791\n",
      "Batch 59, Loss: -1.0495424270629883\n",
      "Batch 60, Loss: -1.5271685123443604\n",
      "Batch 61, Loss: -1.5231432914733887\n",
      "Batch 62, Loss: -1.3352103233337402\n",
      "Batch 63, Loss: -1.172985553741455\n",
      "Batch 64, Loss: 0.18548709154129028\n",
      "Batch 65, Loss: -1.5966606140136719\n",
      "Batch 66, Loss: -1.202059030532837\n",
      "Batch 67, Loss: -1.3844311237335205\n",
      "Batch 68, Loss: 0.24938857555389404\n",
      "Batch 69, Loss: 0.3427577316761017\n",
      "Batch 70, Loss: -1.2769975662231445\n",
      "Batch 71, Loss: -1.007173776626587\n",
      "Batch 72, Loss: 0.7066954374313354\n",
      "Batch 73, Loss: 0.387334942817688\n",
      "Batch 74, Loss: 0.2159983515739441\n",
      "Batch 75, Loss: 0.19235645234584808\n",
      "Batch 76, Loss: 0.10763218998908997\n",
      "Batch 77, Loss: 0.07241794466972351\n",
      "Batch 78, Loss: -1.179498553276062\n",
      "Batch 79, Loss: 0.16276158392429352\n",
      "Batch 80, Loss: -1.2367180585861206\n",
      "Batch 81, Loss: 0.5244573354721069\n",
      "Batch 82, Loss: -1.3680764436721802\n",
      "Batch 83, Loss: -1.6908485889434814\n",
      "Batch 84, Loss: -1.4779713153839111\n",
      "Batch 85, Loss: -1.1498615741729736\n",
      "Batch 86, Loss: 0.45848211646080017\n",
      "Batch 87, Loss: 0.2594515383243561\n",
      "Batch 88, Loss: 0.297136515378952\n",
      "Batch 89, Loss: 0.5713930726051331\n",
      "Batch 90, Loss: -1.4812971353530884\n",
      "Batch 91, Loss: -1.06510329246521\n",
      "Batch 92, Loss: 0.3568873405456543\n",
      "Batch 93, Loss: 0.523280143737793\n",
      "Batch 94, Loss: -1.65012788772583\n",
      "Batch 95, Loss: 0.1477694809436798\n",
      "Batch 96, Loss: 0.5935856103897095\n",
      "Batch 97, Loss: -1.3841811418533325\n",
      "Batch 98, Loss: -1.199533224105835\n",
      "Batch 99, Loss: 0.15553803741931915\n",
      "Batch 100, Loss: -1.5463727712631226\n",
      "Batch 101, Loss: -1.2949867248535156\n",
      "Batch 102, Loss: 0.19263070821762085\n",
      "Batch 103, Loss: 0.22574226558208466\n",
      "Batch 104, Loss: 0.6329689621925354\n",
      "Batch 105, Loss: -1.0766068696975708\n",
      "Batch 106, Loss: 0.018541723489761353\n",
      "Batch 107, Loss: 0.09101271629333496\n",
      "Batch 108, Loss: 0.5812389850616455\n",
      "Batch 109, Loss: -1.6405068635940552\n",
      "Batch 110, Loss: 0.6822490096092224\n",
      "Batch 111, Loss: -1.074728012084961\n",
      "Batch 112, Loss: -1.3196930885314941\n",
      "Batch 113, Loss: 0.6921525001525879\n",
      "Batch 114, Loss: -1.5309897661209106\n",
      "Batch 115, Loss: 0.27418461441993713\n",
      "Batch 116, Loss: 0.06980180740356445\n",
      "Batch 117, Loss: 0.3979921042919159\n",
      "Batch 118, Loss: 0.05808752775192261\n",
      "Batch 119, Loss: 0.31186583638191223\n",
      "Batch 120, Loss: 0.3015436828136444\n",
      "Batch 121, Loss: 0.07767468690872192\n",
      "Batch 122, Loss: -1.2423229217529297\n",
      "Batch 123, Loss: 0.6327738761901855\n",
      "Batch 124, Loss: -1.2902801036834717\n",
      "Batch 125, Loss: -1.6600594520568848\n",
      "Batch 126, Loss: -1.4258620738983154\n",
      "Batch 127, Loss: 0.27202439308166504\n",
      "Batch 128, Loss: -1.3668173551559448\n",
      "Batch 129, Loss: 0.36653363704681396\n",
      "Batch 130, Loss: 0.01539716124534607\n",
      "Batch 131, Loss: -1.1115106344223022\n",
      "Batch 132, Loss: -1.0830967426300049\n",
      "Batch 133, Loss: 0.04140740633010864\n",
      "Batch 134, Loss: 0.1884618103504181\n",
      "Batch 135, Loss: 0.2844499349594116\n",
      "Batch 136, Loss: -1.2178229093551636\n",
      "Batch 137, Loss: 0.6691093444824219\n",
      "Batch 138, Loss: 0.16064275801181793\n",
      "Batch 139, Loss: -1.0968329906463623\n",
      "Batch 140, Loss: -1.2567839622497559\n",
      "Batch 141, Loss: -1.446745753288269\n",
      "Batch 142, Loss: 0.27718889713287354\n",
      "Batch 143, Loss: -1.2716548442840576\n",
      "Batch 144, Loss: -1.4714531898498535\n",
      "Batch 145, Loss: 0.08874377608299255\n",
      "Batch 146, Loss: 0.06334570050239563\n",
      "Batch 147, Loss: 0.35544508695602417\n",
      "Batch 148, Loss: -1.1776325702667236\n",
      "Batch 149, Loss: 0.05589139461517334\n",
      "Batch 150, Loss: 0.4917699098587036\n",
      "Batch 151, Loss: 0.5750725269317627\n",
      "Batch 152, Loss: 0.4475293457508087\n",
      "Batch 153, Loss: 0.2179194688796997\n",
      "Batch 154, Loss: -1.1273579597473145\n",
      "Batch 155, Loss: -1.2471487522125244\n",
      "Batch 156, Loss: -1.1772541999816895\n",
      "Batch 157, Loss: -1.1605883836746216\n",
      "Batch 158, Loss: 0.10674150288105011\n",
      "Batch 159, Loss: 0.26406538486480713\n",
      "Batch 160, Loss: -1.2350600957870483\n",
      "Batch 161, Loss: -1.1637377738952637\n",
      "Batch 162, Loss: 0.29262062907218933\n",
      "Batch 163, Loss: 0.20019465684890747\n",
      "Batch 164, Loss: 0.034101009368896484\n",
      "Batch 165, Loss: -1.7111239433288574\n",
      "Batch 166, Loss: 0.061738818883895874\n",
      "Batch 167, Loss: -1.343329668045044\n",
      "Batch 168, Loss: -1.2856929302215576\n",
      "Batch 169, Loss: 0.03024446964263916\n",
      "Batch 170, Loss: 0.30247771739959717\n",
      "Batch 171, Loss: -1.1992131471633911\n",
      "Batch 172, Loss: 0.0912519097328186\n",
      "Batch 173, Loss: 0.01910582184791565\n",
      "Batch 174, Loss: 0.4025149345397949\n",
      "Batch 175, Loss: 0.04706504940986633\n",
      "Batch 176, Loss: 0.2935929596424103\n",
      "Batch 177, Loss: 0.06768697500228882\n",
      "Batch 178, Loss: 0.4523334205150604\n",
      "Batch 179, Loss: 0.06882765889167786\n",
      "Batch 180, Loss: 0.15257591009140015\n",
      "Batch 181, Loss: 0.4480283260345459\n",
      "Batch 182, Loss: 0.2094900906085968\n",
      "Batch 183, Loss: -1.5252258777618408\n",
      "Batch 184, Loss: 0.3891787528991699\n",
      "Batch 185, Loss: -1.574038028717041\n",
      "Batch 186, Loss: -1.6441543102264404\n",
      "Batch 187, Loss: -1.5480347871780396\n",
      "Batch 188, Loss: -1.2915291786193848\n",
      "Batch 189, Loss: -1.2238856554031372\n",
      "Batch 190, Loss: -1.5116498470306396\n",
      "Batch 191, Loss: 0.6638098359107971\n",
      "Batch 192, Loss: 0.37463808059692383\n",
      "Batch 193, Loss: -1.3831355571746826\n",
      "Batch 194, Loss: -1.3238131999969482\n",
      "Batch 195, Loss: -1.6699634790420532\n",
      "Batch 196, Loss: -1.5912327766418457\n",
      "Batch 197, Loss: -1.3866549730300903\n",
      "Batch 198, Loss: 0.2743587791919708\n",
      "Batch 199, Loss: -1.5194880962371826\n",
      "Batch 200, Loss: -1.678770661354065\n",
      "Batch 201, Loss: -1.5932310819625854\n",
      "Batch 202, Loss: -1.2635607719421387\n",
      "Batch 203, Loss: -1.0441474914550781\n",
      "Batch 204, Loss: -1.678699254989624\n",
      "Batch 205, Loss: -1.1675901412963867\n",
      "Batch 206, Loss: -1.2468456029891968\n",
      "Batch 207, Loss: -1.7362306118011475\n",
      "Batch 208, Loss: 0.4136630892753601\n",
      "Batch 209, Loss: 0.4685303568840027\n",
      "Batch 210, Loss: -1.720496654510498\n",
      "Batch 211, Loss: 0.2623547315597534\n",
      "Batch 212, Loss: -1.5186903476715088\n",
      "Batch 213, Loss: 0.25540637969970703\n",
      "Batch 214, Loss: -1.4039714336395264\n",
      "Batch 215, Loss: -1.0259681940078735\n",
      "Batch 216, Loss: -1.2828915119171143\n",
      "Batch 217, Loss: -1.32399320602417\n",
      "Batch 218, Loss: -1.0253621339797974\n",
      "Batch 219, Loss: 0.21050889790058136\n",
      "Batch 220, Loss: -1.0669631958007812\n",
      "Batch 221, Loss: -1.6266255378723145\n",
      "Batch 222, Loss: -0.988858163356781\n",
      "Batch 223, Loss: 0.3751072883605957\n",
      "Batch 224, Loss: 0.3071035146713257\n",
      "Batch 225, Loss: -1.176424264907837\n",
      "Batch 226, Loss: -1.7504476308822632\n",
      "Batch 227, Loss: 0.2733192443847656\n",
      "Batch 228, Loss: -1.160835862159729\n",
      "Batch 229, Loss: -1.2815202474594116\n",
      "Batch 230, Loss: 0.20676203072071075\n",
      "Batch 231, Loss: -1.0223582983016968\n",
      "Batch 232, Loss: 0.4015997648239136\n",
      "Batch 233, Loss: -1.2944949865341187\n",
      "Batch 234, Loss: -1.4785405397415161\n",
      "Batch 235, Loss: -1.2104560136795044\n",
      "Batch 236, Loss: -1.3757023811340332\n",
      "Batch 237, Loss: 0.5114103555679321\n",
      "Batch 238, Loss: 0.21242156624794006\n",
      "Batch 239, Loss: 0.40119585394859314\n",
      "Batch 240, Loss: -1.1425156593322754\n",
      "Batch 241, Loss: 0.11084724962711334\n",
      "Batch 242, Loss: -1.5072050094604492\n",
      "Batch 243, Loss: -1.2349517345428467\n",
      "Batch 244, Loss: -1.104812502861023\n",
      "Batch 245, Loss: 0.18296729028224945\n",
      "Batch 246, Loss: 0.27063071727752686\n",
      "Batch 247, Loss: 0.17423409223556519\n",
      "Batch 248, Loss: -1.247170329093933\n",
      "Batch 249, Loss: -1.371856927871704\n",
      "Batch 250, Loss: -1.2277172803878784\n",
      "Batch 251, Loss: 0.234022855758667\n",
      "Batch 252, Loss: 0.09165138006210327\n",
      "Batch 253, Loss: 0.008202195167541504\n",
      "Batch 254, Loss: -1.2262187004089355\n",
      "Batch 255, Loss: -1.627511739730835\n",
      "Batch 256, Loss: -1.4139739274978638\n",
      "Batch 257, Loss: -1.4777848720550537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 258, Loss: -1.4160640239715576\n",
      "Batch 259, Loss: 0.2162480652332306\n",
      "Batch 260, Loss: -1.4239917993545532\n",
      "Batch 261, Loss: -1.1458566188812256\n",
      "Batch 262, Loss: -1.1856789588928223\n",
      "Batch 263, Loss: -1.579924464225769\n",
      "Batch 264, Loss: -1.1493914127349854\n",
      "Batch 265, Loss: 0.45231252908706665\n",
      "Batch 266, Loss: 0.5735054612159729\n",
      "Batch 267, Loss: 0.3221668601036072\n",
      "Batch 268, Loss: -1.6508148908615112\n",
      "Batch 269, Loss: 0.6019684672355652\n",
      "Batch 270, Loss: 0.012483000755310059\n",
      "Batch 271, Loss: 0.0008645057678222656\n",
      "Batch 272, Loss: -1.5134204626083374\n",
      "Batch 273, Loss: 0.4044482111930847\n",
      "Batch 274, Loss: 0.4994399845600128\n",
      "Batch 275, Loss: -1.1894373893737793\n",
      "Batch 276, Loss: 0.4877544641494751\n",
      "Batch 277, Loss: 0.6409474015235901\n",
      "Batch 278, Loss: 0.26433509588241577\n",
      "Batch 279, Loss: -1.3272712230682373\n",
      "Batch 280, Loss: 0.2595750689506531\n",
      "Batch 281, Loss: -1.3494181632995605\n",
      "Batch 282, Loss: 0.38933396339416504\n",
      "Batch 283, Loss: 0.2342807650566101\n",
      "Batch 284, Loss: 0.7256218791007996\n",
      "Batch 285, Loss: -1.300265908241272\n",
      "Batch 286, Loss: 0.38125404715538025\n",
      "Batch 287, Loss: -1.2225278615951538\n",
      "Batch 288, Loss: -1.117246150970459\n",
      "Batch 289, Loss: -1.0193935632705688\n",
      "Batch 290, Loss: 0.4686983823776245\n",
      "Batch 291, Loss: 0.0006588101387023926\n",
      "Batch 292, Loss: 0.4518823027610779\n",
      "Batch 293, Loss: -1.6649824380874634\n",
      "Batch 294, Loss: -1.4213411808013916\n",
      "Batch 295, Loss: 0.5287396311759949\n",
      "Batch 296, Loss: -1.590100884437561\n",
      "Batch 297, Loss: 0.5641933679580688\n",
      "Batch 298, Loss: -1.1366056203842163\n",
      "Batch 299, Loss: 0.4007188081741333\n",
      "Batch 300, Loss: 0.2368985116481781\n",
      "Batch 301, Loss: 0.33004122972488403\n",
      "Batch 302, Loss: 0.1500406414270401\n",
      "Batch 303, Loss: -1.2395011186599731\n",
      "Batch 304, Loss: -1.2418709993362427\n",
      "Batch 305, Loss: -1.2436953783035278\n",
      "Batch 306, Loss: -1.4783929586410522\n",
      "Batch 307, Loss: -1.2857638597488403\n",
      "Batch 308, Loss: 0.3440329432487488\n",
      "Batch 309, Loss: -1.7108043432235718\n",
      "Batch 310, Loss: 0.3743705451488495\n",
      "Batch 311, Loss: -1.2833313941955566\n",
      "Batch 312, Loss: -1.2435282468795776\n",
      "Batch 313, Loss: -1.4271740913391113\n",
      "Batch 314, Loss: -1.5029804706573486\n",
      "Batch 315, Loss: -1.2683067321777344\n",
      "Batch 316, Loss: 0.05033215880393982\n",
      "Batch 317, Loss: 0.05306553840637207\n",
      "Batch 318, Loss: -1.4807699918746948\n",
      "Batch 319, Loss: 0.6294224262237549\n",
      "Batch 320, Loss: 0.1823924481868744\n",
      "Batch 321, Loss: 0.1824747920036316\n",
      "Batch 322, Loss: 0.08507409691810608\n",
      "Batch 323, Loss: -1.4149091243743896\n",
      "Batch 324, Loss: -1.2330029010772705\n",
      "Batch 325, Loss: 0.030378997325897217\n",
      "Batch 326, Loss: -1.0035403966903687\n",
      "Batch 327, Loss: -1.005990743637085\n",
      "Batch 328, Loss: 0.6146334409713745\n",
      "Batch 329, Loss: 0.355166494846344\n",
      "Batch 330, Loss: -1.4972662925720215\n",
      "Batch 331, Loss: -1.5122551918029785\n",
      "Batch 332, Loss: -1.5621743202209473\n",
      "Batch 333, Loss: -1.3192226886749268\n",
      "Batch 334, Loss: -1.1670926809310913\n",
      "Batch 335, Loss: 0.5963830947875977\n",
      "Batch 336, Loss: -1.6183195114135742\n",
      "Batch 337, Loss: 0.09108057618141174\n",
      "Batch 338, Loss: 0.2569454610347748\n",
      "Batch 339, Loss: 0.3495418131351471\n",
      "Batch 340, Loss: 0.009646862745285034\n",
      "Batch 341, Loss: 0.42308011651039124\n",
      "Batch 342, Loss: -1.4060759544372559\n",
      "Batch 343, Loss: -1.2290791273117065\n",
      "Batch 344, Loss: 0.1404327154159546\n",
      "Batch 345, Loss: 0.5531007051467896\n",
      "Batch 346, Loss: -1.3678761720657349\n",
      "Batch 347, Loss: -1.67964768409729\n",
      "Batch 348, Loss: -1.2460070848464966\n",
      "Batch 349, Loss: -1.4330382347106934\n",
      "Batch 350, Loss: 0.3214559257030487\n",
      "Batch 351, Loss: 0.002416759729385376\n",
      "Batch 352, Loss: -1.1435457468032837\n",
      "Batch 353, Loss: -1.0829694271087646\n",
      "Batch 354, Loss: -1.6692334413528442\n",
      "Batch 355, Loss: 0.5090502500534058\n",
      "Batch 356, Loss: -1.5625578165054321\n",
      "Batch 357, Loss: 0.30877920985221863\n",
      "Batch 358, Loss: 0.2652513086795807\n",
      "Batch 359, Loss: 0.2271517813205719\n",
      "Batch 360, Loss: -1.482567548751831\n",
      "Batch 361, Loss: 0.49886447191238403\n",
      "Batch 362, Loss: 0.4182675778865814\n",
      "Batch 363, Loss: 0.5124655365943909\n",
      "Batch 364, Loss: -1.455971598625183\n",
      "Batch 365, Loss: -1.1744420528411865\n",
      "Batch 366, Loss: 0.217390239238739\n",
      "Batch 367, Loss: -1.1899508237838745\n",
      "Batch 368, Loss: -1.4730145931243896\n",
      "Batch 369, Loss: -1.5689423084259033\n",
      "Batch 370, Loss: -1.6544339656829834\n",
      "Batch 371, Loss: 0.5881087779998779\n",
      "Batch 372, Loss: 0.2646115720272064\n",
      "Batch 373, Loss: -1.1674214601516724\n",
      "Batch 374, Loss: -1.1506824493408203\n",
      "Batch 375, Loss: 0.5006685256958008\n",
      "Batch 376, Loss: -1.5754051208496094\n",
      "Batch 377, Loss: -1.126990795135498\n",
      "Batch 378, Loss: -1.1464779376983643\n",
      "Batch 379, Loss: -1.178948998451233\n",
      "Batch 380, Loss: -1.6981562376022339\n",
      "Batch 381, Loss: -1.2912607192993164\n",
      "Batch 382, Loss: 0.19962215423583984\n",
      "Batch 383, Loss: 0.21851670742034912\n",
      "Batch 384, Loss: 0.5988466739654541\n",
      "Batch 385, Loss: -1.1968327760696411\n",
      "Batch 386, Loss: -1.1555556058883667\n",
      "Batch 387, Loss: -1.2981665134429932\n",
      "Batch 388, Loss: 0.7118151187896729\n",
      "Batch 389, Loss: -1.0547077655792236\n",
      "Batch 390, Loss: 0.04020705819129944\n",
      "Batch 391, Loss: -1.3316895961761475\n",
      "Batch 392, Loss: 0.053773075342178345\n",
      "Batch 393, Loss: 0.6264972686767578\n",
      "Batch 394, Loss: -1.0568886995315552\n",
      "Batch 395, Loss: 0.3843666613101959\n",
      "Batch 396, Loss: -0.03557524085044861\n",
      "Batch 397, Loss: -1.6071357727050781\n",
      "Batch 398, Loss: -1.3255698680877686\n",
      "Batch 399, Loss: 0.35881251096725464\n",
      "Training [60%]\tLoss: -0.5134\n",
      "Batch 0, Loss: -1.1473438739776611\n",
      "Batch 1, Loss: 0.4589642286300659\n",
      "Batch 2, Loss: 0.6691349744796753\n",
      "Batch 3, Loss: 0.17584508657455444\n",
      "Batch 4, Loss: -1.5214190483093262\n",
      "Batch 5, Loss: 0.2854920029640198\n",
      "Batch 6, Loss: 0.6677054762840271\n",
      "Batch 7, Loss: -1.581507682800293\n",
      "Batch 8, Loss: -1.580260157585144\n",
      "Batch 9, Loss: 0.1163097470998764\n",
      "Batch 10, Loss: 0.40040865540504456\n",
      "Batch 11, Loss: -1.5485591888427734\n",
      "Batch 12, Loss: -1.4926997423171997\n",
      "Batch 13, Loss: 0.4689238667488098\n",
      "Batch 14, Loss: -1.3461837768554688\n",
      "Batch 15, Loss: -1.0386842489242554\n",
      "Batch 16, Loss: 0.724545955657959\n",
      "Batch 17, Loss: 0.3394870460033417\n",
      "Batch 18, Loss: 0.6281213164329529\n",
      "Batch 19, Loss: 0.6827351450920105\n",
      "Batch 20, Loss: -0.9840855598449707\n",
      "Batch 21, Loss: 0.2950687110424042\n",
      "Batch 22, Loss: 0.6132955551147461\n",
      "Batch 23, Loss: -1.4139188528060913\n",
      "Batch 24, Loss: -1.2664847373962402\n",
      "Batch 25, Loss: -1.1945850849151611\n",
      "Batch 26, Loss: 0.24200326204299927\n",
      "Batch 27, Loss: 0.22826041281223297\n",
      "Batch 28, Loss: 0.14812321960926056\n",
      "Batch 29, Loss: 0.42957085371017456\n",
      "Batch 30, Loss: -1.2412903308868408\n",
      "Batch 31, Loss: -1.0494511127471924\n",
      "Batch 32, Loss: -1.4570817947387695\n",
      "Batch 33, Loss: 0.5448656678199768\n",
      "Batch 34, Loss: 0.16034068167209625\n",
      "Batch 35, Loss: -1.4003065824508667\n",
      "Batch 36, Loss: -1.4019949436187744\n",
      "Batch 37, Loss: -1.3787472248077393\n",
      "Batch 38, Loss: 0.33896157145500183\n",
      "Batch 39, Loss: 0.29998230934143066\n",
      "Batch 40, Loss: -1.5289099216461182\n",
      "Batch 41, Loss: -1.3751343488693237\n",
      "Batch 42, Loss: 0.3540215790271759\n",
      "Batch 43, Loss: -1.6552238464355469\n",
      "Batch 44, Loss: 0.48032212257385254\n",
      "Batch 45, Loss: 0.19979169964790344\n",
      "Batch 46, Loss: 0.021165788173675537\n",
      "Batch 47, Loss: 0.7318992018699646\n",
      "Batch 48, Loss: -1.2887938022613525\n",
      "Batch 49, Loss: -1.7280747890472412\n",
      "Batch 50, Loss: -1.036647081375122\n",
      "Batch 51, Loss: -1.1935389041900635\n",
      "Batch 52, Loss: -1.1267582178115845\n",
      "Batch 53, Loss: -1.3591862916946411\n",
      "Batch 54, Loss: 0.3842420279979706\n",
      "Batch 55, Loss: 0.06233292818069458\n",
      "Batch 56, Loss: 0.4804786443710327\n",
      "Batch 57, Loss: -1.1865043640136719\n",
      "Batch 58, Loss: -1.1157054901123047\n",
      "Batch 59, Loss: 0.6780917644500732\n",
      "Batch 60, Loss: 0.6411494016647339\n",
      "Batch 61, Loss: -1.263283610343933\n",
      "Batch 62, Loss: 0.2879239618778229\n",
      "Batch 63, Loss: 0.6997904777526855\n",
      "Batch 64, Loss: 0.29362231492996216\n",
      "Batch 65, Loss: 0.724003791809082\n",
      "Batch 66, Loss: 0.3809671998023987\n",
      "Batch 67, Loss: -1.1106778383255005\n",
      "Batch 68, Loss: -1.646157145500183\n",
      "Batch 69, Loss: -1.3146427869796753\n",
      "Batch 70, Loss: -1.3008016347885132\n",
      "Batch 71, Loss: -1.057374358177185\n",
      "Batch 72, Loss: -1.589930772781372\n",
      "Batch 73, Loss: 0.14637668430805206\n",
      "Batch 74, Loss: -1.1216593980789185\n",
      "Batch 75, Loss: 0.7081199884414673\n",
      "Batch 76, Loss: 0.25258326530456543\n",
      "Batch 77, Loss: -1.703559398651123\n",
      "Batch 78, Loss: -1.7410621643066406\n",
      "Batch 79, Loss: 0.733138918876648\n",
      "Batch 80, Loss: -1.6784067153930664\n",
      "Batch 81, Loss: -1.6339588165283203\n",
      "Batch 82, Loss: -1.6327154636383057\n",
      "Batch 83, Loss: 0.21008500456809998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 84, Loss: -0.9897037744522095\n",
      "Batch 85, Loss: -1.2857941389083862\n",
      "Batch 86, Loss: 0.33655086159706116\n",
      "Batch 87, Loss: 0.17196542024612427\n",
      "Batch 88, Loss: 0.3235577344894409\n",
      "Batch 89, Loss: 0.0912335216999054\n",
      "Batch 90, Loss: -1.37751042842865\n",
      "Batch 91, Loss: -1.3873265981674194\n",
      "Batch 92, Loss: -1.287063479423523\n",
      "Batch 93, Loss: 0.2562646269798279\n",
      "Batch 94, Loss: -1.4674038887023926\n",
      "Batch 95, Loss: 0.16388942301273346\n",
      "Batch 96, Loss: -1.3969230651855469\n",
      "Batch 97, Loss: 0.2087075412273407\n",
      "Batch 98, Loss: -1.5158874988555908\n",
      "Batch 99, Loss: -1.4258731603622437\n",
      "Batch 100, Loss: 0.2692229151725769\n",
      "Batch 101, Loss: -0.029054373502731323\n",
      "Batch 102, Loss: -1.7414734363555908\n",
      "Batch 103, Loss: 0.21521861851215363\n",
      "Batch 104, Loss: 0.49342766404151917\n",
      "Batch 105, Loss: -1.1295756101608276\n",
      "Batch 106, Loss: -1.0824053287506104\n",
      "Batch 107, Loss: -1.1653517484664917\n",
      "Batch 108, Loss: 0.08460187911987305\n",
      "Batch 109, Loss: -1.2823885679244995\n",
      "Batch 110, Loss: 0.3743307590484619\n",
      "Batch 111, Loss: 0.2587113380432129\n",
      "Batch 112, Loss: 0.3628562390804291\n",
      "Batch 113, Loss: 0.606528103351593\n",
      "Batch 114, Loss: -1.2664631605148315\n",
      "Batch 115, Loss: 0.1314733624458313\n",
      "Batch 116, Loss: -1.2117067575454712\n",
      "Batch 117, Loss: 0.6243017315864563\n",
      "Batch 118, Loss: -1.5725315809249878\n",
      "Batch 119, Loss: -0.9968748092651367\n",
      "Batch 120, Loss: -1.6182725429534912\n",
      "Batch 121, Loss: -1.3008038997650146\n",
      "Batch 122, Loss: 0.6106248497962952\n",
      "Batch 123, Loss: -1.4165563583374023\n",
      "Batch 124, Loss: -1.2650599479675293\n",
      "Batch 125, Loss: -1.3003934621810913\n",
      "Batch 126, Loss: 0.3371187746524811\n",
      "Batch 127, Loss: 0.43846139311790466\n",
      "Batch 128, Loss: 0.5377185344696045\n",
      "Batch 129, Loss: -1.3095022439956665\n",
      "Batch 130, Loss: 0.15968692302703857\n",
      "Batch 131, Loss: 0.7459656000137329\n",
      "Batch 132, Loss: -1.3143043518066406\n",
      "Batch 133, Loss: 0.19405719637870789\n",
      "Batch 134, Loss: 0.5304640531539917\n",
      "Batch 135, Loss: 0.015178889036178589\n",
      "Batch 136, Loss: -1.3395158052444458\n",
      "Batch 137, Loss: -1.1960495710372925\n",
      "Batch 138, Loss: -1.2865748405456543\n",
      "Batch 139, Loss: -1.6364383697509766\n",
      "Batch 140, Loss: -1.4538697004318237\n",
      "Batch 141, Loss: -1.0220376253128052\n",
      "Batch 142, Loss: 0.5456753969192505\n",
      "Batch 143, Loss: 0.6975908279418945\n",
      "Batch 144, Loss: -1.7259207963943481\n",
      "Batch 145, Loss: 0.38782432675361633\n",
      "Batch 146, Loss: -1.3249109983444214\n",
      "Batch 147, Loss: 0.7323306798934937\n",
      "Batch 148, Loss: -1.5442914962768555\n",
      "Batch 149, Loss: -1.0123953819274902\n",
      "Batch 150, Loss: -1.114133596420288\n",
      "Batch 151, Loss: 0.7240859866142273\n",
      "Batch 152, Loss: -1.2740492820739746\n",
      "Batch 153, Loss: -1.2320795059204102\n",
      "Batch 154, Loss: -1.2449414730072021\n",
      "Batch 155, Loss: 0.4791940450668335\n",
      "Batch 156, Loss: -1.2883718013763428\n",
      "Batch 157, Loss: -1.3366587162017822\n",
      "Batch 158, Loss: -1.3401789665222168\n",
      "Batch 159, Loss: -1.429337501525879\n",
      "Batch 160, Loss: 0.5526957511901855\n",
      "Batch 161, Loss: -1.5295937061309814\n",
      "Batch 162, Loss: -1.750093698501587\n",
      "Batch 163, Loss: 0.344488263130188\n",
      "Batch 164, Loss: 0.7126325368881226\n",
      "Batch 165, Loss: -1.337706446647644\n",
      "Batch 166, Loss: -1.3244465589523315\n",
      "Batch 167, Loss: 0.342102587223053\n",
      "Batch 168, Loss: 0.3504023551940918\n",
      "Batch 169, Loss: -1.34685218334198\n",
      "Batch 170, Loss: 0.2536298930644989\n",
      "Batch 171, Loss: 0.7081863880157471\n",
      "Batch 172, Loss: 0.2738105356693268\n",
      "Batch 173, Loss: -1.1399390697479248\n",
      "Batch 174, Loss: 0.5187473297119141\n",
      "Batch 175, Loss: -1.3309416770935059\n",
      "Batch 176, Loss: 0.287628173828125\n",
      "Batch 177, Loss: -1.183696985244751\n",
      "Batch 178, Loss: 0.2942884564399719\n",
      "Batch 179, Loss: -1.3550554513931274\n",
      "Batch 180, Loss: 0.49236249923706055\n",
      "Batch 181, Loss: 0.7191638946533203\n",
      "Batch 182, Loss: -1.531295657157898\n",
      "Batch 183, Loss: 0.5405058264732361\n",
      "Batch 184, Loss: 0.6074565052986145\n",
      "Batch 185, Loss: 0.06077086925506592\n",
      "Batch 186, Loss: 0.27032530307769775\n",
      "Batch 187, Loss: 0.4407849907875061\n",
      "Batch 188, Loss: -1.2729452848434448\n",
      "Batch 189, Loss: 0.6829147934913635\n",
      "Batch 190, Loss: -1.6074342727661133\n",
      "Batch 191, Loss: 0.6802279949188232\n",
      "Batch 192, Loss: 0.2790086567401886\n",
      "Batch 193, Loss: 0.06069937348365784\n",
      "Batch 194, Loss: -1.1305158138275146\n",
      "Batch 195, Loss: 0.5705344080924988\n",
      "Batch 196, Loss: 0.0413394570350647\n",
      "Batch 197, Loss: 0.5303436517715454\n",
      "Batch 198, Loss: 0.6722712516784668\n",
      "Batch 199, Loss: 0.19825688004493713\n",
      "Batch 200, Loss: -1.3181685209274292\n",
      "Batch 201, Loss: 0.22227148711681366\n",
      "Batch 202, Loss: -1.340232491493225\n",
      "Batch 203, Loss: 0.24388450384140015\n",
      "Batch 204, Loss: 0.7168397903442383\n",
      "Batch 205, Loss: -1.5814111232757568\n",
      "Batch 206, Loss: 0.6131154298782349\n",
      "Batch 207, Loss: -1.7237350940704346\n",
      "Batch 208, Loss: -1.7079010009765625\n",
      "Batch 209, Loss: 0.3345015347003937\n",
      "Batch 210, Loss: -1.5266170501708984\n",
      "Batch 211, Loss: 0.28899314999580383\n",
      "Batch 212, Loss: 0.4036984145641327\n",
      "Batch 213, Loss: 0.18381890654563904\n",
      "Batch 214, Loss: -1.109034776687622\n",
      "Batch 215, Loss: -1.3179124593734741\n",
      "Batch 216, Loss: 0.26278525590896606\n",
      "Batch 217, Loss: -1.7085634469985962\n",
      "Batch 218, Loss: 0.3286357820034027\n",
      "Batch 219, Loss: 0.21077625453472137\n",
      "Batch 220, Loss: 0.27010783553123474\n",
      "Batch 221, Loss: 0.32029443979263306\n",
      "Batch 222, Loss: -1.503986120223999\n",
      "Batch 223, Loss: 0.4576643705368042\n",
      "Batch 224, Loss: -1.654995322227478\n",
      "Batch 225, Loss: 0.23729833960533142\n",
      "Batch 226, Loss: -1.0934001207351685\n",
      "Batch 227, Loss: 0.3309602737426758\n",
      "Batch 228, Loss: -1.1246975660324097\n",
      "Batch 229, Loss: -1.5601012706756592\n",
      "Batch 230, Loss: -1.2362589836120605\n",
      "Batch 231, Loss: -1.1841697692871094\n",
      "Batch 232, Loss: -1.1059448719024658\n",
      "Batch 233, Loss: -1.3365918397903442\n",
      "Batch 234, Loss: 0.2995486557483673\n",
      "Batch 235, Loss: 0.1945197433233261\n",
      "Batch 236, Loss: 0.6475989818572998\n",
      "Batch 237, Loss: -1.0884380340576172\n",
      "Batch 238, Loss: 0.210017129778862\n",
      "Batch 239, Loss: -1.4618767499923706\n",
      "Batch 240, Loss: 0.410732239484787\n",
      "Batch 241, Loss: 0.2528160810470581\n",
      "Batch 242, Loss: 0.28926414251327515\n",
      "Batch 243, Loss: 0.14670035243034363\n",
      "Batch 244, Loss: 0.7103330492973328\n",
      "Batch 245, Loss: -1.3757765293121338\n",
      "Batch 246, Loss: -1.5510838031768799\n",
      "Batch 247, Loss: 0.47454914450645447\n",
      "Batch 248, Loss: 0.6950535774230957\n",
      "Batch 249, Loss: 0.712644636631012\n",
      "Batch 250, Loss: 0.4000798463821411\n",
      "Batch 251, Loss: 0.6315100789070129\n",
      "Batch 252, Loss: -1.086138367652893\n",
      "Batch 253, Loss: -1.2493889331817627\n",
      "Batch 254, Loss: -1.1324803829193115\n",
      "Batch 255, Loss: -1.2005473375320435\n",
      "Batch 256, Loss: -1.183324933052063\n",
      "Batch 257, Loss: -1.1035876274108887\n",
      "Batch 258, Loss: -1.3437697887420654\n",
      "Batch 259, Loss: 0.6285903453826904\n",
      "Batch 260, Loss: -1.268860101699829\n",
      "Batch 261, Loss: -1.6381099224090576\n",
      "Batch 262, Loss: -1.3643851280212402\n",
      "Batch 263, Loss: 0.26401981711387634\n",
      "Batch 264, Loss: -1.1975945234298706\n",
      "Batch 265, Loss: 0.2646273970603943\n",
      "Batch 266, Loss: 0.4930158853530884\n",
      "Batch 267, Loss: 0.360657662153244\n",
      "Batch 268, Loss: 0.22525450587272644\n",
      "Batch 269, Loss: 0.3956843614578247\n",
      "Batch 270, Loss: -1.3269199132919312\n",
      "Batch 271, Loss: -1.6082991361618042\n",
      "Batch 272, Loss: 0.3052229583263397\n",
      "Batch 273, Loss: 0.16754844784736633\n",
      "Batch 274, Loss: -1.3467059135437012\n",
      "Batch 275, Loss: -1.1635661125183105\n",
      "Batch 276, Loss: 0.31035134196281433\n",
      "Batch 277, Loss: -1.1668263673782349\n",
      "Batch 278, Loss: 0.4921368956565857\n",
      "Batch 279, Loss: -1.3537259101867676\n",
      "Batch 280, Loss: -1.540495753288269\n",
      "Batch 281, Loss: -1.1052786111831665\n",
      "Batch 282, Loss: 0.2151702642440796\n",
      "Batch 283, Loss: -1.606740951538086\n",
      "Batch 284, Loss: -1.5934064388275146\n",
      "Batch 285, Loss: -1.5720559358596802\n",
      "Batch 286, Loss: 0.3378562331199646\n",
      "Batch 287, Loss: 0.46478506922721863\n",
      "Batch 288, Loss: 0.09971946477890015\n",
      "Batch 289, Loss: 0.0798611044883728\n",
      "Batch 290, Loss: -1.2961864471435547\n",
      "Batch 291, Loss: 0.6570895314216614\n",
      "Batch 292, Loss: -1.374125599861145\n",
      "Batch 293, Loss: -1.2574750185012817\n",
      "Batch 294, Loss: 0.35831761360168457\n",
      "Batch 295, Loss: -1.3094521760940552\n",
      "Batch 296, Loss: 0.3808837831020355\n",
      "Batch 297, Loss: -1.684288501739502\n",
      "Batch 298, Loss: -1.2812310457229614\n",
      "Batch 299, Loss: 0.3297657370567322\n",
      "Batch 300, Loss: 0.19737935066223145\n",
      "Batch 301, Loss: -1.0568675994873047\n",
      "Batch 302, Loss: -1.3956398963928223\n",
      "Batch 303, Loss: -1.6225593090057373\n",
      "Batch 304, Loss: -1.6317214965820312\n",
      "Batch 305, Loss: 0.10376444458961487\n",
      "Batch 306, Loss: 0.4623044729232788\n",
      "Batch 307, Loss: 0.23383843898773193\n",
      "Batch 308, Loss: -1.1482126712799072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 309, Loss: 0.4354156255722046\n",
      "Batch 310, Loss: 0.14571112394332886\n",
      "Batch 311, Loss: 0.45841825008392334\n",
      "Batch 312, Loss: -1.2135965824127197\n",
      "Batch 313, Loss: -1.4800435304641724\n",
      "Batch 314, Loss: -1.5111335515975952\n",
      "Batch 315, Loss: 0.5827317833900452\n",
      "Batch 316, Loss: 0.29427435994148254\n",
      "Batch 317, Loss: 0.17084862291812897\n",
      "Batch 318, Loss: -1.3534247875213623\n",
      "Batch 319, Loss: -1.4639747142791748\n",
      "Batch 320, Loss: -1.4743735790252686\n",
      "Batch 321, Loss: -1.0353596210479736\n",
      "Batch 322, Loss: 0.6401186585426331\n",
      "Batch 323, Loss: -1.6237143278121948\n",
      "Batch 324, Loss: 0.27936166524887085\n",
      "Batch 325, Loss: 0.2972673773765564\n",
      "Batch 326, Loss: -1.5628325939178467\n",
      "Batch 327, Loss: 0.08917832374572754\n",
      "Batch 328, Loss: 0.6768831014633179\n",
      "Batch 329, Loss: -1.0737073421478271\n",
      "Batch 330, Loss: -1.182574987411499\n",
      "Batch 331, Loss: -1.0861616134643555\n",
      "Batch 332, Loss: 0.3240499496459961\n",
      "Batch 333, Loss: 0.06255367398262024\n",
      "Batch 334, Loss: -1.224149465560913\n",
      "Batch 335, Loss: 0.38641494512557983\n",
      "Batch 336, Loss: -1.1731531620025635\n",
      "Batch 337, Loss: -1.566551685333252\n",
      "Batch 338, Loss: -1.2981572151184082\n",
      "Batch 339, Loss: 0.5901137590408325\n",
      "Batch 340, Loss: -1.5921127796173096\n",
      "Batch 341, Loss: 0.09382972121238708\n",
      "Batch 342, Loss: 0.5558394193649292\n",
      "Batch 343, Loss: 0.00263330340385437\n",
      "Batch 344, Loss: -1.3908495903015137\n",
      "Batch 345, Loss: -0.00037491321563720703\n",
      "Batch 346, Loss: 0.45913833379745483\n",
      "Batch 347, Loss: 0.5708436965942383\n",
      "Batch 348, Loss: 0.3493359684944153\n",
      "Batch 349, Loss: 0.22631090879440308\n",
      "Batch 350, Loss: -1.37470543384552\n",
      "Batch 351, Loss: -1.427154302597046\n",
      "Batch 352, Loss: 0.5176626443862915\n",
      "Batch 353, Loss: -1.4468683004379272\n",
      "Batch 354, Loss: 0.24498045444488525\n",
      "Batch 355, Loss: -1.719671607017517\n",
      "Batch 356, Loss: 0.18757718801498413\n",
      "Batch 357, Loss: 0.28192901611328125\n",
      "Batch 358, Loss: -1.2708956003189087\n",
      "Batch 359, Loss: 0.17649438977241516\n",
      "Batch 360, Loss: -1.26544189453125\n",
      "Batch 361, Loss: -1.0210844278335571\n",
      "Batch 362, Loss: -1.0587058067321777\n",
      "Batch 363, Loss: -1.2682006359100342\n",
      "Batch 364, Loss: 0.06642159819602966\n",
      "Batch 365, Loss: -1.6358424425125122\n",
      "Batch 366, Loss: -1.0082685947418213\n",
      "Batch 367, Loss: 0.4616556167602539\n",
      "Batch 368, Loss: -1.2083160877227783\n",
      "Batch 369, Loss: -1.1600149869918823\n",
      "Batch 370, Loss: -1.2763011455535889\n",
      "Batch 371, Loss: 0.5960527062416077\n",
      "Batch 372, Loss: 0.23238806426525116\n",
      "Batch 373, Loss: 0.12533551454544067\n",
      "Batch 374, Loss: -1.338666558265686\n",
      "Batch 375, Loss: -1.6747771501541138\n",
      "Batch 376, Loss: -1.3681927919387817\n",
      "Batch 377, Loss: -1.1794610023498535\n",
      "Batch 378, Loss: 0.5108635425567627\n",
      "Batch 379, Loss: 0.552072286605835\n",
      "Batch 380, Loss: -1.0870622396469116\n",
      "Batch 381, Loss: 0.6606078743934631\n",
      "Batch 382, Loss: -1.1032134294509888\n",
      "Batch 383, Loss: -1.7213211059570312\n",
      "Batch 384, Loss: -1.3774374723434448\n",
      "Batch 385, Loss: -1.688635230064392\n",
      "Batch 386, Loss: 0.08227747678756714\n",
      "Batch 387, Loss: -1.6819484233856201\n",
      "Batch 388, Loss: 0.5146886706352234\n",
      "Batch 389, Loss: 0.1704528033733368\n",
      "Batch 390, Loss: 0.31004682183265686\n",
      "Batch 391, Loss: 0.19592244923114777\n",
      "Batch 392, Loss: -1.491821050643921\n",
      "Batch 393, Loss: -1.4988245964050293\n",
      "Batch 394, Loss: 0.3574749231338501\n",
      "Batch 395, Loss: -1.4362543821334839\n",
      "Batch 396, Loss: -1.2282068729400635\n",
      "Batch 397, Loss: 0.5215172171592712\n",
      "Batch 398, Loss: 0.5648136138916016\n",
      "Batch 399, Loss: -1.2494609355926514\n",
      "Training [70%]\tLoss: -0.4882\n",
      "Batch 0, Loss: -1.2594958543777466\n",
      "Batch 1, Loss: -1.3520630598068237\n",
      "Batch 2, Loss: 0.24155151844024658\n",
      "Batch 3, Loss: 0.7201587557792664\n",
      "Batch 4, Loss: 0.5857820510864258\n",
      "Batch 5, Loss: 0.6634329557418823\n",
      "Batch 6, Loss: -1.1156758069992065\n",
      "Batch 7, Loss: -1.6327803134918213\n",
      "Batch 8, Loss: -1.4497380256652832\n",
      "Batch 9, Loss: 0.3550180494785309\n",
      "Batch 10, Loss: 0.6194741129875183\n",
      "Batch 11, Loss: 0.0517696738243103\n",
      "Batch 12, Loss: 0.6338329315185547\n",
      "Batch 13, Loss: -1.5758999586105347\n",
      "Batch 14, Loss: -1.5806978940963745\n",
      "Batch 15, Loss: -1.4768613576889038\n",
      "Batch 16, Loss: -1.1151854991912842\n",
      "Batch 17, Loss: 0.3324171304702759\n",
      "Batch 18, Loss: 0.5859770178794861\n",
      "Batch 19, Loss: 0.5810006856918335\n",
      "Batch 20, Loss: -1.396047830581665\n",
      "Batch 21, Loss: 0.17303510010242462\n",
      "Batch 22, Loss: 0.2686925530433655\n",
      "Batch 23, Loss: 0.2610238194465637\n",
      "Batch 24, Loss: -1.3790721893310547\n",
      "Batch 25, Loss: -1.1088014841079712\n",
      "Batch 26, Loss: 0.25255972146987915\n",
      "Batch 27, Loss: 0.27925530076026917\n",
      "Batch 28, Loss: -1.5543349981307983\n",
      "Batch 29, Loss: -1.5809111595153809\n",
      "Batch 30, Loss: -1.1974502801895142\n",
      "Batch 31, Loss: 0.5325746536254883\n",
      "Batch 32, Loss: -1.2209914922714233\n",
      "Batch 33, Loss: -1.4661144018173218\n",
      "Batch 34, Loss: 0.22291171550750732\n",
      "Batch 35, Loss: -1.212384819984436\n",
      "Batch 36, Loss: -1.3233104944229126\n",
      "Batch 37, Loss: -1.2654082775115967\n",
      "Batch 38, Loss: 0.3928534686565399\n",
      "Batch 39, Loss: 0.424368679523468\n",
      "Batch 40, Loss: 0.315108984708786\n",
      "Batch 41, Loss: 0.18193268775939941\n",
      "Batch 42, Loss: -1.6385767459869385\n",
      "Batch 43, Loss: -1.0160062313079834\n",
      "Batch 44, Loss: 0.010678023099899292\n",
      "Batch 45, Loss: 0.4829574227333069\n",
      "Batch 46, Loss: -1.5478787422180176\n",
      "Batch 47, Loss: 0.4157215654850006\n",
      "Batch 48, Loss: -1.4605845212936401\n",
      "Batch 49, Loss: -1.5319485664367676\n",
      "Batch 50, Loss: -1.0845386981964111\n",
      "Batch 51, Loss: 0.2520439624786377\n",
      "Batch 52, Loss: -1.3031280040740967\n",
      "Batch 53, Loss: 0.34769806265830994\n",
      "Batch 54, Loss: 0.5762924551963806\n",
      "Batch 55, Loss: -1.5921871662139893\n",
      "Batch 56, Loss: 0.6854375600814819\n",
      "Batch 57, Loss: -1.243065595626831\n",
      "Batch 58, Loss: -1.7203956842422485\n",
      "Batch 59, Loss: -1.6849353313446045\n",
      "Batch 60, Loss: -1.1499077081680298\n",
      "Batch 61, Loss: 0.16437801718711853\n",
      "Batch 62, Loss: -1.7012590169906616\n",
      "Batch 63, Loss: 0.4632565975189209\n",
      "Batch 64, Loss: 0.3012542128562927\n",
      "Batch 65, Loss: 0.3876650929450989\n",
      "Batch 66, Loss: 0.164749875664711\n",
      "Batch 67, Loss: -1.205508828163147\n",
      "Batch 68, Loss: 0.10861092805862427\n",
      "Batch 69, Loss: -1.2038192749023438\n",
      "Batch 70, Loss: -1.3765065670013428\n",
      "Batch 71, Loss: 0.11717483401298523\n",
      "Batch 72, Loss: -1.1850305795669556\n",
      "Batch 73, Loss: -1.1383426189422607\n",
      "Batch 74, Loss: 0.642202615737915\n",
      "Batch 75, Loss: 0.24384242296218872\n",
      "Batch 76, Loss: -1.3810652494430542\n",
      "Batch 77, Loss: 0.39098888635635376\n",
      "Batch 78, Loss: 0.08046677708625793\n",
      "Batch 79, Loss: 0.14851488173007965\n",
      "Batch 80, Loss: 0.4219503700733185\n",
      "Batch 81, Loss: 0.3493163585662842\n",
      "Batch 82, Loss: 0.39952877163887024\n",
      "Batch 83, Loss: 0.3724116384983063\n",
      "Batch 84, Loss: 0.14367426931858063\n",
      "Batch 85, Loss: 0.04583558440208435\n",
      "Batch 86, Loss: -1.3793535232543945\n",
      "Batch 87, Loss: 0.2519933879375458\n",
      "Batch 88, Loss: 0.47581642866134644\n",
      "Batch 89, Loss: -1.208489179611206\n",
      "Batch 90, Loss: -1.4838886260986328\n",
      "Batch 91, Loss: 0.6075192093849182\n",
      "Batch 92, Loss: 0.6293476819992065\n",
      "Batch 93, Loss: 0.4704800248146057\n",
      "Batch 94, Loss: 0.2061919867992401\n",
      "Batch 95, Loss: -1.5098296403884888\n",
      "Batch 96, Loss: 0.46306106448173523\n",
      "Batch 97, Loss: 0.21749967336654663\n",
      "Batch 98, Loss: -1.3900121450424194\n",
      "Batch 99, Loss: 0.1702166199684143\n",
      "Batch 100, Loss: 0.6604619026184082\n",
      "Batch 101, Loss: -1.6168674230575562\n",
      "Batch 102, Loss: -1.0724766254425049\n",
      "Batch 103, Loss: 0.17271548509597778\n",
      "Batch 104, Loss: -1.390350103378296\n",
      "Batch 105, Loss: -1.5385109186172485\n",
      "Batch 106, Loss: -1.693509578704834\n",
      "Batch 107, Loss: -1.3857340812683105\n",
      "Batch 108, Loss: -1.3335464000701904\n",
      "Batch 109, Loss: 0.4955669343471527\n",
      "Batch 110, Loss: -1.502199649810791\n",
      "Batch 111, Loss: 0.24556466937065125\n",
      "Batch 112, Loss: 0.49127721786499023\n",
      "Batch 113, Loss: 0.5029615163803101\n",
      "Batch 114, Loss: -1.183784008026123\n",
      "Batch 115, Loss: 0.4249255657196045\n",
      "Batch 116, Loss: -1.2864999771118164\n",
      "Batch 117, Loss: -1.3780195713043213\n",
      "Batch 118, Loss: 0.4999445080757141\n",
      "Batch 119, Loss: 0.6171364188194275\n",
      "Batch 120, Loss: -1.0364357233047485\n",
      "Batch 121, Loss: -1.1857014894485474\n",
      "Batch 122, Loss: -1.2458157539367676\n",
      "Batch 123, Loss: 0.0603557825088501\n",
      "Batch 124, Loss: 0.5990020632743835\n",
      "Batch 125, Loss: 0.030878931283950806\n",
      "Batch 126, Loss: -1.3201513290405273\n",
      "Batch 127, Loss: 0.5414168834686279\n",
      "Batch 128, Loss: 0.47934794425964355\n",
      "Batch 129, Loss: -1.1829133033752441\n",
      "Batch 130, Loss: -1.041036605834961\n",
      "Batch 131, Loss: -1.3068006038665771\n",
      "Batch 132, Loss: -1.3829457759857178\n",
      "Batch 133, Loss: -1.2355084419250488\n",
      "Batch 134, Loss: -1.4655888080596924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 135, Loss: 0.3492078483104706\n",
      "Batch 136, Loss: 0.4792693257331848\n",
      "Batch 137, Loss: 0.1853998750448227\n",
      "Batch 138, Loss: 0.5625036954879761\n",
      "Batch 139, Loss: -1.3047676086425781\n",
      "Batch 140, Loss: 0.07043704390525818\n",
      "Batch 141, Loss: 0.37938088178634644\n",
      "Batch 142, Loss: 0.22028620541095734\n",
      "Batch 143, Loss: 0.4222196042537689\n",
      "Batch 144, Loss: -1.470268726348877\n",
      "Batch 145, Loss: -1.3586217164993286\n",
      "Batch 146, Loss: -1.2253220081329346\n",
      "Batch 147, Loss: -1.5173619985580444\n",
      "Batch 148, Loss: 0.3422822654247284\n",
      "Batch 149, Loss: 0.12937869131565094\n",
      "Batch 150, Loss: -1.6487082242965698\n",
      "Batch 151, Loss: 0.09966495633125305\n",
      "Batch 152, Loss: 0.17687807977199554\n",
      "Batch 153, Loss: -1.201013445854187\n",
      "Batch 154, Loss: -1.164473295211792\n",
      "Batch 155, Loss: 0.18587520718574524\n",
      "Batch 156, Loss: -1.1320257186889648\n",
      "Batch 157, Loss: 0.2586002051830292\n",
      "Batch 158, Loss: 0.003385603427886963\n",
      "Batch 159, Loss: 0.07076716423034668\n",
      "Batch 160, Loss: 0.5845605134963989\n",
      "Batch 161, Loss: -0.009418308734893799\n",
      "Batch 162, Loss: 0.46476808190345764\n",
      "Batch 163, Loss: 0.10216283798217773\n",
      "Batch 164, Loss: 0.2002839297056198\n",
      "Batch 165, Loss: 0.6191532611846924\n",
      "Batch 166, Loss: -1.55324387550354\n",
      "Batch 167, Loss: -1.275649905204773\n",
      "Batch 168, Loss: 0.1622251272201538\n",
      "Batch 169, Loss: -1.5002726316452026\n",
      "Batch 170, Loss: 0.191303089261055\n",
      "Batch 171, Loss: -1.190637230873108\n",
      "Batch 172, Loss: -1.3810522556304932\n",
      "Batch 173, Loss: 0.5053637027740479\n",
      "Batch 174, Loss: -1.149803638458252\n",
      "Batch 175, Loss: 0.04845529794692993\n",
      "Batch 176, Loss: 0.34405627846717834\n",
      "Batch 177, Loss: -1.5130112171173096\n",
      "Batch 178, Loss: 0.3068680167198181\n",
      "Batch 179, Loss: -1.4811546802520752\n",
      "Batch 180, Loss: -1.3916499614715576\n",
      "Batch 181, Loss: 0.16872605681419373\n",
      "Batch 182, Loss: 0.12492229044437408\n",
      "Batch 183, Loss: -1.5624566078186035\n",
      "Batch 184, Loss: -1.4339187145233154\n",
      "Batch 185, Loss: 0.35900330543518066\n",
      "Batch 186, Loss: 0.4528988301753998\n",
      "Batch 187, Loss: -1.5352649688720703\n",
      "Batch 188, Loss: 0.3574230968952179\n",
      "Batch 189, Loss: 0.1191573292016983\n",
      "Batch 190, Loss: -1.2022311687469482\n",
      "Batch 191, Loss: 0.14882582426071167\n",
      "Batch 192, Loss: 0.5291110277175903\n",
      "Batch 193, Loss: 0.2613394260406494\n",
      "Batch 194, Loss: -1.308046817779541\n",
      "Batch 195, Loss: 0.5274776816368103\n",
      "Batch 196, Loss: 0.2882048785686493\n",
      "Batch 197, Loss: -1.335262417793274\n",
      "Batch 198, Loss: -1.1634808778762817\n",
      "Batch 199, Loss: -1.228012204170227\n",
      "Batch 200, Loss: -1.3107223510742188\n",
      "Batch 201, Loss: 0.3797922432422638\n",
      "Batch 202, Loss: -1.267125129699707\n",
      "Batch 203, Loss: 0.23162460327148438\n",
      "Batch 204, Loss: 0.24020066857337952\n",
      "Batch 205, Loss: -1.7086155414581299\n",
      "Batch 206, Loss: 0.5104129910469055\n",
      "Batch 207, Loss: -0.9705746173858643\n",
      "Batch 208, Loss: -1.2060762643814087\n",
      "Batch 209, Loss: -1.5019242763519287\n",
      "Batch 210, Loss: 0.5560963153839111\n",
      "Batch 211, Loss: -1.1534814834594727\n",
      "Batch 212, Loss: -0.010434657335281372\n",
      "Batch 213, Loss: -1.1873831748962402\n",
      "Batch 214, Loss: 0.6611456871032715\n",
      "Batch 215, Loss: 0.2908547818660736\n",
      "Batch 216, Loss: 0.35815727710723877\n",
      "Batch 217, Loss: -1.3806418180465698\n",
      "Batch 218, Loss: 0.3680269420146942\n",
      "Batch 219, Loss: 0.2736837863922119\n",
      "Batch 220, Loss: 0.31104400753974915\n",
      "Batch 221, Loss: 0.5888333916664124\n",
      "Batch 222, Loss: -1.5436148643493652\n",
      "Batch 223, Loss: -1.333111047744751\n",
      "Batch 224, Loss: 0.302105188369751\n",
      "Batch 225, Loss: 0.6380354166030884\n",
      "Batch 226, Loss: -1.289597749710083\n",
      "Batch 227, Loss: 0.15479111671447754\n",
      "Batch 228, Loss: 0.681232213973999\n",
      "Batch 229, Loss: 0.4488919675350189\n",
      "Batch 230, Loss: -1.6520910263061523\n",
      "Batch 231, Loss: 0.3299521803855896\n",
      "Batch 232, Loss: -1.0947294235229492\n",
      "Batch 233, Loss: 0.6896994113922119\n",
      "Batch 234, Loss: -1.0830250978469849\n",
      "Batch 235, Loss: -1.4920310974121094\n",
      "Batch 236, Loss: 0.4245263636112213\n",
      "Batch 237, Loss: 0.16234631836414337\n",
      "Batch 238, Loss: -1.1193479299545288\n",
      "Batch 239, Loss: -1.24372136592865\n",
      "Batch 240, Loss: -1.334037184715271\n",
      "Batch 241, Loss: -1.501863718032837\n",
      "Batch 242, Loss: 0.5821129083633423\n",
      "Batch 243, Loss: 0.48370224237442017\n",
      "Batch 244, Loss: 0.49685096740722656\n",
      "Batch 245, Loss: -1.6638383865356445\n",
      "Batch 246, Loss: 0.46381574869155884\n",
      "Batch 247, Loss: -1.2667992115020752\n",
      "Batch 248, Loss: -1.1442807912826538\n",
      "Batch 249, Loss: -1.246823787689209\n",
      "Batch 250, Loss: 0.5495461225509644\n",
      "Batch 251, Loss: 0.496219277381897\n",
      "Batch 252, Loss: -1.456437349319458\n",
      "Batch 253, Loss: -1.3971728086471558\n",
      "Batch 254, Loss: 0.33693087100982666\n",
      "Batch 255, Loss: 0.3978208601474762\n",
      "Batch 256, Loss: -1.2650549411773682\n",
      "Batch 257, Loss: 0.7035199403762817\n",
      "Batch 258, Loss: -1.3081893920898438\n",
      "Batch 259, Loss: 0.4280206561088562\n",
      "Batch 260, Loss: -1.4903349876403809\n",
      "Batch 261, Loss: -1.434267282485962\n",
      "Batch 262, Loss: -1.4169468879699707\n",
      "Batch 263, Loss: -1.6723061800003052\n",
      "Batch 264, Loss: 0.5439565181732178\n",
      "Batch 265, Loss: -1.3152198791503906\n",
      "Batch 266, Loss: -1.0956650972366333\n",
      "Batch 267, Loss: -1.1129194498062134\n",
      "Batch 268, Loss: -1.1697660684585571\n",
      "Batch 269, Loss: -1.1210225820541382\n",
      "Batch 270, Loss: -1.245177984237671\n",
      "Batch 271, Loss: 0.19178496301174164\n",
      "Batch 272, Loss: -1.459381103515625\n",
      "Batch 273, Loss: 0.39092108607292175\n",
      "Batch 274, Loss: 0.4426652193069458\n",
      "Batch 275, Loss: 0.3943459987640381\n",
      "Batch 276, Loss: 0.20452094078063965\n",
      "Batch 277, Loss: -1.2742679119110107\n",
      "Batch 278, Loss: -1.1850435733795166\n",
      "Batch 279, Loss: -1.5710523128509521\n",
      "Batch 280, Loss: -1.3404674530029297\n",
      "Batch 281, Loss: -1.5982928276062012\n",
      "Batch 282, Loss: 0.21193984150886536\n",
      "Batch 283, Loss: -1.6536840200424194\n",
      "Batch 284, Loss: 0.2118954062461853\n",
      "Batch 285, Loss: 0.20816054940223694\n",
      "Batch 286, Loss: 0.055390238761901855\n",
      "Batch 287, Loss: -1.2415882349014282\n",
      "Batch 288, Loss: 0.3686958849430084\n",
      "Batch 289, Loss: -1.1168973445892334\n",
      "Batch 290, Loss: -1.6901837587356567\n",
      "Batch 291, Loss: 0.24362629652023315\n",
      "Batch 292, Loss: -1.1037572622299194\n",
      "Batch 293, Loss: 0.6104221940040588\n",
      "Batch 294, Loss: -1.6241645812988281\n",
      "Batch 295, Loss: -1.567632794380188\n",
      "Batch 296, Loss: 0.39353498816490173\n",
      "Batch 297, Loss: -1.2008004188537598\n",
      "Batch 298, Loss: 0.2984437048435211\n",
      "Batch 299, Loss: -1.66746985912323\n",
      "Batch 300, Loss: -1.2491931915283203\n",
      "Batch 301, Loss: 0.4863840341567993\n",
      "Batch 302, Loss: 0.27289703488349915\n",
      "Batch 303, Loss: 0.31462422013282776\n",
      "Batch 304, Loss: -1.3033361434936523\n",
      "Batch 305, Loss: -1.4693069458007812\n",
      "Batch 306, Loss: -1.413822054862976\n",
      "Batch 307, Loss: 0.12135308980941772\n",
      "Batch 308, Loss: -1.1546502113342285\n",
      "Batch 309, Loss: 0.1996949464082718\n",
      "Batch 310, Loss: -1.342008352279663\n",
      "Batch 311, Loss: -1.0383484363555908\n",
      "Batch 312, Loss: -1.1417089700698853\n",
      "Batch 313, Loss: 0.09149378538131714\n",
      "Batch 314, Loss: -1.1328049898147583\n",
      "Batch 315, Loss: 0.05023759603500366\n",
      "Batch 316, Loss: 0.3626987040042877\n",
      "Batch 317, Loss: 0.2360394150018692\n",
      "Batch 318, Loss: -1.365364670753479\n",
      "Batch 319, Loss: -1.0887138843536377\n",
      "Batch 320, Loss: 0.2734464108943939\n",
      "Batch 321, Loss: -1.2290658950805664\n",
      "Batch 322, Loss: 0.3666157126426697\n",
      "Batch 323, Loss: 0.16418159008026123\n",
      "Batch 324, Loss: -1.5776352882385254\n",
      "Batch 325, Loss: -1.6788049936294556\n",
      "Batch 326, Loss: 0.35100167989730835\n",
      "Batch 327, Loss: -1.546013593673706\n",
      "Batch 328, Loss: 0.011823832988739014\n",
      "Batch 329, Loss: 0.6187201142311096\n",
      "Batch 330, Loss: -1.2185224294662476\n",
      "Batch 331, Loss: 0.6758072972297668\n",
      "Batch 332, Loss: 0.25273606181144714\n",
      "Batch 333, Loss: -1.2832173109054565\n",
      "Batch 334, Loss: -0.9969820976257324\n",
      "Batch 335, Loss: 0.1675388365983963\n",
      "Batch 336, Loss: 0.6090248823165894\n",
      "Batch 337, Loss: -1.1790884733200073\n",
      "Batch 338, Loss: 0.4756258726119995\n",
      "Batch 339, Loss: -1.0939979553222656\n",
      "Batch 340, Loss: -1.4673984050750732\n",
      "Batch 341, Loss: -1.5706453323364258\n",
      "Batch 342, Loss: -1.680936336517334\n",
      "Batch 343, Loss: -1.6229535341262817\n",
      "Batch 344, Loss: -1.3028624057769775\n",
      "Batch 345, Loss: -1.6785392761230469\n",
      "Batch 346, Loss: -1.0851919651031494\n",
      "Batch 347, Loss: 0.22941607236862183\n",
      "Batch 348, Loss: -1.4838078022003174\n",
      "Batch 349, Loss: -1.5276778936386108\n",
      "Batch 350, Loss: -1.4355942010879517\n",
      "Batch 351, Loss: -1.0897144079208374\n",
      "Batch 352, Loss: -1.6204297542572021\n",
      "Batch 353, Loss: 0.36362025141716003\n",
      "Batch 354, Loss: -1.6752989292144775\n",
      "Batch 355, Loss: 0.406595915555954\n",
      "Batch 356, Loss: -1.562524676322937\n",
      "Batch 357, Loss: 0.21789398789405823\n",
      "Batch 358, Loss: -1.3254785537719727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 359, Loss: -1.2609033584594727\n",
      "Batch 360, Loss: -1.426663875579834\n",
      "Batch 361, Loss: -1.2520349025726318\n",
      "Batch 362, Loss: -1.2189451456069946\n",
      "Batch 363, Loss: -0.03046661615371704\n",
      "Batch 364, Loss: -1.3669291734695435\n",
      "Batch 365, Loss: -1.4953029155731201\n",
      "Batch 366, Loss: 0.3567367494106293\n",
      "Batch 367, Loss: -1.103548288345337\n",
      "Batch 368, Loss: 0.5851681232452393\n",
      "Batch 369, Loss: 0.2324078232049942\n",
      "Batch 370, Loss: 0.46216729283332825\n",
      "Batch 371, Loss: -1.0506702661514282\n",
      "Batch 372, Loss: -1.1939127445220947\n",
      "Batch 373, Loss: 0.38126856088638306\n",
      "Batch 374, Loss: -1.4346967935562134\n",
      "Batch 375, Loss: -1.4028546810150146\n",
      "Batch 376, Loss: 0.3093704283237457\n",
      "Batch 377, Loss: -1.3333081007003784\n",
      "Batch 378, Loss: -1.5049110651016235\n",
      "Batch 379, Loss: 0.6665451526641846\n",
      "Batch 380, Loss: 0.4778435230255127\n",
      "Batch 381, Loss: -1.7050384283065796\n",
      "Batch 382, Loss: -1.5206412076950073\n",
      "Batch 383, Loss: 0.16652552783489227\n",
      "Batch 384, Loss: 0.4072537422180176\n",
      "Batch 385, Loss: -1.003938913345337\n",
      "Batch 386, Loss: 0.19576182961463928\n",
      "Batch 387, Loss: -1.408223271369934\n",
      "Batch 388, Loss: -1.5760629177093506\n",
      "Batch 389, Loss: -1.5862748622894287\n",
      "Batch 390, Loss: 0.05749693512916565\n",
      "Batch 391, Loss: -1.450520396232605\n",
      "Batch 392, Loss: 0.6560773849487305\n",
      "Batch 393, Loss: 0.36827316880226135\n",
      "Batch 394, Loss: 0.06217125058174133\n",
      "Batch 395, Loss: 0.7344514727592468\n",
      "Batch 396, Loss: 0.5536566972732544\n",
      "Batch 397, Loss: -1.1856204271316528\n",
      "Batch 398, Loss: 0.5027067065238953\n",
      "Batch 399, Loss: 0.3592669665813446\n",
      "Training [80%]\tLoss: -0.5035\n",
      "Batch 0, Loss: -1.1349825859069824\n",
      "Batch 1, Loss: -1.197278618812561\n",
      "Batch 2, Loss: -1.4018698930740356\n",
      "Batch 3, Loss: -1.4011423587799072\n",
      "Batch 4, Loss: -1.3574092388153076\n",
      "Batch 5, Loss: -1.2752090692520142\n",
      "Batch 6, Loss: 0.623340368270874\n",
      "Batch 7, Loss: 0.7022556066513062\n",
      "Batch 8, Loss: 0.7044795751571655\n",
      "Batch 9, Loss: -1.520479679107666\n",
      "Batch 10, Loss: 0.2589036226272583\n",
      "Batch 11, Loss: 0.189447820186615\n",
      "Batch 12, Loss: -1.340834617614746\n",
      "Batch 13, Loss: 0.3799784779548645\n",
      "Batch 14, Loss: 0.6088626980781555\n",
      "Batch 15, Loss: -1.3731629848480225\n",
      "Batch 16, Loss: -1.2270280122756958\n",
      "Batch 17, Loss: 0.2979053556919098\n",
      "Batch 18, Loss: -1.3953979015350342\n",
      "Batch 19, Loss: -1.2425037622451782\n",
      "Batch 20, Loss: 0.2942656874656677\n",
      "Batch 21, Loss: 0.1023668646812439\n",
      "Batch 22, Loss: 0.3772687315940857\n",
      "Batch 23, Loss: 0.1855272650718689\n",
      "Batch 24, Loss: 0.61663818359375\n",
      "Batch 25, Loss: 0.20291799306869507\n",
      "Batch 26, Loss: 0.593545138835907\n",
      "Batch 27, Loss: 0.1880095899105072\n",
      "Batch 28, Loss: 0.06086894869804382\n",
      "Batch 29, Loss: -1.7195972204208374\n",
      "Batch 30, Loss: -1.639722228050232\n",
      "Batch 31, Loss: -1.067819595336914\n",
      "Batch 32, Loss: 0.39703407883644104\n",
      "Batch 33, Loss: -1.6137903928756714\n",
      "Batch 34, Loss: -1.6844767332077026\n",
      "Batch 35, Loss: 0.7077434062957764\n",
      "Batch 36, Loss: 0.19000816345214844\n",
      "Batch 37, Loss: -1.681917667388916\n",
      "Batch 38, Loss: 0.48914599418640137\n",
      "Batch 39, Loss: 0.25908181071281433\n",
      "Batch 40, Loss: -1.3948357105255127\n",
      "Batch 41, Loss: -1.3425517082214355\n",
      "Batch 42, Loss: -1.2938215732574463\n",
      "Batch 43, Loss: 0.18213488161563873\n",
      "Batch 44, Loss: 0.29047808051109314\n",
      "Batch 45, Loss: 0.1887221336364746\n",
      "Batch 46, Loss: 0.33793342113494873\n",
      "Batch 47, Loss: -1.0329194068908691\n",
      "Batch 48, Loss: -1.04948890209198\n",
      "Batch 49, Loss: -1.690905213356018\n",
      "Batch 50, Loss: -1.5671933889389038\n",
      "Batch 51, Loss: -1.3227941989898682\n",
      "Batch 52, Loss: -1.5931298732757568\n",
      "Batch 53, Loss: 0.7293242812156677\n",
      "Batch 54, Loss: -1.258318305015564\n",
      "Batch 55, Loss: 0.5983072519302368\n",
      "Batch 56, Loss: -1.541725754737854\n",
      "Batch 57, Loss: -1.3471732139587402\n",
      "Batch 58, Loss: -1.7022645473480225\n",
      "Batch 59, Loss: 0.20490959286689758\n",
      "Batch 60, Loss: -1.5326802730560303\n",
      "Batch 61, Loss: -1.497227430343628\n",
      "Batch 62, Loss: 0.5127074718475342\n",
      "Batch 63, Loss: 0.41753605008125305\n",
      "Batch 64, Loss: 0.6117135286331177\n",
      "Batch 65, Loss: 0.39678213000297546\n",
      "Batch 66, Loss: -1.1816201210021973\n",
      "Batch 67, Loss: -1.0137256383895874\n",
      "Batch 68, Loss: 0.0956951379776001\n",
      "Batch 69, Loss: 0.27800121903419495\n",
      "Batch 70, Loss: 0.37683454155921936\n",
      "Batch 71, Loss: 0.18968404829502106\n",
      "Batch 72, Loss: 0.36408764123916626\n",
      "Batch 73, Loss: 0.37654319405555725\n",
      "Batch 74, Loss: 0.2846408784389496\n",
      "Batch 75, Loss: -1.4289591312408447\n",
      "Batch 76, Loss: -1.7071027755737305\n",
      "Batch 77, Loss: 0.10851460695266724\n",
      "Batch 78, Loss: -1.4831875562667847\n",
      "Batch 79, Loss: -1.201419711112976\n",
      "Batch 80, Loss: -1.1365433931350708\n",
      "Batch 81, Loss: 0.21067096292972565\n",
      "Batch 82, Loss: 0.3337005376815796\n",
      "Batch 83, Loss: 0.5125275254249573\n",
      "Batch 84, Loss: 0.46679800748825073\n",
      "Batch 85, Loss: 0.1586068868637085\n",
      "Batch 86, Loss: -1.3208568096160889\n",
      "Batch 87, Loss: 0.6372972726821899\n",
      "Batch 88, Loss: -1.0441832542419434\n",
      "Batch 89, Loss: -1.265979528427124\n",
      "Batch 90, Loss: -1.3944967985153198\n",
      "Batch 91, Loss: 0.15957285463809967\n",
      "Batch 92, Loss: -1.6288573741912842\n",
      "Batch 93, Loss: -1.2645528316497803\n",
      "Batch 94, Loss: -1.2149698734283447\n",
      "Batch 95, Loss: 0.6334930062294006\n",
      "Batch 96, Loss: 0.7308709025382996\n",
      "Batch 97, Loss: -1.485799789428711\n",
      "Batch 98, Loss: -1.5472838878631592\n",
      "Batch 99, Loss: 0.5203593969345093\n",
      "Batch 100, Loss: -1.2562429904937744\n",
      "Batch 101, Loss: -1.1316248178482056\n",
      "Batch 102, Loss: 0.4219738841056824\n",
      "Batch 103, Loss: -1.7224479913711548\n",
      "Batch 104, Loss: -1.716596245765686\n",
      "Batch 105, Loss: -1.2000094652175903\n",
      "Batch 106, Loss: -1.4115877151489258\n",
      "Batch 107, Loss: -1.7059096097946167\n",
      "Batch 108, Loss: 0.33945512771606445\n",
      "Batch 109, Loss: 0.36114704608917236\n",
      "Batch 110, Loss: 0.5338701009750366\n",
      "Batch 111, Loss: -1.302746057510376\n",
      "Batch 112, Loss: 0.08499625325202942\n",
      "Batch 113, Loss: -1.5877809524536133\n",
      "Batch 114, Loss: 0.28350722789764404\n",
      "Batch 115, Loss: -1.146889328956604\n",
      "Batch 116, Loss: -1.52519690990448\n",
      "Batch 117, Loss: -1.3724884986877441\n",
      "Batch 118, Loss: 0.31488099694252014\n",
      "Batch 119, Loss: -1.2356598377227783\n",
      "Batch 120, Loss: -1.469720482826233\n",
      "Batch 121, Loss: 0.7136398553848267\n",
      "Batch 122, Loss: 0.29230737686157227\n",
      "Batch 123, Loss: 0.6563856601715088\n",
      "Batch 124, Loss: -1.6220662593841553\n",
      "Batch 125, Loss: 0.38390177488327026\n",
      "Batch 126, Loss: -1.45730721950531\n",
      "Batch 127, Loss: -1.313437819480896\n",
      "Batch 128, Loss: 0.7045869827270508\n",
      "Batch 129, Loss: -1.2783342599868774\n",
      "Batch 130, Loss: -1.5810446739196777\n",
      "Batch 131, Loss: -1.2184854745864868\n",
      "Batch 132, Loss: 0.28844553232192993\n",
      "Batch 133, Loss: 0.6513610482215881\n",
      "Batch 134, Loss: -1.3786349296569824\n",
      "Batch 135, Loss: 0.38101452589035034\n",
      "Batch 136, Loss: -1.2417033910751343\n",
      "Batch 137, Loss: 0.2033836394548416\n",
      "Batch 138, Loss: 0.2865382134914398\n",
      "Batch 139, Loss: -1.1725342273712158\n",
      "Batch 140, Loss: 0.5139934420585632\n",
      "Batch 141, Loss: -1.4913840293884277\n",
      "Batch 142, Loss: -1.3730531930923462\n",
      "Batch 143, Loss: -1.4708619117736816\n",
      "Batch 144, Loss: 0.3872615098953247\n",
      "Batch 145, Loss: -1.2076445817947388\n",
      "Batch 146, Loss: 0.14229866862297058\n",
      "Batch 147, Loss: -1.6062068939208984\n",
      "Batch 148, Loss: 0.29098838567733765\n",
      "Batch 149, Loss: -1.4359767436981201\n",
      "Batch 150, Loss: 0.576354444026947\n",
      "Batch 151, Loss: 0.28567320108413696\n",
      "Batch 152, Loss: -1.333288550376892\n",
      "Batch 153, Loss: 0.3752844035625458\n",
      "Batch 154, Loss: 0.3391169011592865\n",
      "Batch 155, Loss: 0.3109310269355774\n",
      "Batch 156, Loss: -1.2133245468139648\n",
      "Batch 157, Loss: 0.30568310618400574\n",
      "Batch 158, Loss: -1.630195140838623\n",
      "Batch 159, Loss: 0.24032413959503174\n",
      "Batch 160, Loss: 0.06516239047050476\n",
      "Batch 161, Loss: -1.3249682188034058\n",
      "Batch 162, Loss: 0.5353338718414307\n",
      "Batch 163, Loss: -1.6068568229675293\n",
      "Batch 164, Loss: -1.3080761432647705\n",
      "Batch 165, Loss: -1.3068310022354126\n",
      "Batch 166, Loss: -1.0579726696014404\n",
      "Batch 167, Loss: 0.7205983400344849\n",
      "Batch 168, Loss: 0.29003602266311646\n",
      "Batch 169, Loss: -1.3821309804916382\n",
      "Batch 170, Loss: -1.7390036582946777\n",
      "Batch 171, Loss: -1.4659374952316284\n",
      "Batch 172, Loss: 0.4097679555416107\n",
      "Batch 173, Loss: 0.18684522807598114\n",
      "Batch 174, Loss: 0.13435900211334229\n",
      "Batch 175, Loss: -1.323638677597046\n",
      "Batch 176, Loss: 0.551630437374115\n",
      "Batch 177, Loss: -1.244666337966919\n",
      "Batch 178, Loss: -1.2414053678512573\n",
      "Batch 179, Loss: 0.1820293813943863\n",
      "Batch 180, Loss: -1.0011253356933594\n",
      "Batch 181, Loss: 0.32844415307044983\n",
      "Batch 182, Loss: -1.207582712173462\n",
      "Batch 183, Loss: 0.19038653373718262\n",
      "Batch 184, Loss: 0.008460819721221924\n",
      "Batch 185, Loss: -1.3103312253952026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 186, Loss: -1.0376334190368652\n",
      "Batch 187, Loss: -1.3149645328521729\n",
      "Batch 188, Loss: 0.5746787786483765\n",
      "Batch 189, Loss: 0.5756492614746094\n",
      "Batch 190, Loss: 0.1062048077583313\n",
      "Batch 191, Loss: 0.6488213539123535\n",
      "Batch 192, Loss: 0.26463228464126587\n",
      "Batch 193, Loss: 0.4235292375087738\n",
      "Batch 194, Loss: -1.6130387783050537\n",
      "Batch 195, Loss: 0.2111896276473999\n",
      "Batch 196, Loss: 0.4042655825614929\n",
      "Batch 197, Loss: -1.5105223655700684\n",
      "Batch 198, Loss: 0.32707980275154114\n",
      "Batch 199, Loss: 0.726469874382019\n",
      "Batch 200, Loss: 0.3601369559764862\n",
      "Batch 201, Loss: 0.5974321365356445\n",
      "Batch 202, Loss: 0.3240366578102112\n",
      "Batch 203, Loss: -1.4980674982070923\n",
      "Batch 204, Loss: -1.4972368478775024\n",
      "Batch 205, Loss: 0.4369693398475647\n",
      "Batch 206, Loss: -1.6459472179412842\n",
      "Batch 207, Loss: 0.7109371423721313\n",
      "Batch 208, Loss: 0.6281172037124634\n",
      "Batch 209, Loss: 0.4419785439968109\n",
      "Batch 210, Loss: 0.18823395669460297\n",
      "Batch 211, Loss: -1.6832382678985596\n",
      "Batch 212, Loss: 0.3231128454208374\n",
      "Batch 213, Loss: 0.5155100226402283\n",
      "Batch 214, Loss: 0.17970292270183563\n",
      "Batch 215, Loss: 0.3740144670009613\n",
      "Batch 216, Loss: -1.337864637374878\n",
      "Batch 217, Loss: 0.6383670568466187\n",
      "Batch 218, Loss: -1.274032711982727\n",
      "Batch 219, Loss: -1.4365979433059692\n",
      "Batch 220, Loss: 0.38777652382850647\n",
      "Batch 221, Loss: -1.4149706363677979\n",
      "Batch 222, Loss: -1.6262917518615723\n",
      "Batch 223, Loss: 0.18484258651733398\n",
      "Batch 224, Loss: -1.2546558380126953\n",
      "Batch 225, Loss: -1.2282769680023193\n",
      "Batch 226, Loss: 0.5324181914329529\n",
      "Batch 227, Loss: -1.4269165992736816\n",
      "Batch 228, Loss: -1.3441481590270996\n",
      "Batch 229, Loss: -1.5016026496887207\n",
      "Batch 230, Loss: -1.6750507354736328\n",
      "Batch 231, Loss: -1.4722479581832886\n",
      "Batch 232, Loss: 0.21028035879135132\n",
      "Batch 233, Loss: -1.3739066123962402\n",
      "Batch 234, Loss: -1.3585940599441528\n",
      "Batch 235, Loss: 0.08634638786315918\n",
      "Batch 236, Loss: -1.1687458753585815\n",
      "Batch 237, Loss: -1.602203130722046\n",
      "Batch 238, Loss: -1.3530387878417969\n",
      "Batch 239, Loss: -1.4053467512130737\n",
      "Batch 240, Loss: 0.23316504061222076\n",
      "Batch 241, Loss: -1.2497234344482422\n",
      "Batch 242, Loss: 0.6860781311988831\n",
      "Batch 243, Loss: -1.5841699838638306\n",
      "Batch 244, Loss: -1.1822800636291504\n",
      "Batch 245, Loss: -1.32156503200531\n",
      "Batch 246, Loss: -1.1677570343017578\n",
      "Batch 247, Loss: -1.705188512802124\n",
      "Batch 248, Loss: 0.37157198786735535\n",
      "Batch 249, Loss: 0.2910977900028229\n",
      "Batch 250, Loss: -1.7023621797561646\n",
      "Batch 251, Loss: -1.2119239568710327\n",
      "Batch 252, Loss: 0.2784615755081177\n",
      "Batch 253, Loss: -1.6054320335388184\n",
      "Batch 254, Loss: -1.3938910961151123\n",
      "Batch 255, Loss: 0.6280267834663391\n",
      "Batch 256, Loss: -1.4785146713256836\n",
      "Batch 257, Loss: -1.5026880502700806\n",
      "Batch 258, Loss: -1.3269855976104736\n",
      "Batch 259, Loss: -1.264128565788269\n",
      "Batch 260, Loss: 0.6396588087081909\n",
      "Batch 261, Loss: -1.0282588005065918\n",
      "Batch 262, Loss: 0.3875546157360077\n",
      "Batch 263, Loss: 0.3390558958053589\n",
      "Batch 264, Loss: -1.2266556024551392\n",
      "Batch 265, Loss: 0.2531813681125641\n",
      "Batch 266, Loss: 0.2554098963737488\n",
      "Batch 267, Loss: 0.21401970088481903\n",
      "Batch 268, Loss: 0.6654072403907776\n",
      "Batch 269, Loss: 0.47051364183425903\n",
      "Batch 270, Loss: 0.35860711336135864\n",
      "Batch 271, Loss: -1.5918554067611694\n",
      "Batch 272, Loss: -1.5288043022155762\n",
      "Batch 273, Loss: 0.6582787036895752\n",
      "Batch 274, Loss: 0.2723340392112732\n",
      "Batch 275, Loss: -1.2450261116027832\n",
      "Batch 276, Loss: -1.385487675666809\n",
      "Batch 277, Loss: -1.4929535388946533\n",
      "Batch 278, Loss: 0.37767958641052246\n",
      "Batch 279, Loss: -1.1337790489196777\n",
      "Batch 280, Loss: 0.165035218000412\n",
      "Batch 281, Loss: -1.1803581714630127\n",
      "Batch 282, Loss: -1.7067700624465942\n",
      "Batch 283, Loss: 0.3874351978302002\n",
      "Batch 284, Loss: 0.3312323987483978\n",
      "Batch 285, Loss: -1.1350815296173096\n",
      "Batch 286, Loss: -1.6192479133605957\n",
      "Batch 287, Loss: -1.4278377294540405\n",
      "Batch 288, Loss: 0.5517926812171936\n",
      "Batch 289, Loss: 0.20545437932014465\n",
      "Batch 290, Loss: -1.6220003366470337\n",
      "Batch 291, Loss: -1.4268558025360107\n",
      "Batch 292, Loss: -1.3398228883743286\n",
      "Batch 293, Loss: 0.5052765011787415\n",
      "Batch 294, Loss: 0.32374879717826843\n",
      "Batch 295, Loss: -1.442291021347046\n",
      "Batch 296, Loss: -1.0404378175735474\n",
      "Batch 297, Loss: 0.3701486885547638\n",
      "Batch 298, Loss: -1.3741037845611572\n",
      "Batch 299, Loss: -1.488251805305481\n",
      "Batch 300, Loss: -1.7159385681152344\n",
      "Batch 301, Loss: 0.4868728220462799\n",
      "Batch 302, Loss: -1.3853744268417358\n",
      "Batch 303, Loss: 0.26540642976760864\n",
      "Batch 304, Loss: 0.4730496108531952\n",
      "Batch 305, Loss: 0.4805699586868286\n",
      "Batch 306, Loss: -0.009861797094345093\n",
      "Batch 307, Loss: 0.5203292369842529\n",
      "Batch 308, Loss: 0.5267564654350281\n",
      "Batch 309, Loss: 0.533470869064331\n",
      "Batch 310, Loss: -1.6028543710708618\n",
      "Batch 311, Loss: 0.13872481882572174\n",
      "Batch 312, Loss: -1.5788004398345947\n",
      "Batch 313, Loss: -1.689589500427246\n",
      "Batch 314, Loss: 0.3364623486995697\n",
      "Batch 315, Loss: -1.6109240055084229\n",
      "Batch 316, Loss: 0.3291413187980652\n",
      "Batch 317, Loss: -1.4179303646087646\n",
      "Batch 318, Loss: -1.7626367807388306\n",
      "Batch 319, Loss: 0.13139064610004425\n",
      "Batch 320, Loss: 0.287945955991745\n",
      "Batch 321, Loss: -1.1700361967086792\n",
      "Batch 322, Loss: 0.12015259265899658\n",
      "Batch 323, Loss: 0.3429630696773529\n",
      "Batch 324, Loss: 0.5693672895431519\n",
      "Batch 325, Loss: -1.0614018440246582\n",
      "Batch 326, Loss: -1.5879895687103271\n",
      "Batch 327, Loss: -1.668968677520752\n",
      "Batch 328, Loss: 0.36775046586990356\n",
      "Batch 329, Loss: -1.0613532066345215\n",
      "Batch 330, Loss: -1.306362509727478\n",
      "Batch 331, Loss: 0.3698395788669586\n",
      "Batch 332, Loss: -1.7033637762069702\n",
      "Batch 333, Loss: -1.1860264539718628\n",
      "Batch 334, Loss: 0.16629989445209503\n",
      "Batch 335, Loss: -1.1002016067504883\n",
      "Batch 336, Loss: 0.16202987730503082\n",
      "Batch 337, Loss: -1.5831338167190552\n",
      "Batch 338, Loss: -0.0020567476749420166\n",
      "Batch 339, Loss: -1.4528274536132812\n",
      "Batch 340, Loss: -0.9916142225265503\n",
      "Batch 341, Loss: 0.6507853269577026\n",
      "Batch 342, Loss: -1.2277134656906128\n",
      "Batch 343, Loss: 0.6377900242805481\n",
      "Batch 344, Loss: -1.6271098852157593\n",
      "Batch 345, Loss: -1.2107288837432861\n",
      "Batch 346, Loss: -1.262412428855896\n",
      "Batch 347, Loss: 0.40821000933647156\n",
      "Batch 348, Loss: -1.274186372756958\n",
      "Batch 349, Loss: -1.195040225982666\n",
      "Batch 350, Loss: -1.5171349048614502\n",
      "Batch 351, Loss: 0.20199015736579895\n",
      "Batch 352, Loss: -1.7044873237609863\n",
      "Batch 353, Loss: 0.5065175890922546\n",
      "Batch 354, Loss: -1.3370171785354614\n",
      "Batch 355, Loss: -1.228664517402649\n",
      "Batch 356, Loss: -1.40233314037323\n",
      "Batch 357, Loss: 0.42792126536369324\n",
      "Batch 358, Loss: 0.009375989437103271\n",
      "Batch 359, Loss: 0.44570696353912354\n",
      "Batch 360, Loss: 0.3691389560699463\n",
      "Batch 361, Loss: 0.4352058172225952\n",
      "Batch 362, Loss: 0.7527588605880737\n",
      "Batch 363, Loss: 0.018444299697875977\n",
      "Batch 364, Loss: 0.6754100322723389\n",
      "Batch 365, Loss: -1.5994876623153687\n",
      "Batch 366, Loss: -1.2454619407653809\n",
      "Batch 367, Loss: -1.4766523838043213\n",
      "Batch 368, Loss: 0.5216997861862183\n",
      "Batch 369, Loss: 0.2174922525882721\n",
      "Batch 370, Loss: 0.4150770902633667\n",
      "Batch 371, Loss: -1.2101459503173828\n",
      "Batch 372, Loss: 0.3381873071193695\n",
      "Batch 373, Loss: -1.44777250289917\n",
      "Batch 374, Loss: 0.23739129304885864\n",
      "Batch 375, Loss: 0.7404266595840454\n",
      "Batch 376, Loss: 0.735576868057251\n",
      "Batch 377, Loss: -1.5008866786956787\n",
      "Batch 378, Loss: -1.2811031341552734\n",
      "Batch 379, Loss: 0.32657590508461\n",
      "Batch 380, Loss: 0.734855055809021\n",
      "Batch 381, Loss: -1.4657623767852783\n",
      "Batch 382, Loss: -1.320035457611084\n",
      "Batch 383, Loss: -0.005762696266174316\n",
      "Batch 384, Loss: 0.02103602886199951\n",
      "Batch 385, Loss: -1.13307785987854\n",
      "Batch 386, Loss: 0.37933149933815\n",
      "Batch 387, Loss: 0.1361449658870697\n",
      "Batch 388, Loss: -1.2102152109146118\n",
      "Batch 389, Loss: 0.2191433608531952\n",
      "Batch 390, Loss: 0.023734450340270996\n",
      "Batch 391, Loss: 0.10206842422485352\n",
      "Batch 392, Loss: 0.5812384486198425\n",
      "Batch 393, Loss: -1.3428434133529663\n",
      "Batch 394, Loss: 0.29452380537986755\n",
      "Batch 395, Loss: -1.1025350093841553\n",
      "Batch 396, Loss: -1.4442908763885498\n",
      "Batch 397, Loss: 0.14438024163246155\n",
      "Batch 398, Loss: 0.565556526184082\n",
      "Batch 399, Loss: 0.48498496413230896\n",
      "Training [90%]\tLoss: -0.5078\n",
      "Batch 0, Loss: 0.16190524399280548\n",
      "Batch 1, Loss: 0.5622406005859375\n",
      "Batch 2, Loss: -1.3930494785308838\n",
      "Batch 3, Loss: 0.4305896759033203\n",
      "Batch 4, Loss: 0.4436402916908264\n",
      "Batch 5, Loss: 0.564224123954773\n",
      "Batch 6, Loss: 0.03911316394805908\n",
      "Batch 7, Loss: 0.5436049699783325\n",
      "Batch 8, Loss: -1.0017459392547607\n",
      "Batch 9, Loss: -1.0735222101211548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10, Loss: -1.1047768592834473\n",
      "Batch 11, Loss: 0.5100514888763428\n",
      "Batch 12, Loss: 0.7004193663597107\n",
      "Batch 13, Loss: 0.016131222248077393\n",
      "Batch 14, Loss: -1.25845205783844\n",
      "Batch 15, Loss: -1.2118604183197021\n",
      "Batch 16, Loss: 0.2303987443447113\n",
      "Batch 17, Loss: 0.15676021575927734\n",
      "Batch 18, Loss: -1.2537446022033691\n",
      "Batch 19, Loss: -1.6398454904556274\n",
      "Batch 20, Loss: 0.35487350821495056\n",
      "Batch 21, Loss: -1.2364492416381836\n",
      "Batch 22, Loss: -1.729350209236145\n",
      "Batch 23, Loss: 0.48101574182510376\n",
      "Batch 24, Loss: -1.5001680850982666\n",
      "Batch 25, Loss: -1.3198883533477783\n",
      "Batch 26, Loss: 0.48176896572113037\n",
      "Batch 27, Loss: 0.6545152068138123\n",
      "Batch 28, Loss: -1.6717276573181152\n",
      "Batch 29, Loss: -1.4030406475067139\n",
      "Batch 30, Loss: 0.24079656600952148\n",
      "Batch 31, Loss: -1.237000823020935\n",
      "Batch 32, Loss: 0.219223290681839\n",
      "Batch 33, Loss: -1.3650312423706055\n",
      "Batch 34, Loss: -1.7260667085647583\n",
      "Batch 35, Loss: -1.3732781410217285\n",
      "Batch 36, Loss: 0.6561254262924194\n",
      "Batch 37, Loss: -1.2457078695297241\n",
      "Batch 38, Loss: -1.2477977275848389\n",
      "Batch 39, Loss: 0.42875319719314575\n",
      "Batch 40, Loss: 0.34414681792259216\n",
      "Batch 41, Loss: -1.3297817707061768\n",
      "Batch 42, Loss: 0.23413732647895813\n",
      "Batch 43, Loss: 0.3920407295227051\n",
      "Batch 44, Loss: 0.2968609035015106\n",
      "Batch 45, Loss: 0.5789170265197754\n",
      "Batch 46, Loss: 0.5351771116256714\n",
      "Batch 47, Loss: 0.5181488990783691\n",
      "Batch 48, Loss: 0.12010906636714935\n",
      "Batch 49, Loss: 0.019272416830062866\n",
      "Batch 50, Loss: -1.2581942081451416\n",
      "Batch 51, Loss: -1.136425495147705\n",
      "Batch 52, Loss: -1.7036067247390747\n",
      "Batch 53, Loss: -1.3615120649337769\n",
      "Batch 54, Loss: 0.3651721775531769\n",
      "Batch 55, Loss: -1.6551010608673096\n",
      "Batch 56, Loss: 0.12156008183956146\n",
      "Batch 57, Loss: -0.9995636343955994\n",
      "Batch 58, Loss: 0.5613803863525391\n",
      "Batch 59, Loss: 0.3752525746822357\n",
      "Batch 60, Loss: 0.38096535205841064\n",
      "Batch 61, Loss: 0.5474386811256409\n",
      "Batch 62, Loss: -1.5926642417907715\n",
      "Batch 63, Loss: 0.5156917572021484\n",
      "Batch 64, Loss: -1.1428073644638062\n",
      "Batch 65, Loss: -1.1118627786636353\n",
      "Batch 66, Loss: 0.33038368821144104\n",
      "Batch 67, Loss: 0.6348932385444641\n",
      "Batch 68, Loss: -1.2552481889724731\n",
      "Batch 69, Loss: 0.38748323917388916\n",
      "Batch 70, Loss: 0.3017267882823944\n",
      "Batch 71, Loss: 0.3465040624141693\n",
      "Batch 72, Loss: -1.3183410167694092\n",
      "Batch 73, Loss: -1.1678781509399414\n",
      "Batch 74, Loss: -1.5182627439498901\n",
      "Batch 75, Loss: -1.1010717153549194\n",
      "Batch 76, Loss: 0.3732353448867798\n",
      "Batch 77, Loss: 0.43395859003067017\n",
      "Batch 78, Loss: -1.2688231468200684\n",
      "Batch 79, Loss: -1.4735056161880493\n",
      "Batch 80, Loss: 0.3745591938495636\n",
      "Batch 81, Loss: -1.1870603561401367\n",
      "Batch 82, Loss: 0.36581388115882874\n",
      "Batch 83, Loss: 0.3384411036968231\n",
      "Batch 84, Loss: 0.5485414266586304\n",
      "Batch 85, Loss: -1.5190309286117554\n",
      "Batch 86, Loss: -1.4849390983581543\n",
      "Batch 87, Loss: -1.4987677335739136\n",
      "Batch 88, Loss: 0.45648008584976196\n",
      "Batch 89, Loss: -1.171634554862976\n",
      "Batch 90, Loss: 0.28006434440612793\n",
      "Batch 91, Loss: 0.256314218044281\n",
      "Batch 92, Loss: 0.23270079493522644\n",
      "Batch 93, Loss: 0.36102917790412903\n",
      "Batch 94, Loss: 0.32972612977027893\n",
      "Batch 95, Loss: 0.25142669677734375\n",
      "Batch 96, Loss: 0.01423346996307373\n",
      "Batch 97, Loss: -1.3575401306152344\n",
      "Batch 98, Loss: 0.516474723815918\n",
      "Batch 99, Loss: -1.3675950765609741\n",
      "Batch 100, Loss: 0.18089668452739716\n",
      "Batch 101, Loss: 0.3959086537361145\n",
      "Batch 102, Loss: -1.3717316389083862\n",
      "Batch 103, Loss: 0.41956236958503723\n",
      "Batch 104, Loss: 0.2415829449892044\n",
      "Batch 105, Loss: -1.656529426574707\n",
      "Batch 106, Loss: -1.5039422512054443\n",
      "Batch 107, Loss: 0.7009512186050415\n",
      "Batch 108, Loss: -1.1629582643508911\n",
      "Batch 109, Loss: -1.1512459516525269\n",
      "Batch 110, Loss: 0.26690220832824707\n",
      "Batch 111, Loss: 0.37999093532562256\n",
      "Batch 112, Loss: -1.5806708335876465\n",
      "Batch 113, Loss: -1.5491684675216675\n",
      "Batch 114, Loss: -1.6739516258239746\n",
      "Batch 115, Loss: 0.04852166771888733\n",
      "Batch 116, Loss: -1.0477778911590576\n",
      "Batch 117, Loss: 0.036595821380615234\n",
      "Batch 118, Loss: 0.7316941022872925\n",
      "Batch 119, Loss: 0.23177753388881683\n",
      "Batch 120, Loss: -1.3404514789581299\n",
      "Batch 121, Loss: -1.3311355113983154\n",
      "Batch 122, Loss: 0.057503312826156616\n",
      "Batch 123, Loss: 0.02477973699569702\n",
      "Batch 124, Loss: 0.26673680543899536\n",
      "Batch 125, Loss: 0.6621168851852417\n",
      "Batch 126, Loss: 0.328827828168869\n",
      "Batch 127, Loss: 0.31435221433639526\n",
      "Batch 128, Loss: 0.5873885154724121\n",
      "Batch 129, Loss: 0.12267850339412689\n",
      "Batch 130, Loss: 0.19119888544082642\n",
      "Batch 131, Loss: -1.5485104322433472\n",
      "Batch 132, Loss: 0.3752024173736572\n",
      "Batch 133, Loss: 0.2819838523864746\n",
      "Batch 134, Loss: -1.321574091911316\n",
      "Batch 135, Loss: 0.23982763290405273\n",
      "Batch 136, Loss: 0.33677783608436584\n",
      "Batch 137, Loss: -1.1117929220199585\n",
      "Batch 138, Loss: 0.32532575726509094\n",
      "Batch 139, Loss: -1.1041111946105957\n",
      "Batch 140, Loss: 0.44762709736824036\n",
      "Batch 141, Loss: -1.2278060913085938\n",
      "Batch 142, Loss: 0.24356180429458618\n",
      "Batch 143, Loss: 0.28682786226272583\n",
      "Batch 144, Loss: -1.0030672550201416\n",
      "Batch 145, Loss: -1.4378581047058105\n",
      "Batch 146, Loss: -1.6408847570419312\n",
      "Batch 147, Loss: -1.3507602214813232\n",
      "Batch 148, Loss: 0.41261550784111023\n",
      "Batch 149, Loss: -1.543952226638794\n",
      "Batch 150, Loss: -1.7052146196365356\n",
      "Batch 151, Loss: 0.2099834680557251\n",
      "Batch 152, Loss: 0.6820615530014038\n",
      "Batch 153, Loss: -1.5233449935913086\n",
      "Batch 154, Loss: 0.43634676933288574\n",
      "Batch 155, Loss: -1.504872441291809\n",
      "Batch 156, Loss: -1.3765859603881836\n",
      "Batch 157, Loss: -1.6259818077087402\n",
      "Batch 158, Loss: 0.18787315487861633\n",
      "Batch 159, Loss: 0.42397207021713257\n",
      "Batch 160, Loss: 0.5335934162139893\n",
      "Batch 161, Loss: 0.6798803806304932\n",
      "Batch 162, Loss: -0.028797388076782227\n",
      "Batch 163, Loss: -1.2106144428253174\n",
      "Batch 164, Loss: 0.487205445766449\n",
      "Batch 165, Loss: 0.12111221253871918\n",
      "Batch 166, Loss: 0.34383997321128845\n",
      "Batch 167, Loss: -1.0605876445770264\n",
      "Batch 168, Loss: 0.4240884780883789\n",
      "Batch 169, Loss: -1.5365707874298096\n",
      "Batch 170, Loss: -1.599250078201294\n",
      "Batch 171, Loss: 0.7300843000411987\n",
      "Batch 172, Loss: -1.3165283203125\n",
      "Batch 173, Loss: 0.05291181802749634\n",
      "Batch 174, Loss: 0.16153591871261597\n",
      "Batch 175, Loss: 0.34581559896469116\n",
      "Batch 176, Loss: -1.0967552661895752\n",
      "Batch 177, Loss: -1.115474820137024\n",
      "Batch 178, Loss: -1.4313405752182007\n",
      "Batch 179, Loss: -1.5428099632263184\n",
      "Batch 180, Loss: 0.3010796010494232\n",
      "Batch 181, Loss: -1.2785537242889404\n",
      "Batch 182, Loss: -1.6499273777008057\n",
      "Batch 183, Loss: -0.9965214729309082\n",
      "Batch 184, Loss: -1.3384302854537964\n",
      "Batch 185, Loss: 0.642371654510498\n",
      "Batch 186, Loss: -1.3398135900497437\n",
      "Batch 187, Loss: 0.6262679100036621\n",
      "Batch 188, Loss: 0.6318042278289795\n",
      "Batch 189, Loss: -1.1661274433135986\n",
      "Batch 190, Loss: 0.5451434254646301\n",
      "Batch 191, Loss: -1.2411342859268188\n",
      "Batch 192, Loss: -1.4764615297317505\n",
      "Batch 193, Loss: -1.3049862384796143\n",
      "Batch 194, Loss: -1.312544584274292\n",
      "Batch 195, Loss: -1.0860334634780884\n",
      "Batch 196, Loss: -1.5269298553466797\n",
      "Batch 197, Loss: 0.6126408576965332\n",
      "Batch 198, Loss: 0.5277912020683289\n",
      "Batch 199, Loss: -1.1455094814300537\n",
      "Batch 200, Loss: -1.2113003730773926\n",
      "Batch 201, Loss: 0.2417161762714386\n",
      "Batch 202, Loss: -1.2146883010864258\n",
      "Batch 203, Loss: -1.2999931573867798\n",
      "Batch 204, Loss: 0.14922308921813965\n",
      "Batch 205, Loss: -1.3817297220230103\n",
      "Batch 206, Loss: 0.0626215934753418\n",
      "Batch 207, Loss: -1.5726730823516846\n",
      "Batch 208, Loss: -1.141493797302246\n",
      "Batch 209, Loss: -1.2812182903289795\n",
      "Batch 210, Loss: -1.6264677047729492\n",
      "Batch 211, Loss: -1.513414740562439\n",
      "Batch 212, Loss: 0.5545329451560974\n",
      "Batch 213, Loss: -1.2979159355163574\n",
      "Batch 214, Loss: 0.040576159954071045\n",
      "Batch 215, Loss: 0.6586595177650452\n",
      "Batch 216, Loss: 0.6243179440498352\n",
      "Batch 217, Loss: 0.4317964017391205\n",
      "Batch 218, Loss: -1.2813222408294678\n",
      "Batch 219, Loss: -1.2260457277297974\n",
      "Batch 220, Loss: -1.1912728548049927\n",
      "Batch 221, Loss: -1.279405117034912\n",
      "Batch 222, Loss: 0.09866911172866821\n",
      "Batch 223, Loss: -1.5633511543273926\n",
      "Batch 224, Loss: -1.452053189277649\n",
      "Batch 225, Loss: 0.5542564392089844\n",
      "Batch 226, Loss: 0.3357258141040802\n",
      "Batch 227, Loss: -1.0449755191802979\n",
      "Batch 228, Loss: -1.5078986883163452\n",
      "Batch 229, Loss: -1.3714970350265503\n",
      "Batch 230, Loss: 0.3457907736301422\n",
      "Batch 231, Loss: 0.4960559606552124\n",
      "Batch 232, Loss: 0.36316847801208496\n",
      "Batch 233, Loss: -0.9846374988555908\n",
      "Batch 234, Loss: -1.060217261314392\n",
      "Batch 235, Loss: -1.5444482564926147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 236, Loss: 0.4266800880432129\n",
      "Batch 237, Loss: -1.067582368850708\n",
      "Batch 238, Loss: -1.3588658571243286\n",
      "Batch 239, Loss: -1.1213841438293457\n",
      "Batch 240, Loss: -1.3574546575546265\n",
      "Batch 241, Loss: 0.29164355993270874\n",
      "Batch 242, Loss: -1.713389277458191\n",
      "Batch 243, Loss: 0.3075452446937561\n",
      "Batch 244, Loss: -1.2582365274429321\n",
      "Batch 245, Loss: 0.29487448930740356\n",
      "Batch 246, Loss: 0.40495380759239197\n",
      "Batch 247, Loss: 0.5611340999603271\n",
      "Batch 248, Loss: 0.18882878124713898\n",
      "Batch 249, Loss: 0.4471490979194641\n",
      "Batch 250, Loss: 0.09332966804504395\n",
      "Batch 251, Loss: -1.3741025924682617\n",
      "Batch 252, Loss: -1.055171251296997\n",
      "Batch 253, Loss: 0.23650366067886353\n",
      "Batch 254, Loss: 0.6482698917388916\n",
      "Batch 255, Loss: -1.1697068214416504\n",
      "Batch 256, Loss: 0.5394764542579651\n",
      "Batch 257, Loss: 0.12609438598155975\n",
      "Batch 258, Loss: 0.5080733299255371\n",
      "Batch 259, Loss: 0.29518842697143555\n",
      "Batch 260, Loss: -1.0476617813110352\n",
      "Batch 261, Loss: 0.4980780780315399\n",
      "Batch 262, Loss: 0.5960279703140259\n",
      "Batch 263, Loss: -1.2367579936981201\n",
      "Batch 264, Loss: -1.6242908239364624\n",
      "Batch 265, Loss: -1.3204309940338135\n",
      "Batch 266, Loss: -1.0667309761047363\n",
      "Batch 267, Loss: 0.3282202184200287\n",
      "Batch 268, Loss: -1.6621546745300293\n",
      "Batch 269, Loss: -1.1711385250091553\n",
      "Batch 270, Loss: 0.4549575448036194\n",
      "Batch 271, Loss: -1.6532890796661377\n",
      "Batch 272, Loss: 0.30255523324012756\n",
      "Batch 273, Loss: 0.5426076054573059\n",
      "Batch 274, Loss: 0.165090411901474\n",
      "Batch 275, Loss: -1.353725552558899\n",
      "Batch 276, Loss: -1.5738906860351562\n",
      "Batch 277, Loss: -1.1490243673324585\n",
      "Batch 278, Loss: -1.3196594715118408\n",
      "Batch 279, Loss: -0.0253581702709198\n",
      "Batch 280, Loss: 0.3446776866912842\n",
      "Batch 281, Loss: -1.54221510887146\n",
      "Batch 282, Loss: -1.1842124462127686\n",
      "Batch 283, Loss: 0.21081876754760742\n",
      "Batch 284, Loss: -1.5554587841033936\n",
      "Batch 285, Loss: 0.3849630057811737\n",
      "Batch 286, Loss: 0.293442040681839\n",
      "Batch 287, Loss: -1.0640711784362793\n",
      "Batch 288, Loss: -0.0039130449295043945\n",
      "Batch 289, Loss: 0.37613338232040405\n",
      "Batch 290, Loss: 0.4094323515892029\n",
      "Batch 291, Loss: -1.2885193824768066\n",
      "Batch 292, Loss: -1.431312084197998\n",
      "Batch 293, Loss: -1.45340895652771\n",
      "Batch 294, Loss: 0.2800421416759491\n",
      "Batch 295, Loss: 0.3398367464542389\n",
      "Batch 296, Loss: -1.4831750392913818\n",
      "Batch 297, Loss: 0.4645470678806305\n",
      "Batch 298, Loss: 0.6676417589187622\n",
      "Batch 299, Loss: 0.5165058374404907\n",
      "Batch 300, Loss: 0.46083498001098633\n",
      "Batch 301, Loss: -1.4985064268112183\n",
      "Batch 302, Loss: -1.1466176509857178\n",
      "Batch 303, Loss: -1.2320678234100342\n",
      "Batch 304, Loss: -1.0371780395507812\n",
      "Batch 305, Loss: 0.449954092502594\n",
      "Batch 306, Loss: -1.3280123472213745\n",
      "Batch 307, Loss: 0.46004384756088257\n",
      "Batch 308, Loss: -1.5536714792251587\n",
      "Batch 309, Loss: 0.3431512713432312\n",
      "Batch 310, Loss: -1.328978419303894\n",
      "Batch 311, Loss: 0.5952491760253906\n",
      "Batch 312, Loss: -1.1375831365585327\n",
      "Batch 313, Loss: 0.5317187905311584\n",
      "Batch 314, Loss: -1.36851167678833\n",
      "Batch 315, Loss: 0.05071714520454407\n",
      "Batch 316, Loss: -1.1540848016738892\n",
      "Batch 317, Loss: -1.4107590913772583\n",
      "Batch 318, Loss: -1.2044148445129395\n",
      "Batch 319, Loss: 0.02023795247077942\n",
      "Batch 320, Loss: -1.053451418876648\n",
      "Batch 321, Loss: 0.17332838475704193\n",
      "Batch 322, Loss: -1.5380619764328003\n",
      "Batch 323, Loss: -1.1873936653137207\n",
      "Batch 324, Loss: 0.7009694576263428\n",
      "Batch 325, Loss: 0.6434328556060791\n",
      "Batch 326, Loss: -1.5747220516204834\n",
      "Batch 327, Loss: -1.309983253479004\n",
      "Batch 328, Loss: -1.3199121952056885\n",
      "Batch 329, Loss: 0.5040664076805115\n",
      "Batch 330, Loss: 0.6973077058792114\n",
      "Batch 331, Loss: 0.32968196272850037\n",
      "Batch 332, Loss: -1.6128348112106323\n",
      "Batch 333, Loss: -1.4222040176391602\n",
      "Batch 334, Loss: -1.2843598127365112\n",
      "Batch 335, Loss: -1.1323511600494385\n",
      "Batch 336, Loss: 0.6384986639022827\n",
      "Batch 337, Loss: 0.5747277140617371\n",
      "Batch 338, Loss: 0.7174466252326965\n",
      "Batch 339, Loss: -0.9830601215362549\n",
      "Batch 340, Loss: -1.0191800594329834\n",
      "Batch 341, Loss: -1.2821274995803833\n",
      "Batch 342, Loss: -1.3656492233276367\n",
      "Batch 343, Loss: -1.142437219619751\n",
      "Batch 344, Loss: 0.1346997618675232\n",
      "Batch 345, Loss: -1.4130128622055054\n",
      "Batch 346, Loss: 0.1412273645401001\n",
      "Batch 347, Loss: -1.4209611415863037\n",
      "Batch 348, Loss: 0.6334972381591797\n",
      "Batch 349, Loss: 0.7053378820419312\n",
      "Batch 350, Loss: -1.4552403688430786\n",
      "Batch 351, Loss: 0.6710386276245117\n",
      "Batch 352, Loss: 0.6844292879104614\n",
      "Batch 353, Loss: -1.212172508239746\n",
      "Batch 354, Loss: -1.4351842403411865\n",
      "Batch 355, Loss: 0.3013022243976593\n",
      "Batch 356, Loss: -1.1644372940063477\n",
      "Batch 357, Loss: -1.035989761352539\n",
      "Batch 358, Loss: 0.28946441411972046\n",
      "Batch 359, Loss: -1.324556827545166\n",
      "Batch 360, Loss: 0.04184940457344055\n",
      "Batch 361, Loss: -1.6264790296554565\n",
      "Batch 362, Loss: -1.1498231887817383\n",
      "Batch 363, Loss: -1.513460397720337\n",
      "Batch 364, Loss: -1.363415002822876\n",
      "Batch 365, Loss: 0.05363863706588745\n",
      "Batch 366, Loss: 0.23850443959236145\n",
      "Batch 367, Loss: -1.4625470638275146\n",
      "Batch 368, Loss: -1.1126151084899902\n",
      "Batch 369, Loss: 0.3096638023853302\n",
      "Batch 370, Loss: -1.2282730340957642\n",
      "Batch 371, Loss: 0.3231547772884369\n",
      "Batch 372, Loss: 0.11263419687747955\n",
      "Batch 373, Loss: 0.42199409008026123\n",
      "Batch 374, Loss: -1.6813169717788696\n",
      "Batch 375, Loss: -1.471609354019165\n",
      "Batch 376, Loss: -1.511669397354126\n",
      "Batch 377, Loss: 0.16275860369205475\n",
      "Batch 378, Loss: -1.3620132207870483\n",
      "Batch 379, Loss: -1.6492748260498047\n",
      "Batch 380, Loss: -1.494314193725586\n",
      "Batch 381, Loss: -1.3288440704345703\n",
      "Batch 382, Loss: -1.2975387573242188\n",
      "Batch 383, Loss: 0.43506985902786255\n",
      "Batch 384, Loss: 0.6650264263153076\n",
      "Batch 385, Loss: 0.1782136857509613\n",
      "Batch 386, Loss: -1.174244999885559\n",
      "Batch 387, Loss: -1.1869968175888062\n",
      "Batch 388, Loss: 0.13096953928470612\n",
      "Batch 389, Loss: 0.5379064679145813\n",
      "Batch 390, Loss: -1.5542742013931274\n",
      "Batch 391, Loss: -1.053976058959961\n",
      "Batch 392, Loss: -1.5761297941207886\n",
      "Batch 393, Loss: 0.6297205686569214\n",
      "Batch 394, Loss: 0.10755178332328796\n",
      "Batch 395, Loss: -1.5968106985092163\n",
      "Batch 396, Loss: 0.04741498827934265\n",
      "Batch 397, Loss: -1.4588181972503662\n",
      "Batch 398, Loss: 0.05738013982772827\n",
      "Batch 399, Loss: 0.16089749336242676\n",
      "Training [100%]\tLoss: -0.4863\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: -0.0007068701088428497 s\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: -0.4885\n",
      "\tAccuracy: 50.0%\n",
      "Test time: 0.00028241705149412155 s\n"
     ]
    }
   ],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
