{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '7']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"hybridqnn_shallow\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "LR = 0.001\n",
    "n_samples_train = 100 #128\n",
    "n_samples_test = 50 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"MNIST\" # CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = ['1','7'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "if network == 'hybridqnn_shallow' or network == 'QSVM':\n",
    "    # Set preprocessing transforms\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "else:\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    \n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: -0.0067272220039740205 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZ),\n",
      "  CircuitStateFn(\n",
      "       ┌──────────────────────────┐┌──────────────────────────────────────┐\n",
      "  q_0: ┤0                         ├┤0                                     ├\n",
      "       │  ZZFeatureMap(x[0],x[1]) ││  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
      "  q_1: ┤1                         ├┤1                                     ├\n",
      "       └──────────────────────────┘└──────────────────────────────────────┘\n",
      "  )\n",
      "])\n",
      "HybridQNN_Shallow(\n",
      "  (conv1): Conv2d(1, 2, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (qnn): TorchConnector()\n",
      "  (fc3): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Network init elapsed time: -0.0005568310152739286 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]+7-4)/2)-4)/2)**2)*16\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: -0.5056832432746887\n",
      "Batch 1, Loss: 0.14385509490966797\n",
      "Batch 2, Loss: -0.488931804895401\n",
      "Batch 3, Loss: -0.4851067066192627\n",
      "Batch 4, Loss: 0.1497575342655182\n",
      "Batch 5, Loss: 0.08839739114046097\n",
      "Batch 6, Loss: -0.5602617263793945\n",
      "Batch 7, Loss: -0.5492416024208069\n",
      "Batch 8, Loss: -0.8980317115783691\n",
      "Batch 9, Loss: -0.04677228629589081\n",
      "Batch 10, Loss: -0.6233863830566406\n",
      "Batch 11, Loss: -1.0160109996795654\n",
      "Batch 12, Loss: -0.1130186915397644\n",
      "Batch 13, Loss: -0.5707395672798157\n",
      "Batch 14, Loss: -0.17768342792987823\n",
      "Batch 15, Loss: -0.6320870518684387\n",
      "Batch 16, Loss: -0.9215679168701172\n",
      "Batch 17, Loss: -0.9640600681304932\n",
      "Batch 18, Loss: -0.5411238670349121\n",
      "Batch 19, Loss: -0.610205888748169\n",
      "Batch 20, Loss: -0.7539561986923218\n",
      "Batch 21, Loss: -0.12581348419189453\n",
      "Batch 22, Loss: -0.29224735498428345\n",
      "Batch 23, Loss: -0.3433738052845001\n",
      "Batch 24, Loss: -0.8078201413154602\n",
      "Batch 25, Loss: -0.47355520725250244\n",
      "Batch 26, Loss: -0.43272918462753296\n",
      "Batch 27, Loss: -0.7483267784118652\n",
      "Batch 28, Loss: -0.6487829685211182\n",
      "Batch 29, Loss: -0.41806653141975403\n",
      "Batch 30, Loss: -0.6727002263069153\n",
      "Batch 31, Loss: -0.42553597688674927\n",
      "Batch 32, Loss: -0.8141156435012817\n",
      "Batch 33, Loss: -0.46041733026504517\n",
      "Batch 34, Loss: -0.8881514668464661\n",
      "Batch 35, Loss: -1.008481740951538\n",
      "Batch 36, Loss: -0.24172380566596985\n",
      "Batch 37, Loss: -0.763078510761261\n",
      "Batch 38, Loss: -1.0114727020263672\n",
      "Batch 39, Loss: -0.2978144884109497\n",
      "Batch 40, Loss: -0.8030983805656433\n",
      "Batch 41, Loss: -1.2679413557052612\n",
      "Batch 42, Loss: -1.186601161956787\n",
      "Batch 43, Loss: -0.26400530338287354\n",
      "Batch 44, Loss: -0.8491361141204834\n",
      "Batch 45, Loss: -0.42645514011383057\n",
      "Batch 46, Loss: -0.7472426891326904\n",
      "Batch 47, Loss: -0.3949752151966095\n",
      "Batch 48, Loss: -0.920443594455719\n",
      "Batch 49, Loss: -1.2459430694580078\n",
      "Batch 50, Loss: -0.8978487253189087\n",
      "Batch 51, Loss: -0.7669011354446411\n",
      "Batch 52, Loss: -1.2859373092651367\n",
      "Batch 53, Loss: -0.7366697788238525\n",
      "Batch 54, Loss: -1.2576749324798584\n",
      "Batch 55, Loss: -0.827784538269043\n",
      "Batch 56, Loss: -0.894565224647522\n",
      "Batch 57, Loss: -0.6924872398376465\n",
      "Batch 58, Loss: -1.2796844244003296\n",
      "Batch 59, Loss: -1.2999999523162842\n",
      "Batch 60, Loss: -0.33081650733947754\n",
      "Batch 61, Loss: -0.8228787183761597\n",
      "Batch 62, Loss: -0.3854297399520874\n",
      "Batch 63, Loss: -0.7512407898902893\n",
      "Batch 64, Loss: -0.7556960582733154\n",
      "Batch 65, Loss: -1.243708848953247\n",
      "Batch 66, Loss: -0.8879836797714233\n",
      "Batch 67, Loss: -0.2448136955499649\n",
      "Batch 68, Loss: -1.2341872453689575\n",
      "Batch 69, Loss: -0.8389776945114136\n",
      "Batch 70, Loss: -0.6662468314170837\n",
      "Batch 71, Loss: -0.9643313884735107\n",
      "Batch 72, Loss: -0.9089177846908569\n",
      "Batch 73, Loss: -1.274300456047058\n",
      "Batch 74, Loss: -0.48257556557655334\n",
      "Batch 75, Loss: -0.9154796600341797\n",
      "Batch 76, Loss: -0.8828839063644409\n",
      "Batch 77, Loss: -1.3038077354431152\n",
      "Batch 78, Loss: -0.9357075095176697\n",
      "Batch 79, Loss: -0.954681932926178\n",
      "Batch 80, Loss: -0.4363483190536499\n",
      "Batch 81, Loss: -0.9100627303123474\n",
      "Batch 82, Loss: -0.7952263951301575\n",
      "Batch 83, Loss: -0.9300405383110046\n",
      "Batch 84, Loss: -0.9230332970619202\n",
      "Batch 85, Loss: -0.5824130773544312\n",
      "Batch 86, Loss: -0.8903683423995972\n",
      "Batch 87, Loss: -0.8650179505348206\n",
      "Batch 88, Loss: -1.211301565170288\n",
      "Batch 89, Loss: -1.364213228225708\n",
      "Batch 90, Loss: -0.4353792071342468\n",
      "Batch 91, Loss: -0.7729958295822144\n",
      "Batch 92, Loss: -0.5363036394119263\n",
      "Batch 93, Loss: -0.7320641875267029\n",
      "Batch 94, Loss: -0.7114877104759216\n",
      "Batch 95, Loss: -1.2704970836639404\n",
      "Batch 96, Loss: -0.5141640305519104\n",
      "Batch 97, Loss: -0.5350133776664734\n",
      "Batch 98, Loss: -1.2818629741668701\n",
      "Batch 99, Loss: -1.15639066696167\n",
      "Training [10%]\tLoss: -0.7263\n",
      "Batch 0, Loss: -1.3735816478729248\n",
      "Batch 1, Loss: -0.44680482149124146\n",
      "Batch 2, Loss: -0.8919070363044739\n",
      "Batch 3, Loss: -0.8634172081947327\n",
      "Batch 4, Loss: -1.1306761503219604\n",
      "Batch 5, Loss: -0.7651816010475159\n",
      "Batch 6, Loss: -1.1768901348114014\n",
      "Batch 7, Loss: -1.352001667022705\n",
      "Batch 8, Loss: -0.7819259166717529\n",
      "Batch 9, Loss: -1.3758479356765747\n",
      "Batch 10, Loss: -0.3998333811759949\n",
      "Batch 11, Loss: -0.9619196653366089\n",
      "Batch 12, Loss: -1.2901349067687988\n",
      "Batch 13, Loss: -0.27953359484672546\n",
      "Batch 14, Loss: -1.237749695777893\n",
      "Batch 15, Loss: -0.897431492805481\n",
      "Batch 16, Loss: -0.836219072341919\n",
      "Batch 17, Loss: -0.5011969804763794\n",
      "Batch 18, Loss: -0.17033447325229645\n",
      "Batch 19, Loss: -0.48413413763046265\n",
      "Batch 20, Loss: -0.5461233854293823\n",
      "Batch 21, Loss: -0.9180518984794617\n",
      "Batch 22, Loss: -1.365943431854248\n",
      "Batch 23, Loss: -0.8555148839950562\n",
      "Batch 24, Loss: -0.8047945499420166\n",
      "Batch 25, Loss: -1.315883994102478\n",
      "Batch 26, Loss: -0.9044210314750671\n",
      "Batch 27, Loss: -0.11297043412923813\n",
      "Batch 28, Loss: -0.4941636025905609\n",
      "Batch 29, Loss: -0.9978752136230469\n",
      "Batch 30, Loss: -1.287804126739502\n",
      "Batch 31, Loss: -0.42331114411354065\n",
      "Batch 32, Loss: -0.5877482295036316\n",
      "Batch 33, Loss: -1.2550022602081299\n",
      "Batch 34, Loss: -0.9865856766700745\n",
      "Batch 35, Loss: -0.5283364653587341\n",
      "Batch 36, Loss: -0.9846361875534058\n",
      "Batch 37, Loss: -0.48659059405326843\n",
      "Batch 38, Loss: -0.4652690291404724\n",
      "Batch 39, Loss: -0.9859782457351685\n",
      "Batch 40, Loss: -0.5220727324485779\n",
      "Batch 41, Loss: -0.9365501403808594\n",
      "Batch 42, Loss: -1.3934803009033203\n",
      "Batch 43, Loss: -0.9279254078865051\n",
      "Batch 44, Loss: -0.8923149108886719\n",
      "Batch 45, Loss: -1.2225762605667114\n",
      "Batch 46, Loss: -1.4103074073791504\n",
      "Batch 47, Loss: -0.5675681233406067\n",
      "Batch 48, Loss: -0.5197878479957581\n",
      "Batch 49, Loss: -1.4046860933303833\n",
      "Batch 50, Loss: -1.4294428825378418\n",
      "Batch 51, Loss: -1.4319448471069336\n",
      "Batch 52, Loss: -0.4895856976509094\n",
      "Batch 53, Loss: -0.8556280732154846\n",
      "Batch 54, Loss: -1.4095537662506104\n",
      "Batch 55, Loss: -1.0248076915740967\n",
      "Batch 56, Loss: -1.019148826599121\n",
      "Batch 57, Loss: -0.964044451713562\n",
      "Batch 58, Loss: -0.9960887432098389\n",
      "Batch 59, Loss: -0.5488997101783752\n",
      "Batch 60, Loss: -0.11262629926204681\n",
      "Batch 61, Loss: -0.6092958450317383\n",
      "Batch 62, Loss: -1.0283093452453613\n",
      "Batch 63, Loss: -1.3272788524627686\n",
      "Batch 64, Loss: -0.8869637250900269\n",
      "Batch 65, Loss: -0.9702593684196472\n",
      "Batch 66, Loss: -1.4627816677093506\n",
      "Batch 67, Loss: -0.8790177702903748\n",
      "Batch 68, Loss: -1.0196213722229004\n",
      "Batch 69, Loss: -1.0046234130859375\n",
      "Batch 70, Loss: -1.0103133916854858\n",
      "Batch 71, Loss: -0.6135859489440918\n",
      "Batch 72, Loss: -1.0534083843231201\n",
      "Batch 73, Loss: -0.6089291572570801\n",
      "Batch 74, Loss: -0.7914125919342041\n",
      "Batch 75, Loss: -1.018130898475647\n",
      "Batch 76, Loss: -0.6231081485748291\n",
      "Batch 77, Loss: -1.0272953510284424\n",
      "Batch 78, Loss: -1.316100835800171\n",
      "Batch 79, Loss: -0.9724167585372925\n",
      "Batch 80, Loss: -1.0330822467803955\n",
      "Batch 81, Loss: -1.4766862392425537\n",
      "Batch 82, Loss: -1.4607939720153809\n",
      "Batch 83, Loss: -0.6282814741134644\n",
      "Batch 84, Loss: -0.9388561844825745\n",
      "Batch 85, Loss: -1.1029279232025146\n",
      "Batch 86, Loss: -1.0256551504135132\n",
      "Batch 87, Loss: -0.6080596446990967\n",
      "Batch 88, Loss: -0.5057520866394043\n",
      "Batch 89, Loss: -0.8245499134063721\n",
      "Batch 90, Loss: -0.6126221418380737\n",
      "Batch 91, Loss: -0.9781270027160645\n",
      "Batch 92, Loss: -1.0075234174728394\n",
      "Batch 93, Loss: -0.6432316303253174\n",
      "Batch 94, Loss: -0.9083350300788879\n",
      "Batch 95, Loss: -0.8191099166870117\n",
      "Batch 96, Loss: -1.0309640169143677\n",
      "Batch 97, Loss: -1.041875958442688\n",
      "Batch 98, Loss: -1.434777021408081\n",
      "Batch 99, Loss: -0.9244887232780457\n",
      "Training [20%]\tLoss: -0.9083\n",
      "Batch 0, Loss: -1.288741946220398\n",
      "Batch 1, Loss: -1.061633586883545\n",
      "Batch 2, Loss: -0.6497937440872192\n",
      "Batch 3, Loss: -1.1659045219421387\n",
      "Batch 4, Loss: -0.5143322944641113\n",
      "Batch 5, Loss: -0.9636111855506897\n",
      "Batch 6, Loss: -1.0537607669830322\n",
      "Batch 7, Loss: -1.0564873218536377\n",
      "Batch 8, Loss: -1.067895770072937\n",
      "Batch 9, Loss: -1.0603306293487549\n",
      "Batch 10, Loss: -1.4880712032318115\n",
      "Batch 11, Loss: -1.0356959104537964\n",
      "Batch 12, Loss: -1.090588092803955\n",
      "Batch 13, Loss: -1.0085039138793945\n",
      "Batch 14, Loss: -1.0835686922073364\n",
      "Batch 15, Loss: -1.1048507690429688\n",
      "Batch 16, Loss: -0.6137715578079224\n",
      "Batch 17, Loss: -1.0204389095306396\n",
      "Batch 18, Loss: -1.0828036069869995\n",
      "Batch 19, Loss: -1.4829331636428833\n",
      "Batch 20, Loss: -0.9945935606956482\n",
      "Batch 21, Loss: -1.0247666835784912\n",
      "Batch 22, Loss: -1.0920474529266357\n",
      "Batch 23, Loss: -0.7033317685127258\n",
      "Batch 24, Loss: -1.0065395832061768\n",
      "Batch 25, Loss: -1.5259321928024292\n",
      "Batch 26, Loss: -0.6509768962860107\n",
      "Batch 27, Loss: -0.6966134309768677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28, Loss: -1.1222434043884277\n",
      "Batch 29, Loss: -0.6587448120117188\n",
      "Batch 30, Loss: -0.9544551372528076\n",
      "Batch 31, Loss: -1.0531328916549683\n",
      "Batch 32, Loss: -1.525923490524292\n",
      "Batch 33, Loss: -1.0911920070648193\n",
      "Batch 34, Loss: -1.118391752243042\n",
      "Batch 35, Loss: -0.7253429889678955\n",
      "Batch 36, Loss: -1.0608036518096924\n",
      "Batch 37, Loss: -1.0931257009506226\n",
      "Batch 38, Loss: -1.0461227893829346\n",
      "Batch 39, Loss: -1.1372745037078857\n",
      "Batch 40, Loss: -1.4375524520874023\n",
      "Batch 41, Loss: -0.7264811396598816\n",
      "Batch 42, Loss: -1.5383902788162231\n",
      "Batch 43, Loss: -1.116580843925476\n",
      "Batch 44, Loss: -0.7024604678153992\n",
      "Batch 45, Loss: -0.7072330117225647\n",
      "Batch 46, Loss: -1.0604522228240967\n",
      "Batch 47, Loss: -1.1441551446914673\n",
      "Batch 48, Loss: -1.1439082622528076\n",
      "Batch 49, Loss: -1.3959978818893433\n",
      "Batch 50, Loss: -1.1517179012298584\n",
      "Batch 51, Loss: -1.1126701831817627\n",
      "Batch 52, Loss: -1.1694824695587158\n",
      "Batch 53, Loss: -1.072394609451294\n",
      "Batch 54, Loss: -1.1414117813110352\n",
      "Batch 55, Loss: -0.965233564376831\n",
      "Batch 56, Loss: -1.443993091583252\n",
      "Batch 57, Loss: -1.4815495014190674\n",
      "Batch 58, Loss: -0.9781489372253418\n",
      "Batch 59, Loss: -1.1162984371185303\n",
      "Batch 60, Loss: -1.1017224788665771\n",
      "Batch 61, Loss: -1.5515307188034058\n",
      "Batch 62, Loss: -1.1690932512283325\n",
      "Batch 63, Loss: -1.4634467363357544\n",
      "Batch 64, Loss: -0.7572910785675049\n",
      "Batch 65, Loss: -0.1958320438861847\n",
      "Batch 66, Loss: -1.143232822418213\n",
      "Batch 67, Loss: -1.1743218898773193\n",
      "Batch 68, Loss: -1.0877023935317993\n",
      "Batch 69, Loss: -1.1066131591796875\n",
      "Batch 70, Loss: -0.7262861728668213\n",
      "Batch 71, Loss: -1.5642409324645996\n",
      "Batch 72, Loss: -1.168891429901123\n",
      "Batch 73, Loss: -1.127401351928711\n",
      "Batch 74, Loss: -1.1366262435913086\n",
      "Batch 75, Loss: -1.1492668390274048\n",
      "Batch 76, Loss: -1.1187667846679688\n",
      "Batch 77, Loss: -1.4280654191970825\n",
      "Batch 78, Loss: -0.8567171096801758\n",
      "Batch 79, Loss: -1.1287823915481567\n",
      "Batch 80, Loss: -0.7355275750160217\n",
      "Batch 81, Loss: -0.7348906993865967\n",
      "Batch 82, Loss: -1.6080901622772217\n",
      "Batch 83, Loss: -1.1218955516815186\n",
      "Batch 84, Loss: -1.1823209524154663\n",
      "Batch 85, Loss: -1.206732153892517\n",
      "Batch 86, Loss: -1.1270267963409424\n",
      "Batch 87, Loss: -1.2010829448699951\n",
      "Batch 88, Loss: -0.7971099615097046\n",
      "Batch 89, Loss: -1.159593105316162\n",
      "Batch 90, Loss: -0.7354325652122498\n",
      "Batch 91, Loss: -0.805626392364502\n",
      "Batch 92, Loss: -1.3556373119354248\n",
      "Batch 93, Loss: -1.1384066343307495\n",
      "Batch 94, Loss: -0.9058623313903809\n",
      "Batch 95, Loss: -0.7830647230148315\n",
      "Batch 96, Loss: -0.7081660628318787\n",
      "Batch 97, Loss: -1.5215028524398804\n",
      "Batch 98, Loss: -1.6210730075836182\n",
      "Batch 99, Loss: -1.0486234426498413\n",
      "Training [30%]\tLoss: -1.0743\n",
      "Batch 0, Loss: -1.0818537473678589\n",
      "Batch 1, Loss: -1.081495761871338\n",
      "Batch 2, Loss: -1.2381675243377686\n",
      "Batch 3, Loss: -1.5791914463043213\n",
      "Batch 4, Loss: -0.9788936376571655\n",
      "Batch 5, Loss: -1.2046167850494385\n",
      "Batch 6, Loss: -1.195831060409546\n",
      "Batch 7, Loss: -0.6714778542518616\n",
      "Batch 8, Loss: -0.9615775346755981\n",
      "Batch 9, Loss: -1.198728084564209\n",
      "Batch 10, Loss: -0.7648285031318665\n",
      "Batch 11, Loss: -0.9470653533935547\n",
      "Batch 12, Loss: -0.6806727647781372\n",
      "Batch 13, Loss: -1.1375550031661987\n",
      "Batch 14, Loss: -0.9117783308029175\n",
      "Batch 15, Loss: -1.1998145580291748\n",
      "Batch 16, Loss: -1.179677128791809\n",
      "Batch 17, Loss: -1.210304856300354\n",
      "Batch 18, Loss: -1.562264323234558\n",
      "Batch 19, Loss: -1.5426664352416992\n",
      "Batch 20, Loss: -1.2295808792114258\n",
      "Batch 21, Loss: -1.226397156715393\n",
      "Batch 22, Loss: -1.648695707321167\n",
      "Batch 23, Loss: -1.154090404510498\n",
      "Batch 24, Loss: -1.184005618095398\n",
      "Batch 25, Loss: -1.087672233581543\n",
      "Batch 26, Loss: -0.6959459185600281\n",
      "Batch 27, Loss: -0.8596572875976562\n",
      "Batch 28, Loss: -0.5459334254264832\n",
      "Batch 29, Loss: -1.2483443021774292\n",
      "Batch 30, Loss: -0.8746878504753113\n",
      "Batch 31, Loss: -1.2435641288757324\n",
      "Batch 32, Loss: -1.6420257091522217\n",
      "Batch 33, Loss: -0.8443927764892578\n",
      "Batch 34, Loss: -0.8092333674430847\n",
      "Batch 35, Loss: -0.8667894601821899\n",
      "Batch 36, Loss: -1.2231945991516113\n",
      "Batch 37, Loss: -1.4683492183685303\n",
      "Batch 38, Loss: -1.1898140907287598\n",
      "Batch 39, Loss: -1.4665002822875977\n",
      "Batch 40, Loss: -0.8053611516952515\n",
      "Batch 41, Loss: -1.6285805702209473\n",
      "Batch 42, Loss: -1.2364848852157593\n",
      "Batch 43, Loss: -1.2196251153945923\n",
      "Batch 44, Loss: -1.6236858367919922\n",
      "Batch 45, Loss: -1.1997814178466797\n",
      "Batch 46, Loss: -1.6175320148468018\n",
      "Batch 47, Loss: -1.2499122619628906\n",
      "Batch 48, Loss: -1.2897650003433228\n",
      "Batch 49, Loss: -0.765525758266449\n",
      "Batch 50, Loss: -1.668419599533081\n",
      "Batch 51, Loss: -0.8098961114883423\n",
      "Batch 52, Loss: -0.9066594839096069\n",
      "Batch 53, Loss: -1.3529400825500488\n",
      "Batch 54, Loss: -1.6784727573394775\n",
      "Batch 55, Loss: -1.2779428958892822\n",
      "Batch 56, Loss: -1.5540151596069336\n",
      "Batch 57, Loss: -1.2691339254379272\n",
      "Batch 58, Loss: -1.2787576913833618\n",
      "Batch 59, Loss: -1.6849455833435059\n",
      "Batch 60, Loss: -1.195349097251892\n",
      "Batch 61, Loss: -1.1531000137329102\n",
      "Batch 62, Loss: -0.9003510475158691\n",
      "Batch 63, Loss: -1.6474769115447998\n",
      "Batch 64, Loss: -1.6745409965515137\n",
      "Batch 65, Loss: -1.304762363433838\n",
      "Batch 66, Loss: -1.1721811294555664\n",
      "Batch 67, Loss: -1.3044373989105225\n",
      "Batch 68, Loss: -1.2274824380874634\n",
      "Batch 69, Loss: -0.8335739374160767\n",
      "Batch 70, Loss: -1.3168996572494507\n",
      "Batch 71, Loss: -1.6600749492645264\n",
      "Batch 72, Loss: -1.2935123443603516\n",
      "Batch 73, Loss: -1.3104326725006104\n",
      "Batch 74, Loss: -0.8432220220565796\n",
      "Batch 75, Loss: -0.9221832752227783\n",
      "Batch 76, Loss: -1.2014343738555908\n",
      "Batch 77, Loss: -1.7176437377929688\n",
      "Batch 78, Loss: -0.9337888956069946\n",
      "Batch 79, Loss: -0.8826393485069275\n",
      "Batch 80, Loss: -1.2262507677078247\n",
      "Batch 81, Loss: -1.5771626234054565\n",
      "Batch 82, Loss: -1.6606507301330566\n",
      "Batch 83, Loss: -1.6888048648834229\n",
      "Batch 84, Loss: -1.6095185279846191\n",
      "Batch 85, Loss: -0.9516357779502869\n",
      "Batch 86, Loss: -0.6951916217803955\n",
      "Batch 87, Loss: -0.6245507597923279\n",
      "Batch 88, Loss: -0.9074274897575378\n",
      "Batch 89, Loss: -1.3105103969573975\n",
      "Batch 90, Loss: -1.3364169597625732\n",
      "Batch 91, Loss: -1.3139467239379883\n",
      "Batch 92, Loss: -1.1252470016479492\n",
      "Batch 93, Loss: -1.3101454973220825\n",
      "Batch 94, Loss: -0.9279322624206543\n",
      "Batch 95, Loss: -1.2997775077819824\n",
      "Batch 96, Loss: -1.6958788633346558\n",
      "Batch 97, Loss: -1.1984575986862183\n",
      "Batch 98, Loss: -0.9604590535163879\n",
      "Batch 99, Loss: -1.3064210414886475\n",
      "Training [40%]\tLoss: -1.2018\n",
      "Batch 0, Loss: -0.9283303022384644\n",
      "Batch 1, Loss: -1.1014517545700073\n",
      "Batch 2, Loss: -1.2442424297332764\n",
      "Batch 3, Loss: -0.8354848623275757\n",
      "Batch 4, Loss: -1.3607593774795532\n",
      "Batch 5, Loss: -0.9587794542312622\n",
      "Batch 6, Loss: -0.9693126678466797\n",
      "Batch 7, Loss: -1.7191886901855469\n",
      "Batch 8, Loss: -1.1470112800598145\n",
      "Batch 9, Loss: -1.3291234970092773\n",
      "Batch 10, Loss: -1.3445171117782593\n",
      "Batch 11, Loss: -1.7288168668746948\n",
      "Batch 12, Loss: -0.9477640390396118\n",
      "Batch 13, Loss: -1.3380379676818848\n",
      "Batch 14, Loss: -1.3072905540466309\n",
      "Batch 15, Loss: -1.7494977712631226\n",
      "Batch 16, Loss: -1.3021941184997559\n",
      "Batch 17, Loss: -0.9986218214035034\n",
      "Batch 18, Loss: -0.9613233804702759\n",
      "Batch 19, Loss: -0.9572079181671143\n",
      "Batch 20, Loss: -0.9919023513793945\n",
      "Batch 21, Loss: -1.3657270669937134\n",
      "Batch 22, Loss: -0.8380023241043091\n",
      "Batch 23, Loss: -1.7108193635940552\n",
      "Batch 24, Loss: -1.4018962383270264\n",
      "Batch 25, Loss: -1.357804536819458\n",
      "Batch 26, Loss: -1.7674100399017334\n",
      "Batch 27, Loss: -1.384467363357544\n",
      "Batch 28, Loss: -1.3332419395446777\n",
      "Batch 29, Loss: -1.2893577814102173\n",
      "Batch 30, Loss: -1.63911771774292\n",
      "Batch 31, Loss: -1.347400426864624\n",
      "Batch 32, Loss: -1.3845398426055908\n",
      "Batch 33, Loss: -1.0074795484542847\n",
      "Batch 34, Loss: -1.3859894275665283\n",
      "Batch 35, Loss: -1.0304828882217407\n",
      "Batch 36, Loss: -1.3841590881347656\n",
      "Batch 37, Loss: -1.7684595584869385\n",
      "Batch 38, Loss: -1.0251739025115967\n",
      "Batch 39, Loss: -1.7604933977127075\n",
      "Batch 40, Loss: -1.3832104206085205\n",
      "Batch 41, Loss: -1.217210292816162\n",
      "Batch 42, Loss: -0.9525586366653442\n",
      "Batch 43, Loss: -1.7871452569961548\n",
      "Batch 44, Loss: -1.3078229427337646\n",
      "Batch 45, Loss: -1.6990437507629395\n",
      "Batch 46, Loss: -0.6751444339752197\n",
      "Batch 47, Loss: -1.7600734233856201\n",
      "Batch 48, Loss: -1.3833050727844238\n",
      "Batch 49, Loss: -0.9363167881965637\n",
      "Batch 50, Loss: -0.9984504580497742\n",
      "Batch 51, Loss: -0.9472044706344604\n",
      "Batch 52, Loss: -1.4155827760696411\n",
      "Batch 53, Loss: -1.695634365081787\n",
      "Batch 54, Loss: -1.0551142692565918\n",
      "Batch 55, Loss: -1.3770828247070312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 56, Loss: -1.3966457843780518\n",
      "Batch 57, Loss: -1.750199794769287\n",
      "Batch 58, Loss: -0.939640462398529\n",
      "Batch 59, Loss: -1.7984416484832764\n",
      "Batch 60, Loss: -1.7300082445144653\n",
      "Batch 61, Loss: -1.4413244724273682\n",
      "Batch 62, Loss: -1.6487374305725098\n",
      "Batch 63, Loss: -1.3737454414367676\n",
      "Batch 64, Loss: -1.1090102195739746\n",
      "Batch 65, Loss: -1.024247169494629\n",
      "Batch 66, Loss: -1.034714698791504\n",
      "Batch 67, Loss: -1.782904863357544\n",
      "Batch 68, Loss: -1.3939954042434692\n",
      "Batch 69, Loss: -1.3977826833724976\n",
      "Batch 70, Loss: -1.451592206954956\n",
      "Batch 71, Loss: -1.437166452407837\n",
      "Batch 72, Loss: -1.3999594449996948\n",
      "Batch 73, Loss: -1.0782432556152344\n",
      "Batch 74, Loss: -0.9758521914482117\n",
      "Batch 75, Loss: -1.4114289283752441\n",
      "Batch 76, Loss: -1.8406801223754883\n",
      "Batch 77, Loss: -1.4370189905166626\n",
      "Batch 78, Loss: -1.2861614227294922\n",
      "Batch 79, Loss: -1.4108068943023682\n",
      "Batch 80, Loss: -0.995294451713562\n",
      "Batch 81, Loss: -1.8374617099761963\n",
      "Batch 82, Loss: -1.8290741443634033\n",
      "Batch 83, Loss: -0.9730249047279358\n",
      "Batch 84, Loss: -1.4025896787643433\n",
      "Batch 85, Loss: -1.0420036315917969\n",
      "Batch 86, Loss: -1.4013710021972656\n",
      "Batch 87, Loss: -1.284393310546875\n",
      "Batch 88, Loss: -1.8321374654769897\n",
      "Batch 89, Loss: -1.4618114233016968\n",
      "Batch 90, Loss: -1.0653643608093262\n",
      "Batch 91, Loss: -1.0079474449157715\n",
      "Batch 92, Loss: -1.8171281814575195\n",
      "Batch 93, Loss: -1.7816927433013916\n",
      "Batch 94, Loss: -1.3219836950302124\n",
      "Batch 95, Loss: -0.9728693962097168\n",
      "Batch 96, Loss: -1.3969837427139282\n",
      "Batch 97, Loss: -1.438979148864746\n",
      "Batch 98, Loss: -1.8644087314605713\n",
      "Batch 99, Loss: -0.9862942695617676\n",
      "Training [50%]\tLoss: -1.3326\n",
      "Batch 0, Loss: -1.0352275371551514\n",
      "Batch 1, Loss: -1.8399955034255981\n",
      "Batch 2, Loss: -1.4781768321990967\n",
      "Batch 3, Loss: -1.8026950359344482\n",
      "Batch 4, Loss: -1.4013619422912598\n",
      "Batch 5, Loss: -1.0906753540039062\n",
      "Batch 6, Loss: -1.8730583190917969\n",
      "Batch 7, Loss: -1.4814363718032837\n",
      "Batch 8, Loss: -1.4288004636764526\n",
      "Batch 9, Loss: -1.4102308750152588\n",
      "Batch 10, Loss: -1.0743076801300049\n",
      "Batch 11, Loss: -1.011286973953247\n",
      "Batch 12, Loss: -1.029295802116394\n",
      "Batch 13, Loss: -1.8542873859405518\n",
      "Batch 14, Loss: -0.988288164138794\n",
      "Batch 15, Loss: -0.9960894584655762\n",
      "Batch 16, Loss: -1.4395380020141602\n",
      "Batch 17, Loss: -1.8777761459350586\n",
      "Batch 18, Loss: -1.0385572910308838\n",
      "Batch 19, Loss: -1.118208885192871\n",
      "Batch 20, Loss: -0.9603433012962341\n",
      "Batch 21, Loss: -1.4392026662826538\n",
      "Batch 22, Loss: -1.4523510932922363\n",
      "Batch 23, Loss: -1.4948267936706543\n",
      "Batch 24, Loss: -1.4912408590316772\n",
      "Batch 25, Loss: -1.1146376132965088\n",
      "Batch 26, Loss: -1.46518075466156\n",
      "Batch 27, Loss: -1.4789907932281494\n",
      "Batch 28, Loss: -1.293757438659668\n",
      "Batch 29, Loss: -1.8330508470535278\n",
      "Batch 30, Loss: -1.4812054634094238\n",
      "Batch 31, Loss: -1.8773022890090942\n",
      "Batch 32, Loss: -1.4707527160644531\n",
      "Batch 33, Loss: -1.4601409435272217\n",
      "Batch 34, Loss: -1.473646640777588\n",
      "Batch 35, Loss: -1.9030741453170776\n",
      "Batch 36, Loss: -1.2336678504943848\n",
      "Batch 37, Loss: -1.092965006828308\n",
      "Batch 38, Loss: -1.1045479774475098\n",
      "Batch 39, Loss: -1.4522547721862793\n",
      "Batch 40, Loss: -1.4560203552246094\n",
      "Batch 41, Loss: -1.505417823791504\n",
      "Batch 42, Loss: -1.5221083164215088\n",
      "Batch 43, Loss: -1.9000245332717896\n",
      "Batch 44, Loss: -1.9139703512191772\n",
      "Batch 45, Loss: -1.8475468158721924\n",
      "Batch 46, Loss: -1.5216600894927979\n",
      "Batch 47, Loss: -1.0917491912841797\n",
      "Batch 48, Loss: -1.5095365047454834\n",
      "Batch 49, Loss: -1.9072871208190918\n",
      "Batch 50, Loss: -1.1541378498077393\n",
      "Batch 51, Loss: -1.1300835609436035\n",
      "Batch 52, Loss: -1.122602939605713\n",
      "Batch 53, Loss: -1.8571851253509521\n",
      "Batch 54, Loss: -1.5208441019058228\n",
      "Batch 55, Loss: -1.2613792419433594\n",
      "Batch 56, Loss: -1.5253865718841553\n",
      "Batch 57, Loss: -1.4894616603851318\n",
      "Batch 58, Loss: -1.3214797973632812\n",
      "Batch 59, Loss: -1.8664450645446777\n",
      "Batch 60, Loss: -1.525341510772705\n",
      "Batch 61, Loss: -1.9237689971923828\n",
      "Batch 62, Loss: -1.1110799312591553\n",
      "Batch 63, Loss: -1.146568775177002\n",
      "Batch 64, Loss: -1.5325534343719482\n",
      "Batch 65, Loss: -1.1276943683624268\n",
      "Batch 66, Loss: -1.9391777515411377\n",
      "Batch 67, Loss: -1.1625996828079224\n",
      "Batch 68, Loss: -1.3858990669250488\n",
      "Batch 69, Loss: -1.9386216402053833\n",
      "Batch 70, Loss: -1.7414323091506958\n",
      "Batch 71, Loss: -1.4997414350509644\n",
      "Batch 72, Loss: -1.9330353736877441\n",
      "Batch 73, Loss: -1.349363088607788\n",
      "Batch 74, Loss: -1.902953863143921\n",
      "Batch 75, Loss: -1.8902899026870728\n",
      "Batch 76, Loss: -1.407320499420166\n",
      "Batch 77, Loss: -1.1115188598632812\n",
      "Batch 78, Loss: -1.1702557802200317\n",
      "Batch 79, Loss: -1.529721736907959\n",
      "Batch 80, Loss: -1.483616590499878\n",
      "Batch 81, Loss: -1.9537032842636108\n",
      "Batch 82, Loss: -1.4427525997161865\n",
      "Batch 83, Loss: -1.5544885396957397\n",
      "Batch 84, Loss: -1.2696490287780762\n",
      "Batch 85, Loss: -1.5630208253860474\n",
      "Batch 86, Loss: -1.1756151914596558\n",
      "Batch 87, Loss: -1.4028260707855225\n",
      "Batch 88, Loss: -1.4946321249008179\n",
      "Batch 89, Loss: -1.5052293539047241\n",
      "Batch 90, Loss: -1.977196455001831\n",
      "Batch 91, Loss: -1.4425842761993408\n",
      "Batch 92, Loss: -1.9616146087646484\n",
      "Batch 93, Loss: -1.144789695739746\n",
      "Batch 94, Loss: -0.07561713457107544\n",
      "Batch 95, Loss: -0.9510201215744019\n",
      "Batch 96, Loss: -1.5836222171783447\n",
      "Batch 97, Loss: -1.1670711040496826\n",
      "Batch 98, Loss: -1.1812334060668945\n",
      "Batch 99, Loss: -1.8853334188461304\n",
      "Training [60%]\tLoss: -1.4481\n",
      "Batch 0, Loss: -1.1355111598968506\n",
      "Batch 1, Loss: -1.5765631198883057\n",
      "Batch 2, Loss: -1.44590163230896\n",
      "Batch 3, Loss: -1.594618320465088\n",
      "Batch 4, Loss: -1.1903584003448486\n",
      "Batch 5, Loss: -1.9689712524414062\n",
      "Batch 6, Loss: -1.5655536651611328\n",
      "Batch 7, Loss: -1.5403097867965698\n",
      "Batch 8, Loss: -1.5377644300460815\n",
      "Batch 9, Loss: -1.5856761932373047\n",
      "Batch 10, Loss: -1.957280158996582\n",
      "Batch 11, Loss: -1.5657216310501099\n",
      "Batch 12, Loss: -1.539367437362671\n",
      "Batch 13, Loss: -1.9798890352249146\n",
      "Batch 14, Loss: -1.0689301490783691\n",
      "Batch 15, Loss: -1.5610620975494385\n",
      "Batch 16, Loss: -1.9678189754486084\n",
      "Batch 17, Loss: -1.5280096530914307\n",
      "Batch 18, Loss: -1.5939916372299194\n",
      "Batch 19, Loss: -1.202226996421814\n",
      "Batch 20, Loss: -1.5845507383346558\n",
      "Batch 21, Loss: -1.6017241477966309\n",
      "Batch 22, Loss: -1.5739161968231201\n",
      "Batch 23, Loss: -1.9863014221191406\n",
      "Batch 24, Loss: -1.4726598262786865\n",
      "Batch 25, Loss: -1.5307526588439941\n",
      "Batch 26, Loss: -1.5779322385787964\n",
      "Batch 27, Loss: -1.1379063129425049\n",
      "Batch 28, Loss: -1.1944432258605957\n",
      "Batch 29, Loss: -1.2282025814056396\n",
      "Batch 30, Loss: -1.166710615158081\n",
      "Batch 31, Loss: -1.583969235420227\n",
      "Batch 32, Loss: -1.5278615951538086\n",
      "Batch 33, Loss: -1.9983820915222168\n",
      "Batch 34, Loss: -1.5883464813232422\n",
      "Batch 35, Loss: -0.9609354138374329\n",
      "Batch 36, Loss: -1.5475441217422485\n",
      "Batch 37, Loss: -1.9393699169158936\n",
      "Batch 38, Loss: -1.602914571762085\n",
      "Batch 39, Loss: -1.595558762550354\n",
      "Batch 40, Loss: -1.9523024559020996\n",
      "Batch 41, Loss: -1.579937219619751\n",
      "Batch 42, Loss: -1.6131291389465332\n",
      "Batch 43, Loss: -1.5989997386932373\n",
      "Batch 44, Loss: -1.6183867454528809\n",
      "Batch 45, Loss: -1.0921552181243896\n",
      "Batch 46, Loss: -1.6233700513839722\n",
      "Batch 47, Loss: -1.5951519012451172\n",
      "Batch 48, Loss: -1.0440876483917236\n",
      "Batch 49, Loss: -1.970867395401001\n",
      "Batch 50, Loss: -1.9610488414764404\n",
      "Batch 51, Loss: -1.6107453107833862\n",
      "Batch 52, Loss: -1.251185417175293\n",
      "Batch 53, Loss: -1.1705822944641113\n",
      "Batch 54, Loss: -1.2308669090270996\n",
      "Batch 55, Loss: -1.07990562915802\n",
      "Batch 56, Loss: -1.6211880445480347\n",
      "Batch 57, Loss: -2.0219719409942627\n",
      "Batch 58, Loss: -1.657662034034729\n",
      "Batch 59, Loss: -1.2241833209991455\n",
      "Batch 60, Loss: -1.2629225254058838\n",
      "Batch 61, Loss: -1.9353232383728027\n",
      "Batch 62, Loss: -1.423513650894165\n",
      "Batch 63, Loss: -1.635272741317749\n",
      "Batch 64, Loss: -1.5785250663757324\n",
      "Batch 65, Loss: -1.6380951404571533\n",
      "Batch 66, Loss: -1.4879770278930664\n",
      "Batch 67, Loss: -1.598402976989746\n",
      "Batch 68, Loss: -1.5649774074554443\n",
      "Batch 69, Loss: -1.653642177581787\n",
      "Batch 70, Loss: -1.5559685230255127\n",
      "Batch 71, Loss: -1.2674195766448975\n",
      "Batch 72, Loss: -2.054506778717041\n",
      "Batch 73, Loss: -2.046654462814331\n",
      "Batch 74, Loss: -1.6468720436096191\n",
      "Batch 75, Loss: -1.5354870557785034\n",
      "Batch 76, Loss: -2.054866075515747\n",
      "Batch 77, Loss: -2.0381851196289062\n",
      "Batch 78, Loss: -1.1818453073501587\n",
      "Batch 79, Loss: -1.1919969320297241\n",
      "Batch 80, Loss: -1.9805697202682495\n",
      "Batch 81, Loss: -1.99064040184021\n",
      "Batch 82, Loss: -1.651193618774414\n",
      "Batch 83, Loss: -1.6122660636901855\n",
      "Batch 84, Loss: -1.4691303968429565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 85, Loss: -2.0535340309143066\n",
      "Batch 86, Loss: -1.6887116432189941\n",
      "Batch 87, Loss: -1.3254656791687012\n",
      "Batch 88, Loss: -1.3017351627349854\n",
      "Batch 89, Loss: -1.26919424533844\n",
      "Batch 90, Loss: -1.289764404296875\n",
      "Batch 91, Loss: -1.6264653205871582\n",
      "Batch 92, Loss: -1.5572776794433594\n",
      "Batch 93, Loss: -1.0064953565597534\n",
      "Batch 94, Loss: -1.5264439582824707\n",
      "Batch 95, Loss: -1.5675703287124634\n",
      "Batch 96, Loss: -1.657626986503601\n",
      "Batch 97, Loss: -1.151540756225586\n",
      "Batch 98, Loss: -2.0199766159057617\n",
      "Batch 99, Loss: -1.3049629926681519\n",
      "Training [70%]\tLoss: -1.5543\n",
      "Batch 0, Loss: -2.089456558227539\n",
      "Batch 1, Loss: -1.7033250331878662\n",
      "Batch 2, Loss: -1.6901851892471313\n",
      "Batch 3, Loss: -1.3100296258926392\n",
      "Batch 4, Loss: -1.6669607162475586\n",
      "Batch 5, Loss: -1.2262487411499023\n",
      "Batch 6, Loss: -1.9898145198822021\n",
      "Batch 7, Loss: -1.9915109872817993\n",
      "Batch 8, Loss: -2.0393707752227783\n",
      "Batch 9, Loss: -0.9293298721313477\n",
      "Batch 10, Loss: -1.2982280254364014\n",
      "Batch 11, Loss: -1.8917369842529297\n",
      "Batch 12, Loss: -2.0257229804992676\n",
      "Batch 13, Loss: -1.2309398651123047\n",
      "Batch 14, Loss: -1.7090264558792114\n",
      "Batch 15, Loss: -1.1646366119384766\n",
      "Batch 16, Loss: -1.2647016048431396\n",
      "Batch 17, Loss: -1.5239579677581787\n",
      "Batch 18, Loss: -1.3207612037658691\n",
      "Batch 19, Loss: -2.0583434104919434\n",
      "Batch 20, Loss: -1.6597044467926025\n",
      "Batch 21, Loss: -1.6233524084091187\n",
      "Batch 22, Loss: -1.222381353378296\n",
      "Batch 23, Loss: -1.2956762313842773\n",
      "Batch 24, Loss: -1.6965036392211914\n",
      "Batch 25, Loss: -1.3029351234436035\n",
      "Batch 26, Loss: -1.140191674232483\n",
      "Batch 27, Loss: -1.6511318683624268\n",
      "Batch 28, Loss: -1.7193586826324463\n",
      "Batch 29, Loss: -1.7219915390014648\n",
      "Batch 30, Loss: -1.5975615978240967\n",
      "Batch 31, Loss: -2.094665050506592\n",
      "Batch 32, Loss: -2.077972888946533\n",
      "Batch 33, Loss: -1.7513043880462646\n",
      "Batch 34, Loss: -1.7336692810058594\n",
      "Batch 35, Loss: -1.7300913333892822\n",
      "Batch 36, Loss: -1.0942494869232178\n",
      "Batch 37, Loss: -1.2430758476257324\n",
      "Batch 38, Loss: -2.1219301223754883\n",
      "Batch 39, Loss: -1.367899775505066\n",
      "Batch 40, Loss: -1.753580093383789\n",
      "Batch 41, Loss: -2.0917787551879883\n",
      "Batch 42, Loss: -2.066704750061035\n",
      "Batch 43, Loss: -1.374363899230957\n",
      "Batch 44, Loss: -1.7649071216583252\n",
      "Batch 45, Loss: -1.7184298038482666\n",
      "Batch 46, Loss: -1.718273639678955\n",
      "Batch 47, Loss: -1.7350428104400635\n",
      "Batch 48, Loss: -1.1603195667266846\n",
      "Batch 49, Loss: -1.8409358263015747\n",
      "Batch 50, Loss: -2.122652769088745\n",
      "Batch 51, Loss: -1.7333979606628418\n",
      "Batch 52, Loss: -1.7564932107925415\n",
      "Batch 53, Loss: -1.7660496234893799\n",
      "Batch 54, Loss: -1.7196449041366577\n",
      "Batch 55, Loss: -1.4017040729522705\n",
      "Batch 56, Loss: -1.3978660106658936\n",
      "Batch 57, Loss: -1.7117812633514404\n",
      "Batch 58, Loss: -2.1382789611816406\n",
      "Batch 59, Loss: -1.7823607921600342\n",
      "Batch 60, Loss: -1.7870469093322754\n",
      "Batch 61, Loss: -1.7681810855865479\n",
      "Batch 62, Loss: -1.766805648803711\n",
      "Batch 63, Loss: -2.150829315185547\n",
      "Batch 64, Loss: -1.7762577533721924\n",
      "Batch 65, Loss: -2.1079933643341064\n",
      "Batch 66, Loss: -1.7219085693359375\n",
      "Batch 67, Loss: -1.7869272232055664\n",
      "Batch 68, Loss: -1.8550999164581299\n",
      "Batch 69, Loss: -1.1462199687957764\n",
      "Batch 70, Loss: -1.7732281684875488\n",
      "Batch 71, Loss: -1.7304370403289795\n",
      "Batch 72, Loss: -1.7995028495788574\n",
      "Batch 73, Loss: -1.401503086090088\n",
      "Batch 74, Loss: -2.158205986022949\n",
      "Batch 75, Loss: -2.16062331199646\n",
      "Batch 76, Loss: -2.099358558654785\n",
      "Batch 77, Loss: -0.9845949411392212\n",
      "Batch 78, Loss: -2.148146152496338\n",
      "Batch 79, Loss: -1.7988064289093018\n",
      "Batch 80, Loss: -1.388075828552246\n",
      "Batch 81, Loss: -1.8084033727645874\n",
      "Batch 82, Loss: -1.418853521347046\n",
      "Batch 83, Loss: -2.1633083820343018\n",
      "Batch 84, Loss: -2.1756043434143066\n",
      "Batch 85, Loss: -0.8039371371269226\n",
      "Batch 86, Loss: -1.4323689937591553\n",
      "Batch 87, Loss: -2.1643824577331543\n",
      "Batch 88, Loss: -1.827280044555664\n",
      "Batch 89, Loss: -1.7984285354614258\n",
      "Batch 90, Loss: -1.7737376689910889\n",
      "Batch 91, Loss: -1.7188429832458496\n",
      "Batch 92, Loss: -1.7572580575942993\n",
      "Batch 93, Loss: -2.095371961593628\n",
      "Batch 94, Loss: -1.7502126693725586\n",
      "Batch 95, Loss: -1.4415510892868042\n",
      "Batch 96, Loss: -1.831531286239624\n",
      "Batch 97, Loss: -1.775179386138916\n",
      "Batch 98, Loss: -1.4615387916564941\n",
      "Batch 99, Loss: -1.8290379047393799\n",
      "Training [80%]\tLoss: -1.6906\n",
      "Batch 0, Loss: -1.7980819940567017\n",
      "Batch 1, Loss: -1.7727434635162354\n",
      "Batch 2, Loss: -2.206904888153076\n",
      "Batch 3, Loss: -2.147672653198242\n",
      "Batch 4, Loss: -1.1645228862762451\n",
      "Batch 5, Loss: -1.8139269351959229\n",
      "Batch 6, Loss: -1.8461947441101074\n",
      "Batch 7, Loss: -1.842928171157837\n",
      "Batch 8, Loss: -1.3853557109832764\n",
      "Batch 9, Loss: -1.454596996307373\n",
      "Batch 10, Loss: -1.8527905941009521\n",
      "Batch 11, Loss: -2.2137725353240967\n",
      "Batch 12, Loss: -1.8457666635513306\n",
      "Batch 13, Loss: -1.8413870334625244\n",
      "Batch 14, Loss: -1.4746310710906982\n",
      "Batch 15, Loss: -1.294219732284546\n",
      "Batch 16, Loss: -1.835864543914795\n",
      "Batch 17, Loss: -1.8614400625228882\n",
      "Batch 18, Loss: -2.0943000316619873\n",
      "Batch 19, Loss: -1.8609247207641602\n",
      "Batch 20, Loss: -1.8140716552734375\n",
      "Batch 21, Loss: -2.200061798095703\n",
      "Batch 22, Loss: -2.156965732574463\n",
      "Batch 23, Loss: -2.2194485664367676\n",
      "Batch 24, Loss: -2.194185733795166\n",
      "Batch 25, Loss: -1.8451389074325562\n",
      "Batch 26, Loss: -1.8539700508117676\n",
      "Batch 27, Loss: -1.4987754821777344\n",
      "Batch 28, Loss: -2.2311716079711914\n",
      "Batch 29, Loss: -1.4985727071762085\n",
      "Batch 30, Loss: -1.4963932037353516\n",
      "Batch 31, Loss: -1.822557806968689\n",
      "Batch 32, Loss: -1.4812469482421875\n",
      "Batch 33, Loss: -2.217585802078247\n",
      "Batch 34, Loss: -1.4971023797988892\n",
      "Batch 35, Loss: -1.5698939561843872\n",
      "Batch 36, Loss: -1.326775074005127\n",
      "Batch 37, Loss: -1.8872145414352417\n",
      "Batch 38, Loss: -2.2568628787994385\n",
      "Batch 39, Loss: -1.8401802778244019\n",
      "Batch 40, Loss: -2.2485239505767822\n",
      "Batch 41, Loss: -2.2568857669830322\n",
      "Batch 42, Loss: -1.8788714408874512\n",
      "Batch 43, Loss: -1.790210247039795\n",
      "Batch 44, Loss: -1.5059256553649902\n",
      "Batch 45, Loss: -2.2275614738464355\n",
      "Batch 46, Loss: -1.4320018291473389\n",
      "Batch 47, Loss: -1.8944822549819946\n",
      "Batch 48, Loss: -1.870909333229065\n",
      "Batch 49, Loss: -1.8959736824035645\n",
      "Batch 50, Loss: -1.8099055290222168\n",
      "Batch 51, Loss: -1.4650237560272217\n",
      "Batch 52, Loss: -2.1792354583740234\n",
      "Batch 53, Loss: -1.8283631801605225\n",
      "Batch 54, Loss: -2.2716917991638184\n",
      "Batch 55, Loss: -1.1963794231414795\n",
      "Batch 56, Loss: -1.5303913354873657\n",
      "Batch 57, Loss: -1.912821888923645\n",
      "Batch 58, Loss: -2.2684576511383057\n",
      "Batch 59, Loss: -1.4938428401947021\n",
      "Batch 60, Loss: -1.905997395515442\n",
      "Batch 61, Loss: -1.8949453830718994\n",
      "Batch 62, Loss: -2.262000560760498\n",
      "Batch 63, Loss: -1.553870439529419\n",
      "Batch 64, Loss: -2.2761149406433105\n",
      "Batch 65, Loss: -1.4098583459854126\n",
      "Batch 66, Loss: -1.8038864135742188\n",
      "Batch 67, Loss: -1.9008678197860718\n",
      "Batch 68, Loss: -2.2285118103027344\n",
      "Batch 69, Loss: -1.5474958419799805\n",
      "Batch 70, Loss: -1.9186837673187256\n",
      "Batch 71, Loss: -1.572188377380371\n",
      "Batch 72, Loss: -1.9340524673461914\n",
      "Batch 73, Loss: -1.8860384225845337\n",
      "Batch 74, Loss: -1.582869291305542\n",
      "Batch 75, Loss: -1.4258675575256348\n",
      "Batch 76, Loss: -1.8871415853500366\n",
      "Batch 77, Loss: -1.5775156021118164\n",
      "Batch 78, Loss: -1.92403244972229\n",
      "Batch 79, Loss: -2.296785831451416\n",
      "Batch 80, Loss: -1.5466084480285645\n",
      "Batch 81, Loss: -1.9469096660614014\n",
      "Batch 82, Loss: -1.5399454832077026\n",
      "Batch 83, Loss: -1.8807992935180664\n",
      "Batch 84, Loss: -1.942008376121521\n",
      "Batch 85, Loss: -2.0783584117889404\n",
      "Batch 86, Loss: -1.8045299053192139\n",
      "Batch 87, Loss: -1.9484652280807495\n",
      "Batch 88, Loss: -1.5944159030914307\n",
      "Batch 89, Loss: -2.302189588546753\n",
      "Batch 90, Loss: -2.279175281524658\n",
      "Batch 91, Loss: -1.958036184310913\n",
      "Batch 92, Loss: -1.949157476425171\n",
      "Batch 93, Loss: -2.2901735305786133\n",
      "Batch 94, Loss: -1.869224190711975\n",
      "Batch 95, Loss: -1.5918288230895996\n",
      "Batch 96, Loss: -2.305711269378662\n",
      "Batch 97, Loss: -1.9169466495513916\n",
      "Batch 98, Loss: -1.5817965269088745\n",
      "Batch 99, Loss: -1.9493448734283447\n",
      "Training [90%]\tLoss: -1.8454\n",
      "Batch 0, Loss: -2.319833755493164\n",
      "Batch 1, Loss: -1.6165649890899658\n",
      "Batch 2, Loss: -1.970947504043579\n",
      "Batch 3, Loss: -1.623084545135498\n",
      "Batch 4, Loss: -2.2708792686462402\n",
      "Batch 5, Loss: -1.967677354812622\n",
      "Batch 6, Loss: -1.9743382930755615\n",
      "Batch 7, Loss: -2.3211638927459717\n",
      "Batch 8, Loss: -1.9844434261322021\n",
      "Batch 9, Loss: -1.9704419374465942\n",
      "Batch 10, Loss: -1.9750614166259766\n",
      "Batch 11, Loss: -1.624072551727295\n",
      "Batch 12, Loss: -2.2956247329711914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13, Loss: -1.9673006534576416\n",
      "Batch 14, Loss: -2.157700538635254\n",
      "Batch 15, Loss: -2.3290579319000244\n",
      "Batch 16, Loss: -1.9889602661132812\n",
      "Batch 17, Loss: -2.321800470352173\n",
      "Batch 18, Loss: -1.6489629745483398\n",
      "Batch 19, Loss: -1.998464822769165\n",
      "Batch 20, Loss: -2.3343653678894043\n",
      "Batch 21, Loss: -1.9995334148406982\n",
      "Batch 22, Loss: -1.619199275970459\n",
      "Batch 23, Loss: -2.3356099128723145\n",
      "Batch 24, Loss: -1.989515781402588\n",
      "Batch 25, Loss: -1.659395456314087\n",
      "Batch 26, Loss: -1.9558807611465454\n",
      "Batch 27, Loss: -1.9728153944015503\n",
      "Batch 28, Loss: -1.9956066608428955\n",
      "Batch 29, Loss: -1.9844880104064941\n",
      "Batch 30, Loss: -1.99170982837677\n",
      "Batch 31, Loss: -1.7736663818359375\n",
      "Batch 32, Loss: -1.6657981872558594\n",
      "Batch 33, Loss: -2.3555312156677246\n",
      "Batch 34, Loss: -2.0076301097869873\n",
      "Batch 35, Loss: -1.6736407279968262\n",
      "Batch 36, Loss: -1.6505255699157715\n",
      "Batch 37, Loss: -2.023285150527954\n",
      "Batch 38, Loss: -2.012758255004883\n",
      "Batch 39, Loss: -2.354506015777588\n",
      "Batch 40, Loss: -1.9951558113098145\n",
      "Batch 41, Loss: -2.364994525909424\n",
      "Batch 42, Loss: -2.3685922622680664\n",
      "Batch 43, Loss: -1.5535571575164795\n",
      "Batch 44, Loss: -2.029236078262329\n",
      "Batch 45, Loss: -1.5874567031860352\n",
      "Batch 46, Loss: -1.6598916053771973\n",
      "Batch 47, Loss: -2.0012831687927246\n",
      "Batch 48, Loss: -2.0114755630493164\n",
      "Batch 49, Loss: -2.0246293544769287\n",
      "Batch 50, Loss: -1.6964478492736816\n",
      "Batch 51, Loss: -2.357046604156494\n",
      "Batch 52, Loss: -2.0254554748535156\n",
      "Batch 53, Loss: -2.3770854473114014\n",
      "Batch 54, Loss: -2.036714792251587\n",
      "Batch 55, Loss: -2.036555290222168\n",
      "Batch 56, Loss: -1.6983420848846436\n",
      "Batch 57, Loss: -2.0480313301086426\n",
      "Batch 58, Loss: -2.0507707595825195\n",
      "Batch 59, Loss: -1.7040815353393555\n",
      "Batch 60, Loss: -2.0543630123138428\n",
      "Batch 61, Loss: -2.0446436405181885\n",
      "Batch 62, Loss: -1.6960583925247192\n",
      "Batch 63, Loss: -1.707972526550293\n",
      "Batch 64, Loss: -2.033308506011963\n",
      "Batch 65, Loss: -2.028867483139038\n",
      "Batch 66, Loss: -1.9292073249816895\n",
      "Batch 67, Loss: -1.1745482683181763\n",
      "Batch 68, Loss: -2.313694477081299\n",
      "Batch 69, Loss: -1.8441662788391113\n",
      "Batch 70, Loss: -1.9823827743530273\n",
      "Batch 71, Loss: -1.6434824466705322\n",
      "Batch 72, Loss: -2.0362677574157715\n",
      "Batch 73, Loss: -2.0562446117401123\n",
      "Batch 74, Loss: -2.042888641357422\n",
      "Batch 75, Loss: -2.290351390838623\n",
      "Batch 76, Loss: -2.3774795532226562\n",
      "Batch 77, Loss: -2.394742012023926\n",
      "Batch 78, Loss: -2.0410892963409424\n",
      "Batch 79, Loss: -2.071742057800293\n",
      "Batch 80, Loss: -2.3982064723968506\n",
      "Batch 81, Loss: -1.7173676490783691\n",
      "Batch 82, Loss: -2.3480448722839355\n",
      "Batch 83, Loss: -2.3964128494262695\n",
      "Batch 84, Loss: -1.8315874338150024\n",
      "Batch 85, Loss: -1.9398585557937622\n",
      "Batch 86, Loss: -2.3960533142089844\n",
      "Batch 87, Loss: -1.6884992122650146\n",
      "Batch 88, Loss: -1.566340446472168\n",
      "Batch 89, Loss: -1.8898781538009644\n",
      "Batch 90, Loss: -1.950484037399292\n",
      "Batch 91, Loss: -1.7224920988082886\n",
      "Batch 92, Loss: -2.0426182746887207\n",
      "Batch 93, Loss: -2.059372901916504\n",
      "Batch 94, Loss: -1.7330131530761719\n",
      "Batch 95, Loss: -2.0752620697021484\n",
      "Batch 96, Loss: -1.7330635786056519\n",
      "Batch 97, Loss: -1.7062687873840332\n",
      "Batch 98, Loss: -1.999253749847412\n",
      "Batch 99, Loss: -2.088068962097168\n",
      "Training [100%]\tLoss: -1.9835\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.007594972965307534 s\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: -1.9941\n",
      "\tAccuracy: 99.0%\n",
      "Test time: -0.0025430559762753546 s\n"
     ]
    }
   ],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
