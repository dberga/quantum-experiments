{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "#sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baby', 'man']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"QSVM\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "LR = 0.001\n",
    "n_samples_train = 200 #128\n",
    "n_samples_test = 50 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"CIFAR100\" # MNIST / CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = [\"baby\",\"man\"] # ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "if network == 'hybridqnn_shallow' or network == 'QSVM':\n",
    "    # Set preprocessing transforms\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "else:\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    \n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: 0.0007143672555685043 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network init elapsed time: -0.00019026361405849457 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        #n_filts_fc1 = int(((((input_resolution[0])/2)-4)/2)**2)*16 # +7\n",
    "        n_filts_fc1 = 256\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "    \n",
    "    '''\n",
    "    # Reduce dimensionality\n",
    "    n_dim = 2\n",
    "    pca = PCA(n_components=n_dim).fit(sample_train)\n",
    "    sample_train = pca.transform(sample_train)\n",
    "    sample_test = pca.transform(sample_test)\n",
    "\n",
    "    # Normalise\n",
    "    std_scale = StandardScaler().fit(sample_train)\n",
    "    sample_train = std_scale.transform(sample_train)\n",
    "    sample_test = std_scale.transform(sample_test)\n",
    "\n",
    "    # Scale\n",
    "    samples = np.append(sample_train, sample_test, axis=0)\n",
    "    minmax_scale = MinMaxScaler((-1, 1)).fit(samples)\n",
    "    sample_train = minmax_scale.transform(sample_train)\n",
    "    sample_test = minmax_scale.transform(sample_test)\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "   '''\n",
    "elif network=='altqnn':\n",
    "    sampler_classifier = NeuralNetworkClassifier(\n",
    "        neural_network=sampler_qnn, optimizer=COBYLA(maxiter=30), callback=callback_graph\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m network \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQSVM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msvm\u001b[49m\u001b[38;5;241m.\u001b[39mrun(quantum_instance)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
