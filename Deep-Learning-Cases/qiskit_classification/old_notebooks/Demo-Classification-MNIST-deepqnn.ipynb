{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"resnet18\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "LR = 0.0001\n",
    "n_samples_train = 128 #128\n",
    "n_samples_test = 32 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"MNIST\" # CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "# Set preprocessing transforms\n",
    "list_preprocessing = [\n",
    "    transforms.Resize(input_resolution),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: -0.0013296129764057696 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPEAAACHCAYAAADHsL/VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAIsElEQVR4nO3dX2hU6RnH8d9jXE0MpbootN02udC1ECuIFFNpLhZRixfVKERB1y24VtQLb2opFqVSNFkvhFZWaZVixYqhRmuh1qo3S6SyVIoUKrqKuElEFjZqKrVq/XN6MePpec9mJpM4k5ln8v1A4H14z8m8k5PfvPPOmTNjURQJgF/jyj0AAK+HEAPOEWLAOUIMOEeIAecIMeCcyxCb2Udmtt7Tvhg+jnNhyhpiM/vUzBaWcwylYmbfMrNzZtZvZmP6ZDzHubRczsROPJP0e0nvl3sgKKmyH+eKDLGZTTGzP5nZ52b2INv+emqz6Wb2NzN7aGZ/NLM3E/t/x8wumdmAmf3DzN7Jc1vrzOxa9nbOmVljom+RmV03s3+Z2YeSrND7EEXRJ1EU/UbS1YLv+BjDcS6OigyxMuM6LKlRUoOkx5I+TG3znqR1kr4q6bmkfZJkZm9JOiNpl6Q3JW2VdNLMpqVvxMyWSfqppBWSpkm6KOl4tm+qpFOStkuaKumWpO8m9m3I/vM0FOUej00c52KIoqhsP5I+lbSwgO3mSHqQqD+S9EGibpL0X0k1kn4i6Whq/3OSfpDYd322fVbS+4ntxkn6jzL/VO9J+jjRZ5LuvNp3GPdxRubPXL6/c7l/OM6l/anImdjMJpnZr82sx8weSuqWNNnMahKb9SXaPZLeUOaRtFFSW/bRc8DMBiS1KPNIntYo6ZeJ7e4rcxDfkvS15G1EmSPVN8jvwAhxnItjfLkHkMOPJH1TUnMURZ+Z2RxJVxSuVb6RaDco8wJDvzIH4GgURT8s4Hb6JO2OouhYusPM3k7ehplZ6jbx+jjORVAJM/EbZlab+Bkv6UvKrI8Gsi9k/GyQ/d41syYzmyTp55K6oih6Iel3kr5vZt8zs5rs73xnkBdMJOlXkraZ2SxJMrMvm1lbtu+MpFlmtiI7pi2SvlLonbKMWkkTsnWtmU0sdP8qxHEukUoI8Z+VOZCvfnZK+oWkOmUecT+W9JdB9jsq6beSPpNUq8wfX1EU9Ul69ULG58o8Cv9Yg9zXKIr+IGmPpM7s07l/SlqS7euX1CbpA0n3JL0t6a+v9s2+4PHvPC94NGbvz6tXLR9L+iTvX6K6cZxLxLKLcgBOVcJMDOA1EGLAOUIMOEeIAecIMeDcsN7sYWP8krpKFUVRwW/YHwrHuGL1R1H0hfeFS8zEgBc9uToIMeAcIQacI8SAc4QYcI4QA84RYsA5Qgw4R4gB5wgx4BwhBpwjxIBzhBhwrlI/snZENmzYENQHDhzIue3s2bOD+tq1ayUZE3xqbm6O22fOnAn6Hjx4ELcXLFgQ9PX1jf5HVjMTA84RYsC5qno6nfby5cucfUeOHAnqefPmlXo4qGB1dXVBvX///rg9efLkoG/KlClxu6WlJeg7fvx48Qc3BGZiwDlCDDhHiAHnqnpNnE9DQ/jVOmvWrInbx4594cvzUOXq6+uDuqmpKee2J0+ejNtdXV0lG1OhmIkB5wgx4NyYfTrd398f1N3d3WUaCcphxowZQd3R0RHUEyZMyLlvb29v3H727FlxBzYCzMSAc4QYcI4QA85V9Zp43Ljcj1HpUwjbtm2L25s3by7ZmFAZWltbg3rp0qU5t+3s7Azq9vb2UgxpxJiJAecIMeAcIQacq+o1cb5LEdOiiK/lrXarVq2K23v27Mm77d27d+N2+rLV+/fvF3dgr4mZGHCOEAPOVfXTaSAp+VbLoZZPhw4dituXLl0q2ZiKgZkYcI4QA84RYsA51sRZK1asiNsXLlwI+k6fPj3Ko0Ex7N27N6g3btyYc9t79+4FdfIU1NOnT4s7sCJjJgacI8SAc4QYcK6q1sQHDx4M6rlz58bt9evXB33pyxSnTZsWt6dOnVqC0WG0TZ8+Pahra2vjdvo88dmzZ4O60tfBSczEgHOEGHCuqp5OpyWfMg11RdNwrnhCZVq3bl1QL168OOe2t27dCuodO3aUZEyjgZkYcI4QA84RYsC5ql4To/qNH///f+G2tragb+LEiUGdPK3Y09MT9KW/EcQTZmLAOUIMOEeIAedYE8O1Xbt2xe1FixYFfem3Vt6+fTtur1y5Muh7/PhxCUY3OpiJAecIMeBcVT+dvnPnTtxOX5VSX18/2sNBEcyaNSuo83353ZMnT4I6eaXSwMBAUcdVTszEgHOEGHCOEAPOVfWaePfu3XF72bJlQV/yUz8kLkWsVDNnzgzqnTt3BvWkSZNy7ps8/pLU0dFRtHFVEmZiwDlCDDhHiAHnqnpNnNTe3h7UXV1dObddvXp1UJ8/fz6oe3t7izcw5LVly5agXr58ec5t08fp8OHDJRlTpWEmBpwjxIBzNtSXLQcbmxW+cYV7/vx5UOc7xdTc3BzUV65cKcmYRiqKIivW76qEY9zZ2Rm301cbpf9fk0ubhQsXBn3pT7R07u9RFH17sA5mYsA5Qgw4R4gB58bMKaa0mpqagre9fPlyUN+4cSNuL1myJOhLf4oihjZ//vycdXoNnK63b98et6tsDVwwZmLAOUIMODdmTzGdOHEiqNNXOeWT/BDy7u7uoC/5SRPXr18f4eiGx9spprq6uqC+ePFiUM+ZMyc5nqDv6tWrQZ186v3o0aMijbAicYoJqFaEGHCOEAPOjdlTTJs2bQrq5HoqfRVTPi0tLTnr0VoTe/PixYugfvjwYc5tb968GdTpT/ao8nVwQZiJAecIMeAcIQacG7Nr4vSXSm/dujVupz9BsbW1NaiTa921a9cGfX19fUUaYfVKX/aZ/qaGpH379gX1qVOnSjImz5iJAecIMeDcmH3bZTXx9rZLjAhvuwSqFSEGnCPEgHOEGHCOEAPOEWLAOUIMOEeIAecIMeAcIQacI8SAc4QYcI4QA84RYsC54X6yR78kvjGssjQW+fdxjCtTzuM8rOuJAVQenk4DzhFiwDlCDDhHiAHnCDHgHCEGnCPEgHOEGHCOEAPO/Q+iTqnxozq0FQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZ),\n",
      "  CircuitStateFn(\n",
      "       ┌──────────────────────────┐┌──────────────────────────────────────┐\n",
      "  q_0: ┤0                         ├┤0                                     ├\n",
      "       │  ZZFeatureMap(x[0],x[1]) ││  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
      "  q_1: ┤1                         ├┤1                                     ├\n",
      "       └──────────────────────────┘└──────────────────────────────────────┘\n",
      "  )\n",
      "])\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
      "    (1): TorchConnector()\n",
      "    (2): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Network init elapsed time: -0.0003073320258408785 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]+7-4)/2)-4)/2)**2)*16\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 0.6203333139419556\n",
      "Batch 1, Loss: 0.5367475152015686\n",
      "Batch 2, Loss: -0.4995359480381012\n",
      "Batch 3, Loss: -0.5383416414260864\n",
      "Batch 4, Loss: -1.4964802265167236\n",
      "Batch 5, Loss: 0.4927327036857605\n",
      "Batch 6, Loss: -1.4178295135498047\n",
      "Batch 7, Loss: -0.5543209314346313\n",
      "Batch 8, Loss: -0.5091373920440674\n",
      "Batch 9, Loss: 0.599835991859436\n",
      "Batch 10, Loss: -1.516977071762085\n",
      "Batch 11, Loss: -1.50580894947052\n",
      "Batch 12, Loss: -1.578896403312683\n",
      "Batch 13, Loss: -0.43068280816078186\n",
      "Batch 14, Loss: -0.4254828691482544\n",
      "Batch 15, Loss: 0.42442411184310913\n",
      "Batch 16, Loss: 0.2982161045074463\n",
      "Batch 17, Loss: 0.5530551671981812\n",
      "Batch 18, Loss: -1.3616595268249512\n",
      "Batch 19, Loss: -0.6043495535850525\n",
      "Batch 20, Loss: -0.5118367671966553\n",
      "Batch 21, Loss: -0.5990685224533081\n",
      "Batch 22, Loss: -0.4800107479095459\n",
      "Batch 23, Loss: -0.6422582268714905\n",
      "Batch 24, Loss: -0.4338560104370117\n",
      "Batch 25, Loss: -0.5957666039466858\n",
      "Batch 26, Loss: 0.4671710133552551\n",
      "Batch 27, Loss: -1.6259174346923828\n",
      "Batch 28, Loss: 0.5967158079147339\n",
      "Batch 29, Loss: -0.3616667091846466\n",
      "Batch 30, Loss: -0.5763230919837952\n",
      "Batch 31, Loss: -1.5197551250457764\n",
      "Batch 32, Loss: -0.44475752115249634\n",
      "Batch 33, Loss: -0.5593995451927185\n",
      "Batch 34, Loss: -0.5038745999336243\n",
      "Batch 35, Loss: -0.5804780125617981\n",
      "Batch 36, Loss: -1.5266146659851074\n",
      "Batch 37, Loss: -0.6504996418952942\n",
      "Batch 38, Loss: 0.3658847212791443\n",
      "Batch 39, Loss: -0.37015289068222046\n",
      "Batch 40, Loss: 0.4497263431549072\n",
      "Batch 41, Loss: -0.5916433334350586\n",
      "Batch 42, Loss: 0.607147753238678\n",
      "Batch 43, Loss: -1.4080448150634766\n",
      "Batch 44, Loss: -0.3883521556854248\n",
      "Batch 45, Loss: -0.520535945892334\n",
      "Batch 46, Loss: -0.4757368862628937\n",
      "Batch 47, Loss: -1.4712631702423096\n",
      "Batch 48, Loss: -0.5389505624771118\n",
      "Batch 49, Loss: -0.48412489891052246\n",
      "Batch 50, Loss: -0.4803497791290283\n",
      "Batch 51, Loss: -0.5583653450012207\n",
      "Batch 52, Loss: -0.4972202479839325\n",
      "Batch 53, Loss: 0.5832371115684509\n",
      "Batch 54, Loss: -0.3572404384613037\n",
      "Batch 55, Loss: -1.2935314178466797\n",
      "Batch 56, Loss: -1.6124705076217651\n",
      "Batch 57, Loss: -1.4014490842819214\n",
      "Batch 58, Loss: -0.4294380247592926\n",
      "Batch 59, Loss: -0.6889781951904297\n",
      "Batch 60, Loss: -0.52947998046875\n",
      "Batch 61, Loss: -0.5109162926673889\n",
      "Batch 62, Loss: 0.5310879945755005\n",
      "Batch 63, Loss: -1.4633604288101196\n",
      "Batch 64, Loss: -0.48287728428840637\n",
      "Batch 65, Loss: -0.4734504222869873\n",
      "Batch 66, Loss: -0.42980772256851196\n",
      "Batch 67, Loss: -0.33418601751327515\n",
      "Batch 68, Loss: -1.4553558826446533\n",
      "Batch 69, Loss: 0.4397176504135132\n",
      "Batch 70, Loss: -0.49412012100219727\n",
      "Batch 71, Loss: -1.4883244037628174\n",
      "Batch 72, Loss: -0.34299397468566895\n",
      "Batch 73, Loss: -1.4579403400421143\n",
      "Batch 74, Loss: 0.5557129383087158\n",
      "Batch 75, Loss: -0.5092560648918152\n",
      "Batch 76, Loss: 0.41032421588897705\n",
      "Batch 77, Loss: -1.5550804138183594\n",
      "Batch 78, Loss: 0.40816956758499146\n",
      "Batch 79, Loss: -0.5441988706588745\n",
      "Batch 80, Loss: -0.5758137702941895\n",
      "Batch 81, Loss: -1.6098685264587402\n",
      "Batch 82, Loss: -0.491269052028656\n",
      "Batch 83, Loss: 0.5707520246505737\n",
      "Batch 84, Loss: -0.45797133445739746\n",
      "Batch 85, Loss: -0.5393631458282471\n",
      "Batch 86, Loss: -0.5502970218658447\n",
      "Batch 87, Loss: -1.6072888374328613\n",
      "Batch 88, Loss: -1.5488009452819824\n",
      "Batch 89, Loss: -0.5499669313430786\n",
      "Batch 90, Loss: -0.39820557832717896\n",
      "Batch 91, Loss: -0.43781018257141113\n",
      "Batch 92, Loss: -0.432779997587204\n",
      "Batch 93, Loss: 0.5482074618339539\n",
      "Batch 94, Loss: -1.5313154458999634\n",
      "Batch 95, Loss: -1.5604205131530762\n",
      "Batch 96, Loss: 0.4821823239326477\n",
      "Batch 97, Loss: 0.450276255607605\n",
      "Batch 98, Loss: -0.6100362539291382\n",
      "Batch 99, Loss: -1.4275016784667969\n",
      "Batch 100, Loss: 0.4880436062812805\n",
      "Batch 101, Loss: 0.4852009415626526\n",
      "Batch 102, Loss: -0.502249538898468\n",
      "Batch 103, Loss: -0.5200726985931396\n",
      "Batch 104, Loss: -0.5616858005523682\n",
      "Batch 105, Loss: -0.5475612878799438\n",
      "Batch 106, Loss: -0.5818008184432983\n",
      "Batch 107, Loss: 0.4761720895767212\n",
      "Batch 108, Loss: -1.513344168663025\n",
      "Batch 109, Loss: -0.47174105048179626\n",
      "Batch 110, Loss: -0.37157928943634033\n",
      "Batch 111, Loss: -0.5857537984848022\n",
      "Batch 112, Loss: -1.4882113933563232\n",
      "Batch 113, Loss: -1.5483653545379639\n",
      "Batch 114, Loss: -0.6854177713394165\n",
      "Batch 115, Loss: -1.5848052501678467\n",
      "Batch 116, Loss: -0.45609837770462036\n",
      "Batch 117, Loss: 0.3470907211303711\n",
      "Batch 118, Loss: -0.48412439227104187\n",
      "Batch 119, Loss: -0.4223095774650574\n",
      "Batch 120, Loss: -0.4109044671058655\n",
      "Batch 121, Loss: -0.6664984822273254\n",
      "Batch 122, Loss: 0.420370876789093\n",
      "Batch 123, Loss: -0.43257153034210205\n",
      "Batch 124, Loss: 0.5185032486915588\n",
      "Batch 125, Loss: -0.49493616819381714\n",
      "Batch 126, Loss: -0.40506625175476074\n",
      "Batch 127, Loss: 0.3840835690498352\n",
      "Training [10%]\tLoss: -0.5050\n",
      "Batch 0, Loss: -1.490125060081482\n",
      "Batch 1, Loss: 0.4406082332134247\n",
      "Batch 2, Loss: -1.531989574432373\n",
      "Batch 3, Loss: 0.5547882318496704\n",
      "Batch 4, Loss: -0.45180124044418335\n",
      "Batch 5, Loss: -0.5216159820556641\n",
      "Batch 6, Loss: -1.4999929666519165\n",
      "Batch 7, Loss: 0.6295962333679199\n",
      "Batch 8, Loss: -1.5320340394973755\n",
      "Batch 9, Loss: -1.405473232269287\n",
      "Batch 10, Loss: -1.4931269884109497\n",
      "Batch 11, Loss: -0.561959981918335\n",
      "Batch 12, Loss: -0.33532607555389404\n",
      "Batch 13, Loss: -1.427910327911377\n",
      "Batch 14, Loss: -0.4209768772125244\n",
      "Batch 15, Loss: 0.48162996768951416\n",
      "Batch 16, Loss: -0.6289308667182922\n",
      "Batch 17, Loss: -1.5703482627868652\n",
      "Batch 18, Loss: -0.45616430044174194\n",
      "Batch 19, Loss: -0.5799946784973145\n",
      "Batch 20, Loss: -0.43654966354370117\n",
      "Batch 21, Loss: -0.5188531875610352\n",
      "Batch 22, Loss: -0.45388275384902954\n",
      "Batch 23, Loss: -0.4680744409561157\n",
      "Batch 24, Loss: -0.5291247367858887\n",
      "Batch 25, Loss: 0.39291083812713623\n",
      "Batch 26, Loss: -1.6136960983276367\n",
      "Batch 27, Loss: -1.4325460195541382\n",
      "Batch 28, Loss: 0.405806303024292\n",
      "Batch 29, Loss: -1.5527758598327637\n",
      "Batch 30, Loss: 0.34997978806495667\n",
      "Batch 31, Loss: -0.5645954608917236\n",
      "Batch 32, Loss: -0.7175468802452087\n",
      "Batch 33, Loss: -0.5668066143989563\n",
      "Batch 34, Loss: -0.6051376461982727\n",
      "Batch 35, Loss: -0.513842761516571\n",
      "Batch 36, Loss: -1.4882060289382935\n",
      "Batch 37, Loss: -0.49512431025505066\n",
      "Batch 38, Loss: -1.5716464519500732\n",
      "Batch 39, Loss: -0.38563209772109985\n",
      "Batch 40, Loss: -0.492725133895874\n",
      "Batch 41, Loss: -0.5548040866851807\n",
      "Batch 42, Loss: 0.6040636301040649\n",
      "Batch 43, Loss: 0.37632039189338684\n",
      "Batch 44, Loss: -0.5552219152450562\n",
      "Batch 45, Loss: 0.41971465945243835\n",
      "Batch 46, Loss: -1.4896879196166992\n",
      "Batch 47, Loss: 0.4043772518634796\n",
      "Batch 48, Loss: -0.5634225010871887\n",
      "Batch 49, Loss: -1.452636957168579\n",
      "Batch 50, Loss: -0.5369802713394165\n",
      "Batch 51, Loss: 0.5772702693939209\n",
      "Batch 52, Loss: -0.5819836258888245\n",
      "Batch 53, Loss: 0.6013134121894836\n",
      "Batch 54, Loss: -1.5883994102478027\n",
      "Batch 55, Loss: -1.5440499782562256\n",
      "Batch 56, Loss: -1.4228293895721436\n",
      "Batch 57, Loss: 0.5625013113021851\n",
      "Batch 58, Loss: -0.4025874137878418\n",
      "Batch 59, Loss: 0.48250287771224976\n",
      "Batch 60, Loss: -1.3642271757125854\n",
      "Batch 61, Loss: 0.48028552532196045\n",
      "Batch 62, Loss: 0.5022202730178833\n",
      "Batch 63, Loss: 0.6385138034820557\n",
      "Batch 64, Loss: 0.49808192253112793\n",
      "Batch 65, Loss: -1.6236376762390137\n",
      "Batch 66, Loss: -1.4320708513259888\n",
      "Batch 67, Loss: -1.5335805416107178\n",
      "Batch 68, Loss: -1.6298973560333252\n",
      "Batch 69, Loss: 0.40793663263320923\n",
      "Batch 70, Loss: -0.4303337633609772\n",
      "Batch 71, Loss: -0.4667883515357971\n",
      "Batch 72, Loss: -0.49203944206237793\n",
      "Batch 73, Loss: -0.5890893936157227\n",
      "Batch 74, Loss: 0.5138280987739563\n",
      "Batch 75, Loss: -1.4849834442138672\n",
      "Batch 76, Loss: 0.5511252284049988\n",
      "Batch 77, Loss: -1.4268983602523804\n",
      "Batch 78, Loss: -0.44830310344696045\n",
      "Batch 79, Loss: 0.43626827001571655\n",
      "Batch 80, Loss: 0.46754807233810425\n",
      "Batch 81, Loss: 0.6253173351287842\n",
      "Batch 82, Loss: -0.605770468711853\n",
      "Batch 83, Loss: -1.5928927659988403\n",
      "Batch 84, Loss: -0.4245683550834656\n",
      "Batch 85, Loss: 0.47355347871780396\n",
      "Batch 86, Loss: 0.42746761441230774\n",
      "Batch 87, Loss: -0.5731062889099121\n",
      "Batch 88, Loss: -1.4763442277908325\n",
      "Batch 89, Loss: -0.5948467254638672\n",
      "Batch 90, Loss: -0.5177105069160461\n",
      "Batch 91, Loss: -0.6119847297668457\n",
      "Batch 92, Loss: -1.3595565557479858\n",
      "Batch 93, Loss: -1.5309940576553345\n",
      "Batch 94, Loss: -0.31686288118362427\n",
      "Batch 95, Loss: -0.5644569993019104\n",
      "Batch 96, Loss: -1.6601029634475708\n",
      "Batch 97, Loss: -0.45966702699661255\n",
      "Batch 98, Loss: 0.469127893447876\n",
      "Batch 99, Loss: -0.5351085066795349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: -0.5584790706634521\n",
      "Batch 101, Loss: 0.46992242336273193\n",
      "Batch 102, Loss: -1.4356852769851685\n",
      "Batch 103, Loss: 0.3962637484073639\n",
      "Batch 104, Loss: -1.3976835012435913\n",
      "Batch 105, Loss: -0.38895949721336365\n",
      "Batch 106, Loss: -0.4863955080509186\n",
      "Batch 107, Loss: -0.6445367932319641\n",
      "Batch 108, Loss: 0.3204236328601837\n",
      "Batch 109, Loss: -0.454794317483902\n",
      "Batch 110, Loss: 0.31605201959609985\n",
      "Batch 111, Loss: 0.6543455123901367\n",
      "Batch 112, Loss: 0.4205969572067261\n",
      "Batch 113, Loss: -1.6150810718536377\n",
      "Batch 114, Loss: -0.5155202746391296\n",
      "Batch 115, Loss: 0.45641928911209106\n",
      "Batch 116, Loss: -0.3777918219566345\n",
      "Batch 117, Loss: -1.3924193382263184\n",
      "Batch 118, Loss: -0.5676878690719604\n",
      "Batch 119, Loss: 0.5884114503860474\n",
      "Batch 120, Loss: -1.4412611722946167\n",
      "Batch 121, Loss: -0.43781977891921997\n",
      "Batch 122, Loss: 0.4240599274635315\n",
      "Batch 123, Loss: -1.433241367340088\n",
      "Batch 124, Loss: -1.496884822845459\n",
      "Batch 125, Loss: -0.555941104888916\n",
      "Batch 126, Loss: -0.5404581427574158\n",
      "Batch 127, Loss: -0.55402672290802\n",
      "Training [20%]\tLoss: -0.5096\n",
      "Batch 0, Loss: -0.4990990161895752\n",
      "Batch 1, Loss: -0.6325110197067261\n",
      "Batch 2, Loss: -0.521278977394104\n",
      "Batch 3, Loss: -0.4666321873664856\n",
      "Batch 4, Loss: -0.649936318397522\n",
      "Batch 5, Loss: 0.35475099086761475\n",
      "Batch 6, Loss: -0.508886456489563\n",
      "Batch 7, Loss: 0.4142190217971802\n",
      "Batch 8, Loss: 0.5542377233505249\n",
      "Batch 9, Loss: -1.5884240865707397\n",
      "Batch 10, Loss: 0.5160965919494629\n",
      "Batch 11, Loss: -0.543205738067627\n",
      "Batch 12, Loss: -1.4114038944244385\n",
      "Batch 13, Loss: -0.6318855285644531\n",
      "Batch 14, Loss: -1.5352022647857666\n",
      "Batch 15, Loss: 0.4757702052593231\n",
      "Batch 16, Loss: -0.6081963181495667\n",
      "Batch 17, Loss: 0.3100053668022156\n",
      "Batch 18, Loss: -1.4863171577453613\n",
      "Batch 19, Loss: 0.4604869484901428\n",
      "Batch 20, Loss: -0.5975104570388794\n",
      "Batch 21, Loss: -0.5430303812026978\n",
      "Batch 22, Loss: -1.4283933639526367\n",
      "Batch 23, Loss: -0.41654783487319946\n",
      "Batch 24, Loss: -0.584999680519104\n",
      "Batch 25, Loss: 0.5496053099632263\n",
      "Batch 26, Loss: -1.40486478805542\n",
      "Batch 27, Loss: -0.5742111802101135\n",
      "Batch 28, Loss: 0.5640994310379028\n",
      "Batch 29, Loss: -1.5354878902435303\n",
      "Batch 30, Loss: -0.457966148853302\n",
      "Batch 31, Loss: -1.4294496774673462\n",
      "Batch 32, Loss: 0.42570045590400696\n",
      "Batch 33, Loss: -1.5494511127471924\n",
      "Batch 34, Loss: -0.501473069190979\n",
      "Batch 35, Loss: -0.6143732070922852\n",
      "Batch 36, Loss: -0.6191597580909729\n",
      "Batch 37, Loss: 0.5866789221763611\n",
      "Batch 38, Loss: -0.46757596731185913\n",
      "Batch 39, Loss: -0.41805633902549744\n",
      "Batch 40, Loss: -0.40951231122016907\n",
      "Batch 41, Loss: 0.3120810091495514\n",
      "Batch 42, Loss: 0.4517340362071991\n",
      "Batch 43, Loss: 0.5388138890266418\n",
      "Batch 44, Loss: -0.522812008857727\n",
      "Batch 45, Loss: -0.5356782674789429\n",
      "Batch 46, Loss: -0.43139949440956116\n",
      "Batch 47, Loss: -0.4602954387664795\n",
      "Batch 48, Loss: 0.48420119285583496\n",
      "Batch 49, Loss: -0.3693768084049225\n",
      "Batch 50, Loss: -1.4890508651733398\n",
      "Batch 51, Loss: -1.4262948036193848\n",
      "Batch 52, Loss: -0.34300506114959717\n",
      "Batch 53, Loss: -0.49029600620269775\n",
      "Batch 54, Loss: -1.4715335369110107\n",
      "Batch 55, Loss: -0.4320441782474518\n",
      "Batch 56, Loss: 0.6020199060440063\n",
      "Batch 57, Loss: -1.5436424016952515\n",
      "Batch 58, Loss: -1.536338210105896\n",
      "Batch 59, Loss: -0.5798261761665344\n",
      "Batch 60, Loss: -1.5568774938583374\n",
      "Batch 61, Loss: -0.5654412508010864\n",
      "Batch 62, Loss: -1.5239756107330322\n",
      "Batch 63, Loss: -1.4171867370605469\n",
      "Batch 64, Loss: -0.4343344271183014\n",
      "Batch 65, Loss: -0.304520845413208\n",
      "Batch 66, Loss: 0.506759524345398\n",
      "Batch 67, Loss: -1.3759417533874512\n",
      "Batch 68, Loss: -0.513097882270813\n",
      "Batch 69, Loss: -0.43198883533477783\n",
      "Batch 70, Loss: -0.5929321646690369\n",
      "Batch 71, Loss: 0.37620043754577637\n",
      "Batch 72, Loss: -1.6089491844177246\n",
      "Batch 73, Loss: 0.4261493682861328\n",
      "Batch 74, Loss: -0.5212733745574951\n",
      "Batch 75, Loss: 0.33796805143356323\n",
      "Batch 76, Loss: -0.5455970764160156\n",
      "Batch 77, Loss: -0.508401095867157\n",
      "Batch 78, Loss: -0.5257366299629211\n",
      "Batch 79, Loss: 0.4971234202384949\n",
      "Batch 80, Loss: -0.5013604164123535\n",
      "Batch 81, Loss: -0.5646036267280579\n",
      "Batch 82, Loss: 0.4328184127807617\n",
      "Batch 83, Loss: 0.5654269456863403\n",
      "Batch 84, Loss: -1.4204764366149902\n",
      "Batch 85, Loss: 0.38520991802215576\n",
      "Batch 86, Loss: -1.5084223747253418\n",
      "Batch 87, Loss: 0.5861880779266357\n",
      "Batch 88, Loss: -0.5198680758476257\n",
      "Batch 89, Loss: -0.42742305994033813\n",
      "Batch 90, Loss: -0.44833290576934814\n",
      "Batch 91, Loss: -1.543088436126709\n",
      "Batch 92, Loss: -0.4363845884799957\n",
      "Batch 93, Loss: -1.439002275466919\n",
      "Batch 94, Loss: -0.4157879054546356\n",
      "Batch 95, Loss: 0.3296431601047516\n",
      "Batch 96, Loss: -0.4691966772079468\n",
      "Batch 97, Loss: -1.5414378643035889\n",
      "Batch 98, Loss: -1.488097906112671\n",
      "Batch 99, Loss: -0.4280666410923004\n",
      "Batch 100, Loss: -0.4524182677268982\n",
      "Batch 101, Loss: -0.574508786201477\n",
      "Batch 102, Loss: -1.4165046215057373\n",
      "Batch 103, Loss: -0.5373562574386597\n",
      "Batch 104, Loss: 0.5717931985855103\n",
      "Batch 105, Loss: -0.4468975067138672\n",
      "Batch 106, Loss: -0.5231781601905823\n",
      "Batch 107, Loss: -0.4393213987350464\n",
      "Batch 108, Loss: -1.564815878868103\n",
      "Batch 109, Loss: -0.4380756914615631\n",
      "Batch 110, Loss: -0.5115550756454468\n",
      "Batch 111, Loss: -0.56296706199646\n",
      "Batch 112, Loss: -1.4995050430297852\n",
      "Batch 113, Loss: -1.4506664276123047\n",
      "Batch 114, Loss: -0.44774383306503296\n",
      "Batch 115, Loss: -0.3832629323005676\n",
      "Batch 116, Loss: 0.5780496597290039\n",
      "Batch 117, Loss: -0.4020438492298126\n",
      "Batch 118, Loss: -0.48812294006347656\n",
      "Batch 119, Loss: -1.4585928916931152\n",
      "Batch 120, Loss: -1.6448314189910889\n",
      "Batch 121, Loss: -1.5563058853149414\n",
      "Batch 122, Loss: -0.647434651851654\n",
      "Batch 123, Loss: 0.5477855801582336\n",
      "Batch 124, Loss: -0.6443129777908325\n",
      "Batch 125, Loss: 0.5430856943130493\n",
      "Batch 126, Loss: 0.5608645677566528\n",
      "Batch 127, Loss: 0.5595648288726807\n",
      "Training [30%]\tLoss: -0.5041\n",
      "Batch 0, Loss: -0.44227689504623413\n",
      "Batch 1, Loss: -0.40491271018981934\n",
      "Batch 2, Loss: -0.5699060559272766\n",
      "Batch 3, Loss: -0.5499035716056824\n",
      "Batch 4, Loss: -0.5336369872093201\n",
      "Batch 5, Loss: -0.6265424489974976\n",
      "Batch 6, Loss: -1.444568157196045\n",
      "Batch 7, Loss: 0.4663841724395752\n",
      "Batch 8, Loss: -0.4199334383010864\n",
      "Batch 9, Loss: -0.5376153588294983\n",
      "Batch 10, Loss: -0.592298686504364\n",
      "Batch 11, Loss: -0.5077486634254456\n",
      "Batch 12, Loss: -0.4508962035179138\n",
      "Batch 13, Loss: -1.4779222011566162\n",
      "Batch 14, Loss: -1.332991361618042\n",
      "Batch 15, Loss: -1.430488109588623\n",
      "Batch 16, Loss: -0.5125492811203003\n",
      "Batch 17, Loss: -0.4326118528842926\n",
      "Batch 18, Loss: -0.5143182277679443\n",
      "Batch 19, Loss: -0.4310988187789917\n",
      "Batch 20, Loss: -1.6262567043304443\n",
      "Batch 21, Loss: -0.4742850661277771\n",
      "Batch 22, Loss: 0.3793398141860962\n",
      "Batch 23, Loss: -0.4144822061061859\n",
      "Batch 24, Loss: -0.48852986097335815\n",
      "Batch 25, Loss: 0.40825793147087097\n",
      "Batch 26, Loss: 0.40975630283355713\n",
      "Batch 27, Loss: 0.3728652596473694\n",
      "Batch 28, Loss: 0.610622763633728\n",
      "Batch 29, Loss: 0.5907196402549744\n",
      "Batch 30, Loss: -1.512729525566101\n",
      "Batch 31, Loss: -0.3228740692138672\n",
      "Batch 32, Loss: -0.6481732130050659\n",
      "Batch 33, Loss: -0.4929676651954651\n",
      "Batch 34, Loss: 0.514953076839447\n",
      "Batch 35, Loss: -0.5187017917633057\n",
      "Batch 36, Loss: -1.4869908094406128\n",
      "Batch 37, Loss: -1.4694291353225708\n",
      "Batch 38, Loss: -0.5658198595046997\n",
      "Batch 39, Loss: -1.512641191482544\n",
      "Batch 40, Loss: 0.5292708277702332\n",
      "Batch 41, Loss: 0.4211646318435669\n",
      "Batch 42, Loss: -0.481136292219162\n",
      "Batch 43, Loss: -0.4596429467201233\n",
      "Batch 44, Loss: -1.3784115314483643\n",
      "Batch 45, Loss: -1.4957044124603271\n",
      "Batch 46, Loss: 0.4834136962890625\n",
      "Batch 47, Loss: -1.403785228729248\n",
      "Batch 48, Loss: -0.4410216808319092\n",
      "Batch 49, Loss: -0.5175201892852783\n",
      "Batch 50, Loss: -0.48158106207847595\n",
      "Batch 51, Loss: -0.47868508100509644\n",
      "Batch 52, Loss: -0.5145474672317505\n",
      "Batch 53, Loss: 0.44112247228622437\n",
      "Batch 54, Loss: -0.4317581057548523\n",
      "Batch 55, Loss: -0.4368341863155365\n",
      "Batch 56, Loss: 0.5025514960289001\n",
      "Batch 57, Loss: 0.3943333029747009\n",
      "Batch 58, Loss: -1.561478853225708\n",
      "Batch 59, Loss: 0.6505135297775269\n",
      "Batch 60, Loss: -0.5760430693626404\n",
      "Batch 61, Loss: -0.44337520003318787\n",
      "Batch 62, Loss: -0.34425032138824463\n",
      "Batch 63, Loss: 0.5429038405418396\n",
      "Batch 64, Loss: -0.44069892168045044\n",
      "Batch 65, Loss: -0.37730222940444946\n",
      "Batch 66, Loss: -1.416073203086853\n",
      "Batch 67, Loss: 0.5170413255691528\n",
      "Batch 68, Loss: -0.5499293804168701\n",
      "Batch 69, Loss: -0.5568403601646423\n",
      "Batch 70, Loss: -1.433517575263977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 71, Loss: -0.7221547961235046\n",
      "Batch 72, Loss: -1.5041124820709229\n",
      "Batch 73, Loss: -0.6067348718643188\n",
      "Batch 74, Loss: -0.5789905786514282\n",
      "Batch 75, Loss: -0.39492183923721313\n",
      "Batch 76, Loss: -0.6204352974891663\n",
      "Batch 77, Loss: -1.3759511709213257\n",
      "Batch 78, Loss: -0.5021383762359619\n",
      "Batch 79, Loss: 0.5612117648124695\n",
      "Batch 80, Loss: -0.48125937581062317\n",
      "Batch 81, Loss: -0.557915449142456\n",
      "Batch 82, Loss: -1.4855897426605225\n",
      "Batch 83, Loss: -0.5007556676864624\n",
      "Batch 84, Loss: 0.544059157371521\n",
      "Batch 85, Loss: -0.4449693560600281\n",
      "Batch 86, Loss: -0.4766855835914612\n",
      "Batch 87, Loss: -0.5777261853218079\n",
      "Batch 88, Loss: -0.35272127389907837\n",
      "Batch 89, Loss: -1.4747995138168335\n",
      "Batch 90, Loss: -1.46510910987854\n",
      "Batch 91, Loss: -0.36073386669158936\n",
      "Batch 92, Loss: -0.4873790740966797\n",
      "Batch 93, Loss: -0.35244959592819214\n",
      "Batch 94, Loss: -0.44249987602233887\n",
      "Batch 95, Loss: 0.5696088075637817\n",
      "Batch 96, Loss: -1.578809142112732\n",
      "Batch 97, Loss: 0.5495387315750122\n",
      "Batch 98, Loss: 0.42616885900497437\n",
      "Batch 99, Loss: 0.5152730345726013\n",
      "Batch 100, Loss: 0.37315070629119873\n",
      "Batch 101, Loss: -1.5995570421218872\n",
      "Batch 102, Loss: -0.46716248989105225\n",
      "Batch 103, Loss: -1.4769586324691772\n",
      "Batch 104, Loss: -1.4798498153686523\n",
      "Batch 105, Loss: 0.5107695460319519\n",
      "Batch 106, Loss: 0.535094678401947\n",
      "Batch 107, Loss: -0.40479445457458496\n",
      "Batch 108, Loss: 0.532995879650116\n",
      "Batch 109, Loss: 0.5182652473449707\n",
      "Batch 110, Loss: -0.47514861822128296\n",
      "Batch 111, Loss: -1.5156195163726807\n",
      "Batch 112, Loss: -0.4527198076248169\n",
      "Batch 113, Loss: -0.5392393469810486\n",
      "Batch 114, Loss: -0.4048106074333191\n",
      "Batch 115, Loss: -0.5606046915054321\n",
      "Batch 116, Loss: 0.6226041913032532\n",
      "Batch 117, Loss: 0.39646923542022705\n",
      "Batch 118, Loss: -1.4749139547348022\n",
      "Batch 119, Loss: -1.4865082502365112\n",
      "Batch 120, Loss: 0.5567051768302917\n",
      "Batch 121, Loss: -1.5184929370880127\n",
      "Batch 122, Loss: -1.5704607963562012\n",
      "Batch 123, Loss: -1.5128445625305176\n",
      "Batch 124, Loss: -1.404083251953125\n",
      "Batch 125, Loss: -0.38605523109436035\n",
      "Batch 126, Loss: 0.4356856346130371\n",
      "Batch 127, Loss: -1.5083321332931519\n",
      "Training [40%]\tLoss: -0.4896\n",
      "Batch 0, Loss: -1.35553777217865\n",
      "Batch 1, Loss: 0.3058556616306305\n",
      "Batch 2, Loss: 0.31812965869903564\n",
      "Batch 3, Loss: -0.5208865404129028\n",
      "Batch 4, Loss: -0.39153343439102173\n",
      "Batch 5, Loss: -0.5032473206520081\n",
      "Batch 6, Loss: -1.4925159215927124\n",
      "Batch 7, Loss: -0.4450840353965759\n",
      "Batch 8, Loss: -0.5228143334388733\n",
      "Batch 9, Loss: 0.3886081576347351\n",
      "Batch 10, Loss: -0.49461597204208374\n",
      "Batch 11, Loss: -1.5548564195632935\n",
      "Batch 12, Loss: -1.4749784469604492\n",
      "Batch 13, Loss: -0.6034498810768127\n",
      "Batch 14, Loss: -0.4142533838748932\n",
      "Batch 15, Loss: -0.5092284679412842\n",
      "Batch 16, Loss: 0.5408129096031189\n",
      "Batch 17, Loss: -0.5093799233436584\n",
      "Batch 18, Loss: -0.5962843298912048\n",
      "Batch 19, Loss: 0.5649645328521729\n",
      "Batch 20, Loss: -0.35811489820480347\n",
      "Batch 21, Loss: -0.49530917406082153\n",
      "Batch 22, Loss: -0.560960590839386\n",
      "Batch 23, Loss: -1.462970495223999\n",
      "Batch 24, Loss: -0.5042504072189331\n",
      "Batch 25, Loss: -0.5084043741226196\n",
      "Batch 26, Loss: -0.5640523433685303\n",
      "Batch 27, Loss: -0.487096905708313\n",
      "Batch 28, Loss: -0.37755951285362244\n",
      "Batch 29, Loss: 0.4095568358898163\n",
      "Batch 30, Loss: -1.4286797046661377\n",
      "Batch 31, Loss: 0.5369795560836792\n",
      "Batch 32, Loss: -1.568711280822754\n",
      "Batch 33, Loss: -1.5345293283462524\n",
      "Batch 34, Loss: -0.48950207233428955\n",
      "Batch 35, Loss: -1.4179819822311401\n",
      "Batch 36, Loss: -1.6231770515441895\n",
      "Batch 37, Loss: -0.6292192935943604\n",
      "Batch 38, Loss: 0.5220823287963867\n",
      "Batch 39, Loss: -0.5620502829551697\n",
      "Batch 40, Loss: -0.5291274189949036\n",
      "Batch 41, Loss: -1.3953474760055542\n",
      "Batch 42, Loss: 0.4728248715400696\n",
      "Batch 43, Loss: -1.5511910915374756\n",
      "Batch 44, Loss: -0.47190067172050476\n",
      "Batch 45, Loss: 0.4260964095592499\n",
      "Batch 46, Loss: -1.5396349430084229\n",
      "Batch 47, Loss: -0.5859893560409546\n",
      "Batch 48, Loss: -0.5019937753677368\n",
      "Batch 49, Loss: 0.4660103917121887\n",
      "Batch 50, Loss: -1.5344810485839844\n",
      "Batch 51, Loss: -0.4133851230144501\n",
      "Batch 52, Loss: 0.49448612332344055\n",
      "Batch 53, Loss: -0.5249608755111694\n",
      "Batch 54, Loss: -0.5542664527893066\n",
      "Batch 55, Loss: -1.6008830070495605\n",
      "Batch 56, Loss: 0.4856877326965332\n",
      "Batch 57, Loss: -0.48397642374038696\n",
      "Batch 58, Loss: 0.5352511405944824\n",
      "Batch 59, Loss: 0.5237938761711121\n",
      "Batch 60, Loss: -0.34518346190452576\n",
      "Batch 61, Loss: -0.4239477217197418\n",
      "Batch 62, Loss: -0.5255191922187805\n",
      "Batch 63, Loss: -0.6222695112228394\n",
      "Batch 64, Loss: -1.5044540166854858\n",
      "Batch 65, Loss: -0.4971938729286194\n",
      "Batch 66, Loss: -0.5460625290870667\n",
      "Batch 67, Loss: -0.5706297755241394\n",
      "Batch 68, Loss: -1.5010826587677002\n",
      "Batch 69, Loss: -0.3566873073577881\n",
      "Batch 70, Loss: -1.5617554187774658\n",
      "Batch 71, Loss: -0.44919782876968384\n",
      "Batch 72, Loss: -0.5071101188659668\n",
      "Batch 73, Loss: -1.4649252891540527\n",
      "Batch 74, Loss: 0.5630840063095093\n",
      "Batch 75, Loss: 0.4342767298221588\n",
      "Batch 76, Loss: -1.3732426166534424\n",
      "Batch 77, Loss: -1.4142578840255737\n",
      "Batch 78, Loss: -0.40019047260284424\n",
      "Batch 79, Loss: -0.5525206327438354\n",
      "Batch 80, Loss: -0.5801128149032593\n",
      "Batch 81, Loss: -1.5426602363586426\n",
      "Batch 82, Loss: 0.6140100359916687\n",
      "Batch 83, Loss: -0.6177898645401001\n",
      "Batch 84, Loss: -0.675898551940918\n",
      "Batch 85, Loss: 0.4933183193206787\n",
      "Batch 86, Loss: -0.37940332293510437\n",
      "Batch 87, Loss: 0.44268912076950073\n",
      "Batch 88, Loss: 0.5009809136390686\n",
      "Batch 89, Loss: 0.5634582042694092\n",
      "Batch 90, Loss: -0.5221055150032043\n",
      "Batch 91, Loss: -0.422441303730011\n",
      "Batch 92, Loss: -0.6126906275749207\n",
      "Batch 93, Loss: 0.5402523279190063\n",
      "Batch 94, Loss: -0.4297235608100891\n",
      "Batch 95, Loss: -0.4504346549510956\n",
      "Batch 96, Loss: -0.597205638885498\n",
      "Batch 97, Loss: 0.4603484272956848\n",
      "Batch 98, Loss: -1.4661617279052734\n",
      "Batch 99, Loss: 0.5126726627349854\n",
      "Batch 100, Loss: -0.4540088474750519\n",
      "Batch 101, Loss: -0.38624662160873413\n",
      "Batch 102, Loss: -0.5663430690765381\n",
      "Batch 103, Loss: -1.4533408880233765\n",
      "Batch 104, Loss: -0.45778095722198486\n",
      "Batch 105, Loss: -0.5282042026519775\n",
      "Batch 106, Loss: -0.5582340359687805\n",
      "Batch 107, Loss: -0.46223652362823486\n",
      "Batch 108, Loss: 0.4418190121650696\n",
      "Batch 109, Loss: -0.3627829849720001\n",
      "Batch 110, Loss: -0.48602378368377686\n",
      "Batch 111, Loss: -0.6048616766929626\n",
      "Batch 112, Loss: -0.42586299777030945\n",
      "Batch 113, Loss: -0.43463024497032166\n",
      "Batch 114, Loss: -0.45304766297340393\n",
      "Batch 115, Loss: -1.494347095489502\n",
      "Batch 116, Loss: -0.3170751929283142\n",
      "Batch 117, Loss: -0.4980180859565735\n",
      "Batch 118, Loss: -0.5669999122619629\n",
      "Batch 119, Loss: -0.48120251297950745\n",
      "Batch 120, Loss: -1.3956093788146973\n",
      "Batch 121, Loss: -0.4272220730781555\n",
      "Batch 122, Loss: -1.381605863571167\n",
      "Batch 123, Loss: -0.589460015296936\n",
      "Batch 124, Loss: -0.3746030032634735\n",
      "Batch 125, Loss: -0.4957959055900574\n",
      "Batch 126, Loss: -0.49398112297058105\n",
      "Batch 127, Loss: 0.370596706867218\n",
      "Training [50%]\tLoss: -0.4983\n",
      "Batch 0, Loss: -0.4319790303707123\n",
      "Batch 1, Loss: -0.5547387599945068\n",
      "Batch 2, Loss: -0.5315199494361877\n",
      "Batch 3, Loss: -1.3115808963775635\n",
      "Batch 4, Loss: -1.3876930475234985\n",
      "Batch 5, Loss: 0.596315324306488\n",
      "Batch 6, Loss: 0.4904252290725708\n",
      "Batch 7, Loss: -0.6140899658203125\n",
      "Batch 8, Loss: -0.4782333970069885\n",
      "Batch 9, Loss: -1.3209004402160645\n",
      "Batch 10, Loss: 0.6564223766326904\n",
      "Batch 11, Loss: -1.4631714820861816\n",
      "Batch 12, Loss: -0.45131197571754456\n",
      "Batch 13, Loss: -0.5526814460754395\n",
      "Batch 14, Loss: -0.41345828771591187\n",
      "Batch 15, Loss: -0.5039211511611938\n",
      "Batch 16, Loss: -0.5746638774871826\n",
      "Batch 17, Loss: -0.47240912914276123\n",
      "Batch 18, Loss: 0.4982013702392578\n",
      "Batch 19, Loss: -0.6095668077468872\n",
      "Batch 20, Loss: -0.5428018569946289\n",
      "Batch 21, Loss: 0.4878968596458435\n",
      "Batch 22, Loss: -1.4636080265045166\n",
      "Batch 23, Loss: -0.5571510791778564\n",
      "Batch 24, Loss: -0.6407936811447144\n",
      "Batch 25, Loss: -1.427600622177124\n",
      "Batch 26, Loss: -1.5804164409637451\n",
      "Batch 27, Loss: -0.40588995814323425\n",
      "Batch 28, Loss: -1.3452248573303223\n",
      "Batch 29, Loss: -1.4471582174301147\n",
      "Batch 30, Loss: 0.42008811235427856\n",
      "Batch 31, Loss: -0.5309733152389526\n",
      "Batch 32, Loss: -0.4529893100261688\n",
      "Batch 33, Loss: -0.5350846648216248\n",
      "Batch 34, Loss: 0.5431328415870667\n",
      "Batch 35, Loss: -0.60072261095047\n",
      "Batch 36, Loss: -0.5687944889068604\n",
      "Batch 37, Loss: -0.546376645565033\n",
      "Batch 38, Loss: -1.5024046897888184\n",
      "Batch 39, Loss: 0.470766544342041\n",
      "Batch 40, Loss: -0.4015119969844818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 41, Loss: 0.4071049392223358\n",
      "Batch 42, Loss: -0.4586257338523865\n",
      "Batch 43, Loss: -1.4498238563537598\n",
      "Batch 44, Loss: -0.6355332136154175\n",
      "Batch 45, Loss: -0.633643627166748\n",
      "Batch 46, Loss: -0.41044673323631287\n",
      "Batch 47, Loss: -0.4991663694381714\n",
      "Batch 48, Loss: -1.5511870384216309\n",
      "Batch 49, Loss: -0.5891933441162109\n",
      "Batch 50, Loss: -0.5486889481544495\n",
      "Batch 51, Loss: -1.492058277130127\n",
      "Batch 52, Loss: -0.5785899758338928\n",
      "Batch 53, Loss: 0.45945680141448975\n",
      "Batch 54, Loss: 0.4498296082019806\n",
      "Batch 55, Loss: -1.3742351531982422\n",
      "Batch 56, Loss: -0.5976629853248596\n",
      "Batch 57, Loss: -1.3743149042129517\n",
      "Batch 58, Loss: 0.4905184209346771\n",
      "Batch 59, Loss: -0.43953320384025574\n",
      "Batch 60, Loss: -1.5528979301452637\n",
      "Batch 61, Loss: -1.5596961975097656\n",
      "Batch 62, Loss: -0.539635419845581\n",
      "Batch 63, Loss: -0.4741567075252533\n",
      "Batch 64, Loss: -0.35231295228004456\n",
      "Batch 65, Loss: -1.5737426280975342\n",
      "Batch 66, Loss: -0.5571724772453308\n",
      "Batch 67, Loss: 0.49781155586242676\n",
      "Batch 68, Loss: -1.4901535511016846\n",
      "Batch 69, Loss: -1.455237627029419\n",
      "Batch 70, Loss: -1.55613374710083\n",
      "Batch 71, Loss: -0.40733927488327026\n",
      "Batch 72, Loss: -0.5468763709068298\n",
      "Batch 73, Loss: -0.3393528163433075\n",
      "Batch 74, Loss: 0.470866858959198\n",
      "Batch 75, Loss: -0.4882148504257202\n",
      "Batch 76, Loss: -1.5214539766311646\n",
      "Batch 77, Loss: -0.6118680238723755\n",
      "Batch 78, Loss: -0.3519575595855713\n",
      "Batch 79, Loss: -0.5589198470115662\n",
      "Batch 80, Loss: -0.5124702453613281\n",
      "Batch 81, Loss: -0.47071635723114014\n",
      "Batch 82, Loss: 0.46939337253570557\n",
      "Batch 83, Loss: -0.5434895157814026\n",
      "Batch 84, Loss: -0.4016643464565277\n",
      "Batch 85, Loss: 0.4753488600254059\n",
      "Batch 86, Loss: -0.6026279926300049\n",
      "Batch 87, Loss: -0.5331381559371948\n",
      "Batch 88, Loss: -1.6124283075332642\n",
      "Batch 89, Loss: -1.4944945573806763\n",
      "Batch 90, Loss: -1.461364507675171\n",
      "Batch 91, Loss: -0.6649473309516907\n",
      "Batch 92, Loss: -0.6202043294906616\n",
      "Batch 93, Loss: -0.5104907751083374\n",
      "Batch 94, Loss: -0.5515619516372681\n",
      "Batch 95, Loss: -0.6652750372886658\n",
      "Batch 96, Loss: 0.5119917988777161\n",
      "Batch 97, Loss: 0.4124627709388733\n",
      "Batch 98, Loss: -0.6371840238571167\n",
      "Batch 99, Loss: -0.418462872505188\n",
      "Batch 100, Loss: -1.3972070217132568\n",
      "Batch 101, Loss: 0.3777695894241333\n",
      "Batch 102, Loss: 0.42997491359710693\n",
      "Batch 103, Loss: -0.6457446813583374\n",
      "Batch 104, Loss: -0.5998126864433289\n",
      "Batch 105, Loss: -1.526090145111084\n",
      "Batch 106, Loss: 0.6426529288291931\n",
      "Batch 107, Loss: -0.6236730217933655\n",
      "Batch 108, Loss: 0.5849047303199768\n",
      "Batch 109, Loss: -0.5448826551437378\n",
      "Batch 110, Loss: 0.40599408745765686\n",
      "Batch 111, Loss: 0.5200006365776062\n",
      "Batch 112, Loss: -1.4071400165557861\n",
      "Batch 113, Loss: 0.4395056962966919\n",
      "Batch 114, Loss: -0.5635182857513428\n",
      "Batch 115, Loss: -0.6368131041526794\n",
      "Batch 116, Loss: 0.5266668796539307\n",
      "Batch 117, Loss: -0.6521536111831665\n",
      "Batch 118, Loss: -1.5083425045013428\n",
      "Batch 119, Loss: -0.6342178583145142\n",
      "Batch 120, Loss: -0.6544100046157837\n",
      "Batch 121, Loss: -0.4675106406211853\n",
      "Batch 122, Loss: -1.4015042781829834\n",
      "Batch 123, Loss: 0.46634480357170105\n",
      "Batch 124, Loss: 0.649229884147644\n",
      "Batch 125, Loss: 0.4696899354457855\n",
      "Batch 126, Loss: -0.4913947582244873\n",
      "Batch 127, Loss: 0.3799108564853668\n",
      "Training [60%]\tLoss: -0.5121\n",
      "Batch 0, Loss: -0.656629204750061\n",
      "Batch 1, Loss: -0.5959103107452393\n",
      "Batch 2, Loss: -0.472697377204895\n",
      "Batch 3, Loss: -1.54581880569458\n",
      "Batch 4, Loss: 0.46774011850357056\n",
      "Batch 5, Loss: 0.2694040536880493\n",
      "Batch 6, Loss: -1.3777105808258057\n",
      "Batch 7, Loss: 0.6333555579185486\n",
      "Batch 8, Loss: -1.324576497077942\n",
      "Batch 9, Loss: 0.4182804822921753\n",
      "Batch 10, Loss: -1.4540823698043823\n",
      "Batch 11, Loss: -1.5678410530090332\n",
      "Batch 12, Loss: -0.4278275668621063\n",
      "Batch 13, Loss: -0.6547539234161377\n",
      "Batch 14, Loss: 0.43455690145492554\n",
      "Batch 15, Loss: 0.5616841316223145\n",
      "Batch 16, Loss: 0.4000551700592041\n",
      "Batch 17, Loss: -0.475477933883667\n",
      "Batch 18, Loss: -0.5537647008895874\n",
      "Batch 19, Loss: -0.5086457133293152\n",
      "Batch 20, Loss: -0.4515184462070465\n",
      "Batch 21, Loss: -0.6037617921829224\n",
      "Batch 22, Loss: 0.5364921689033508\n",
      "Batch 23, Loss: -1.5413899421691895\n",
      "Batch 24, Loss: -0.5323675274848938\n",
      "Batch 25, Loss: -1.4589760303497314\n",
      "Batch 26, Loss: -0.5387054681777954\n",
      "Batch 27, Loss: 0.5277518033981323\n",
      "Batch 28, Loss: 0.5121908783912659\n",
      "Batch 29, Loss: -0.4587553143501282\n",
      "Batch 30, Loss: -0.5468525886535645\n",
      "Batch 31, Loss: -0.48622000217437744\n",
      "Batch 32, Loss: -0.42811644077301025\n",
      "Batch 33, Loss: -0.4529077410697937\n",
      "Batch 34, Loss: 0.4323393702507019\n",
      "Batch 35, Loss: -0.5566334128379822\n",
      "Batch 36, Loss: -0.44997483491897583\n",
      "Batch 37, Loss: -0.5261861085891724\n",
      "Batch 38, Loss: 0.5835971832275391\n",
      "Batch 39, Loss: 0.499093621969223\n",
      "Batch 40, Loss: -1.3889201879501343\n",
      "Batch 41, Loss: 0.46269261837005615\n",
      "Batch 42, Loss: -0.5595362186431885\n",
      "Batch 43, Loss: -0.446790486574173\n",
      "Batch 44, Loss: -0.5593843460083008\n",
      "Batch 45, Loss: -0.4691735804080963\n",
      "Batch 46, Loss: -1.451714038848877\n",
      "Batch 47, Loss: -1.4452574253082275\n",
      "Batch 48, Loss: -0.5144831538200378\n",
      "Batch 49, Loss: -0.6347492337226868\n",
      "Batch 50, Loss: -1.4557093381881714\n",
      "Batch 51, Loss: -0.48083677887916565\n",
      "Batch 52, Loss: 0.4245734214782715\n",
      "Batch 53, Loss: -1.3302397727966309\n",
      "Batch 54, Loss: 0.5292780995368958\n",
      "Batch 55, Loss: -0.5455296039581299\n",
      "Batch 56, Loss: -0.5469438433647156\n",
      "Batch 57, Loss: 0.47718197107315063\n",
      "Batch 58, Loss: -0.5279513597488403\n",
      "Batch 59, Loss: -0.5082252025604248\n",
      "Batch 60, Loss: -1.4789249897003174\n",
      "Batch 61, Loss: -1.5637035369873047\n",
      "Batch 62, Loss: -0.5129324197769165\n",
      "Batch 63, Loss: -0.5039736032485962\n",
      "Batch 64, Loss: -0.5133175849914551\n",
      "Batch 65, Loss: 0.5331360697746277\n",
      "Batch 66, Loss: -0.5351941585540771\n",
      "Batch 67, Loss: -0.5121861696243286\n",
      "Batch 68, Loss: -1.4868626594543457\n",
      "Batch 69, Loss: -0.6103948354721069\n",
      "Batch 70, Loss: 0.43690744042396545\n",
      "Batch 71, Loss: 0.6137683391571045\n",
      "Batch 72, Loss: -0.565761148929596\n",
      "Batch 73, Loss: 0.4654431939125061\n",
      "Batch 74, Loss: -1.564328908920288\n",
      "Batch 75, Loss: -0.6436519622802734\n",
      "Batch 76, Loss: 0.44992220401763916\n",
      "Batch 77, Loss: -0.65293288230896\n",
      "Batch 78, Loss: 0.556848406791687\n",
      "Batch 79, Loss: -0.6719666719436646\n",
      "Batch 80, Loss: -0.515476644039154\n",
      "Batch 81, Loss: -0.6315094232559204\n",
      "Batch 82, Loss: -1.5107313394546509\n",
      "Batch 83, Loss: -1.4933092594146729\n",
      "Batch 84, Loss: -0.6352169513702393\n",
      "Batch 85, Loss: -1.565929889678955\n",
      "Batch 86, Loss: -1.433062195777893\n",
      "Batch 87, Loss: -1.5506110191345215\n",
      "Batch 88, Loss: -0.5785980224609375\n",
      "Batch 89, Loss: -0.5314439535140991\n",
      "Batch 90, Loss: -0.6932422518730164\n",
      "Batch 91, Loss: 0.35823336243629456\n",
      "Batch 92, Loss: -0.6756191253662109\n",
      "Batch 93, Loss: -0.5377024412155151\n",
      "Batch 94, Loss: 0.3938177525997162\n",
      "Batch 95, Loss: -0.6682902574539185\n",
      "Batch 96, Loss: -0.6794577836990356\n",
      "Batch 97, Loss: -0.6091609001159668\n",
      "Batch 98, Loss: -0.6375093460083008\n",
      "Batch 99, Loss: -0.5741686224937439\n",
      "Batch 100, Loss: -1.4820382595062256\n",
      "Batch 101, Loss: -0.46919870376586914\n",
      "Batch 102, Loss: -0.5919452905654907\n",
      "Batch 103, Loss: -0.6641727685928345\n",
      "Batch 104, Loss: -1.539633870124817\n",
      "Batch 105, Loss: -0.6941289305686951\n",
      "Batch 106, Loss: -1.6110727787017822\n",
      "Batch 107, Loss: -1.385972499847412\n",
      "Batch 108, Loss: -0.631255567073822\n",
      "Batch 109, Loss: 0.5367482900619507\n",
      "Batch 110, Loss: -0.6151189208030701\n",
      "Batch 111, Loss: -0.5724966526031494\n",
      "Batch 112, Loss: -0.6389161348342896\n",
      "Batch 113, Loss: -0.6356124877929688\n",
      "Batch 114, Loss: -0.6516410708427429\n",
      "Batch 115, Loss: -1.51717209815979\n",
      "Batch 116, Loss: -0.623190701007843\n",
      "Batch 117, Loss: -0.6708113551139832\n",
      "Batch 118, Loss: -0.6454980969429016\n",
      "Batch 119, Loss: -1.6052131652832031\n",
      "Batch 120, Loss: -0.6151621341705322\n",
      "Batch 121, Loss: -1.5533405542373657\n",
      "Batch 122, Loss: -0.6120846271514893\n",
      "Batch 123, Loss: -0.5750992298126221\n",
      "Batch 124, Loss: 0.4578624963760376\n",
      "Batch 125, Loss: 0.4724106788635254\n",
      "Batch 126, Loss: -0.5614554286003113\n",
      "Batch 127, Loss: -0.5526784658432007\n",
      "Training [70%]\tLoss: -0.5400\n",
      "Batch 0, Loss: -0.4465724229812622\n",
      "Batch 1, Loss: -0.5834422707557678\n",
      "Batch 2, Loss: 0.43984919786453247\n",
      "Batch 3, Loss: -0.4055333435535431\n",
      "Batch 4, Loss: -1.5160529613494873\n",
      "Batch 5, Loss: -0.571004331111908\n",
      "Batch 6, Loss: -0.44237977266311646\n",
      "Batch 7, Loss: -0.6878178119659424\n",
      "Batch 8, Loss: -0.4605157971382141\n",
      "Batch 9, Loss: -1.5258333683013916\n",
      "Batch 10, Loss: -1.4234514236450195\n",
      "Batch 11, Loss: 0.40792787075042725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12, Loss: -1.409783124923706\n",
      "Batch 13, Loss: -0.6208609938621521\n",
      "Batch 14, Loss: -0.49421295523643494\n",
      "Batch 15, Loss: -1.582824468612671\n",
      "Batch 16, Loss: 0.4877547323703766\n",
      "Batch 17, Loss: -0.4229162633419037\n",
      "Batch 18, Loss: 0.3396177589893341\n",
      "Batch 19, Loss: -0.544754147529602\n",
      "Batch 20, Loss: 0.3910951614379883\n",
      "Batch 21, Loss: 0.5048235654830933\n",
      "Batch 22, Loss: -0.6488536596298218\n",
      "Batch 23, Loss: 0.5495760440826416\n",
      "Batch 24, Loss: -0.651607871055603\n",
      "Batch 25, Loss: -1.3688734769821167\n",
      "Batch 26, Loss: -0.6020755767822266\n",
      "Batch 27, Loss: -0.5576571226119995\n",
      "Batch 28, Loss: -0.5516939163208008\n",
      "Batch 29, Loss: -1.421389102935791\n",
      "Batch 30, Loss: 0.5235464572906494\n",
      "Batch 31, Loss: -0.6896528601646423\n",
      "Batch 32, Loss: -0.6238536834716797\n",
      "Batch 33, Loss: -0.6015360355377197\n",
      "Batch 34, Loss: 0.4941079020500183\n",
      "Batch 35, Loss: 0.43376457691192627\n",
      "Batch 36, Loss: -1.476656436920166\n",
      "Batch 37, Loss: -1.413181185722351\n",
      "Batch 38, Loss: -0.5650221705436707\n",
      "Batch 39, Loss: 0.49339863657951355\n",
      "Batch 40, Loss: -0.6013621091842651\n",
      "Batch 41, Loss: -0.4809250235557556\n",
      "Batch 42, Loss: -0.512823760509491\n",
      "Batch 43, Loss: -0.5030391216278076\n",
      "Batch 44, Loss: -0.5060025453567505\n",
      "Batch 45, Loss: -0.45355725288391113\n",
      "Batch 46, Loss: -0.5282164216041565\n",
      "Batch 47, Loss: -1.4705833196640015\n",
      "Batch 48, Loss: -0.5582954287528992\n",
      "Batch 49, Loss: 0.42741858959198\n",
      "Batch 50, Loss: -0.5888780355453491\n",
      "Batch 51, Loss: -0.5888743996620178\n",
      "Batch 52, Loss: -1.4961795806884766\n",
      "Batch 53, Loss: -0.5255850553512573\n",
      "Batch 54, Loss: 0.5003037452697754\n",
      "Batch 55, Loss: -0.5839055776596069\n",
      "Batch 56, Loss: 0.5820446014404297\n",
      "Batch 57, Loss: -0.5003467798233032\n",
      "Batch 58, Loss: -0.5125154256820679\n",
      "Batch 59, Loss: -1.4401724338531494\n",
      "Batch 60, Loss: -0.5839921236038208\n",
      "Batch 61, Loss: -1.4535012245178223\n",
      "Batch 62, Loss: -1.512836217880249\n",
      "Batch 63, Loss: -1.4489259719848633\n",
      "Batch 64, Loss: -1.4778295755386353\n",
      "Batch 65, Loss: -0.42010968923568726\n",
      "Batch 66, Loss: 0.43350663781166077\n",
      "Batch 67, Loss: -0.5762948393821716\n",
      "Batch 68, Loss: -0.6079333424568176\n",
      "Batch 69, Loss: 0.49244779348373413\n",
      "Batch 70, Loss: -0.6176527738571167\n",
      "Batch 71, Loss: 0.47137773036956787\n",
      "Batch 72, Loss: 0.49933385848999023\n",
      "Batch 73, Loss: -1.4151818752288818\n",
      "Batch 74, Loss: -1.4719138145446777\n",
      "Batch 75, Loss: -1.4274976253509521\n",
      "Batch 76, Loss: -0.5573632121086121\n",
      "Batch 77, Loss: -0.5686490535736084\n",
      "Batch 78, Loss: -0.5352822542190552\n",
      "Batch 79, Loss: 0.4053240120410919\n",
      "Batch 80, Loss: -0.6713887453079224\n",
      "Batch 81, Loss: -1.4618690013885498\n",
      "Batch 82, Loss: 0.489946186542511\n",
      "Batch 83, Loss: -0.5720607042312622\n",
      "Batch 84, Loss: -1.5421092510223389\n",
      "Batch 85, Loss: -1.3979650735855103\n",
      "Batch 86, Loss: -0.5184553861618042\n",
      "Batch 87, Loss: -0.6909534931182861\n",
      "Batch 88, Loss: -0.5200824737548828\n",
      "Batch 89, Loss: -0.4932861626148224\n",
      "Batch 90, Loss: -0.5618261098861694\n",
      "Batch 91, Loss: 0.5223857164382935\n",
      "Batch 92, Loss: 0.3716069459915161\n",
      "Batch 93, Loss: -0.49545928835868835\n",
      "Batch 94, Loss: -0.56494140625\n",
      "Batch 95, Loss: -0.6381670236587524\n",
      "Batch 96, Loss: -1.4717190265655518\n",
      "Batch 97, Loss: -1.5051928758621216\n",
      "Batch 98, Loss: -1.503299355506897\n",
      "Batch 99, Loss: -1.4249927997589111\n",
      "Batch 100, Loss: 0.5666279792785645\n",
      "Batch 101, Loss: 0.5150473117828369\n",
      "Batch 102, Loss: -1.6425977945327759\n",
      "Batch 103, Loss: -1.3653616905212402\n",
      "Batch 104, Loss: -0.5651766657829285\n",
      "Batch 105, Loss: -0.6206173300743103\n",
      "Batch 106, Loss: -0.5288786888122559\n",
      "Batch 107, Loss: 0.4633316695690155\n",
      "Batch 108, Loss: 0.4524807333946228\n",
      "Batch 109, Loss: -0.5785356163978577\n",
      "Batch 110, Loss: 0.4823313057422638\n",
      "Batch 111, Loss: -0.6210015416145325\n",
      "Batch 112, Loss: -0.6152166128158569\n",
      "Batch 113, Loss: -0.6228359341621399\n",
      "Batch 114, Loss: -1.5146422386169434\n",
      "Batch 115, Loss: -0.5528417825698853\n",
      "Batch 116, Loss: 0.43853896856307983\n",
      "Batch 117, Loss: 0.45425039529800415\n",
      "Batch 118, Loss: -0.6626216769218445\n",
      "Batch 119, Loss: 0.38880398869514465\n",
      "Batch 120, Loss: -1.5476257801055908\n",
      "Batch 121, Loss: 0.404485285282135\n",
      "Batch 122, Loss: 0.319333016872406\n",
      "Batch 123, Loss: -0.6235029697418213\n",
      "Batch 124, Loss: -1.565030813217163\n",
      "Batch 125, Loss: -1.3697521686553955\n",
      "Batch 126, Loss: -0.6726552844047546\n",
      "Batch 127, Loss: -0.5593039393424988\n",
      "Training [80%]\tLoss: -0.5340\n",
      "Batch 0, Loss: -1.5092964172363281\n",
      "Batch 1, Loss: 0.5394885540008545\n",
      "Batch 2, Loss: -0.5417502522468567\n",
      "Batch 3, Loss: -0.6603743433952332\n",
      "Batch 4, Loss: -1.4190715551376343\n",
      "Batch 5, Loss: -0.5840996503829956\n",
      "Batch 6, Loss: -0.6902358531951904\n",
      "Batch 7, Loss: -0.5931249260902405\n",
      "Batch 8, Loss: -1.509666919708252\n",
      "Batch 9, Loss: -0.6447543501853943\n",
      "Batch 10, Loss: -0.628442108631134\n",
      "Batch 11, Loss: -0.6220523118972778\n",
      "Batch 12, Loss: -0.5512349009513855\n",
      "Batch 13, Loss: -0.6478968262672424\n",
      "Batch 14, Loss: -0.5101915597915649\n",
      "Batch 15, Loss: 0.4216078519821167\n",
      "Batch 16, Loss: 0.5035569071769714\n",
      "Batch 17, Loss: 0.3621498644351959\n",
      "Batch 18, Loss: 0.426403671503067\n",
      "Batch 19, Loss: -0.5944051742553711\n",
      "Batch 20, Loss: 0.381878525018692\n",
      "Batch 21, Loss: -0.6252355575561523\n",
      "Batch 22, Loss: 0.5051531791687012\n",
      "Batch 23, Loss: -0.5978184938430786\n",
      "Batch 24, Loss: -0.5227156281471252\n",
      "Batch 25, Loss: 0.49077385663986206\n",
      "Batch 26, Loss: -1.5535576343536377\n",
      "Batch 27, Loss: -1.5529048442840576\n",
      "Batch 28, Loss: -0.5798190832138062\n",
      "Batch 29, Loss: 0.45818400382995605\n",
      "Batch 30, Loss: -0.6451708674430847\n",
      "Batch 31, Loss: -0.584966242313385\n",
      "Batch 32, Loss: -0.5641175508499146\n",
      "Batch 33, Loss: -1.4858593940734863\n",
      "Batch 34, Loss: -1.5521430969238281\n",
      "Batch 35, Loss: -0.6844090223312378\n",
      "Batch 36, Loss: -0.5901737213134766\n",
      "Batch 37, Loss: -1.507076382637024\n",
      "Batch 38, Loss: -1.4551749229431152\n",
      "Batch 39, Loss: -1.5653549432754517\n",
      "Batch 40, Loss: -1.5000483989715576\n",
      "Batch 41, Loss: 0.4931199252605438\n",
      "Batch 42, Loss: -0.4280396103858948\n",
      "Batch 43, Loss: -1.4293283224105835\n",
      "Batch 44, Loss: -1.5376286506652832\n",
      "Batch 45, Loss: -0.36360669136047363\n",
      "Batch 46, Loss: -0.4398173689842224\n",
      "Batch 47, Loss: 0.6130155324935913\n",
      "Batch 48, Loss: 0.5744588375091553\n",
      "Batch 49, Loss: -0.5002071857452393\n",
      "Batch 50, Loss: 0.40047597885131836\n",
      "Batch 51, Loss: -1.4748347997665405\n",
      "Batch 52, Loss: -0.6112766265869141\n",
      "Batch 53, Loss: -0.5618798732757568\n",
      "Batch 54, Loss: -0.6395635008811951\n",
      "Batch 55, Loss: -0.5774000287055969\n",
      "Batch 56, Loss: -0.6515092253684998\n",
      "Batch 57, Loss: -0.6556821465492249\n",
      "Batch 58, Loss: -0.6376368999481201\n",
      "Batch 59, Loss: -0.643487274646759\n",
      "Batch 60, Loss: -1.5226081609725952\n",
      "Batch 61, Loss: -0.6680094003677368\n",
      "Batch 62, Loss: -1.586338758468628\n",
      "Batch 63, Loss: 0.5176666975021362\n",
      "Batch 64, Loss: -0.492514044046402\n",
      "Batch 65, Loss: -0.5063713192939758\n",
      "Batch 66, Loss: -0.6260663270950317\n",
      "Batch 67, Loss: -0.4193521738052368\n",
      "Batch 68, Loss: -0.4876054525375366\n",
      "Batch 69, Loss: -0.5867273807525635\n",
      "Batch 70, Loss: -0.632005512714386\n",
      "Batch 71, Loss: -0.6711888313293457\n",
      "Batch 72, Loss: 0.419547438621521\n",
      "Batch 73, Loss: -0.35769134759902954\n",
      "Batch 74, Loss: -1.5192630290985107\n",
      "Batch 75, Loss: -0.5878013372421265\n",
      "Batch 76, Loss: -1.351527452468872\n",
      "Batch 77, Loss: -0.6420184373855591\n",
      "Batch 78, Loss: -1.5378625392913818\n",
      "Batch 79, Loss: -0.652695894241333\n",
      "Batch 80, Loss: -0.6259496808052063\n",
      "Batch 81, Loss: -0.6700059175491333\n",
      "Batch 82, Loss: -0.6637577414512634\n",
      "Batch 83, Loss: -1.5791168212890625\n",
      "Batch 84, Loss: 0.43704575300216675\n",
      "Batch 85, Loss: 0.4754026532173157\n",
      "Batch 86, Loss: -0.6531829833984375\n",
      "Batch 87, Loss: -1.5036089420318604\n",
      "Batch 88, Loss: -0.6510725021362305\n",
      "Batch 89, Loss: -0.5389388799667358\n",
      "Batch 90, Loss: 0.5511857271194458\n",
      "Batch 91, Loss: 0.45881325006484985\n",
      "Batch 92, Loss: -1.4055454730987549\n",
      "Batch 93, Loss: -0.660197377204895\n",
      "Batch 94, Loss: -1.5141639709472656\n",
      "Batch 95, Loss: -0.6784801483154297\n",
      "Batch 96, Loss: -0.6593515872955322\n",
      "Batch 97, Loss: -1.4891016483306885\n",
      "Batch 98, Loss: 0.4434644281864166\n",
      "Batch 99, Loss: -0.5150187611579895\n",
      "Batch 100, Loss: -1.5795193910598755\n",
      "Batch 101, Loss: -0.6206871271133423\n",
      "Batch 102, Loss: 0.5073606371879578\n",
      "Batch 103, Loss: -0.6851202249526978\n",
      "Batch 104, Loss: -1.6441638469696045\n",
      "Batch 105, Loss: -1.3780694007873535\n",
      "Batch 106, Loss: -0.6937454342842102\n",
      "Batch 107, Loss: 0.48662614822387695\n",
      "Batch 108, Loss: -1.5116132497787476\n",
      "Batch 109, Loss: -0.6355332732200623\n",
      "Batch 110, Loss: 0.5259634852409363\n",
      "Batch 111, Loss: 0.49704205989837646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 112, Loss: 0.593044638633728\n",
      "Batch 113, Loss: 0.4849753975868225\n",
      "Batch 114, Loss: 0.4010970890522003\n",
      "Batch 115, Loss: 0.5460138320922852\n",
      "Batch 116, Loss: -0.4625868499279022\n",
      "Batch 117, Loss: -0.6189586520195007\n",
      "Batch 118, Loss: 0.5407217741012573\n",
      "Batch 119, Loss: -0.5192348957061768\n",
      "Batch 120, Loss: -0.598384439945221\n",
      "Batch 121, Loss: -1.3499689102172852\n",
      "Batch 122, Loss: -1.3898823261260986\n",
      "Batch 123, Loss: -0.6741741895675659\n",
      "Batch 124, Loss: -0.5783237218856812\n",
      "Batch 125, Loss: 0.5680116415023804\n",
      "Batch 126, Loss: -1.4137134552001953\n",
      "Batch 127, Loss: 0.5070372819900513\n",
      "Training [90%]\tLoss: -0.5492\n",
      "Batch 0, Loss: 0.3806006610393524\n",
      "Batch 1, Loss: -1.4078508615493774\n",
      "Batch 2, Loss: -1.5894758701324463\n",
      "Batch 3, Loss: -0.41959381103515625\n",
      "Batch 4, Loss: -1.4489407539367676\n",
      "Batch 5, Loss: -0.5416423678398132\n",
      "Batch 6, Loss: -0.5054429173469543\n",
      "Batch 7, Loss: -0.5180999636650085\n",
      "Batch 8, Loss: 0.48552507162094116\n",
      "Batch 9, Loss: 0.436032772064209\n",
      "Batch 10, Loss: -0.4148343801498413\n",
      "Batch 11, Loss: 0.5268173217773438\n",
      "Batch 12, Loss: -0.5831875801086426\n",
      "Batch 13, Loss: -0.42542555928230286\n",
      "Batch 14, Loss: -1.4339697360992432\n",
      "Batch 15, Loss: -0.4797772169113159\n",
      "Batch 16, Loss: -1.4167304039001465\n",
      "Batch 17, Loss: -1.520031213760376\n",
      "Batch 18, Loss: -1.521167516708374\n",
      "Batch 19, Loss: -0.5896321535110474\n",
      "Batch 20, Loss: 0.3967590928077698\n",
      "Batch 21, Loss: -0.6547442078590393\n",
      "Batch 22, Loss: -0.4540601372718811\n",
      "Batch 23, Loss: -1.4937314987182617\n",
      "Batch 24, Loss: 0.43256276845932007\n",
      "Batch 25, Loss: -0.4450858235359192\n",
      "Batch 26, Loss: 0.4080171585083008\n",
      "Batch 27, Loss: -0.591187596321106\n",
      "Batch 28, Loss: -0.5850844383239746\n",
      "Batch 29, Loss: 0.5317144989967346\n",
      "Batch 30, Loss: 0.42864030599594116\n",
      "Batch 31, Loss: -1.363424301147461\n",
      "Batch 32, Loss: -0.668071985244751\n",
      "Batch 33, Loss: 0.23702377080917358\n",
      "Batch 34, Loss: -0.5293298959732056\n",
      "Batch 35, Loss: -0.4544333219528198\n",
      "Batch 36, Loss: 0.5740424394607544\n",
      "Batch 37, Loss: -1.5332591533660889\n",
      "Batch 38, Loss: 0.5615373849868774\n",
      "Batch 39, Loss: -1.4144885540008545\n",
      "Batch 40, Loss: -1.589678406715393\n",
      "Batch 41, Loss: -1.5724769830703735\n",
      "Batch 42, Loss: 0.4698169529438019\n",
      "Batch 43, Loss: 0.459261953830719\n",
      "Batch 44, Loss: -0.5630201101303101\n",
      "Batch 45, Loss: -0.5042247772216797\n",
      "Batch 46, Loss: -1.5511305332183838\n",
      "Batch 47, Loss: -0.584989070892334\n",
      "Batch 48, Loss: -1.5002835988998413\n",
      "Batch 49, Loss: 0.45792311429977417\n",
      "Batch 50, Loss: -1.52297043800354\n",
      "Batch 51, Loss: -1.5007920265197754\n",
      "Batch 52, Loss: -0.5121929049491882\n",
      "Batch 53, Loss: 0.5315213203430176\n",
      "Batch 54, Loss: -0.5806766748428345\n",
      "Batch 55, Loss: -0.412247896194458\n",
      "Batch 56, Loss: -1.476712942123413\n",
      "Batch 57, Loss: -1.4140361547470093\n",
      "Batch 58, Loss: -0.40117156505584717\n",
      "Batch 59, Loss: -0.41675037145614624\n",
      "Batch 60, Loss: -1.4855295419692993\n",
      "Batch 61, Loss: -1.5378751754760742\n",
      "Batch 62, Loss: -0.35255977511405945\n",
      "Batch 63, Loss: -0.40158602595329285\n",
      "Batch 64, Loss: 0.31035876274108887\n",
      "Batch 65, Loss: -0.38693273067474365\n",
      "Batch 66, Loss: 0.3947286009788513\n",
      "Batch 67, Loss: -0.3656640648841858\n",
      "Batch 68, Loss: -0.3904462456703186\n",
      "Batch 69, Loss: 0.45366230607032776\n",
      "Batch 70, Loss: -0.7203168869018555\n",
      "Batch 71, Loss: -0.45273101329803467\n",
      "Batch 72, Loss: -0.3929356336593628\n",
      "Batch 73, Loss: -1.5453619956970215\n",
      "Batch 74, Loss: -0.5525042414665222\n",
      "Batch 75, Loss: 0.35189390182495117\n",
      "Batch 76, Loss: 0.41881263256073\n",
      "Batch 77, Loss: -0.5407992005348206\n",
      "Batch 78, Loss: 0.5859599113464355\n",
      "Batch 79, Loss: -0.4319969415664673\n",
      "Batch 80, Loss: -1.4067708253860474\n",
      "Batch 81, Loss: -0.517215371131897\n",
      "Batch 82, Loss: -0.4111618995666504\n",
      "Batch 83, Loss: -0.4957740306854248\n",
      "Batch 84, Loss: -1.440772294998169\n",
      "Batch 85, Loss: -0.46513593196868896\n",
      "Batch 86, Loss: 0.49553048610687256\n",
      "Batch 87, Loss: -1.3842171430587769\n",
      "Batch 88, Loss: -0.5012431740760803\n",
      "Batch 89, Loss: -0.5302422642707825\n",
      "Batch 90, Loss: -1.551279067993164\n",
      "Batch 91, Loss: -0.6092507839202881\n",
      "Batch 92, Loss: -0.5600894689559937\n",
      "Batch 93, Loss: -0.5261273384094238\n",
      "Batch 94, Loss: -0.43817007541656494\n",
      "Batch 95, Loss: -0.5658726692199707\n",
      "Batch 96, Loss: -0.41544267535209656\n",
      "Batch 97, Loss: -1.6737608909606934\n",
      "Batch 98, Loss: -0.4613685607910156\n",
      "Batch 99, Loss: -0.37229984998703003\n",
      "Batch 100, Loss: -0.41753625869750977\n",
      "Batch 101, Loss: -1.6464190483093262\n",
      "Batch 102, Loss: -0.6173336505889893\n",
      "Batch 103, Loss: -0.45832598209381104\n",
      "Batch 104, Loss: 0.47817063331604004\n",
      "Batch 105, Loss: -0.45125752687454224\n",
      "Batch 106, Loss: -1.4355534315109253\n",
      "Batch 107, Loss: -0.4287329614162445\n",
      "Batch 108, Loss: 0.4482296109199524\n",
      "Batch 109, Loss: -1.568096399307251\n",
      "Batch 110, Loss: -0.49664562940597534\n",
      "Batch 111, Loss: 0.4410131871700287\n",
      "Batch 112, Loss: -0.5351566076278687\n",
      "Batch 113, Loss: 0.596267580986023\n",
      "Batch 114, Loss: -0.44357404112815857\n",
      "Batch 115, Loss: -1.564188003540039\n",
      "Batch 116, Loss: -0.5672947764396667\n",
      "Batch 117, Loss: 0.46746933460235596\n",
      "Batch 118, Loss: -0.40493038296699524\n",
      "Batch 119, Loss: -0.5329880714416504\n",
      "Batch 120, Loss: 0.522285521030426\n",
      "Batch 121, Loss: -0.6242655515670776\n",
      "Batch 122, Loss: -0.5454599857330322\n",
      "Batch 123, Loss: -0.44543272256851196\n",
      "Batch 124, Loss: 0.4942174553871155\n",
      "Batch 125, Loss: 0.4493172764778137\n",
      "Batch 126, Loss: -1.551015853881836\n",
      "Batch 127, Loss: 0.48285090923309326\n",
      "Training [100%]\tLoss: -0.5079\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.0008359469939023256 s\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: -0.5093\n",
      "\tAccuracy: 50.0%\n",
      "Test time: -0.00015425699530169368 s\n"
     ]
    }
   ],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
