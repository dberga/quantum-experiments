{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"hybridqnn_shallow\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "LR = 0.001\n",
    "n_samples_train = 200 #128\n",
    "n_samples_test = 50 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"CIFAR10\" # MNIST / CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = [\"cat\",\"dog\"] # ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "if network == 'hybridqnn_shallow' or network == 'QSVM':\n",
    "    # Set preprocessing transforms\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "else:\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    \n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: -2.62801768258214e-06 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIEAAACRCAYAAADkdtvLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAASVklEQVR4nO1dW4yd11X+1n859+Nz5uqxx+PxeHy3lSZpTKIkTaI0gRCpIAp94lKJVlRCAgkhQCAEFYpE+sADUh/gBSEaQcW9Kgj1ASVAFSB9oSRtYtM0dibjeG6ec7/9Nx7mxOf/lj1DjyXHDl6fNJI/7///9/7PWWevtddae21JkgSGexvOnR6A4c7DhMBgQmAwITDAhMAAEwIDPmJCICKviMjnP2L3PiUi793KvR8W7ogQiMglEXnmTvRtuBEfqZnAcHtwVwmBiEyIyD+IyIaIbA//fUhdtiwir4lIQ0S+JiKTqfsfEZFXRaQmIt8Wkaf26OvnReTNYT/fEJHFVNuzIvKWiNRF5MsAZIx3yIvInw6f+10A51X76aF6qYnId0Tkx1JtUyLy9eG7fUtEXhCRb/6gfd8ykiT50P8AXALwzE3+fwrATwIoACgD+CsAf59qfwXAKoBzAIoA/gbAS8O2eQBbAJ7HjnA/O+QzqXs/P/z3jwP4HoDTADwAvw3g1WHbNIAmgJ8C4AP4FQBh6t7DAGoADu/ybi8C+DcAkwAWALwB4L1hmz/s97cAZAA8Pezr5LD9q8O/AoAzAFYAfPO2fx93kxDc5Lr7AWwrIXgxxc8AGABwAfwGgK+o+78B4LM3EYJ/AvC51HUOgA6ARQA/B+A/Um0C4L0P7v0Bxvx9AM+l+C+khOATAK4CcFLtfwHgi8N3CD4QiGHbCx+GENxt6qAgIn8sIpdFpAHgXwFURcRNXbaS+vdl7Py6prHzBX5mOM3WRKQG4HEAB27S1SKAP0xddw07X/Y8gIPpPpKdb2PlJs/YDXT/cIzUliRJrNrnAcxgZ1ZK3ztOv7eMu0oIAPwqgJMAHk6SZB+AJ4b/n9bJC6l/H8bOr2cTOx/YV5Ikqab+ikmSvHiTflYAfEFdm0+S5FUA76f7EBFRff5foPuHY/wAVwAsiIij2lcBbGBH7aRtoHH6vWXcSSHwRSSX+vOwYwd0AdSGBt/v3uS+nxGRMyJSAPB7AP46SZIIwEsAPiUiPyIi7vCZT93EsASAPwLwmyJyFgBEpCIinxm2/SOAsyLy6eGYfhnA3Bjv9ZfDZ08M+/6lVNt/Ykft/LqI+EPD9VMAvjp8h78F8MXhjHgKO6rp9uMO2gSJ+nsBO9PlKwBaAC4C+MKwzUvp9d8H8BqABoCvA5hOPfdhAP+Cnel9Aztf6GFtEwz5zwJ4fficFQB/kmp7bth/HcCXh89MG4Yt7G4YFgD8GXaMx+8C+DUMbYJh+9nh8+rD9p9Itc0Mx9wA8C0AXwLwz7f7+5Bh54a7ECLyJQBzSZJ89nb2c7fZBPc0ROSUiNwnO/ghAJ8D8He3u1/vdndgGAtl7CwZDwJYA/AHAL52uzs1dWAwdWAwITBgTJvA9bzEy2Sucx1VEccl7mWz3K7u6AcBcb+4j7m6P4pj4mEwID7odInHPeYQVn1+oax4SfXH14eqf2hNGqj+gj5RV/Xn+vx5JerzGLSaxEV1mKtU+XmZHF/vjD7vzsYa+s36TQNhYwmBl8ngwLHj13lWDcot86Cmjxzl+8Unfvn9DeJz5z9JfHaZ7292W8Q33n+f+Op/vU68ffEN4hD+UmbPP0n84IOPE693+EvZbrSJh0omvFXuL7n6DvF9DzzB/MAE8ejKVeLv/PvLxHMOj//4j36aeHXxJHHfH/1gX/6dX8RuMHVgGHOJKC6c/PXwParCv4z5g1V+uMu/pEyOu2t7PX58i2eGkvL4lhLub67A0+nyySXinRLPfvXGNvH1Ac9k3Sa3uzmevsXn34yErI5il98vqteJh1cuEndmKdUArTY/Lwr588vv51iYl58iPuDLkWA0VSVq1qJx7N5kuFdgQmAwITCMaxN4PuLqzHWqVCL8kHVsVpnPbshLqKUZXpI1B6yTo3ffIl4O2CaYyhS5v6kC8czsWeKxsMxfarO1faFXI952+Pqyy++TDHi14ivrPcnx9cXOGvE5l20af+4I8dZRHn+hwO/nZvn9o4THGw5G/cd7OIZtJjCYEBhMCAwY122czWJy+dh1Llus0+MBuzkdhz2EwSAiXkq5oAFg611eR7evsQ6tzlR5QJVJokHAMp2o53tZ1qmLFb7e77DifGebd4/Nuux38CL2c5Sr3J4/d4T4yvo14tmoQ3z+2AniW/XLxEW5yb0cu9VDZWOEUcpPgN1hM4HBhMBgQmDA2LEDB0l2FK5sl+epeTDYJL6/wjZBbZ2jZIMB67h+Y4t4r8Pr8EqehytKhjMZ7s/pq9drNYgW1bp7MuHrex22CSr78sR9FQtxleJtxypyq2IR1RmOBbiTVeJ5V9k4yuZxHPV5eGwDSdqvIbtvp7SZwGBCYDAhMGBMmyAYBFhdGWXzJNNHqD0H5SuP2G+Qi3ldvKEyaZot9r3HPtsMG5vrxLc22QapVirE80rnd7ocu5ienubxqnX3NGdroeyx0vd81sGhwzdcabFNM32G8wfcicPEN2ocG4nV19Ntc2yl0eDP16uyzeKk8ht0ah9dt2uL4Z6BCYHBhMAwpk2QRDGCRspffpB17pqw77qifO0LZc6JizKcgzcI2WZw1f25POvszXW2EcKQffnZDuvoXo9tjHyBnxcrX77TY66zq/cd4nX+OnePepbbZ48/SHxN5QRe2GCdH+ZVvoDHNtB2k2MROZVfUc2lbIQ9dprZTGAwITCYEBhwC1vTnVSMOuywzgxzbCNsl7jKS6HI7XGb/Qpende93R77+tttXnfrKHmtXuOxtthGmZtT4ymwzdBSsYXZEvv6ixX2K1xaZx39+iqPb9Xj/hrr/H7X+nz/5jb7Cdw6P082eF/GIMfjHyRsQ/mFkY0QR8oAScFmAoMJgcGEwIBx/QRxiKBdG/1HjW2CaEJtPS/xurVX4b2FuUO8bg/eeZP49hbnF4R9ji2USirHMeYcRk/l3G2qWEOtxn6K/bOs8/MHeW9jUOScxve2eVf0ekgUXZ8/3rU17r/VqvH1LbYJsh32GwQrXNsyqfPn05/m8fRT+QaDNtsjadhMYDAhMJgQGDB27CBEkMoDzKn9/EnMOq0RqZy8PrdPtHldXlY6vuFzzqDv8XCr1SpxR+011BH0KGKb4epVzmfIK79AV+UcDjpsw2Tz7Pc4vJ+vb3k8gjBim0DU59Grsx8gUhyqnI5TZRsljNRvmnIOLZ/AsAdMCAwmBIYxbQKBwMNITzs59t1LQT1O6eieqhlU22CdvDjJ6/RoP/vey2XWiadOniLuuWxD6E35V9eu8Hg6vHaenaoSD7rsR9iq8T4EP8P5CMtlznFElmMjATjhoFXlz2tLVH6Gqqm0mmGbpV3k8bou5zwibWN5u3/VNhMYTAgMJgQGjGsT+Flk5q4fH4hErdOzqsxqTunopMY6uaHyBzDDsYaPnbuPeF7tC5iY5HVyrHz3BZXfMFllnToxwTbG0nGuCKp9/d0W2wgZT+UoKhuopPIdnAzHMnh0QLnI188eZpuo2OYXfFv5LeoqdhKl/AbxHoUMbSYwmBAYTAgMGDd24HoIJmevc8dlneh4nPPmxLwu9oV1WqR89Z2AdeLcHOcfxGpfwYqKr3dUbeBjR48RP3nqOPHDRxeJeyW2Ma7VVV1CcA2lfsDv0x6o942UTdRhvdzv8fN1bCOjaj4tl5gXPP4Nv6GKGw9Spe7be5z2bDOBwYTAYEJgwC3ULIrckd73lR+gVGQbAVdYZ4fbrFPdCp/84YJ1mqNOStncYt/96irztTXOuatW+DidpYiPHk4cVpQTJb7eU8flZBRPVE2hrPID9NTxOy1Vr0DXbMqo4358j20Oz2Ob4VBRHdezj22sFW/EGyrfMg2bCQwmBAYTAgPG3osokNTaVcfvp3Icz46VX2C9z+vons86zClzTmKpzLEEiTjfIJNhGT44w/UPSiV+vVqN9/MvneRawmWlUyO1f29e1SPIqZy/RpNjIc3G7rn+ACCqtqBeyjvqvAUVmoCf8Od5RJ3B1MmMbJgV8xMY9oIJgcGEwDBuPoEDeKm1cFaltDkB701sN1gHt5SO7AaqFnGBc/RKJY5FTJcOEj+0n22EUMXTY6Vzq7Nci/noMp8xtK7qJAYDXucvzLNN0As4FrC9XSOeUectuGqtrnW+p/IAfbXvQpRRkKhzEzMO72Mo90btbqSSLdLj2LXFcM/AhMBgQmAY0yZwHEGhONJTVVVLN1RHx283OCevp2oLJwG35w4oX3jIeqyvDmIMumyDQMUCKtOzxOcPcb2B8j5Vg+jyJX5+wPF+XSOpp2o26aPt86qmUKz2QYQZtmH0kQSOwzZEou5XJgniiMeb2aqNnh1azSLDHjAhMJgQGMb1EyQxcoNRTYFsR+W0dXmdGigbIGyxn8B3+f6Cy/H+flvdH7FNIKxSkS2yDi6ovYHlMucLZFRNoa6yMSKlR311va69nKgcQUfFVnSsxfe0o4VpouoRByqnMUxUf8qGKpdGNpvr7P57t5nAYEJgMCEwYFybIAzgpvIEe13WWddUDaDm6rvEI1U3MKfq+XsJ6+C+qlvoqM2GReVbL6icu5KyAXJ59mtoX71KIbwhFnDtGucwRurcw0DtO9B62FNpfolqd5VfwNFnHyudr/MLgpAdB262mLrWbALDHjAhMJgQGMa0CbxcDtMnTo+4wzq73uF1faRyCqHqFuqjhDuqhlCEGeI3+NYzPHxPreO1zk9u2KOvzjQq8z6IuqqfcPEC114+doJrJimVfoNfQ2cRqvQHxOAHaD0+UPkNQZ95s6vOgJKR3ySJrT6BYQ+YEBhMCAxj2gRxIuhGI73lxixDscu+e1F58FqnBzH7BbY77EcIodbFah2vbYpBwM9rNlXtZHWeQKnKOrtSZZsgr/wKb77FNsHUDOcjVCrsp2g2ORYxUPkWPVVPINRFl/ROBKXXu122yUKwkTHhjj5PRwdaUrCZwGBCYDAhMGBMmyAKAjRS8QHfUfUEVM6dr9bNXpn335+47wjxpXOsYzc7XN8gE3PlP9djHRmqvXltdZ5AvsixikmVg5jJcSxiZo79FP/97VeJv33xO8Qfe+xx4ony5TebbBPUtrheQ1vZRNXJKvFinm0WvfbP59XZ1IsjG8XPWH0Cwx4wITCYEBjG9ROEIVpbo/2FXo/3GvY3V4nnVE2fRx57mPipc1ynsKXORt7cZC4D1pl5nx0H5X28V3B2nvcuzh3ivYjFCtsI+icxr66vqNrIq6tss1y9wucSnj71APG1q3ymUb3OtZObTbaxoj77MdqROju6w/zkMn+eT33y/PV///lLL2M32ExgMCEwmBAYMPa5iBHC+ugco87qRWp32qzj7nvwHPEzp5f5+pD9CrmQh3NohnVyVp0vcHyZ4/mnz7EOnls4Qrxc5X0Irkr6ExXLmFA2QybLsZGrmzXiFy5cIn7+oU8Q9+Z5n4E+k6ml8i1Cte8hVrGUQ4s8vqeeuZ/48ROjfRw5XUwiBZsJDCYEBhMCA8a1CeIQYWuk9121rj9zlNepj95/hngxzzHtvjrDZ1qd/TupagwdnOfzCY4u8vNn5tgv4Cg/QhSr+L7ae3ht+23mNfYDNFUOZBDyb6iYrxIvqbOfw1DrfPb9O47OmeTPa2GJn//ks+x3eeAhtpGyudHzxNm9kKHNBAYTAoMJgQHj7jtwEkwVRnps4dRRan/i46yjK1le1w5UncOKOtfwwDSfSzgxdZj4zJzaW1hmnTkIWYcjUvseHLZBQnVGUxDz/bOzrEc//jGuhVzIsg305KNPE5+a4vj/pXe/T7wfcr6Dl+HP67SKrTz9ww8RP3WWx5PLqxzP1EYIXUc5DZsJDCYEBhMCA8a0CSYrFfz0c89f57mAdWrS5djBNZVDt3CMdf7cPMcSJopsI2SyvM4XcP5CGG4Tz+VYpn1f6UFOIURG5TskEZ9N/MCZR4kvzTxIvFTmHMViifcpbG7zGU37pvl9Di7x9UWVA/nc85yzuHyM/SS+zzbADb/pdA6iqn+0x12GexEmBAYTAsOYNkEpU8QjSyO9+D9vv0bt31M5hvtVjt/CIT5foJTjnD3X4XMDA/D5A4UM5xNk1RlJvto76IqqDeywrz6JVL2ALtsknTr7IbodzhEsT/A+guw+jv8vzVaJL5zgdb1f4OctHmadf/zEEeKuOptaRBdB4veLk933H6ZhM4HBhMBgQmAAILp+7p4Xi2wAuHz7hmO4jVhMkmTmZg1jCYHh/ydMHRhMCAwmBAaYEBhgQmCACYEBJgQGmBAYYEJgAPC/wEMvHQbDs78AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZ),\n",
      "  CircuitStateFn(\n",
      "       ┌──────────────────────────┐┌──────────────────────────────────────┐\n",
      "  q_0: ┤0                         ├┤0                                     ├\n",
      "       │  ZZFeatureMap(x[0],x[1]) ││  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
      "  q_1: ┤1                         ├┤1                                     ├\n",
      "       └──────────────────────────┘└──────────────────────────────────────┘\n",
      "  )\n",
      "])\n",
      "HybridQNN_Shallow(\n",
      "  (conv1): Conv2d(3, 2, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (qnn): TorchConnector()\n",
      "  (fc3): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Network init elapsed time: 7.213797653093934e-05 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        #n_filts_fc1 = int(((((input_resolution[0])/2)-4)/2)**2)*16 # +7\n",
    "        n_filts_fc1 = 256\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: -0.0471804141998291\n",
      "Batch 1, Loss: -1.2077360153198242\n",
      "Batch 2, Loss: -0.5857294797897339\n",
      "Batch 3, Loss: -0.6541830897331238\n",
      "Batch 4, Loss: -1.1278027296066284\n",
      "Batch 5, Loss: 0.37837839126586914\n",
      "Batch 6, Loss: -0.4457964301109314\n",
      "Batch 7, Loss: 0.4251919984817505\n",
      "Batch 8, Loss: -0.0036475062370300293\n",
      "Batch 9, Loss: 0.24016046524047852\n",
      "Batch 10, Loss: -1.0864797830581665\n",
      "Batch 11, Loss: 0.41481471061706543\n",
      "Batch 12, Loss: -0.10268402099609375\n",
      "Batch 13, Loss: 0.11600828170776367\n",
      "Batch 14, Loss: 0.3911564350128174\n",
      "Batch 15, Loss: -1.231160044670105\n",
      "Batch 16, Loss: -1.4112062454223633\n",
      "Batch 17, Loss: -0.8632773756980896\n",
      "Batch 18, Loss: -0.3690594434738159\n",
      "Batch 19, Loss: -1.083810806274414\n",
      "Batch 20, Loss: -1.3164782524108887\n",
      "Batch 21, Loss: -0.8451248407363892\n",
      "Batch 22, Loss: -0.1272820234298706\n",
      "Batch 23, Loss: 0.41173577308654785\n",
      "Batch 24, Loss: -0.030517637729644775\n",
      "Batch 25, Loss: -1.250834584236145\n",
      "Batch 26, Loss: -0.9966474771499634\n",
      "Batch 27, Loss: -0.8151662945747375\n",
      "Batch 28, Loss: -0.37811386585235596\n",
      "Batch 29, Loss: -0.8352733850479126\n",
      "Batch 30, Loss: -0.029068350791931152\n",
      "Batch 31, Loss: -1.313612937927246\n",
      "Batch 32, Loss: -0.9064117074012756\n",
      "Batch 33, Loss: -0.6792894005775452\n",
      "Batch 34, Loss: -1.3770904541015625\n",
      "Batch 35, Loss: -0.04004085063934326\n",
      "Batch 36, Loss: -1.3693745136260986\n",
      "Batch 37, Loss: 0.19718313217163086\n",
      "Batch 38, Loss: -0.7388634085655212\n",
      "Batch 39, Loss: -0.695415735244751\n",
      "Batch 40, Loss: 0.37730836868286133\n",
      "Batch 41, Loss: -0.828521728515625\n",
      "Batch 42, Loss: -1.064438819885254\n",
      "Batch 43, Loss: -1.4211435317993164\n",
      "Batch 44, Loss: -1.2725512981414795\n",
      "Batch 45, Loss: -0.005635201930999756\n",
      "Batch 46, Loss: -0.6146456003189087\n",
      "Batch 47, Loss: 0.3673551082611084\n",
      "Batch 48, Loss: 0.4099646806716919\n",
      "Batch 49, Loss: 0.07545995712280273\n",
      "Batch 50, Loss: -0.9685986042022705\n",
      "Batch 51, Loss: -1.2042505741119385\n",
      "Batch 52, Loss: -0.1379847526550293\n",
      "Batch 53, Loss: -0.1447739601135254\n",
      "Batch 54, Loss: -0.2952609360218048\n",
      "Batch 55, Loss: -0.2445012927055359\n",
      "Batch 56, Loss: -0.06206774711608887\n",
      "Batch 57, Loss: -0.7324962615966797\n",
      "Batch 58, Loss: -1.0002888441085815\n",
      "Batch 59, Loss: -0.4068296551704407\n",
      "Batch 60, Loss: -0.3752022087574005\n",
      "Batch 61, Loss: -0.5110188722610474\n",
      "Batch 62, Loss: -1.0259461402893066\n",
      "Batch 63, Loss: -0.760265052318573\n",
      "Batch 64, Loss: -0.3374641537666321\n",
      "Batch 65, Loss: -0.04684114456176758\n",
      "Batch 66, Loss: -0.9932723641395569\n",
      "Batch 67, Loss: -0.6338621377944946\n",
      "Batch 68, Loss: -0.255801260471344\n",
      "Batch 69, Loss: -0.8457688093185425\n",
      "Batch 70, Loss: -0.10230910778045654\n",
      "Batch 71, Loss: -0.14252346754074097\n",
      "Batch 72, Loss: -0.0004902482032775879\n",
      "Batch 73, Loss: -0.11253398656845093\n",
      "Batch 74, Loss: -0.24357980489730835\n",
      "Batch 75, Loss: -0.574802041053772\n",
      "Batch 76, Loss: -0.4206414222717285\n",
      "Batch 77, Loss: 0.212310791015625\n",
      "Batch 78, Loss: -0.3537715673446655\n",
      "Batch 79, Loss: -0.996282696723938\n",
      "Batch 80, Loss: -0.34013450145721436\n",
      "Batch 81, Loss: -0.5873203277587891\n",
      "Batch 82, Loss: -0.7870596051216125\n",
      "Batch 83, Loss: -0.6102235317230225\n",
      "Batch 84, Loss: -0.6225858330726624\n",
      "Batch 85, Loss: -0.026718854904174805\n",
      "Batch 86, Loss: -0.621788501739502\n",
      "Batch 87, Loss: -0.4404555559158325\n",
      "Batch 88, Loss: -0.9952203631401062\n",
      "Batch 89, Loss: -0.8959073424339294\n",
      "Batch 90, Loss: -0.38989606499671936\n",
      "Batch 91, Loss: -0.17052602767944336\n",
      "Batch 92, Loss: -0.9184507727622986\n",
      "Batch 93, Loss: -0.2971336245536804\n",
      "Batch 94, Loss: -0.34643828868865967\n",
      "Batch 95, Loss: -0.38238513469696045\n",
      "Batch 96, Loss: -0.35091543197631836\n",
      "Batch 97, Loss: -0.029060184955596924\n",
      "Batch 98, Loss: -0.5787530541419983\n",
      "Batch 99, Loss: -0.7263652086257935\n",
      "Batch 100, Loss: -0.3887374997138977\n",
      "Batch 101, Loss: -0.3157652020454407\n",
      "Batch 102, Loss: -0.19020652770996094\n",
      "Batch 103, Loss: -0.7140821218490601\n",
      "Batch 104, Loss: -0.11117249727249146\n",
      "Batch 105, Loss: -0.6911464333534241\n",
      "Batch 106, Loss: -0.3133094310760498\n",
      "Batch 107, Loss: -0.6664656400680542\n",
      "Batch 108, Loss: -0.6060031056404114\n",
      "Batch 109, Loss: -0.8002720475196838\n",
      "Batch 110, Loss: -0.8962745070457458\n",
      "Batch 111, Loss: -0.6309356093406677\n",
      "Batch 112, Loss: -0.4309665560722351\n",
      "Batch 113, Loss: -0.2364445924758911\n",
      "Batch 114, Loss: -0.09623980522155762\n",
      "Batch 115, Loss: -0.6966134309768677\n",
      "Batch 116, Loss: -0.6807206273078918\n",
      "Batch 117, Loss: -0.7228336334228516\n",
      "Batch 118, Loss: -0.9234569072723389\n",
      "Batch 119, Loss: -0.6460884809494019\n",
      "Batch 120, Loss: -0.38787662982940674\n",
      "Batch 121, Loss: -0.9536824226379395\n",
      "Batch 122, Loss: -0.7901260256767273\n",
      "Batch 123, Loss: -0.7012580633163452\n",
      "Batch 124, Loss: -0.27726882696151733\n",
      "Batch 125, Loss: -0.3804996609687805\n",
      "Batch 126, Loss: -0.16504085063934326\n",
      "Batch 127, Loss: -0.15605199337005615\n",
      "Batch 128, Loss: -0.38061952590942383\n",
      "Batch 129, Loss: -0.04848074913024902\n",
      "Batch 130, Loss: -0.14075183868408203\n",
      "Batch 131, Loss: -0.10588288307189941\n",
      "Batch 132, Loss: -0.7788912057876587\n",
      "Batch 133, Loss: -1.0014183521270752\n",
      "Batch 134, Loss: -0.37198173999786377\n",
      "Batch 135, Loss: -0.03947949409484863\n",
      "Batch 136, Loss: -0.944916844367981\n",
      "Batch 137, Loss: -0.7889557480812073\n",
      "Batch 138, Loss: -0.24667680263519287\n",
      "Batch 139, Loss: 0.01920032501220703\n",
      "Batch 140, Loss: -0.2280430793762207\n",
      "Batch 141, Loss: -0.77969890832901\n",
      "Batch 142, Loss: -0.4164055585861206\n",
      "Batch 143, Loss: -0.8807626366615295\n",
      "Batch 144, Loss: -0.08269178867340088\n",
      "Batch 145, Loss: -0.911152184009552\n",
      "Batch 146, Loss: -0.4620306193828583\n",
      "Batch 147, Loss: 0.001466989517211914\n",
      "Batch 148, Loss: -0.8488664627075195\n",
      "Batch 149, Loss: -0.17914611101150513\n",
      "Batch 150, Loss: -0.7207220196723938\n",
      "Batch 151, Loss: -0.7607622146606445\n",
      "Batch 152, Loss: 0.35468053817749023\n",
      "Batch 153, Loss: -1.0033092498779297\n",
      "Batch 154, Loss: -0.006651580333709717\n",
      "Batch 155, Loss: -0.046499669551849365\n",
      "Batch 156, Loss: -0.2266474962234497\n",
      "Batch 157, Loss: -0.980925440788269\n",
      "Batch 158, Loss: -0.09283030033111572\n",
      "Batch 159, Loss: -0.6797413229942322\n",
      "Batch 160, Loss: -0.9831932783126831\n",
      "Batch 161, Loss: -0.5240998864173889\n",
      "Batch 162, Loss: -0.9920721054077148\n",
      "Batch 163, Loss: -0.6968328952789307\n",
      "Batch 164, Loss: -0.7714389562606812\n",
      "Batch 165, Loss: -0.8497211933135986\n",
      "Batch 166, Loss: -0.8398807644844055\n",
      "Batch 167, Loss: 0.009622573852539062\n",
      "Batch 168, Loss: -0.6848255395889282\n",
      "Batch 169, Loss: -0.4059563875198364\n",
      "Batch 170, Loss: -0.9905970692634583\n",
      "Batch 171, Loss: -1.0964231491088867\n",
      "Batch 172, Loss: -0.5104238986968994\n",
      "Batch 173, Loss: -0.5945111513137817\n",
      "Batch 174, Loss: -0.12340182065963745\n",
      "Batch 175, Loss: -0.27096474170684814\n",
      "Batch 176, Loss: -0.24689874053001404\n",
      "Batch 177, Loss: 0.2146904468536377\n",
      "Batch 178, Loss: -0.8442578911781311\n",
      "Batch 179, Loss: -1.2033469676971436\n",
      "Batch 180, Loss: -0.138464093208313\n",
      "Batch 181, Loss: -0.6403409242630005\n",
      "Batch 182, Loss: -0.7922809720039368\n",
      "Batch 183, Loss: -0.2877133786678314\n",
      "Batch 184, Loss: -0.010795414447784424\n",
      "Batch 185, Loss: 0.003723442554473877\n",
      "Batch 186, Loss: -0.739971935749054\n",
      "Batch 187, Loss: -0.49187248945236206\n",
      "Batch 188, Loss: -0.43047988414764404\n",
      "Batch 189, Loss: -0.31063634157180786\n",
      "Batch 190, Loss: -0.201793372631073\n",
      "Batch 191, Loss: -0.4938540458679199\n",
      "Batch 192, Loss: -1.2877020835876465\n",
      "Batch 193, Loss: -0.26943421363830566\n",
      "Batch 194, Loss: -1.3036408424377441\n",
      "Batch 195, Loss: -0.43056148290634155\n",
      "Batch 196, Loss: -0.4283266067504883\n",
      "Batch 197, Loss: -0.7470772862434387\n",
      "Batch 198, Loss: -0.6587494611740112\n",
      "Batch 199, Loss: -1.1300196647644043\n",
      "Batch 200, Loss: -0.3754073679447174\n",
      "Batch 201, Loss: -0.10763949155807495\n",
      "Batch 202, Loss: -0.01717466115951538\n",
      "Batch 203, Loss: -0.9784823060035706\n",
      "Batch 204, Loss: -0.7044879198074341\n",
      "Batch 205, Loss: -0.3232519030570984\n",
      "Batch 206, Loss: -1.2270311117172241\n",
      "Batch 207, Loss: -0.5983080863952637\n",
      "Batch 208, Loss: -0.6027472019195557\n",
      "Batch 209, Loss: -0.6296150088310242\n",
      "Batch 210, Loss: -0.9713493585586548\n",
      "Batch 211, Loss: -0.9233798384666443\n",
      "Batch 212, Loss: -1.2323585748672485\n",
      "Batch 213, Loss: 0.2576627731323242\n",
      "Batch 214, Loss: -0.0022788047790527344\n",
      "Batch 215, Loss: -0.944032609462738\n",
      "Batch 216, Loss: -0.08068525791168213\n",
      "Batch 217, Loss: -0.4083523750305176\n",
      "Batch 218, Loss: -0.1724821925163269\n",
      "Batch 219, Loss: -0.7669315934181213\n",
      "Batch 220, Loss: -0.3813108205795288\n",
      "Batch 221, Loss: 0.17550063133239746\n",
      "Batch 222, Loss: -0.4590711295604706\n",
      "Batch 223, Loss: -0.2265062928199768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 224, Loss: 0.016841530799865723\n",
      "Batch 225, Loss: -0.2122408151626587\n",
      "Batch 226, Loss: -0.9246516227722168\n",
      "Batch 227, Loss: -0.5686277151107788\n",
      "Batch 228, Loss: -0.9207239747047424\n",
      "Batch 229, Loss: -0.2774442732334137\n",
      "Batch 230, Loss: -1.3061354160308838\n",
      "Batch 231, Loss: -0.11678427457809448\n",
      "Batch 232, Loss: -0.039724111557006836\n",
      "Batch 233, Loss: -0.5353087782859802\n",
      "Batch 234, Loss: -0.0668635368347168\n",
      "Batch 235, Loss: -0.5992588996887207\n",
      "Batch 236, Loss: -0.41155123710632324\n",
      "Batch 237, Loss: -0.19423454999923706\n",
      "Batch 238, Loss: -0.08426588773727417\n",
      "Batch 239, Loss: -1.196860671043396\n",
      "Batch 240, Loss: -0.198630690574646\n",
      "Batch 241, Loss: -0.3184165060520172\n",
      "Batch 242, Loss: -0.22734075784683228\n",
      "Batch 243, Loss: -0.14355987310409546\n",
      "Batch 244, Loss: -0.8939827680587769\n",
      "Batch 245, Loss: -0.8310934901237488\n",
      "Batch 246, Loss: -0.3099334239959717\n",
      "Batch 247, Loss: -0.18720215559005737\n",
      "Batch 248, Loss: -0.5799584984779358\n",
      "Batch 249, Loss: 0.17072820663452148\n",
      "Batch 250, Loss: -0.3899702727794647\n",
      "Batch 251, Loss: -0.07756644487380981\n",
      "Batch 252, Loss: -0.6363650560379028\n",
      "Batch 253, Loss: -0.1747502088546753\n",
      "Batch 254, Loss: -0.418417364358902\n",
      "Batch 255, Loss: -0.5179118514060974\n",
      "Batch 256, Loss: -0.732022762298584\n",
      "Batch 257, Loss: -0.1342206597328186\n",
      "Batch 258, Loss: -0.6644266843795776\n",
      "Batch 259, Loss: -0.8809630870819092\n",
      "Batch 260, Loss: -0.16457408666610718\n",
      "Batch 261, Loss: -0.386897474527359\n",
      "Batch 262, Loss: 0.1335536241531372\n",
      "Batch 263, Loss: -1.0911262035369873\n",
      "Batch 264, Loss: -0.47144728899002075\n",
      "Batch 265, Loss: -0.7104779481887817\n",
      "Batch 266, Loss: -0.07134771347045898\n",
      "Batch 267, Loss: -1.077212929725647\n",
      "Batch 268, Loss: 0.12862706184387207\n",
      "Batch 269, Loss: -1.1826761960983276\n",
      "Batch 270, Loss: -0.4552805721759796\n",
      "Batch 271, Loss: -0.9304546117782593\n",
      "Batch 272, Loss: -0.895080029964447\n",
      "Batch 273, Loss: -0.8051522374153137\n",
      "Batch 274, Loss: -0.6518049240112305\n",
      "Batch 275, Loss: -0.8699339032173157\n",
      "Batch 276, Loss: -0.9375650882720947\n",
      "Batch 277, Loss: -0.048661231994628906\n",
      "Batch 278, Loss: -0.6181254386901855\n",
      "Batch 279, Loss: -0.34694796800613403\n",
      "Batch 280, Loss: -0.27454620599746704\n",
      "Batch 281, Loss: -0.5144287347793579\n",
      "Batch 282, Loss: -0.28948038816452026\n",
      "Batch 283, Loss: 0.12520253658294678\n",
      "Batch 284, Loss: -0.7039490342140198\n",
      "Batch 285, Loss: -0.5676597952842712\n",
      "Batch 286, Loss: -0.7764682173728943\n",
      "Batch 287, Loss: -0.5183089971542358\n",
      "Batch 288, Loss: -0.9921179413795471\n",
      "Batch 289, Loss: -0.48589175939559937\n",
      "Batch 290, Loss: -0.8028866052627563\n",
      "Batch 291, Loss: 0.08069479465484619\n",
      "Batch 292, Loss: -0.0401192307472229\n",
      "Batch 293, Loss: -0.9650387167930603\n",
      "Batch 294, Loss: -0.8135026097297668\n",
      "Batch 295, Loss: -0.057293474674224854\n",
      "Batch 296, Loss: 0.35403120517730713\n",
      "Batch 297, Loss: 0.13137507438659668\n",
      "Batch 298, Loss: -0.48656922578811646\n",
      "Batch 299, Loss: -0.7390578389167786\n",
      "Batch 300, Loss: -1.082800030708313\n",
      "Batch 301, Loss: -0.022987186908721924\n",
      "Batch 302, Loss: -0.13195210695266724\n",
      "Batch 303, Loss: -1.0265355110168457\n",
      "Batch 304, Loss: -0.2816773056983948\n",
      "Batch 305, Loss: -0.4288899004459381\n",
      "Batch 306, Loss: -0.1423531174659729\n",
      "Batch 307, Loss: -0.4261559545993805\n",
      "Batch 308, Loss: -1.1737079620361328\n",
      "Batch 309, Loss: -0.008806467056274414\n",
      "Batch 310, Loss: -1.0087864398956299\n",
      "Batch 311, Loss: 0.07061219215393066\n",
      "Batch 312, Loss: -0.07303494215011597\n",
      "Batch 313, Loss: -0.6641260981559753\n",
      "Batch 314, Loss: -0.001328885555267334\n",
      "Batch 315, Loss: 0.0910179615020752\n",
      "Batch 316, Loss: -0.3163434565067291\n",
      "Batch 317, Loss: -0.7456637024879456\n",
      "Batch 318, Loss: -0.2321007251739502\n",
      "Batch 319, Loss: -0.42974230647087097\n",
      "Batch 320, Loss: -0.4029984474182129\n",
      "Batch 321, Loss: -0.18690574169158936\n",
      "Batch 322, Loss: -0.860239565372467\n",
      "Batch 323, Loss: -0.7814792990684509\n",
      "Batch 324, Loss: -1.3548911809921265\n",
      "Batch 325, Loss: 0.2883596420288086\n",
      "Batch 326, Loss: -0.4374955892562866\n",
      "Batch 327, Loss: 0.1555163860321045\n",
      "Batch 328, Loss: -0.3156823515892029\n",
      "Batch 329, Loss: -0.2964550852775574\n",
      "Batch 330, Loss: 0.3847963809967041\n",
      "Batch 331, Loss: -0.14592909812927246\n",
      "Batch 332, Loss: -0.6617429852485657\n",
      "Batch 333, Loss: -0.664650559425354\n",
      "Batch 334, Loss: -0.07778000831604004\n",
      "Batch 335, Loss: -0.8702525496482849\n",
      "Batch 336, Loss: -0.6113151907920837\n",
      "Batch 337, Loss: -1.36448335647583\n",
      "Batch 338, Loss: -0.6847177743911743\n",
      "Batch 339, Loss: -0.6622818112373352\n",
      "Batch 340, Loss: -0.16690903902053833\n",
      "Batch 341, Loss: -0.7538260221481323\n",
      "Batch 342, Loss: -0.7350783348083496\n",
      "Batch 343, Loss: -0.18367773294448853\n",
      "Batch 344, Loss: -0.35277795791625977\n",
      "Batch 345, Loss: -0.6794858574867249\n",
      "Batch 346, Loss: -0.7404757142066956\n",
      "Batch 347, Loss: -0.8698697686195374\n",
      "Batch 348, Loss: -0.6423442959785461\n",
      "Batch 349, Loss: -0.33574819564819336\n",
      "Batch 350, Loss: -0.8222930431365967\n",
      "Batch 351, Loss: -0.86421799659729\n",
      "Batch 352, Loss: -0.504790186882019\n",
      "Batch 353, Loss: 0.3590381145477295\n",
      "Batch 354, Loss: -0.647925615310669\n",
      "Batch 355, Loss: -0.6005827188491821\n",
      "Batch 356, Loss: -0.2902449369430542\n",
      "Batch 357, Loss: -0.043417274951934814\n",
      "Batch 358, Loss: -1.4186890125274658\n",
      "Batch 359, Loss: -1.1120597124099731\n",
      "Batch 360, Loss: -0.4719400405883789\n",
      "Batch 361, Loss: -0.7063179612159729\n",
      "Batch 362, Loss: -0.4357532858848572\n",
      "Batch 363, Loss: -1.3576891422271729\n",
      "Batch 364, Loss: 0.08775806427001953\n",
      "Batch 365, Loss: 0.3434807062149048\n",
      "Batch 366, Loss: -0.11651766300201416\n",
      "Batch 367, Loss: -0.8507983684539795\n",
      "Batch 368, Loss: -0.9420380592346191\n",
      "Batch 369, Loss: -0.015344679355621338\n",
      "Batch 370, Loss: -0.5125600099563599\n",
      "Batch 371, Loss: -0.24929100275039673\n",
      "Batch 372, Loss: -0.9424000382423401\n",
      "Batch 373, Loss: -0.08893662691116333\n",
      "Batch 374, Loss: -0.4545191526412964\n",
      "Batch 375, Loss: -1.2201393842697144\n",
      "Batch 376, Loss: -0.1287996768951416\n",
      "Batch 377, Loss: -1.0211137533187866\n",
      "Batch 378, Loss: 0.1931910514831543\n",
      "Batch 379, Loss: -0.0950479507446289\n",
      "Batch 380, Loss: -0.09786272048950195\n",
      "Batch 381, Loss: -0.4849590063095093\n",
      "Batch 382, Loss: -0.305448055267334\n",
      "Batch 383, Loss: -0.3471370339393616\n",
      "Batch 384, Loss: -0.8446311950683594\n",
      "Batch 385, Loss: -1.3024685382843018\n",
      "Batch 386, Loss: -0.18435609340667725\n",
      "Batch 387, Loss: -0.36439982056617737\n",
      "Batch 388, Loss: -1.0874463319778442\n",
      "Batch 389, Loss: -0.12634116411209106\n",
      "Batch 390, Loss: -0.062388598918914795\n",
      "Batch 391, Loss: -0.2346569299697876\n",
      "Batch 392, Loss: -0.7719615697860718\n",
      "Batch 393, Loss: -0.8926441669464111\n",
      "Batch 394, Loss: -0.6922847032546997\n",
      "Batch 395, Loss: -1.2429215908050537\n",
      "Batch 396, Loss: -0.5403594970703125\n",
      "Batch 397, Loss: -0.7353333234786987\n",
      "Batch 398, Loss: -0.6638545989990234\n",
      "Batch 399, Loss: -0.617511510848999\n",
      "Training [10%]\tLoss: -0.4935\n",
      "Batch 0, Loss: -0.4360949993133545\n",
      "Batch 1, Loss: -0.8824185729026794\n",
      "Batch 2, Loss: -0.05089759826660156\n",
      "Batch 3, Loss: -0.06354248523712158\n",
      "Batch 4, Loss: -0.15187698602676392\n",
      "Batch 5, Loss: -0.26327794790267944\n",
      "Batch 6, Loss: -0.2821403741836548\n",
      "Batch 7, Loss: -0.9178546667098999\n",
      "Batch 8, Loss: -1.3436803817749023\n",
      "Batch 9, Loss: -0.30255836248397827\n",
      "Batch 10, Loss: -0.7559103965759277\n",
      "Batch 11, Loss: -0.8703223466873169\n",
      "Batch 12, Loss: -0.3109928071498871\n",
      "Batch 13, Loss: -0.28369471430778503\n",
      "Batch 14, Loss: -0.8769270777702332\n",
      "Batch 15, Loss: -0.8002923727035522\n",
      "Batch 16, Loss: -0.461742639541626\n",
      "Batch 17, Loss: -0.36876678466796875\n",
      "Batch 18, Loss: -0.7438914775848389\n",
      "Batch 19, Loss: -0.5417232513427734\n",
      "Batch 20, Loss: -0.8295824527740479\n",
      "Batch 21, Loss: -0.4482957124710083\n",
      "Batch 22, Loss: -0.4558927118778229\n",
      "Batch 23, Loss: -0.7924523949623108\n",
      "Batch 24, Loss: -0.6269778609275818\n",
      "Batch 25, Loss: -0.7021490335464478\n",
      "Batch 26, Loss: -0.14234155416488647\n",
      "Batch 27, Loss: -0.4486478567123413\n",
      "Batch 28, Loss: 0.13725757598876953\n",
      "Batch 29, Loss: -0.7342497110366821\n",
      "Batch 30, Loss: 0.12157726287841797\n",
      "Batch 31, Loss: -0.809627890586853\n",
      "Batch 32, Loss: 0.22993683815002441\n",
      "Batch 33, Loss: -0.4337853193283081\n",
      "Batch 34, Loss: 0.0007613301277160645\n",
      "Batch 35, Loss: -0.3763541877269745\n",
      "Batch 36, Loss: -0.7861133217811584\n",
      "Batch 37, Loss: -0.9950705766677856\n",
      "Batch 38, Loss: -0.7768973112106323\n",
      "Batch 39, Loss: -1.2395691871643066\n",
      "Batch 40, Loss: -0.522978663444519\n",
      "Batch 41, Loss: -0.45623141527175903\n",
      "Batch 42, Loss: -0.7147827744483948\n",
      "Batch 43, Loss: -0.3357332944869995\n",
      "Batch 44, Loss: -0.2810540795326233\n",
      "Batch 45, Loss: -0.4249180257320404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 46, Loss: -0.5908381342887878\n",
      "Batch 47, Loss: -1.170830249786377\n",
      "Batch 48, Loss: -0.8263470530509949\n",
      "Batch 49, Loss: -0.1934497356414795\n",
      "Batch 50, Loss: -0.16382712125778198\n",
      "Batch 51, Loss: -0.18079215288162231\n",
      "Batch 52, Loss: -0.014569461345672607\n",
      "Batch 53, Loss: -0.14512121677398682\n",
      "Batch 54, Loss: 0.004820764064788818\n",
      "Batch 55, Loss: -0.3451251685619354\n",
      "Batch 56, Loss: -0.8019998669624329\n",
      "Batch 57, Loss: -0.6063450574874878\n",
      "Batch 58, Loss: -0.06476438045501709\n",
      "Batch 59, Loss: -0.218927264213562\n",
      "Batch 60, Loss: -0.12077975273132324\n",
      "Batch 61, Loss: -0.5698192119598389\n",
      "Batch 62, Loss: 0.08205556869506836\n",
      "Batch 63, Loss: 0.0962139368057251\n",
      "Batch 64, Loss: -0.626164436340332\n",
      "Batch 65, Loss: 0.03434133529663086\n",
      "Batch 66, Loss: -0.979503870010376\n",
      "Batch 67, Loss: -0.37785279750823975\n",
      "Batch 68, Loss: -0.8647335171699524\n",
      "Batch 69, Loss: -1.1830041408538818\n",
      "Batch 70, Loss: -0.9451022744178772\n",
      "Batch 71, Loss: -0.5292971730232239\n",
      "Batch 72, Loss: -0.8708144426345825\n",
      "Batch 73, Loss: -0.37587910890579224\n",
      "Batch 74, Loss: -1.101660132408142\n",
      "Batch 75, Loss: -0.93024742603302\n",
      "Batch 76, Loss: -1.2146689891815186\n",
      "Batch 77, Loss: -0.2730482220649719\n",
      "Batch 78, Loss: 0.11818325519561768\n",
      "Batch 79, Loss: -0.878255307674408\n",
      "Batch 80, Loss: -0.5674818754196167\n",
      "Batch 81, Loss: -0.20187515020370483\n",
      "Batch 82, Loss: 0.16717326641082764\n",
      "Batch 83, Loss: -0.03260320425033569\n",
      "Batch 84, Loss: -0.7313981652259827\n",
      "Batch 85, Loss: -0.30792000889778137\n",
      "Batch 86, Loss: -0.6523749828338623\n",
      "Batch 87, Loss: -0.7503800392150879\n",
      "Batch 88, Loss: -0.7645907402038574\n",
      "Batch 89, Loss: -0.5680974721908569\n",
      "Batch 90, Loss: -1.3104023933410645\n",
      "Batch 91, Loss: -0.7215710282325745\n",
      "Batch 92, Loss: -0.8116469383239746\n",
      "Batch 93, Loss: -0.7498518228530884\n",
      "Batch 94, Loss: -0.09687411785125732\n",
      "Batch 95, Loss: -0.3800400495529175\n",
      "Batch 96, Loss: -0.27236324548721313\n",
      "Batch 97, Loss: 0.3586277961730957\n",
      "Batch 98, Loss: -1.0967226028442383\n",
      "Batch 99, Loss: -1.0824880599975586\n",
      "Batch 100, Loss: -0.508409321308136\n",
      "Batch 101, Loss: -0.2249324917793274\n",
      "Batch 102, Loss: -0.6269224882125854\n",
      "Batch 103, Loss: -0.52902752161026\n",
      "Batch 104, Loss: 0.16484403610229492\n",
      "Batch 105, Loss: -0.696995198726654\n",
      "Batch 106, Loss: -0.8591562509536743\n",
      "Batch 107, Loss: -0.37045174837112427\n",
      "Batch 108, Loss: -0.24763751029968262\n",
      "Batch 109, Loss: -0.02695918083190918\n",
      "Batch 110, Loss: -0.6234826445579529\n",
      "Batch 111, Loss: 0.05554819107055664\n",
      "Batch 112, Loss: 0.013424277305603027\n",
      "Batch 113, Loss: -0.4214949607849121\n",
      "Batch 114, Loss: -1.0020074844360352\n",
      "Batch 115, Loss: -0.16551905870437622\n",
      "Batch 116, Loss: -0.15546143054962158\n",
      "Batch 117, Loss: -0.24717432260513306\n",
      "Batch 118, Loss: 0.018257856369018555\n",
      "Batch 119, Loss: 0.029365241527557373\n",
      "Batch 120, Loss: -0.7222095131874084\n",
      "Batch 121, Loss: -0.4494660496711731\n",
      "Batch 122, Loss: -0.43819040060043335\n",
      "Batch 123, Loss: -0.8206568956375122\n",
      "Batch 124, Loss: -0.4637890160083771\n",
      "Batch 125, Loss: 0.07974463701248169\n",
      "Batch 126, Loss: -1.0173263549804688\n",
      "Batch 127, Loss: -1.0739277601242065\n",
      "Batch 128, Loss: 0.11914992332458496\n",
      "Batch 129, Loss: 0.17054510116577148\n",
      "Batch 130, Loss: -0.42444536089897156\n",
      "Batch 131, Loss: -0.7989779710769653\n",
      "Batch 132, Loss: -1.069151520729065\n",
      "Batch 133, Loss: -0.34463608264923096\n",
      "Batch 134, Loss: -1.1387808322906494\n",
      "Batch 135, Loss: -0.5393742322921753\n",
      "Batch 136, Loss: -0.6142272353172302\n",
      "Batch 137, Loss: -0.5529928207397461\n",
      "Batch 138, Loss: -1.2253777980804443\n",
      "Batch 139, Loss: -0.824630856513977\n",
      "Batch 140, Loss: -0.6904154419898987\n",
      "Batch 141, Loss: 0.15822458267211914\n",
      "Batch 142, Loss: -0.08114457130432129\n",
      "Batch 143, Loss: 0.00189971923828125\n",
      "Batch 144, Loss: 0.3681448698043823\n",
      "Batch 145, Loss: -0.28993552923202515\n",
      "Batch 146, Loss: -0.8140807747840881\n",
      "Batch 147, Loss: -0.6175779104232788\n",
      "Batch 148, Loss: -0.05285459756851196\n",
      "Batch 149, Loss: -0.5837140679359436\n",
      "Batch 150, Loss: -1.2165303230285645\n",
      "Batch 151, Loss: -1.1533751487731934\n",
      "Batch 152, Loss: -0.29372280836105347\n",
      "Batch 153, Loss: 0.16434121131896973\n",
      "Batch 154, Loss: -0.16176068782806396\n",
      "Batch 155, Loss: -0.5711314678192139\n",
      "Batch 156, Loss: 0.09141945838928223\n",
      "Batch 157, Loss: -0.48635491728782654\n",
      "Batch 158, Loss: -0.22834289073944092\n",
      "Batch 159, Loss: -0.09557080268859863\n",
      "Batch 160, Loss: 0.08658957481384277\n",
      "Batch 161, Loss: -0.4415908455848694\n",
      "Batch 162, Loss: -0.320535272359848\n",
      "Batch 163, Loss: -1.08525550365448\n",
      "Batch 164, Loss: -0.4023593068122864\n",
      "Batch 165, Loss: -0.9530120491981506\n",
      "Batch 166, Loss: 0.3116971254348755\n",
      "Batch 167, Loss: 0.16107523441314697\n",
      "Batch 168, Loss: -0.3363500237464905\n",
      "Batch 169, Loss: -1.4720675945281982\n",
      "Batch 170, Loss: -0.9121520519256592\n",
      "Batch 171, Loss: -0.6763193011283875\n",
      "Batch 172, Loss: -0.6946494579315186\n",
      "Batch 173, Loss: -0.2801438570022583\n",
      "Batch 174, Loss: -1.2678072452545166\n",
      "Batch 175, Loss: -1.090492606163025\n",
      "Batch 176, Loss: -1.0497010946273804\n",
      "Batch 177, Loss: -0.09993702173233032\n",
      "Batch 178, Loss: -0.6433465480804443\n",
      "Batch 179, Loss: -0.5804774761199951\n",
      "Batch 180, Loss: -0.2528717517852783\n",
      "Batch 181, Loss: 0.17931926250457764\n",
      "Batch 182, Loss: -0.4582340121269226\n",
      "Batch 183, Loss: 0.05357933044433594\n",
      "Batch 184, Loss: -0.2728368043899536\n",
      "Batch 185, Loss: -0.36165887117385864\n",
      "Batch 186, Loss: -1.021324872970581\n",
      "Batch 187, Loss: -0.6575493216514587\n",
      "Batch 188, Loss: -0.6489839553833008\n",
      "Batch 189, Loss: -0.4205162823200226\n",
      "Batch 190, Loss: -0.4911888837814331\n",
      "Batch 191, Loss: -0.7229501605033875\n",
      "Batch 192, Loss: -0.4626982808113098\n",
      "Batch 193, Loss: 0.2812134027481079\n",
      "Batch 194, Loss: -0.14248275756835938\n",
      "Batch 195, Loss: -0.4900634288787842\n",
      "Batch 196, Loss: -0.5661208033561707\n",
      "Batch 197, Loss: -0.48221641778945923\n",
      "Batch 198, Loss: -0.6542872786521912\n",
      "Batch 199, Loss: -0.4264633059501648\n",
      "Batch 200, Loss: 0.38840341567993164\n",
      "Batch 201, Loss: -0.23276662826538086\n",
      "Batch 202, Loss: -0.5459043383598328\n",
      "Batch 203, Loss: -0.9933515787124634\n",
      "Batch 204, Loss: -0.7495894432067871\n",
      "Batch 205, Loss: -0.4984074831008911\n",
      "Batch 206, Loss: -0.33985596895217896\n",
      "Batch 207, Loss: -0.8600462079048157\n",
      "Batch 208, Loss: 0.3388521671295166\n",
      "Batch 209, Loss: -1.4358956813812256\n",
      "Batch 210, Loss: -0.1719411015510559\n",
      "Batch 211, Loss: 0.10372292995452881\n",
      "Batch 212, Loss: -0.96006840467453\n",
      "Batch 213, Loss: -0.5108468532562256\n",
      "Batch 214, Loss: -0.5931320190429688\n",
      "Batch 215, Loss: -0.49413660168647766\n",
      "Batch 216, Loss: -1.2389171123504639\n",
      "Batch 217, Loss: 0.003988027572631836\n",
      "Batch 218, Loss: -0.7595300078392029\n",
      "Batch 219, Loss: -0.20479851961135864\n",
      "Batch 220, Loss: -0.41958320140838623\n",
      "Batch 221, Loss: -0.5983363389968872\n",
      "Batch 222, Loss: -0.20722442865371704\n",
      "Batch 223, Loss: -0.9105536937713623\n",
      "Batch 224, Loss: -0.2535775303840637\n",
      "Batch 225, Loss: -0.5102593898773193\n",
      "Batch 226, Loss: -1.181872844696045\n",
      "Batch 227, Loss: -0.6122841835021973\n",
      "Batch 228, Loss: -0.14591258764266968\n",
      "Batch 229, Loss: -0.7158890962600708\n",
      "Batch 230, Loss: -0.6024821996688843\n",
      "Batch 231, Loss: -0.7058071494102478\n",
      "Batch 232, Loss: -1.1421236991882324\n",
      "Batch 233, Loss: 0.00655210018157959\n",
      "Batch 234, Loss: -0.16020655632019043\n",
      "Batch 235, Loss: 0.3708674907684326\n",
      "Batch 236, Loss: 0.3047412633895874\n",
      "Batch 237, Loss: -0.6751477122306824\n",
      "Batch 238, Loss: -0.31302204728126526\n",
      "Batch 239, Loss: -0.39010655879974365\n",
      "Batch 240, Loss: -0.4978441894054413\n",
      "Batch 241, Loss: 0.1295177936553955\n",
      "Batch 242, Loss: -0.5583743453025818\n",
      "Batch 243, Loss: -0.22685164213180542\n",
      "Batch 244, Loss: -0.3061313033103943\n",
      "Batch 245, Loss: -0.47868406772613525\n",
      "Batch 246, Loss: -0.61946702003479\n",
      "Batch 247, Loss: -1.3436667919158936\n",
      "Batch 248, Loss: -0.28272125124931335\n",
      "Batch 249, Loss: -0.29042983055114746\n",
      "Batch 250, Loss: -0.44052135944366455\n",
      "Batch 251, Loss: -0.6942277550697327\n",
      "Batch 252, Loss: -0.1141437292098999\n",
      "Batch 253, Loss: -0.7858359217643738\n",
      "Batch 254, Loss: -0.35261720418930054\n",
      "Batch 255, Loss: 0.291215181350708\n",
      "Batch 256, Loss: -0.9303448796272278\n",
      "Batch 257, Loss: -0.8213610649108887\n",
      "Batch 258, Loss: -0.8985919952392578\n",
      "Batch 259, Loss: 0.3090895414352417\n",
      "Batch 260, Loss: -0.11413544416427612\n",
      "Batch 261, Loss: -0.8189632892608643\n",
      "Batch 262, Loss: -0.800723135471344\n",
      "Batch 263, Loss: -0.46220555901527405\n",
      "Batch 264, Loss: -1.0920782089233398\n",
      "Batch 265, Loss: -0.4223441779613495\n",
      "Batch 266, Loss: -0.4403057098388672\n",
      "Batch 267, Loss: -0.7157019376754761\n",
      "Batch 268, Loss: -0.9272542595863342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 269, Loss: -0.9124452471733093\n",
      "Batch 270, Loss: -0.8983371257781982\n",
      "Batch 271, Loss: -0.6947662830352783\n",
      "Batch 272, Loss: -0.8588568568229675\n",
      "Batch 273, Loss: -1.3311102390289307\n",
      "Batch 274, Loss: -1.2274651527404785\n",
      "Batch 275, Loss: -0.7222590446472168\n",
      "Batch 276, Loss: -0.4188627004623413\n",
      "Batch 277, Loss: -0.6051852703094482\n",
      "Batch 278, Loss: -0.5857970714569092\n",
      "Batch 279, Loss: -0.5089774131774902\n",
      "Batch 280, Loss: -0.016912341117858887\n",
      "Batch 281, Loss: -0.6361171007156372\n",
      "Batch 282, Loss: -0.7989070415496826\n",
      "Batch 283, Loss: -0.46718886494636536\n",
      "Batch 284, Loss: -0.07913678884506226\n",
      "Batch 285, Loss: -0.45641231536865234\n",
      "Batch 286, Loss: -0.41191965341567993\n",
      "Batch 287, Loss: -0.8676363825798035\n",
      "Batch 288, Loss: -0.8596714735031128\n",
      "Batch 289, Loss: 0.07851135730743408\n",
      "Batch 290, Loss: -0.09454613924026489\n",
      "Batch 291, Loss: -0.9040815830230713\n",
      "Batch 292, Loss: 0.260967493057251\n",
      "Batch 293, Loss: -0.5490272045135498\n",
      "Batch 294, Loss: -0.9711819291114807\n",
      "Batch 295, Loss: -1.0741522312164307\n",
      "Batch 296, Loss: -0.027984976768493652\n",
      "Batch 297, Loss: -0.1275586485862732\n",
      "Batch 298, Loss: -0.21612274646759033\n",
      "Batch 299, Loss: -0.12009161710739136\n",
      "Batch 300, Loss: -0.4388251006603241\n",
      "Batch 301, Loss: -0.3450208902359009\n",
      "Batch 302, Loss: -0.5108213424682617\n",
      "Batch 303, Loss: -0.012914896011352539\n",
      "Batch 304, Loss: -0.7204962372779846\n",
      "Batch 305, Loss: -0.20392918586730957\n",
      "Batch 306, Loss: -0.3634597063064575\n",
      "Batch 307, Loss: -0.15604805946350098\n",
      "Batch 308, Loss: 0.296545147895813\n",
      "Batch 309, Loss: -0.5361332893371582\n",
      "Batch 310, Loss: 0.2847774028778076\n",
      "Batch 311, Loss: -0.7118256688117981\n",
      "Batch 312, Loss: -1.0656027793884277\n",
      "Batch 313, Loss: -0.5632736086845398\n",
      "Batch 314, Loss: -0.6994607448577881\n",
      "Batch 315, Loss: -0.6122089624404907\n",
      "Batch 316, Loss: 0.29367637634277344\n",
      "Batch 317, Loss: -0.20462387800216675\n",
      "Batch 318, Loss: 0.2647514343261719\n",
      "Batch 319, Loss: 0.37083864212036133\n",
      "Batch 320, Loss: -1.1270450353622437\n",
      "Batch 321, Loss: -0.9877477884292603\n",
      "Batch 322, Loss: -0.22157412767410278\n",
      "Batch 323, Loss: -0.6208044290542603\n",
      "Batch 324, Loss: -1.1344960927963257\n",
      "Batch 325, Loss: 0.3236849308013916\n",
      "Batch 326, Loss: -0.45791566371917725\n",
      "Batch 327, Loss: 0.12496232986450195\n",
      "Batch 328, Loss: -0.05514061450958252\n",
      "Batch 329, Loss: -0.6421952247619629\n",
      "Batch 330, Loss: -0.2608323395252228\n",
      "Batch 331, Loss: -0.7417224645614624\n",
      "Batch 332, Loss: -0.26997801661491394\n",
      "Batch 333, Loss: -0.7070228457450867\n",
      "Batch 334, Loss: -0.054212331771850586\n",
      "Batch 335, Loss: -0.06233793497085571\n",
      "Batch 336, Loss: -0.7670225501060486\n",
      "Batch 337, Loss: -0.6822893023490906\n",
      "Batch 338, Loss: -0.6556031107902527\n",
      "Batch 339, Loss: -0.025849878787994385\n",
      "Batch 340, Loss: -0.7441033124923706\n",
      "Batch 341, Loss: -1.1472539901733398\n",
      "Batch 342, Loss: -0.17907631397247314\n",
      "Batch 343, Loss: -0.5367214679718018\n",
      "Batch 344, Loss: -0.8684718608856201\n",
      "Batch 345, Loss: -1.253819465637207\n",
      "Batch 346, Loss: -0.167230486869812\n",
      "Batch 347, Loss: -0.05690610408782959\n",
      "Batch 348, Loss: -0.6744863986968994\n",
      "Batch 349, Loss: 0.2547719478607178\n",
      "Batch 350, Loss: -0.9093531966209412\n",
      "Batch 351, Loss: -0.29936206340789795\n",
      "Batch 352, Loss: -0.6292471289634705\n",
      "Batch 353, Loss: -0.9420700669288635\n",
      "Batch 354, Loss: -0.585759162902832\n",
      "Batch 355, Loss: -0.7430755496025085\n",
      "Batch 356, Loss: -0.4312143921852112\n",
      "Batch 357, Loss: -0.5665925145149231\n",
      "Batch 358, Loss: -0.9266403317451477\n",
      "Batch 359, Loss: -0.932201623916626\n",
      "Batch 360, Loss: -0.4427417814731598\n",
      "Batch 361, Loss: -0.8864853978157043\n",
      "Batch 362, Loss: -0.34414243698120117\n",
      "Batch 363, Loss: 0.1839604377746582\n",
      "Batch 364, Loss: -1.190137267112732\n",
      "Batch 365, Loss: -0.5915395021438599\n",
      "Batch 366, Loss: -0.7935731410980225\n",
      "Batch 367, Loss: -0.735840916633606\n",
      "Batch 368, Loss: -0.9465468525886536\n",
      "Batch 369, Loss: -1.0552990436553955\n",
      "Batch 370, Loss: -1.1541786193847656\n",
      "Batch 371, Loss: -0.6507421731948853\n",
      "Batch 372, Loss: -0.5776309967041016\n",
      "Batch 373, Loss: -0.8437159657478333\n",
      "Batch 374, Loss: -0.016747593879699707\n",
      "Batch 375, Loss: -0.7716887593269348\n",
      "Batch 376, Loss: -0.6297554969787598\n",
      "Batch 377, Loss: -1.320756196975708\n",
      "Batch 378, Loss: -1.097644567489624\n",
      "Batch 379, Loss: -0.6596554517745972\n",
      "Batch 380, Loss: -0.3462538719177246\n",
      "Batch 381, Loss: 0.30539846420288086\n",
      "Batch 382, Loss: -0.17507827281951904\n",
      "Batch 383, Loss: -1.3180992603302002\n",
      "Batch 384, Loss: -0.7729745507240295\n",
      "Batch 385, Loss: 0.28981268405914307\n",
      "Batch 386, Loss: -0.4855169653892517\n",
      "Batch 387, Loss: -0.35337942838668823\n",
      "Batch 388, Loss: -0.9136844277381897\n",
      "Batch 389, Loss: -0.23082387447357178\n",
      "Batch 390, Loss: -0.8982267379760742\n",
      "Batch 391, Loss: -1.2873425483703613\n",
      "Batch 392, Loss: -0.9582930207252502\n",
      "Batch 393, Loss: -0.486591100692749\n",
      "Batch 394, Loss: -0.7384381294250488\n",
      "Batch 395, Loss: -0.2449093461036682\n",
      "Batch 396, Loss: -0.857316792011261\n",
      "Batch 397, Loss: -0.2514957785606384\n",
      "Batch 398, Loss: -1.0429177284240723\n",
      "Batch 399, Loss: -0.8119990229606628\n",
      "Training [20%]\tLoss: -0.4984\n",
      "Batch 0, Loss: -0.47233936190605164\n",
      "Batch 1, Loss: -0.7998685240745544\n",
      "Batch 2, Loss: -0.8396000266075134\n",
      "Batch 3, Loss: -0.10394811630249023\n",
      "Batch 4, Loss: -0.513137698173523\n",
      "Batch 5, Loss: -0.3285476565361023\n",
      "Batch 6, Loss: -0.2740592658519745\n",
      "Batch 7, Loss: -1.0587220191955566\n",
      "Batch 8, Loss: -0.3179836869239807\n",
      "Batch 9, Loss: 0.16634297370910645\n",
      "Batch 10, Loss: -0.5431300401687622\n",
      "Batch 11, Loss: 0.32863593101501465\n",
      "Batch 12, Loss: -0.058615148067474365\n",
      "Batch 13, Loss: -0.5051318407058716\n",
      "Batch 14, Loss: -1.065131664276123\n",
      "Batch 15, Loss: -0.9757367968559265\n",
      "Batch 16, Loss: -0.17228639125823975\n",
      "Batch 17, Loss: -0.12977272272109985\n",
      "Batch 18, Loss: -0.161698579788208\n",
      "Batch 19, Loss: -0.3897778391838074\n",
      "Batch 20, Loss: -0.5597290992736816\n",
      "Batch 21, Loss: -0.40492910146713257\n",
      "Batch 22, Loss: -0.6785435676574707\n",
      "Batch 23, Loss: -0.7035540342330933\n",
      "Batch 24, Loss: -0.6121377944946289\n",
      "Batch 25, Loss: -0.8589662313461304\n",
      "Batch 26, Loss: -0.763829231262207\n",
      "Batch 27, Loss: -0.1674056053161621\n",
      "Batch 28, Loss: -1.188277006149292\n",
      "Batch 29, Loss: -0.12242573499679565\n",
      "Batch 30, Loss: 0.358972430229187\n",
      "Batch 31, Loss: -0.5212574005126953\n",
      "Batch 32, Loss: -0.14616024494171143\n",
      "Batch 33, Loss: -0.32335150241851807\n",
      "Batch 34, Loss: -0.419228732585907\n",
      "Batch 35, Loss: -0.10360181331634521\n",
      "Batch 36, Loss: -1.297004222869873\n",
      "Batch 37, Loss: -0.13914966583251953\n",
      "Batch 38, Loss: 0.31324076652526855\n",
      "Batch 39, Loss: -0.9945937395095825\n",
      "Batch 40, Loss: -0.7026378512382507\n",
      "Batch 41, Loss: -0.2698771357536316\n",
      "Batch 42, Loss: -0.5141303539276123\n",
      "Batch 43, Loss: -0.8580991625785828\n",
      "Batch 44, Loss: -0.9840339422225952\n",
      "Batch 45, Loss: -0.8061733841896057\n",
      "Batch 46, Loss: 0.27245140075683594\n",
      "Batch 47, Loss: -0.6288975477218628\n",
      "Batch 48, Loss: -1.3881585597991943\n",
      "Batch 49, Loss: -0.847717821598053\n",
      "Batch 50, Loss: -1.5125200748443604\n",
      "Batch 51, Loss: -0.41689831018447876\n",
      "Batch 52, Loss: -0.660408079624176\n",
      "Batch 53, Loss: -0.6084835529327393\n",
      "Batch 54, Loss: -0.23271477222442627\n",
      "Batch 55, Loss: -0.5290753841400146\n",
      "Batch 56, Loss: -0.6088311076164246\n",
      "Batch 57, Loss: -0.7113481760025024\n",
      "Batch 58, Loss: -0.8980289697647095\n",
      "Batch 59, Loss: -0.7332894802093506\n",
      "Batch 60, Loss: -0.6700471639633179\n",
      "Batch 61, Loss: -0.9473507404327393\n",
      "Batch 62, Loss: -0.6233178973197937\n",
      "Batch 63, Loss: -0.3935883045196533\n",
      "Batch 64, Loss: -0.7598982453346252\n",
      "Batch 65, Loss: -0.6347106099128723\n",
      "Batch 66, Loss: -0.7862570881843567\n",
      "Batch 67, Loss: 0.2386775016784668\n",
      "Batch 68, Loss: -0.8190646171569824\n",
      "Batch 69, Loss: -0.36595359444618225\n",
      "Batch 70, Loss: -0.5519302487373352\n",
      "Batch 71, Loss: -0.6202683448791504\n",
      "Batch 72, Loss: 0.2708885669708252\n",
      "Batch 73, Loss: -0.40900087356567383\n",
      "Batch 74, Loss: -0.3871702551841736\n",
      "Batch 75, Loss: -0.5660769939422607\n",
      "Batch 76, Loss: -0.04552769660949707\n",
      "Batch 77, Loss: -0.2901313900947571\n",
      "Batch 78, Loss: -0.14035212993621826\n",
      "Batch 79, Loss: -0.9053323268890381\n",
      "Batch 80, Loss: 0.08153128623962402\n",
      "Batch 81, Loss: 0.029422998428344727\n",
      "Batch 82, Loss: -0.0456012487411499\n",
      "Batch 83, Loss: -0.9658091068267822\n",
      "Batch 84, Loss: -0.8773797750473022\n",
      "Batch 85, Loss: -0.8526487946510315\n",
      "Batch 86, Loss: -0.8141598105430603\n",
      "Batch 87, Loss: -0.634093701839447\n",
      "Batch 88, Loss: -0.0745995044708252\n",
      "Batch 89, Loss: -0.6410166025161743\n",
      "Batch 90, Loss: -0.2798011302947998\n",
      "Batch 91, Loss: -0.23444747924804688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 92, Loss: -0.44971930980682373\n",
      "Batch 93, Loss: -0.2703913450241089\n",
      "Batch 94, Loss: -0.5639961957931519\n",
      "Batch 95, Loss: -0.15262234210968018\n",
      "Batch 96, Loss: -0.9116050601005554\n",
      "Batch 97, Loss: -0.9587178826332092\n",
      "Batch 98, Loss: -1.0800652503967285\n",
      "Batch 99, Loss: 0.1760721206665039\n",
      "Batch 100, Loss: 0.06240344047546387\n",
      "Batch 101, Loss: -0.3220975399017334\n",
      "Batch 102, Loss: -0.5447179675102234\n",
      "Batch 103, Loss: -0.5264976024627686\n",
      "Batch 104, Loss: 0.04520249366760254\n",
      "Batch 105, Loss: -0.7648378014564514\n",
      "Batch 106, Loss: -0.33370816707611084\n",
      "Batch 107, Loss: -0.2682434320449829\n",
      "Batch 108, Loss: -0.11754071712493896\n",
      "Batch 109, Loss: -1.435943603515625\n",
      "Batch 110, Loss: -0.1605655550956726\n",
      "Batch 111, Loss: 0.19624745845794678\n",
      "Batch 112, Loss: -0.5088059902191162\n",
      "Batch 113, Loss: -0.037320077419281006\n",
      "Batch 114, Loss: -0.3551030158996582\n",
      "Batch 115, Loss: -1.2787307500839233\n",
      "Batch 116, Loss: -0.6502885222434998\n",
      "Batch 117, Loss: -0.4352636933326721\n",
      "Batch 118, Loss: -0.41386088728904724\n",
      "Batch 119, Loss: -0.015861809253692627\n",
      "Batch 120, Loss: -0.918685793876648\n",
      "Batch 121, Loss: -0.9610510468482971\n",
      "Batch 122, Loss: -0.14040791988372803\n",
      "Batch 123, Loss: -0.09881424903869629\n",
      "Batch 124, Loss: -0.29851624369621277\n",
      "Batch 125, Loss: -0.9613019824028015\n",
      "Batch 126, Loss: -0.44522160291671753\n",
      "Batch 127, Loss: 0.05527836084365845\n",
      "Batch 128, Loss: -0.3632321059703827\n",
      "Batch 129, Loss: -0.8267337083816528\n",
      "Batch 130, Loss: -0.34256279468536377\n",
      "Batch 131, Loss: -0.550797700881958\n",
      "Batch 132, Loss: -0.6768674254417419\n",
      "Batch 133, Loss: -0.3248540759086609\n",
      "Batch 134, Loss: -0.41671687364578247\n",
      "Batch 135, Loss: -0.1620575189590454\n",
      "Batch 136, Loss: 0.05521118640899658\n",
      "Batch 137, Loss: 0.25253725051879883\n",
      "Batch 138, Loss: -0.6138534545898438\n",
      "Batch 139, Loss: -0.4059407413005829\n",
      "Batch 140, Loss: -0.5085616111755371\n",
      "Batch 141, Loss: -0.18370437622070312\n",
      "Batch 142, Loss: 0.1019967794418335\n",
      "Batch 143, Loss: -0.32351943850517273\n",
      "Batch 144, Loss: -0.0426788330078125\n",
      "Batch 145, Loss: -1.0321013927459717\n",
      "Batch 146, Loss: -0.17669135332107544\n",
      "Batch 147, Loss: -0.32128700613975525\n",
      "Batch 148, Loss: -0.24920248985290527\n",
      "Batch 149, Loss: 0.2966083288192749\n",
      "Batch 150, Loss: -0.1459241509437561\n",
      "Batch 151, Loss: -0.475172221660614\n",
      "Batch 152, Loss: -0.6420106291770935\n",
      "Batch 153, Loss: -0.819709062576294\n",
      "Batch 154, Loss: -0.952957034111023\n",
      "Batch 155, Loss: -0.7998981475830078\n",
      "Batch 156, Loss: -0.8321565389633179\n",
      "Batch 157, Loss: -0.6654685735702515\n",
      "Batch 158, Loss: -0.46485966444015503\n",
      "Batch 159, Loss: -1.0978586673736572\n",
      "Batch 160, Loss: -0.783907949924469\n",
      "Batch 161, Loss: -0.3270949721336365\n",
      "Batch 162, Loss: -0.4690324068069458\n",
      "Batch 163, Loss: -0.7126627564430237\n",
      "Batch 164, Loss: -0.953662633895874\n",
      "Batch 165, Loss: -0.7061343193054199\n",
      "Batch 166, Loss: -0.7955784201622009\n",
      "Batch 167, Loss: -0.05736905336380005\n",
      "Batch 168, Loss: -0.6614075899124146\n",
      "Batch 169, Loss: -0.9846652746200562\n",
      "Batch 170, Loss: -0.30425894260406494\n",
      "Batch 171, Loss: -0.7947266101837158\n",
      "Batch 172, Loss: -0.24503940343856812\n",
      "Batch 173, Loss: -0.11103641986846924\n",
      "Batch 174, Loss: -0.8261957168579102\n",
      "Batch 175, Loss: -0.41343432664871216\n",
      "Batch 176, Loss: -0.2367221713066101\n",
      "Batch 177, Loss: -0.5581674575805664\n",
      "Batch 178, Loss: -0.2245650291442871\n",
      "Batch 179, Loss: -1.3203766345977783\n",
      "Batch 180, Loss: -0.6072313189506531\n",
      "Batch 181, Loss: -0.6416428089141846\n",
      "Batch 182, Loss: 0.28919506072998047\n",
      "Batch 183, Loss: -0.1966586709022522\n",
      "Batch 184, Loss: -0.04615509510040283\n",
      "Batch 185, Loss: -0.2673894166946411\n",
      "Batch 186, Loss: -0.5794958472251892\n",
      "Batch 187, Loss: -0.029848873615264893\n",
      "Batch 188, Loss: 0.44516801834106445\n",
      "Batch 189, Loss: -0.4099840819835663\n",
      "Batch 190, Loss: -0.853654682636261\n",
      "Batch 191, Loss: 0.2939567565917969\n",
      "Batch 192, Loss: -0.9486207365989685\n",
      "Batch 193, Loss: -0.540716826915741\n",
      "Batch 194, Loss: -0.5040817856788635\n",
      "Batch 195, Loss: -0.9486073851585388\n",
      "Batch 196, Loss: -0.22962737083435059\n",
      "Batch 197, Loss: -0.49690961837768555\n",
      "Batch 198, Loss: -0.5400392413139343\n",
      "Batch 199, Loss: -0.1563686728477478\n",
      "Batch 200, Loss: -0.26219868659973145\n",
      "Batch 201, Loss: -0.5795681476593018\n",
      "Batch 202, Loss: -0.9504529237747192\n",
      "Batch 203, Loss: -1.3533084392547607\n",
      "Batch 204, Loss: -0.4718419313430786\n",
      "Batch 205, Loss: -0.9041565656661987\n",
      "Batch 206, Loss: -0.7181562781333923\n",
      "Batch 207, Loss: -1.2074334621429443\n",
      "Batch 208, Loss: -0.44074708223342896\n",
      "Batch 209, Loss: -0.45674777030944824\n",
      "Batch 210, Loss: -0.6621588468551636\n",
      "Batch 211, Loss: -0.22808945178985596\n",
      "Batch 212, Loss: -0.3060915470123291\n",
      "Batch 213, Loss: -0.6041673421859741\n",
      "Batch 214, Loss: -0.7710987329483032\n",
      "Batch 215, Loss: -0.5269457101821899\n",
      "Batch 216, Loss: -0.43885424733161926\n",
      "Batch 217, Loss: -0.14480453729629517\n",
      "Batch 218, Loss: -0.12495702505111694\n",
      "Batch 219, Loss: -0.37327441573143005\n",
      "Batch 220, Loss: -0.28673914074897766\n",
      "Batch 221, Loss: -1.083860993385315\n",
      "Batch 222, Loss: -0.4642168879508972\n",
      "Batch 223, Loss: -0.16758668422698975\n",
      "Batch 224, Loss: -0.9472655057907104\n",
      "Batch 225, Loss: -0.5342504382133484\n",
      "Batch 226, Loss: -0.3610457479953766\n",
      "Batch 227, Loss: -0.6347447633743286\n",
      "Batch 228, Loss: -0.6227637529373169\n",
      "Batch 229, Loss: -1.0594764947891235\n",
      "Batch 230, Loss: -0.5064210891723633\n",
      "Batch 231, Loss: -0.41381627321243286\n",
      "Batch 232, Loss: -0.560068666934967\n",
      "Batch 233, Loss: -0.01166689395904541\n",
      "Batch 234, Loss: -0.6260446310043335\n",
      "Batch 235, Loss: 0.4366031885147095\n",
      "Batch 236, Loss: -0.9842327833175659\n",
      "Batch 237, Loss: -0.059089720249176025\n",
      "Batch 238, Loss: -1.3294196128845215\n",
      "Batch 239, Loss: -0.6804277896881104\n",
      "Batch 240, Loss: -0.1316128969192505\n",
      "Batch 241, Loss: 0.0382007360458374\n",
      "Batch 242, Loss: -1.0248498916625977\n",
      "Batch 243, Loss: -1.0571837425231934\n",
      "Batch 244, Loss: -1.2953262329101562\n",
      "Batch 245, Loss: -0.6502178907394409\n",
      "Batch 246, Loss: -0.16401654481887817\n",
      "Batch 247, Loss: -0.9331352710723877\n",
      "Batch 248, Loss: 0.14206206798553467\n",
      "Batch 249, Loss: -0.1354372501373291\n",
      "Batch 250, Loss: -0.2539888024330139\n",
      "Batch 251, Loss: 0.3803713321685791\n",
      "Batch 252, Loss: -0.7227690815925598\n",
      "Batch 253, Loss: -0.39193952083587646\n",
      "Batch 254, Loss: -0.9642348289489746\n",
      "Batch 255, Loss: -0.6454585194587708\n",
      "Batch 256, Loss: 0.030521631240844727\n",
      "Batch 257, Loss: -0.7492368221282959\n",
      "Batch 258, Loss: -0.7058455944061279\n",
      "Batch 259, Loss: -0.6281487941741943\n",
      "Batch 260, Loss: -0.8266456723213196\n",
      "Batch 261, Loss: -0.7157116532325745\n",
      "Batch 262, Loss: -0.3096280097961426\n",
      "Batch 263, Loss: -0.4626672863960266\n",
      "Batch 264, Loss: -1.064717173576355\n",
      "Batch 265, Loss: -0.11786103248596191\n",
      "Batch 266, Loss: -0.5645604729652405\n",
      "Batch 267, Loss: -0.07377022504806519\n",
      "Batch 268, Loss: -0.6645396947860718\n",
      "Batch 269, Loss: -0.3288412392139435\n",
      "Batch 270, Loss: -0.4998463988304138\n",
      "Batch 271, Loss: -0.921386182308197\n",
      "Batch 272, Loss: 0.1749051809310913\n",
      "Batch 273, Loss: -0.13164496421813965\n",
      "Batch 274, Loss: 0.18298101425170898\n",
      "Batch 275, Loss: -0.4216131567955017\n",
      "Batch 276, Loss: -0.6637680530548096\n",
      "Batch 277, Loss: 0.09507155418395996\n",
      "Batch 278, Loss: -1.4350495338439941\n",
      "Batch 279, Loss: -0.49642664194107056\n",
      "Batch 280, Loss: -0.9112834334373474\n",
      "Batch 281, Loss: -0.5547457933425903\n",
      "Batch 282, Loss: -0.9998169541358948\n",
      "Batch 283, Loss: -0.8376466631889343\n",
      "Batch 284, Loss: -0.40804004669189453\n",
      "Batch 285, Loss: 0.28356802463531494\n",
      "Batch 286, Loss: -0.8160808682441711\n",
      "Batch 287, Loss: -0.7891561388969421\n",
      "Batch 288, Loss: -0.6325379610061646\n",
      "Batch 289, Loss: -0.26689815521240234\n",
      "Batch 290, Loss: -1.1728966236114502\n",
      "Batch 291, Loss: -0.45339614152908325\n",
      "Batch 292, Loss: -0.2677997350692749\n",
      "Batch 293, Loss: -0.13551586866378784\n",
      "Batch 294, Loss: -0.4345908761024475\n",
      "Batch 295, Loss: -0.10208356380462646\n",
      "Batch 296, Loss: -0.13371998071670532\n",
      "Batch 297, Loss: -0.35137778520584106\n",
      "Batch 298, Loss: -0.36970430612564087\n",
      "Batch 299, Loss: -0.09276193380355835\n",
      "Batch 300, Loss: -0.03700661659240723\n",
      "Batch 301, Loss: -0.276920348405838\n",
      "Batch 302, Loss: -0.3067072033882141\n",
      "Batch 303, Loss: -0.28244560956954956\n",
      "Batch 304, Loss: -0.5923386812210083\n",
      "Batch 305, Loss: -1.3011893033981323\n",
      "Batch 306, Loss: -0.23404330015182495\n",
      "Batch 307, Loss: -1.0763633251190186\n",
      "Batch 308, Loss: -0.949794352054596\n",
      "Batch 309, Loss: -0.8929736018180847\n",
      "Batch 310, Loss: -0.10043525695800781\n",
      "Batch 311, Loss: -0.8206086158752441\n",
      "Batch 312, Loss: -0.09942042827606201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 313, Loss: -0.5803765654563904\n",
      "Batch 314, Loss: -0.7418956756591797\n",
      "Batch 315, Loss: -0.24922895431518555\n",
      "Batch 316, Loss: -0.9252843260765076\n",
      "Batch 317, Loss: -0.16447323560714722\n",
      "Batch 318, Loss: -0.36595776677131653\n",
      "Batch 319, Loss: 0.22586047649383545\n",
      "Batch 320, Loss: -0.6559416055679321\n",
      "Batch 321, Loss: 0.15973472595214844\n",
      "Batch 322, Loss: -0.7633379101753235\n",
      "Batch 323, Loss: -0.7855792045593262\n",
      "Batch 324, Loss: -0.6518457531929016\n",
      "Batch 325, Loss: -0.38543349504470825\n",
      "Batch 326, Loss: 0.03534245491027832\n",
      "Batch 327, Loss: 0.08720481395721436\n",
      "Batch 328, Loss: -0.7544101476669312\n",
      "Batch 329, Loss: -0.8840689063072205\n",
      "Batch 330, Loss: 0.05688345432281494\n",
      "Batch 331, Loss: -0.32212284207344055\n",
      "Batch 332, Loss: -0.03156173229217529\n",
      "Batch 333, Loss: -0.3342764377593994\n",
      "Batch 334, Loss: -1.1734857559204102\n",
      "Batch 335, Loss: -0.16308486461639404\n",
      "Batch 336, Loss: 0.3441956043243408\n",
      "Batch 337, Loss: -0.729875922203064\n",
      "Batch 338, Loss: -0.8970298767089844\n",
      "Batch 339, Loss: -0.6581058502197266\n",
      "Batch 340, Loss: -0.8688368797302246\n",
      "Batch 341, Loss: -0.9490834474563599\n",
      "Batch 342, Loss: -0.3126434087753296\n",
      "Batch 343, Loss: 0.044533371925354004\n",
      "Batch 344, Loss: -1.2096195220947266\n",
      "Batch 345, Loss: -0.8692246079444885\n",
      "Batch 346, Loss: -0.7969194650650024\n",
      "Batch 347, Loss: 0.3982975482940674\n",
      "Batch 348, Loss: 0.3262507915496826\n",
      "Batch 349, Loss: -0.2934880256652832\n",
      "Batch 350, Loss: -0.3315610885620117\n",
      "Batch 351, Loss: -0.2582445442676544\n",
      "Batch 352, Loss: -0.8748800158500671\n",
      "Batch 353, Loss: -0.644825279712677\n",
      "Batch 354, Loss: -0.6917453408241272\n",
      "Batch 355, Loss: -0.2694084048271179\n",
      "Batch 356, Loss: -0.3630608916282654\n",
      "Batch 357, Loss: -1.337471604347229\n",
      "Batch 358, Loss: -0.6309722065925598\n",
      "Batch 359, Loss: 0.02135789394378662\n",
      "Batch 360, Loss: -0.6510617136955261\n",
      "Batch 361, Loss: -0.9471679925918579\n",
      "Batch 362, Loss: -0.6816337704658508\n",
      "Batch 363, Loss: -0.9561676979064941\n",
      "Batch 364, Loss: -0.4068516492843628\n",
      "Batch 365, Loss: -0.1783425211906433\n",
      "Batch 366, Loss: 0.2666836977005005\n",
      "Batch 367, Loss: -1.3146052360534668\n",
      "Batch 368, Loss: -0.8626610040664673\n",
      "Batch 369, Loss: -1.167922019958496\n",
      "Batch 370, Loss: -1.1301697492599487\n",
      "Batch 371, Loss: -0.14600306749343872\n",
      "Batch 372, Loss: -1.2823947668075562\n",
      "Batch 373, Loss: -0.8017525672912598\n",
      "Batch 374, Loss: -0.15901213884353638\n",
      "Batch 375, Loss: -0.9238080978393555\n",
      "Batch 376, Loss: -0.3290758728981018\n",
      "Batch 377, Loss: -0.014911949634552002\n",
      "Batch 378, Loss: -0.2636353075504303\n",
      "Batch 379, Loss: -1.0237786769866943\n",
      "Batch 380, Loss: -0.7490742206573486\n",
      "Batch 381, Loss: -0.40684375166893005\n",
      "Batch 382, Loss: -0.9349023103713989\n",
      "Batch 383, Loss: 0.2855706214904785\n",
      "Batch 384, Loss: -0.44808658957481384\n",
      "Batch 385, Loss: -0.6913230419158936\n",
      "Batch 386, Loss: -0.09159517288208008\n",
      "Batch 387, Loss: -0.5170981884002686\n",
      "Batch 388, Loss: -0.07200056314468384\n",
      "Batch 389, Loss: -0.30908018350601196\n",
      "Batch 390, Loss: -0.14302116632461548\n",
      "Batch 391, Loss: -0.7247848510742188\n",
      "Batch 392, Loss: 0.1789618730545044\n",
      "Batch 393, Loss: -0.06449615955352783\n",
      "Batch 394, Loss: -0.6925311088562012\n",
      "Batch 395, Loss: 0.21137893199920654\n",
      "Batch 396, Loss: -0.25981730222702026\n",
      "Batch 397, Loss: -0.5743575096130371\n",
      "Batch 398, Loss: -0.7500896453857422\n",
      "Batch 399, Loss: -0.6134220361709595\n",
      "Training [30%]\tLoss: -0.4835\n",
      "Batch 0, Loss: -0.8017566800117493\n",
      "Batch 1, Loss: -0.8490540385246277\n",
      "Batch 2, Loss: -0.8279356956481934\n",
      "Batch 3, Loss: -0.15079689025878906\n",
      "Batch 4, Loss: -0.09672856330871582\n",
      "Batch 5, Loss: -1.315354347229004\n",
      "Batch 6, Loss: -0.45468848943710327\n",
      "Batch 7, Loss: -0.5019427537918091\n",
      "Batch 8, Loss: -0.5580415725708008\n",
      "Batch 9, Loss: -0.9150691628456116\n",
      "Batch 10, Loss: -0.5964152216911316\n",
      "Batch 11, Loss: -1.258033275604248\n",
      "Batch 12, Loss: -0.24207109212875366\n",
      "Batch 13, Loss: -0.3836449980735779\n",
      "Batch 14, Loss: 0.035776615142822266\n",
      "Batch 15, Loss: -0.1392127275466919\n",
      "Batch 16, Loss: -0.8455554246902466\n",
      "Batch 17, Loss: -0.014661431312561035\n",
      "Batch 18, Loss: -0.8940109014511108\n",
      "Batch 19, Loss: -0.4425755739212036\n",
      "Batch 20, Loss: -0.2901719808578491\n",
      "Batch 21, Loss: 0.3063514232635498\n",
      "Batch 22, Loss: -0.18151229619979858\n",
      "Batch 23, Loss: -0.377682626247406\n",
      "Batch 24, Loss: -0.3322833776473999\n",
      "Batch 25, Loss: -0.3834754228591919\n",
      "Batch 26, Loss: -0.4297584295272827\n",
      "Batch 27, Loss: -0.8092799186706543\n",
      "Batch 28, Loss: -0.18469196557998657\n",
      "Batch 29, Loss: -0.20958060026168823\n",
      "Batch 30, Loss: -0.7661499381065369\n",
      "Batch 31, Loss: -0.48719272017478943\n",
      "Batch 32, Loss: -0.49489033222198486\n",
      "Batch 33, Loss: -0.22038966417312622\n",
      "Batch 34, Loss: 0.2917957305908203\n",
      "Batch 35, Loss: -0.6170333623886108\n",
      "Batch 36, Loss: 0.20148849487304688\n",
      "Batch 37, Loss: -1.1152763366699219\n",
      "Batch 38, Loss: -0.3245198130607605\n",
      "Batch 39, Loss: -0.2757975459098816\n",
      "Batch 40, Loss: -0.8883548378944397\n",
      "Batch 41, Loss: -0.11815595626831055\n",
      "Batch 42, Loss: -0.4161728620529175\n",
      "Batch 43, Loss: -1.4702305793762207\n",
      "Batch 44, Loss: -0.9012126326560974\n",
      "Batch 45, Loss: -0.19293886423110962\n",
      "Batch 46, Loss: -0.6653173565864563\n",
      "Batch 47, Loss: -0.3375975489616394\n",
      "Batch 48, Loss: -0.7937427163124084\n",
      "Batch 49, Loss: -0.8850599527359009\n",
      "Batch 50, Loss: -0.903141438961029\n",
      "Batch 51, Loss: -0.7733010649681091\n",
      "Batch 52, Loss: -0.4053022861480713\n",
      "Batch 53, Loss: -0.06847620010375977\n",
      "Batch 54, Loss: -0.7138748168945312\n",
      "Batch 55, Loss: -0.4905548095703125\n",
      "Batch 56, Loss: -0.09781014919281006\n",
      "Batch 57, Loss: -0.4951734244823456\n",
      "Batch 58, Loss: -0.41039779782295227\n",
      "Batch 59, Loss: -0.7984135150909424\n",
      "Batch 60, Loss: -0.2608097195625305\n",
      "Batch 61, Loss: -0.18629640340805054\n",
      "Batch 62, Loss: -0.22443008422851562\n",
      "Batch 63, Loss: -0.19701290130615234\n",
      "Batch 64, Loss: -0.3636264204978943\n",
      "Batch 65, Loss: -0.8635665774345398\n",
      "Batch 66, Loss: -0.3934800624847412\n",
      "Batch 67, Loss: 0.17816650867462158\n",
      "Batch 68, Loss: -0.824652910232544\n",
      "Batch 69, Loss: -0.48877131938934326\n",
      "Batch 70, Loss: -0.6794021129608154\n",
      "Batch 71, Loss: -0.24558335542678833\n",
      "Batch 72, Loss: -0.589627742767334\n",
      "Batch 73, Loss: -0.2769736051559448\n",
      "Batch 74, Loss: -0.21026581525802612\n",
      "Batch 75, Loss: -0.21360939741134644\n",
      "Batch 76, Loss: -0.1720699667930603\n",
      "Batch 77, Loss: -0.8498193621635437\n",
      "Batch 78, Loss: -0.2719748020172119\n",
      "Batch 79, Loss: 0.0868234634399414\n",
      "Batch 80, Loss: -0.7584646344184875\n",
      "Batch 81, Loss: -0.7703807353973389\n",
      "Batch 82, Loss: 0.1651097536087036\n",
      "Batch 83, Loss: 0.00604248046875\n",
      "Batch 84, Loss: -1.1541008949279785\n",
      "Batch 85, Loss: -0.1368141770362854\n",
      "Batch 86, Loss: 0.3458653688430786\n",
      "Batch 87, Loss: -0.6323552131652832\n",
      "Batch 88, Loss: -0.20337927341461182\n",
      "Batch 89, Loss: -0.5164751410484314\n",
      "Batch 90, Loss: -0.6106816530227661\n",
      "Batch 91, Loss: -0.7862894535064697\n",
      "Batch 92, Loss: -0.18294745683670044\n",
      "Batch 93, Loss: -0.8137288689613342\n",
      "Batch 94, Loss: -0.3385249972343445\n",
      "Batch 95, Loss: -0.5827052593231201\n",
      "Batch 96, Loss: -0.29318714141845703\n",
      "Batch 97, Loss: 0.16723871231079102\n",
      "Batch 98, Loss: 0.11586654186248779\n",
      "Batch 99, Loss: -0.25393253564834595\n",
      "Batch 100, Loss: -0.28775835037231445\n",
      "Batch 101, Loss: -1.291920781135559\n",
      "Batch 102, Loss: -0.9881226420402527\n",
      "Batch 103, Loss: -0.4561607539653778\n",
      "Batch 104, Loss: -0.688056468963623\n",
      "Batch 105, Loss: -0.9100458025932312\n",
      "Batch 106, Loss: -0.9713146686553955\n",
      "Batch 107, Loss: -0.1960291862487793\n",
      "Batch 108, Loss: -0.7072030305862427\n",
      "Batch 109, Loss: -0.8749145269393921\n",
      "Batch 110, Loss: -0.3512665033340454\n",
      "Batch 111, Loss: -0.3928658366203308\n",
      "Batch 112, Loss: -0.8395566940307617\n",
      "Batch 113, Loss: -0.7843615412712097\n",
      "Batch 114, Loss: -0.7801565527915955\n",
      "Batch 115, Loss: -0.6375254988670349\n",
      "Batch 116, Loss: -0.695725679397583\n",
      "Batch 117, Loss: 0.2454136610031128\n",
      "Batch 118, Loss: -0.18252485990524292\n",
      "Batch 119, Loss: -0.47355687618255615\n",
      "Batch 120, Loss: -1.002709150314331\n",
      "Batch 121, Loss: -0.6756235361099243\n",
      "Batch 122, Loss: 0.005741596221923828\n",
      "Batch 123, Loss: -0.6542410850524902\n",
      "Batch 124, Loss: -0.5250727534294128\n",
      "Batch 125, Loss: -0.2097611427307129\n",
      "Batch 126, Loss: -0.32209229469299316\n",
      "Batch 127, Loss: -0.7107456922531128\n",
      "Batch 128, Loss: 0.22438466548919678\n",
      "Batch 129, Loss: -0.6389675140380859\n",
      "Batch 130, Loss: -0.6322271823883057\n",
      "Batch 131, Loss: 0.021170377731323242\n",
      "Batch 132, Loss: -0.21452850103378296\n",
      "Batch 133, Loss: -0.9710071086883545\n",
      "Batch 134, Loss: -0.780627429485321\n",
      "Batch 135, Loss: -0.7167718410491943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 136, Loss: -0.5230062007904053\n",
      "Batch 137, Loss: -0.617941677570343\n",
      "Batch 138, Loss: -0.18757927417755127\n",
      "Batch 139, Loss: 0.29384827613830566\n",
      "Batch 140, Loss: -0.848432183265686\n",
      "Batch 141, Loss: -0.37471580505371094\n",
      "Batch 142, Loss: 0.2981492280960083\n",
      "Batch 143, Loss: -0.2463971972465515\n",
      "Batch 144, Loss: 0.2863210439682007\n",
      "Batch 145, Loss: -1.0501763820648193\n",
      "Batch 146, Loss: -0.5554355382919312\n",
      "Batch 147, Loss: -0.5991805791854858\n",
      "Batch 148, Loss: -0.30654817819595337\n",
      "Batch 149, Loss: 0.11923348903656006\n",
      "Batch 150, Loss: -1.126651644706726\n",
      "Batch 151, Loss: -0.6789324879646301\n",
      "Batch 152, Loss: -0.0008236169815063477\n",
      "Batch 153, Loss: 0.010729670524597168\n",
      "Batch 154, Loss: -0.7354800701141357\n",
      "Batch 155, Loss: -0.8836468458175659\n",
      "Batch 156, Loss: -0.9863393902778625\n",
      "Batch 157, Loss: -1.0102514028549194\n",
      "Batch 158, Loss: -0.5242273807525635\n",
      "Batch 159, Loss: -0.3177862763404846\n",
      "Batch 160, Loss: -0.8092264533042908\n",
      "Batch 161, Loss: -0.980665922164917\n",
      "Batch 162, Loss: -1.1751452684402466\n",
      "Batch 163, Loss: -0.5655796527862549\n",
      "Batch 164, Loss: -0.23996883630752563\n",
      "Batch 165, Loss: -0.18593841791152954\n",
      "Batch 166, Loss: -0.9661077857017517\n",
      "Batch 167, Loss: 0.4288029670715332\n",
      "Batch 168, Loss: -1.3058782815933228\n",
      "Batch 169, Loss: 0.13114380836486816\n",
      "Batch 170, Loss: -0.3861652612686157\n",
      "Batch 171, Loss: -0.10088825225830078\n",
      "Batch 172, Loss: -0.9534648656845093\n",
      "Batch 173, Loss: 0.025814294815063477\n",
      "Batch 174, Loss: -0.6570836305618286\n",
      "Batch 175, Loss: -0.04348266124725342\n",
      "Batch 176, Loss: -0.6681458950042725\n",
      "Batch 177, Loss: -0.1841312050819397\n",
      "Batch 178, Loss: -0.5712196826934814\n",
      "Batch 179, Loss: -0.5043042302131653\n",
      "Batch 180, Loss: -0.7028281092643738\n",
      "Batch 181, Loss: -0.8661884069442749\n",
      "Batch 182, Loss: -0.21242862939834595\n",
      "Batch 183, Loss: -0.8886953592300415\n",
      "Batch 184, Loss: -0.12647902965545654\n",
      "Batch 185, Loss: -0.8647773861885071\n",
      "Batch 186, Loss: -0.4935799837112427\n",
      "Batch 187, Loss: 0.34691500663757324\n",
      "Batch 188, Loss: -0.9346732497215271\n",
      "Batch 189, Loss: -0.7453125715255737\n",
      "Batch 190, Loss: -0.21622663736343384\n",
      "Batch 191, Loss: -0.19945454597473145\n",
      "Batch 192, Loss: -0.19735366106033325\n",
      "Batch 193, Loss: -0.4763857126235962\n",
      "Batch 194, Loss: -0.2127496600151062\n",
      "Batch 195, Loss: -0.6891621351242065\n",
      "Batch 196, Loss: -0.8466967344284058\n",
      "Batch 197, Loss: -0.3969290256500244\n",
      "Batch 198, Loss: -0.6337906122207642\n",
      "Batch 199, Loss: -0.12742233276367188\n",
      "Batch 200, Loss: -0.2689287066459656\n",
      "Batch 201, Loss: -0.36272692680358887\n",
      "Batch 202, Loss: -0.5417082905769348\n",
      "Batch 203, Loss: -1.060444951057434\n",
      "Batch 204, Loss: -1.1900010108947754\n",
      "Batch 205, Loss: -1.0024865865707397\n",
      "Batch 206, Loss: -0.39990729093551636\n",
      "Batch 207, Loss: -0.624488115310669\n",
      "Batch 208, Loss: -0.6439039707183838\n",
      "Batch 209, Loss: -0.5335862636566162\n",
      "Batch 210, Loss: -0.13944917917251587\n",
      "Batch 211, Loss: -0.12409114837646484\n",
      "Batch 212, Loss: -0.7034274339675903\n",
      "Batch 213, Loss: -0.19498521089553833\n",
      "Batch 214, Loss: -0.19841039180755615\n",
      "Batch 215, Loss: -0.7273072004318237\n",
      "Batch 216, Loss: -0.8416182398796082\n",
      "Batch 217, Loss: -0.6164489984512329\n",
      "Batch 218, Loss: -0.09224390983581543\n",
      "Batch 219, Loss: -0.7779828310012817\n",
      "Batch 220, Loss: -0.1109122633934021\n",
      "Batch 221, Loss: -0.06893163919448853\n",
      "Batch 222, Loss: -0.8189750909805298\n",
      "Batch 223, Loss: -0.6525421142578125\n",
      "Batch 224, Loss: -1.3209580183029175\n",
      "Batch 225, Loss: -0.48666173219680786\n",
      "Batch 226, Loss: -1.0057400465011597\n",
      "Batch 227, Loss: -0.7474671602249146\n",
      "Batch 228, Loss: -0.1704864501953125\n",
      "Batch 229, Loss: -0.009080171585083008\n",
      "Batch 230, Loss: -0.37466081976890564\n",
      "Batch 231, Loss: -0.3431415557861328\n",
      "Batch 232, Loss: -0.2881263196468353\n",
      "Batch 233, Loss: -0.1736217737197876\n",
      "Batch 234, Loss: -0.5657610297203064\n",
      "Batch 235, Loss: -0.4047144651412964\n",
      "Batch 236, Loss: -0.1247941255569458\n",
      "Batch 237, Loss: -0.16898506879806519\n",
      "Batch 238, Loss: -0.2279214859008789\n",
      "Batch 239, Loss: -1.035662293434143\n",
      "Batch 240, Loss: -1.03147292137146\n",
      "Batch 241, Loss: -0.23833078145980835\n",
      "Batch 242, Loss: -0.15015655755996704\n",
      "Batch 243, Loss: -0.9001200795173645\n",
      "Batch 244, Loss: -0.7874444723129272\n",
      "Batch 245, Loss: -1.0077461004257202\n",
      "Batch 246, Loss: -0.4703044891357422\n",
      "Batch 247, Loss: -0.8166040182113647\n",
      "Batch 248, Loss: -0.37545424699783325\n",
      "Batch 249, Loss: -1.0133421421051025\n",
      "Batch 250, Loss: -0.386691689491272\n",
      "Batch 251, Loss: 0.02744656801223755\n",
      "Batch 252, Loss: -0.18473124504089355\n",
      "Batch 253, Loss: -0.9389160871505737\n",
      "Batch 254, Loss: 0.05856013298034668\n",
      "Batch 255, Loss: -0.13693618774414062\n",
      "Batch 256, Loss: -0.6070564985275269\n",
      "Batch 257, Loss: -0.12196499109268188\n",
      "Batch 258, Loss: -0.9469318389892578\n",
      "Batch 259, Loss: -0.24310877919197083\n",
      "Batch 260, Loss: -0.5447331666946411\n",
      "Batch 261, Loss: -0.5503427982330322\n",
      "Batch 262, Loss: -0.2528897523880005\n",
      "Batch 263, Loss: -0.18665343523025513\n",
      "Batch 264, Loss: -0.3253772258758545\n",
      "Batch 265, Loss: -0.7879013419151306\n",
      "Batch 266, Loss: -0.6663111448287964\n",
      "Batch 267, Loss: 0.4480435848236084\n",
      "Batch 268, Loss: -0.8832535147666931\n",
      "Batch 269, Loss: -0.6096603274345398\n",
      "Batch 270, Loss: -0.21194106340408325\n",
      "Batch 271, Loss: -0.09669327735900879\n",
      "Batch 272, Loss: -0.294677734375\n",
      "Batch 273, Loss: -0.6749382019042969\n",
      "Batch 274, Loss: -0.42239731550216675\n",
      "Batch 275, Loss: -1.0378775596618652\n",
      "Batch 276, Loss: -0.10307955741882324\n",
      "Batch 277, Loss: -0.729010820388794\n",
      "Batch 278, Loss: -1.3280372619628906\n",
      "Batch 279, Loss: -0.5066608786582947\n",
      "Batch 280, Loss: -0.6489329934120178\n",
      "Batch 281, Loss: -0.9069295525550842\n",
      "Batch 282, Loss: 0.02907240390777588\n",
      "Batch 283, Loss: -0.636771559715271\n",
      "Batch 284, Loss: -1.0145162343978882\n",
      "Batch 285, Loss: -0.39897897839546204\n",
      "Batch 286, Loss: -0.9141647219657898\n",
      "Batch 287, Loss: -1.0238749980926514\n",
      "Batch 288, Loss: -0.3542126417160034\n",
      "Batch 289, Loss: -0.2451426386833191\n",
      "Batch 290, Loss: -1.1630033254623413\n",
      "Batch 291, Loss: -0.3824973702430725\n",
      "Batch 292, Loss: -0.283610999584198\n",
      "Batch 293, Loss: -0.5129786133766174\n",
      "Batch 294, Loss: -0.24306917190551758\n",
      "Batch 295, Loss: -0.16256344318389893\n",
      "Batch 296, Loss: -0.48980140686035156\n",
      "Batch 297, Loss: -0.521560788154602\n",
      "Batch 298, Loss: -0.5382687449455261\n",
      "Batch 299, Loss: -0.48269131779670715\n",
      "Batch 300, Loss: -0.9297308921813965\n",
      "Batch 301, Loss: -0.437164306640625\n",
      "Batch 302, Loss: -0.3704298138618469\n",
      "Batch 303, Loss: -0.5775678157806396\n",
      "Batch 304, Loss: -0.5551067590713501\n",
      "Batch 305, Loss: -1.1911492347717285\n",
      "Batch 306, Loss: -0.02607893943786621\n",
      "Batch 307, Loss: -0.90696120262146\n",
      "Batch 308, Loss: -0.7815669178962708\n",
      "Batch 309, Loss: 0.31557726860046387\n",
      "Batch 310, Loss: -0.43666917085647583\n",
      "Batch 311, Loss: -0.6701053977012634\n",
      "Batch 312, Loss: -0.8943368196487427\n",
      "Batch 313, Loss: -1.3129360675811768\n",
      "Batch 314, Loss: -1.2158077955245972\n",
      "Batch 315, Loss: 0.19974708557128906\n",
      "Batch 316, Loss: -0.9009464383125305\n",
      "Batch 317, Loss: -1.3499442338943481\n",
      "Batch 318, Loss: -0.6923472881317139\n",
      "Batch 319, Loss: -0.5723385810852051\n",
      "Batch 320, Loss: -0.24996274709701538\n",
      "Batch 321, Loss: -0.2708534002304077\n",
      "Batch 322, Loss: 0.07618999481201172\n",
      "Batch 323, Loss: -0.8948690295219421\n",
      "Batch 324, Loss: -0.5720493793487549\n",
      "Batch 325, Loss: -0.6730650067329407\n",
      "Batch 326, Loss: -0.10408580303192139\n",
      "Batch 327, Loss: -0.37331175804138184\n",
      "Batch 328, Loss: -0.6521329879760742\n",
      "Batch 329, Loss: -0.07096940279006958\n",
      "Batch 330, Loss: -0.3734712600708008\n",
      "Batch 331, Loss: -0.9619150757789612\n",
      "Batch 332, Loss: -0.2166140079498291\n",
      "Batch 333, Loss: -0.16926920413970947\n",
      "Batch 334, Loss: -0.5107804536819458\n",
      "Batch 335, Loss: -0.30263209342956543\n",
      "Batch 336, Loss: -0.5254677534103394\n",
      "Batch 337, Loss: -0.2621402442455292\n",
      "Batch 338, Loss: -0.8084821105003357\n",
      "Batch 339, Loss: -0.5176145434379578\n",
      "Batch 340, Loss: 0.0009489059448242188\n",
      "Batch 341, Loss: -0.6389187574386597\n",
      "Batch 342, Loss: -0.7294446229934692\n",
      "Batch 343, Loss: -0.863435685634613\n",
      "Batch 344, Loss: -0.5642473697662354\n",
      "Batch 345, Loss: -0.443867027759552\n",
      "Batch 346, Loss: -0.532692551612854\n",
      "Batch 347, Loss: -0.3349840044975281\n",
      "Batch 348, Loss: -0.8764192461967468\n",
      "Batch 349, Loss: -1.1766562461853027\n",
      "Batch 350, Loss: -0.18343907594680786\n",
      "Batch 351, Loss: -0.8814494609832764\n",
      "Batch 352, Loss: -0.6690701842308044\n",
      "Batch 353, Loss: -0.5507653951644897\n",
      "Batch 354, Loss: -0.4271298348903656\n",
      "Batch 355, Loss: -0.25280699133872986\n",
      "Batch 356, Loss: -0.47234249114990234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 357, Loss: -0.802819013595581\n",
      "Batch 358, Loss: -0.34114277362823486\n",
      "Batch 359, Loss: -0.05256545543670654\n",
      "Batch 360, Loss: -0.9751788377761841\n",
      "Batch 361, Loss: -0.3463855981826782\n",
      "Batch 362, Loss: -0.9661628007888794\n",
      "Batch 363, Loss: -0.40081146359443665\n",
      "Batch 364, Loss: -0.5326160192489624\n",
      "Batch 365, Loss: -0.2671511173248291\n",
      "Batch 366, Loss: 0.267122745513916\n",
      "Batch 367, Loss: -0.4204327464103699\n",
      "Batch 368, Loss: -0.9885850548744202\n",
      "Batch 369, Loss: -0.6730139255523682\n",
      "Batch 370, Loss: 0.3284287452697754\n",
      "Batch 371, Loss: -0.32849758863449097\n",
      "Batch 372, Loss: -1.4676933288574219\n",
      "Batch 373, Loss: -0.6893483400344849\n",
      "Batch 374, Loss: -0.256200909614563\n",
      "Batch 375, Loss: -0.8852499127388\n",
      "Batch 376, Loss: -0.5994067192077637\n",
      "Batch 377, Loss: -0.3606640100479126\n",
      "Batch 378, Loss: -0.6834394335746765\n",
      "Batch 379, Loss: -0.47974711656570435\n",
      "Batch 380, Loss: -0.5308228731155396\n",
      "Batch 381, Loss: -0.08902531862258911\n",
      "Batch 382, Loss: -0.8178085684776306\n",
      "Batch 383, Loss: -0.11718976497650146\n",
      "Batch 384, Loss: -0.7930178046226501\n",
      "Batch 385, Loss: -0.19945430755615234\n",
      "Batch 386, Loss: -0.44921207427978516\n",
      "Batch 387, Loss: -0.09747910499572754\n",
      "Batch 388, Loss: 0.1532512903213501\n",
      "Batch 389, Loss: -0.5632094740867615\n",
      "Batch 390, Loss: 0.07199501991271973\n",
      "Batch 391, Loss: -0.5823615789413452\n",
      "Batch 392, Loss: -0.8011842966079712\n",
      "Batch 393, Loss: -0.4255794584751129\n",
      "Batch 394, Loss: -0.2959059774875641\n",
      "Batch 395, Loss: -0.6007716059684753\n",
      "Batch 396, Loss: -0.7773339748382568\n",
      "Batch 397, Loss: -0.5836839079856873\n",
      "Batch 398, Loss: -0.7680969834327698\n",
      "Batch 399, Loss: 0.35571765899658203\n",
      "Training [40%]\tLoss: -0.4903\n",
      "Batch 0, Loss: -0.2821967899799347\n",
      "Batch 1, Loss: -0.1447431445121765\n",
      "Batch 2, Loss: -0.32144948840141296\n",
      "Batch 3, Loss: -0.4312518835067749\n",
      "Batch 4, Loss: -1.1514633893966675\n",
      "Batch 5, Loss: -0.22833597660064697\n",
      "Batch 6, Loss: -0.22997450828552246\n",
      "Batch 7, Loss: -0.7233725190162659\n",
      "Batch 8, Loss: -0.6096764802932739\n",
      "Batch 9, Loss: -0.3408889174461365\n",
      "Batch 10, Loss: -0.48320555686950684\n",
      "Batch 11, Loss: -1.084968090057373\n",
      "Batch 12, Loss: -0.16155391931533813\n",
      "Batch 13, Loss: -0.15705442428588867\n",
      "Batch 14, Loss: -1.1908024549484253\n",
      "Batch 15, Loss: -0.0631483793258667\n",
      "Batch 16, Loss: -0.35857754945755005\n",
      "Batch 17, Loss: -0.45781853795051575\n",
      "Batch 18, Loss: -0.2072082757949829\n",
      "Batch 19, Loss: -0.8147745728492737\n",
      "Batch 20, Loss: -0.600041389465332\n",
      "Batch 21, Loss: -0.7754088640213013\n",
      "Batch 22, Loss: -0.7661311030387878\n",
      "Batch 23, Loss: -0.5073946714401245\n",
      "Batch 24, Loss: -0.05249863862991333\n",
      "Batch 25, Loss: -1.3218284845352173\n",
      "Batch 26, Loss: -1.2534608840942383\n",
      "Batch 27, Loss: -0.8386827707290649\n",
      "Batch 28, Loss: -0.5172524452209473\n",
      "Batch 29, Loss: -0.44732117652893066\n",
      "Batch 30, Loss: -0.5503655076026917\n",
      "Batch 31, Loss: -0.4199223518371582\n",
      "Batch 32, Loss: -0.4022216200828552\n",
      "Batch 33, Loss: -0.9626684188842773\n",
      "Batch 34, Loss: -0.8158323764801025\n",
      "Batch 35, Loss: -1.1747536659240723\n",
      "Batch 36, Loss: 0.28696274757385254\n",
      "Batch 37, Loss: 0.3601033687591553\n",
      "Batch 38, Loss: -0.38229089975357056\n",
      "Batch 39, Loss: -0.6092697381973267\n",
      "Batch 40, Loss: -0.3589150607585907\n",
      "Batch 41, Loss: -0.8818657398223877\n",
      "Batch 42, Loss: -0.5693041086196899\n",
      "Batch 43, Loss: -0.6832475662231445\n",
      "Batch 44, Loss: -0.6222218871116638\n",
      "Batch 45, Loss: -0.9914103746414185\n",
      "Batch 46, Loss: 0.11660933494567871\n",
      "Batch 47, Loss: 0.021922767162322998\n",
      "Batch 48, Loss: -0.5818139314651489\n",
      "Batch 49, Loss: -0.8066876530647278\n",
      "Batch 50, Loss: 0.090049147605896\n",
      "Batch 51, Loss: -0.5216104984283447\n",
      "Batch 52, Loss: -0.28694072365760803\n",
      "Batch 53, Loss: -0.25913897156715393\n",
      "Batch 54, Loss: -0.08623355627059937\n",
      "Batch 55, Loss: -0.8955751061439514\n",
      "Batch 56, Loss: 0.18339776992797852\n",
      "Batch 57, Loss: -1.2235736846923828\n",
      "Batch 58, Loss: -0.5248066186904907\n",
      "Batch 59, Loss: -0.6062474846839905\n",
      "Batch 60, Loss: -0.8628193736076355\n",
      "Batch 61, Loss: -1.2883105278015137\n",
      "Batch 62, Loss: -0.47400832176208496\n",
      "Batch 63, Loss: -0.2503143548965454\n",
      "Batch 64, Loss: -0.43244993686676025\n",
      "Batch 65, Loss: -0.7901778221130371\n",
      "Batch 66, Loss: -0.4720822870731354\n",
      "Batch 67, Loss: -0.31047967076301575\n",
      "Batch 68, Loss: -0.7375695705413818\n",
      "Batch 69, Loss: -0.42947113513946533\n",
      "Batch 70, Loss: 0.39990758895874023\n",
      "Batch 71, Loss: -0.5312623381614685\n",
      "Batch 72, Loss: -0.6859840750694275\n",
      "Batch 73, Loss: -0.4095217287540436\n",
      "Batch 74, Loss: -0.5251356363296509\n",
      "Batch 75, Loss: 0.29288601875305176\n",
      "Batch 76, Loss: -0.17197203636169434\n",
      "Batch 77, Loss: -0.525715708732605\n",
      "Batch 78, Loss: 0.0792229175567627\n",
      "Batch 79, Loss: -0.5267054438591003\n",
      "Batch 80, Loss: -0.7046172618865967\n",
      "Batch 81, Loss: -0.8870156407356262\n",
      "Batch 82, Loss: -0.7906179428100586\n",
      "Batch 83, Loss: -0.20133280754089355\n",
      "Batch 84, Loss: -1.2843858003616333\n",
      "Batch 85, Loss: -0.1325446367263794\n",
      "Batch 86, Loss: -0.7843115925788879\n",
      "Batch 87, Loss: -0.23026502132415771\n",
      "Batch 88, Loss: -1.1804581880569458\n",
      "Batch 89, Loss: 0.32045888900756836\n",
      "Batch 90, Loss: -1.2643489837646484\n",
      "Batch 91, Loss: -0.22339504957199097\n",
      "Batch 92, Loss: -0.5184558629989624\n",
      "Batch 93, Loss: -0.1475893259048462\n",
      "Batch 94, Loss: -0.5669984817504883\n",
      "Batch 95, Loss: -1.3834552764892578\n",
      "Batch 96, Loss: 0.35048413276672363\n",
      "Batch 97, Loss: -0.034868478775024414\n",
      "Batch 98, Loss: -0.7011666297912598\n",
      "Batch 99, Loss: -0.9510722160339355\n",
      "Batch 100, Loss: -0.172593355178833\n",
      "Batch 101, Loss: -0.7853313088417053\n",
      "Batch 102, Loss: -0.15131515264511108\n",
      "Batch 103, Loss: 0.022848904132843018\n",
      "Batch 104, Loss: -0.8899691104888916\n",
      "Batch 105, Loss: -1.171781301498413\n",
      "Batch 106, Loss: -0.21523833274841309\n",
      "Batch 107, Loss: -0.6584890484809875\n",
      "Batch 108, Loss: -0.3344865143299103\n",
      "Batch 109, Loss: -0.13034790754318237\n",
      "Batch 110, Loss: -0.868323028087616\n",
      "Batch 111, Loss: -0.40389299392700195\n",
      "Batch 112, Loss: -1.2385220527648926\n",
      "Batch 113, Loss: -0.5915399193763733\n",
      "Batch 114, Loss: -0.9379953742027283\n",
      "Batch 115, Loss: -0.8343685865402222\n",
      "Batch 116, Loss: -0.3450597822666168\n",
      "Batch 117, Loss: -1.265799880027771\n",
      "Batch 118, Loss: -1.1922768354415894\n",
      "Batch 119, Loss: -0.2597031593322754\n",
      "Batch 120, Loss: -0.5392083525657654\n",
      "Batch 121, Loss: -0.12162184715270996\n",
      "Batch 122, Loss: -0.3138800859451294\n",
      "Batch 123, Loss: -0.23419249057769775\n",
      "Batch 124, Loss: -0.2001028060913086\n",
      "Batch 125, Loss: 0.12117338180541992\n",
      "Batch 126, Loss: -0.3774954378604889\n",
      "Batch 127, Loss: -0.04462796449661255\n",
      "Batch 128, Loss: -0.16984760761260986\n",
      "Batch 129, Loss: 0.08299911022186279\n",
      "Batch 130, Loss: -0.148434579372406\n",
      "Batch 131, Loss: -0.23533064126968384\n",
      "Batch 132, Loss: 0.22962641716003418\n",
      "Batch 133, Loss: -0.21413594484329224\n",
      "Batch 134, Loss: -0.4286072254180908\n",
      "Batch 135, Loss: -0.9140494465827942\n",
      "Batch 136, Loss: -0.2452201247215271\n",
      "Batch 137, Loss: -0.21155214309692383\n",
      "Batch 138, Loss: -0.6707034111022949\n",
      "Batch 139, Loss: -0.17738300561904907\n",
      "Batch 140, Loss: -0.28326839208602905\n",
      "Batch 141, Loss: 0.488653302192688\n",
      "Batch 142, Loss: -0.1312628984451294\n",
      "Batch 143, Loss: -0.49631214141845703\n",
      "Batch 144, Loss: -0.7457566857337952\n",
      "Batch 145, Loss: -0.8297194838523865\n",
      "Batch 146, Loss: -0.764531672000885\n",
      "Batch 147, Loss: -0.0482599139213562\n",
      "Batch 148, Loss: -0.4464778006076813\n",
      "Batch 149, Loss: 0.19515037536621094\n",
      "Batch 150, Loss: -0.583220899105072\n",
      "Batch 151, Loss: -0.16556686162948608\n",
      "Batch 152, Loss: -0.6513287425041199\n",
      "Batch 153, Loss: -1.0311907529830933\n",
      "Batch 154, Loss: -0.8925361633300781\n",
      "Batch 155, Loss: -0.6508569121360779\n",
      "Batch 156, Loss: -0.3545985817909241\n",
      "Batch 157, Loss: 0.4646732807159424\n",
      "Batch 158, Loss: -0.36832550168037415\n",
      "Batch 159, Loss: -1.0831520557403564\n",
      "Batch 160, Loss: -0.5368496179580688\n",
      "Batch 161, Loss: -0.6207005977630615\n",
      "Batch 162, Loss: -0.16434091329574585\n",
      "Batch 163, Loss: -0.863979160785675\n",
      "Batch 164, Loss: -0.5446231365203857\n",
      "Batch 165, Loss: -0.5399757027626038\n",
      "Batch 166, Loss: -0.14845001697540283\n",
      "Batch 167, Loss: -0.1943410038948059\n",
      "Batch 168, Loss: -0.8371856212615967\n",
      "Batch 169, Loss: 0.32387804985046387\n",
      "Batch 170, Loss: -0.38935720920562744\n",
      "Batch 171, Loss: -0.19428890943527222\n",
      "Batch 172, Loss: -0.6990100741386414\n",
      "Batch 173, Loss: -0.3426715135574341\n",
      "Batch 174, Loss: 0.058323025703430176\n",
      "Batch 175, Loss: 0.3031778335571289\n",
      "Batch 176, Loss: -0.7812171578407288\n",
      "Batch 177, Loss: -0.11196929216384888\n",
      "Batch 178, Loss: -0.9089085459709167\n",
      "Batch 179, Loss: 0.29355907440185547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 180, Loss: -0.5627949237823486\n",
      "Batch 181, Loss: -0.38472068309783936\n",
      "Batch 182, Loss: -0.2596637010574341\n",
      "Batch 183, Loss: 0.14205074310302734\n",
      "Batch 184, Loss: -0.22373569011688232\n",
      "Batch 185, Loss: -1.0890637636184692\n",
      "Batch 186, Loss: -0.3064596951007843\n",
      "Batch 187, Loss: -0.2887641489505768\n",
      "Batch 188, Loss: -0.1542721390724182\n",
      "Batch 189, Loss: -0.23644894361495972\n",
      "Batch 190, Loss: -0.6802207231521606\n",
      "Batch 191, Loss: -0.5290476083755493\n",
      "Batch 192, Loss: -0.07746309041976929\n",
      "Batch 193, Loss: 0.23510980606079102\n",
      "Batch 194, Loss: -1.1451616287231445\n",
      "Batch 195, Loss: -1.0194827318191528\n",
      "Batch 196, Loss: -0.18463516235351562\n",
      "Batch 197, Loss: -0.9060295224189758\n",
      "Batch 198, Loss: -0.6016595959663391\n",
      "Batch 199, Loss: -0.5494098663330078\n",
      "Batch 200, Loss: -0.6120449304580688\n",
      "Batch 201, Loss: -0.20941942930221558\n",
      "Batch 202, Loss: -0.8459445238113403\n",
      "Batch 203, Loss: -0.609959602355957\n",
      "Batch 204, Loss: 0.03943312168121338\n",
      "Batch 205, Loss: -0.4424303472042084\n",
      "Batch 206, Loss: -1.0840855836868286\n",
      "Batch 207, Loss: -0.2393399477005005\n",
      "Batch 208, Loss: -0.44828152656555176\n",
      "Batch 209, Loss: -0.13818824291229248\n",
      "Batch 210, Loss: -0.6977969408035278\n",
      "Batch 211, Loss: -0.8221868276596069\n",
      "Batch 212, Loss: -0.43556123971939087\n",
      "Batch 213, Loss: 0.2440943717956543\n",
      "Batch 214, Loss: -0.842642605304718\n",
      "Batch 215, Loss: -0.3145298361778259\n",
      "Batch 216, Loss: -0.8900874257087708\n",
      "Batch 217, Loss: -0.21779865026474\n",
      "Batch 218, Loss: -0.7737708687782288\n",
      "Batch 219, Loss: -0.388480007648468\n",
      "Batch 220, Loss: -0.41763073205947876\n",
      "Batch 221, Loss: -0.9343292713165283\n",
      "Batch 222, Loss: -0.21920788288116455\n",
      "Batch 223, Loss: -0.23867493867874146\n",
      "Batch 224, Loss: -0.9150230884552002\n",
      "Batch 225, Loss: -1.0496325492858887\n",
      "Batch 226, Loss: -0.3810689449310303\n",
      "Batch 227, Loss: -1.2431281805038452\n",
      "Batch 228, Loss: -0.3390943706035614\n",
      "Batch 229, Loss: -0.5838794708251953\n",
      "Batch 230, Loss: -0.9514693021774292\n",
      "Batch 231, Loss: -0.936898946762085\n",
      "Batch 232, Loss: -0.002283334732055664\n",
      "Batch 233, Loss: -0.4190196990966797\n",
      "Batch 234, Loss: -0.8111178278923035\n",
      "Batch 235, Loss: -0.8010998964309692\n",
      "Batch 236, Loss: -0.5497750043869019\n",
      "Batch 237, Loss: -0.23203688859939575\n",
      "Batch 238, Loss: -0.36652886867523193\n",
      "Batch 239, Loss: 0.0320432186126709\n",
      "Batch 240, Loss: 0.070648193359375\n",
      "Batch 241, Loss: -0.31596893072128296\n",
      "Batch 242, Loss: -0.3830820918083191\n",
      "Batch 243, Loss: -0.93159419298172\n",
      "Batch 244, Loss: -0.4173732399940491\n",
      "Batch 245, Loss: -0.5618817806243896\n",
      "Batch 246, Loss: -0.40540868043899536\n",
      "Batch 247, Loss: -0.8339788913726807\n",
      "Batch 248, Loss: -0.07699781656265259\n",
      "Batch 249, Loss: -0.6056044101715088\n",
      "Batch 250, Loss: -0.1756238341331482\n",
      "Batch 251, Loss: -0.6444712281227112\n",
      "Batch 252, Loss: -0.7162809371948242\n",
      "Batch 253, Loss: -0.1805594563484192\n",
      "Batch 254, Loss: -0.5187035799026489\n",
      "Batch 255, Loss: -0.16618067026138306\n",
      "Batch 256, Loss: -0.969939649105072\n",
      "Batch 257, Loss: -0.4656912088394165\n",
      "Batch 258, Loss: -0.6606600284576416\n",
      "Batch 259, Loss: -0.41628530621528625\n",
      "Batch 260, Loss: -0.8268667459487915\n",
      "Batch 261, Loss: -0.637121856212616\n",
      "Batch 262, Loss: -0.5637516975402832\n",
      "Batch 263, Loss: 0.1973022222518921\n",
      "Batch 264, Loss: -0.21093636751174927\n",
      "Batch 265, Loss: -0.5416410565376282\n",
      "Batch 266, Loss: -0.2763289213180542\n",
      "Batch 267, Loss: -0.2592850923538208\n",
      "Batch 268, Loss: -0.9201238751411438\n",
      "Batch 269, Loss: -0.7940776944160461\n",
      "Batch 270, Loss: -0.7095207571983337\n",
      "Batch 271, Loss: -0.5376681089401245\n",
      "Batch 272, Loss: -0.4048842191696167\n",
      "Batch 273, Loss: -0.5956593751907349\n",
      "Batch 274, Loss: -0.011176526546478271\n",
      "Batch 275, Loss: -0.03743010759353638\n",
      "Batch 276, Loss: -0.6446574926376343\n",
      "Batch 277, Loss: -0.553911566734314\n",
      "Batch 278, Loss: -0.5315437316894531\n",
      "Batch 279, Loss: -0.4351066052913666\n",
      "Batch 280, Loss: -0.6264290809631348\n",
      "Batch 281, Loss: -0.44369032979011536\n",
      "Batch 282, Loss: -0.2051175832748413\n",
      "Batch 283, Loss: 0.4157484769821167\n",
      "Batch 284, Loss: -0.1357753872871399\n",
      "Batch 285, Loss: -0.6729156970977783\n",
      "Batch 286, Loss: -0.5389549136161804\n",
      "Batch 287, Loss: -0.5812633633613586\n",
      "Batch 288, Loss: -0.4970119297504425\n",
      "Batch 289, Loss: -0.8975337743759155\n",
      "Batch 290, Loss: -0.6941835284233093\n",
      "Batch 291, Loss: -0.009332478046417236\n",
      "Batch 292, Loss: -0.5125318765640259\n",
      "Batch 293, Loss: -0.07277065515518188\n",
      "Batch 294, Loss: -0.9285821318626404\n",
      "Batch 295, Loss: -0.7393023371696472\n",
      "Batch 296, Loss: -0.14203619956970215\n",
      "Batch 297, Loss: -0.303077757358551\n",
      "Batch 298, Loss: -1.1080201864242554\n",
      "Batch 299, Loss: -0.4020421504974365\n",
      "Batch 300, Loss: -0.06676352024078369\n",
      "Batch 301, Loss: -1.2646503448486328\n",
      "Batch 302, Loss: -0.26707881689071655\n",
      "Batch 303, Loss: -0.6254110336303711\n",
      "Batch 304, Loss: -0.3250095844268799\n",
      "Batch 305, Loss: -1.299100399017334\n",
      "Batch 306, Loss: -0.49174240231513977\n",
      "Batch 307, Loss: -0.6697860956192017\n",
      "Batch 308, Loss: -1.135603666305542\n",
      "Batch 309, Loss: -0.14384222030639648\n",
      "Batch 310, Loss: -0.4801716208457947\n",
      "Batch 311, Loss: -0.9398032426834106\n",
      "Batch 312, Loss: -0.522572934627533\n",
      "Batch 313, Loss: -0.8369097113609314\n",
      "Batch 314, Loss: -0.6199369430541992\n",
      "Batch 315, Loss: -0.5828216075897217\n",
      "Batch 316, Loss: -1.0041723251342773\n",
      "Batch 317, Loss: -0.6516250371932983\n",
      "Batch 318, Loss: -0.31550133228302\n",
      "Batch 319, Loss: -0.6540245413780212\n",
      "Batch 320, Loss: -0.39525675773620605\n",
      "Batch 321, Loss: -1.3542234897613525\n",
      "Batch 322, Loss: -0.5856063961982727\n",
      "Batch 323, Loss: -0.23801767826080322\n",
      "Batch 324, Loss: -0.5094776153564453\n",
      "Batch 325, Loss: -0.7563621997833252\n",
      "Batch 326, Loss: -0.3769339323043823\n",
      "Batch 327, Loss: -0.643578827381134\n",
      "Batch 328, Loss: -1.4497700929641724\n",
      "Batch 329, Loss: -0.6886533498764038\n",
      "Batch 330, Loss: -0.2569171190261841\n",
      "Batch 331, Loss: -1.2635962963104248\n",
      "Batch 332, Loss: -0.32559412717819214\n",
      "Batch 333, Loss: -0.2288110852241516\n",
      "Batch 334, Loss: -0.3300403952598572\n",
      "Batch 335, Loss: -0.8882209658622742\n",
      "Batch 336, Loss: -0.8424766063690186\n",
      "Batch 337, Loss: -0.8062956929206848\n",
      "Batch 338, Loss: 0.3974752426147461\n",
      "Batch 339, Loss: -0.814487874507904\n",
      "Batch 340, Loss: -0.927861213684082\n",
      "Batch 341, Loss: -0.15299028158187866\n",
      "Batch 342, Loss: -0.31672555208206177\n",
      "Batch 343, Loss: -0.14269018173217773\n",
      "Batch 344, Loss: -0.7360106110572815\n",
      "Batch 345, Loss: -0.9393609166145325\n",
      "Batch 346, Loss: -0.8365681767463684\n",
      "Batch 347, Loss: -1.2577342987060547\n",
      "Batch 348, Loss: -0.3020244240760803\n",
      "Batch 349, Loss: -0.8004966974258423\n",
      "Batch 350, Loss: -0.2794955372810364\n",
      "Batch 351, Loss: -0.7019526362419128\n",
      "Batch 352, Loss: -0.5818033814430237\n",
      "Batch 353, Loss: -0.9304866194725037\n",
      "Batch 354, Loss: -0.468120813369751\n",
      "Batch 355, Loss: 0.00790262222290039\n",
      "Batch 356, Loss: 0.04626321792602539\n",
      "Batch 357, Loss: -0.495227575302124\n",
      "Batch 358, Loss: 0.3989671468734741\n",
      "Batch 359, Loss: -0.19751697778701782\n",
      "Batch 360, Loss: -1.3745229244232178\n",
      "Batch 361, Loss: -0.39954400062561035\n",
      "Batch 362, Loss: -0.46466296911239624\n",
      "Batch 363, Loss: -0.5402964353561401\n",
      "Batch 364, Loss: -0.37488579750061035\n",
      "Batch 365, Loss: -0.21603339910507202\n",
      "Batch 366, Loss: -0.7714447975158691\n",
      "Batch 367, Loss: -0.6104556322097778\n",
      "Batch 368, Loss: -0.2882741093635559\n",
      "Batch 369, Loss: -0.08303815126419067\n",
      "Batch 370, Loss: -0.6852351427078247\n",
      "Batch 371, Loss: -0.5060588121414185\n",
      "Batch 372, Loss: -0.05327177047729492\n",
      "Batch 373, Loss: -0.40878090262413025\n",
      "Batch 374, Loss: -0.8429399132728577\n",
      "Batch 375, Loss: -0.6941425800323486\n",
      "Batch 376, Loss: -0.18839538097381592\n",
      "Batch 377, Loss: -0.16994017362594604\n",
      "Batch 378, Loss: -0.30964910984039307\n",
      "Batch 379, Loss: -1.3002523183822632\n",
      "Batch 380, Loss: 0.2004249095916748\n",
      "Batch 381, Loss: -0.7731552124023438\n",
      "Batch 382, Loss: -0.2507270574569702\n",
      "Batch 383, Loss: -0.6668863892555237\n",
      "Batch 384, Loss: -0.4889814853668213\n",
      "Batch 385, Loss: -0.44007042050361633\n",
      "Batch 386, Loss: -0.19856655597686768\n",
      "Batch 387, Loss: -1.08984375\n",
      "Batch 388, Loss: -0.6417174339294434\n",
      "Batch 389, Loss: -0.5301181077957153\n",
      "Batch 390, Loss: -0.5649733543395996\n",
      "Batch 391, Loss: -0.7588979601860046\n",
      "Batch 392, Loss: -0.2398582100868225\n",
      "Batch 393, Loss: -0.5610886216163635\n",
      "Batch 394, Loss: -0.7825042009353638\n",
      "Batch 395, Loss: -0.566214919090271\n",
      "Batch 396, Loss: 0.34020090103149414\n",
      "Batch 397, Loss: -0.5051528215408325\n",
      "Batch 398, Loss: -0.32040107250213623\n",
      "Batch 399, Loss: -0.6138268113136292\n",
      "Training [50%]\tLoss: -0.4886\n",
      "Batch 0, Loss: -0.7617911696434021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Loss: -0.7832902669906616\n",
      "Batch 2, Loss: -0.6353173851966858\n",
      "Batch 3, Loss: -0.219832181930542\n",
      "Batch 4, Loss: -0.5375924110412598\n",
      "Batch 5, Loss: 0.12272334098815918\n",
      "Batch 6, Loss: -0.1569974422454834\n",
      "Batch 7, Loss: -0.7514232397079468\n",
      "Batch 8, Loss: -0.3014187216758728\n",
      "Batch 9, Loss: -0.23346972465515137\n",
      "Batch 10, Loss: -0.4401896595954895\n",
      "Batch 11, Loss: -0.7392719984054565\n",
      "Batch 12, Loss: 0.004989266395568848\n",
      "Batch 13, Loss: -0.09596741199493408\n",
      "Batch 14, Loss: -1.0396976470947266\n",
      "Batch 15, Loss: -0.9890342354774475\n",
      "Batch 16, Loss: -0.7202389240264893\n",
      "Batch 17, Loss: -0.6342625617980957\n",
      "Batch 18, Loss: -1.116074800491333\n",
      "Batch 19, Loss: -0.4347797930240631\n",
      "Batch 20, Loss: -0.8800151348114014\n",
      "Batch 21, Loss: -0.7357051968574524\n",
      "Batch 22, Loss: -0.734886646270752\n",
      "Batch 23, Loss: -0.70721834897995\n",
      "Batch 24, Loss: -0.24902021884918213\n",
      "Batch 25, Loss: -0.5369084477424622\n",
      "Batch 26, Loss: -0.09417879581451416\n",
      "Batch 27, Loss: -0.22363340854644775\n",
      "Batch 28, Loss: -0.6090275049209595\n",
      "Batch 29, Loss: -0.49302202463150024\n",
      "Batch 30, Loss: -0.7828845977783203\n",
      "Batch 31, Loss: -0.5291609764099121\n",
      "Batch 32, Loss: -1.1869947910308838\n",
      "Batch 33, Loss: -0.3958326280117035\n",
      "Batch 34, Loss: -0.4432467818260193\n",
      "Batch 35, Loss: 0.30082952976226807\n",
      "Batch 36, Loss: -0.523408830165863\n",
      "Batch 37, Loss: -1.231757640838623\n",
      "Batch 38, Loss: -0.17265164852142334\n",
      "Batch 39, Loss: -0.40958040952682495\n",
      "Batch 40, Loss: -1.189932942390442\n",
      "Batch 41, Loss: -0.7166016697883606\n",
      "Batch 42, Loss: -0.4207446277141571\n",
      "Batch 43, Loss: -0.9122521281242371\n",
      "Batch 44, Loss: -0.7138233780860901\n",
      "Batch 45, Loss: -0.5467475652694702\n",
      "Batch 46, Loss: -1.0156958103179932\n",
      "Batch 47, Loss: -1.1781660318374634\n",
      "Batch 48, Loss: -0.9916289448738098\n",
      "Batch 49, Loss: -0.40004396438598633\n",
      "Batch 50, Loss: -0.8134163022041321\n",
      "Batch 51, Loss: -0.27969861030578613\n",
      "Batch 52, Loss: -0.5432050228118896\n",
      "Batch 53, Loss: -0.08881598711013794\n",
      "Batch 54, Loss: -0.2878781855106354\n",
      "Batch 55, Loss: -0.2979664206504822\n",
      "Batch 56, Loss: -0.6639294624328613\n",
      "Batch 57, Loss: -1.0009593963623047\n",
      "Batch 58, Loss: -0.04211384057998657\n",
      "Batch 59, Loss: -0.155195951461792\n",
      "Batch 60, Loss: -0.7465630173683167\n",
      "Batch 61, Loss: -0.6356918215751648\n",
      "Batch 62, Loss: -0.4910886585712433\n",
      "Batch 63, Loss: -0.16918236017227173\n",
      "Batch 64, Loss: -0.5874384641647339\n",
      "Batch 65, Loss: -0.5682173371315002\n",
      "Batch 66, Loss: -0.6412457823753357\n",
      "Batch 67, Loss: -0.3304141163825989\n",
      "Batch 68, Loss: -1.2860634326934814\n",
      "Batch 69, Loss: -0.08280247449874878\n",
      "Batch 70, Loss: 0.26476025581359863\n",
      "Batch 71, Loss: -1.2682512998580933\n",
      "Batch 72, Loss: -0.03200632333755493\n",
      "Batch 73, Loss: -0.3749917149543762\n",
      "Batch 74, Loss: 0.1507171392440796\n",
      "Batch 75, Loss: -0.15582549571990967\n",
      "Batch 76, Loss: -1.1521990299224854\n",
      "Batch 77, Loss: -0.20345306396484375\n",
      "Batch 78, Loss: 0.16932547092437744\n",
      "Batch 79, Loss: -0.5269229412078857\n",
      "Batch 80, Loss: -1.225892186164856\n",
      "Batch 81, Loss: 0.05542099475860596\n",
      "Batch 82, Loss: -0.06481713056564331\n",
      "Batch 83, Loss: -0.29313981533050537\n",
      "Batch 84, Loss: -0.2326943278312683\n",
      "Batch 85, Loss: 0.04352676868438721\n",
      "Batch 86, Loss: -0.6173890829086304\n",
      "Batch 87, Loss: 0.13013780117034912\n",
      "Batch 88, Loss: -0.1378786563873291\n",
      "Batch 89, Loss: -1.0581293106079102\n",
      "Batch 90, Loss: -1.082878589630127\n",
      "Batch 91, Loss: -0.2795097827911377\n",
      "Batch 92, Loss: 0.06400537490844727\n",
      "Batch 93, Loss: -0.37915152311325073\n",
      "Batch 94, Loss: -0.03548091650009155\n",
      "Batch 95, Loss: -0.9537577629089355\n",
      "Batch 96, Loss: -0.5590212941169739\n",
      "Batch 97, Loss: 0.30701637268066406\n",
      "Batch 98, Loss: -1.061584711074829\n",
      "Batch 99, Loss: -0.6179080009460449\n",
      "Batch 100, Loss: -0.4790211319923401\n",
      "Batch 101, Loss: -0.04937005043029785\n",
      "Batch 102, Loss: -0.3233548402786255\n",
      "Batch 103, Loss: -0.7536255717277527\n",
      "Batch 104, Loss: -1.2861032485961914\n",
      "Batch 105, Loss: -0.030144810676574707\n",
      "Batch 106, Loss: -0.2768101394176483\n",
      "Batch 107, Loss: -0.8022785186767578\n",
      "Batch 108, Loss: -0.3001633286476135\n",
      "Batch 109, Loss: 0.2318267822265625\n",
      "Batch 110, Loss: -0.4727240204811096\n",
      "Batch 111, Loss: -0.7810655236244202\n",
      "Batch 112, Loss: -0.35972049832344055\n",
      "Batch 113, Loss: -0.5112071633338928\n",
      "Batch 114, Loss: -0.542678952217102\n",
      "Batch 115, Loss: -0.36496642231941223\n",
      "Batch 116, Loss: -0.40145471692085266\n",
      "Batch 117, Loss: -0.33637887239456177\n",
      "Batch 118, Loss: 0.43619489669799805\n",
      "Batch 119, Loss: -1.0225483179092407\n",
      "Batch 120, Loss: -0.6215882301330566\n",
      "Batch 121, Loss: -0.8903378248214722\n",
      "Batch 122, Loss: -0.05246269702911377\n",
      "Batch 123, Loss: -0.8929643630981445\n",
      "Batch 124, Loss: -0.9896292090415955\n",
      "Batch 125, Loss: 0.07532095909118652\n",
      "Batch 126, Loss: -0.5482885837554932\n",
      "Batch 127, Loss: -0.05447089672088623\n",
      "Batch 128, Loss: -0.374325692653656\n",
      "Batch 129, Loss: -0.21783661842346191\n",
      "Batch 130, Loss: 0.4890885353088379\n",
      "Batch 131, Loss: 0.2114710807800293\n",
      "Batch 132, Loss: -0.29895830154418945\n",
      "Batch 133, Loss: -0.2077426314353943\n",
      "Batch 134, Loss: -0.12470203638076782\n",
      "Batch 135, Loss: -1.1396123170852661\n",
      "Batch 136, Loss: -0.3792403042316437\n",
      "Batch 137, Loss: 0.1928161382675171\n",
      "Batch 138, Loss: -0.39486098289489746\n",
      "Batch 139, Loss: -0.0009400844573974609\n",
      "Batch 140, Loss: -1.048734426498413\n",
      "Batch 141, Loss: -0.5916635990142822\n",
      "Batch 142, Loss: -1.424166202545166\n",
      "Batch 143, Loss: -1.2537797689437866\n",
      "Batch 144, Loss: -0.24159455299377441\n",
      "Batch 145, Loss: -0.7182662487030029\n",
      "Batch 146, Loss: -0.7469179034233093\n",
      "Batch 147, Loss: -0.40695708990097046\n",
      "Batch 148, Loss: -0.5903444290161133\n",
      "Batch 149, Loss: -0.010223984718322754\n",
      "Batch 150, Loss: 0.05185270309448242\n",
      "Batch 151, Loss: -0.3087237775325775\n",
      "Batch 152, Loss: -0.864848256111145\n",
      "Batch 153, Loss: -0.8946890830993652\n",
      "Batch 154, Loss: -0.8221375346183777\n",
      "Batch 155, Loss: -0.7066541314125061\n",
      "Batch 156, Loss: -0.7819141149520874\n",
      "Batch 157, Loss: -0.33001983165740967\n",
      "Batch 158, Loss: -0.5863710641860962\n",
      "Batch 159, Loss: -0.40964409708976746\n",
      "Batch 160, Loss: -0.3710458278656006\n",
      "Batch 161, Loss: -0.08937376737594604\n",
      "Batch 162, Loss: -0.40912312269210815\n",
      "Batch 163, Loss: -0.2952481508255005\n",
      "Batch 164, Loss: 0.173569917678833\n",
      "Batch 165, Loss: -0.8924644589424133\n",
      "Batch 166, Loss: -0.51004958152771\n",
      "Batch 167, Loss: 0.2530078887939453\n",
      "Batch 168, Loss: 0.11743617057800293\n",
      "Batch 169, Loss: -0.2700110971927643\n",
      "Batch 170, Loss: -0.15107297897338867\n",
      "Batch 171, Loss: 0.3835446834564209\n",
      "Batch 172, Loss: -0.6618431210517883\n",
      "Batch 173, Loss: -0.32037875056266785\n",
      "Batch 174, Loss: -0.5861823558807373\n",
      "Batch 175, Loss: -0.11440211534500122\n",
      "Batch 176, Loss: -0.39519423246383667\n",
      "Batch 177, Loss: -0.41421857476234436\n",
      "Batch 178, Loss: -0.6175813674926758\n",
      "Batch 179, Loss: -0.49174749851226807\n",
      "Batch 180, Loss: -0.7260705232620239\n",
      "Batch 181, Loss: 0.3245910406112671\n",
      "Batch 182, Loss: -0.8956722021102905\n",
      "Batch 183, Loss: -0.7418240308761597\n",
      "Batch 184, Loss: -1.2639706134796143\n",
      "Batch 185, Loss: -0.3590874969959259\n",
      "Batch 186, Loss: 0.06220364570617676\n",
      "Batch 187, Loss: -0.20005488395690918\n",
      "Batch 188, Loss: -0.6750876307487488\n",
      "Batch 189, Loss: -0.01898711919784546\n",
      "Batch 190, Loss: -0.17473864555358887\n",
      "Batch 191, Loss: -1.043306827545166\n",
      "Batch 192, Loss: -0.42178553342819214\n",
      "Batch 193, Loss: 0.12785649299621582\n",
      "Batch 194, Loss: -0.7680448293685913\n",
      "Batch 195, Loss: -0.822858989238739\n",
      "Batch 196, Loss: -0.937070906162262\n",
      "Batch 197, Loss: -0.13351339101791382\n",
      "Batch 198, Loss: -0.860539436340332\n",
      "Batch 199, Loss: -0.42238447070121765\n",
      "Batch 200, Loss: -0.511901319026947\n",
      "Batch 201, Loss: -0.5570720434188843\n",
      "Batch 202, Loss: -0.5719994306564331\n",
      "Batch 203, Loss: -1.157710075378418\n",
      "Batch 204, Loss: -0.3872659504413605\n",
      "Batch 205, Loss: -1.3102084398269653\n",
      "Batch 206, Loss: -0.5676854848861694\n",
      "Batch 207, Loss: -0.535788893699646\n",
      "Batch 208, Loss: -0.2470548152923584\n",
      "Batch 209, Loss: -0.3261220455169678\n",
      "Batch 210, Loss: -1.0897313356399536\n",
      "Batch 211, Loss: -0.8348034620285034\n",
      "Batch 212, Loss: -0.3998558521270752\n",
      "Batch 213, Loss: -0.34391748905181885\n",
      "Batch 214, Loss: -0.8377155065536499\n",
      "Batch 215, Loss: -1.1127574443817139\n",
      "Batch 216, Loss: -0.6615961194038391\n",
      "Batch 217, Loss: -0.47086209058761597\n",
      "Batch 218, Loss: -0.5673527121543884\n",
      "Batch 219, Loss: -0.8173190355300903\n",
      "Batch 220, Loss: -0.3241104781627655\n",
      "Batch 221, Loss: 0.18971168994903564\n",
      "Batch 222, Loss: -0.7207813858985901\n",
      "Batch 223, Loss: -0.529914140701294\n",
      "Batch 224, Loss: -0.7405098080635071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 225, Loss: -0.48962610960006714\n",
      "Batch 226, Loss: -1.162458062171936\n",
      "Batch 227, Loss: -0.5473237037658691\n",
      "Batch 228, Loss: -0.5635169744491577\n",
      "Batch 229, Loss: -0.9410465955734253\n",
      "Batch 230, Loss: -0.46866345405578613\n",
      "Batch 231, Loss: -0.26422810554504395\n",
      "Batch 232, Loss: -0.8776795864105225\n",
      "Batch 233, Loss: -1.0428804159164429\n",
      "Batch 234, Loss: -0.3662940263748169\n",
      "Batch 235, Loss: -0.41736888885498047\n",
      "Batch 236, Loss: 0.018929004669189453\n",
      "Batch 237, Loss: -0.1830543875694275\n",
      "Batch 238, Loss: -0.6731042265892029\n",
      "Batch 239, Loss: -0.800768256187439\n",
      "Batch 240, Loss: 0.07209551334381104\n",
      "Batch 241, Loss: 0.026400327682495117\n",
      "Batch 242, Loss: -0.6243735551834106\n",
      "Batch 243, Loss: -0.608039140701294\n",
      "Batch 244, Loss: -0.6432470083236694\n",
      "Batch 245, Loss: -0.11289447546005249\n",
      "Batch 246, Loss: -0.501750648021698\n",
      "Batch 247, Loss: 0.045415520668029785\n",
      "Batch 248, Loss: -0.450764536857605\n",
      "Batch 249, Loss: -0.5779764652252197\n",
      "Batch 250, Loss: -0.5834662914276123\n",
      "Batch 251, Loss: -0.5914562940597534\n",
      "Batch 252, Loss: -0.31983256340026855\n",
      "Batch 253, Loss: -0.6864451766014099\n",
      "Batch 254, Loss: -1.1385456323623657\n",
      "Batch 255, Loss: -0.5128076672554016\n",
      "Batch 256, Loss: -0.8300274014472961\n",
      "Batch 257, Loss: 0.2594935894012451\n",
      "Batch 258, Loss: -1.3097180128097534\n",
      "Batch 259, Loss: -0.6690535545349121\n",
      "Batch 260, Loss: -0.6899306774139404\n",
      "Batch 261, Loss: 0.10100007057189941\n",
      "Batch 262, Loss: -0.3385410010814667\n",
      "Batch 263, Loss: -0.47895291447639465\n",
      "Batch 264, Loss: -0.16377520561218262\n",
      "Batch 265, Loss: -1.013828158378601\n",
      "Batch 266, Loss: -0.8860456347465515\n",
      "Batch 267, Loss: -1.2504280805587769\n",
      "Batch 268, Loss: -0.022176086902618408\n",
      "Batch 269, Loss: -0.7447912693023682\n",
      "Batch 270, Loss: -0.3780565857887268\n",
      "Batch 271, Loss: -1.321821928024292\n",
      "Batch 272, Loss: -0.6920456886291504\n",
      "Batch 273, Loss: -1.0255388021469116\n",
      "Batch 274, Loss: -1.143643856048584\n",
      "Batch 275, Loss: -0.13943910598754883\n",
      "Batch 276, Loss: -0.5591304302215576\n",
      "Batch 277, Loss: -0.16667604446411133\n",
      "Batch 278, Loss: 0.023600459098815918\n",
      "Batch 279, Loss: -0.6437562108039856\n",
      "Batch 280, Loss: -0.49266940355300903\n",
      "Batch 281, Loss: -0.08627951145172119\n",
      "Batch 282, Loss: -0.40694257616996765\n",
      "Batch 283, Loss: -0.2844608426094055\n",
      "Batch 284, Loss: -0.5452466607093811\n",
      "Batch 285, Loss: -0.13318467140197754\n",
      "Batch 286, Loss: -0.7650905847549438\n",
      "Batch 287, Loss: -0.4614521861076355\n",
      "Batch 288, Loss: -0.1076730489730835\n",
      "Batch 289, Loss: -0.4399547278881073\n",
      "Batch 290, Loss: -0.9329887628555298\n",
      "Batch 291, Loss: -0.5497050881385803\n",
      "Batch 292, Loss: -1.1867616176605225\n",
      "Batch 293, Loss: -0.15527504682540894\n",
      "Batch 294, Loss: -0.6982710957527161\n",
      "Batch 295, Loss: -0.3948325514793396\n",
      "Batch 296, Loss: -0.2287600040435791\n",
      "Batch 297, Loss: -0.1939525604248047\n",
      "Batch 298, Loss: -0.8832622170448303\n",
      "Batch 299, Loss: -0.5211958885192871\n",
      "Batch 300, Loss: 0.33403444290161133\n",
      "Batch 301, Loss: -0.18032026290893555\n",
      "Batch 302, Loss: -0.2787587642669678\n",
      "Batch 303, Loss: -0.30922994017601013\n",
      "Batch 304, Loss: -0.35050398111343384\n",
      "Batch 305, Loss: 0.11841809749603271\n",
      "Batch 306, Loss: -0.4894459843635559\n",
      "Batch 307, Loss: -0.9577716588973999\n",
      "Batch 308, Loss: -0.566766619682312\n",
      "Batch 309, Loss: -0.5439823865890503\n",
      "Batch 310, Loss: -0.5596379637718201\n",
      "Batch 311, Loss: -0.08004271984100342\n",
      "Batch 312, Loss: -0.6741048097610474\n",
      "Batch 313, Loss: -0.7457970380783081\n",
      "Batch 314, Loss: -1.257091999053955\n",
      "Batch 315, Loss: -0.6874130964279175\n",
      "Batch 316, Loss: -0.0340723991394043\n",
      "Batch 317, Loss: -0.5828059911727905\n",
      "Batch 318, Loss: -0.6612473726272583\n",
      "Batch 319, Loss: 0.19151568412780762\n",
      "Batch 320, Loss: -0.50287926197052\n",
      "Batch 321, Loss: -0.3653676509857178\n",
      "Batch 322, Loss: 0.31116366386413574\n",
      "Batch 323, Loss: -0.754167914390564\n",
      "Batch 324, Loss: -0.8157903552055359\n",
      "Batch 325, Loss: -0.734397292137146\n",
      "Batch 326, Loss: -0.4456586539745331\n",
      "Batch 327, Loss: -0.1059943437576294\n",
      "Batch 328, Loss: -0.6999207139015198\n",
      "Batch 329, Loss: -0.22397923469543457\n",
      "Batch 330, Loss: -0.5282987356185913\n",
      "Batch 331, Loss: -1.2259576320648193\n",
      "Batch 332, Loss: -0.8374437093734741\n",
      "Batch 333, Loss: -0.1784711480140686\n",
      "Batch 334, Loss: -0.9812559485435486\n",
      "Batch 335, Loss: -0.4966544508934021\n",
      "Batch 336, Loss: 0.11179304122924805\n",
      "Batch 337, Loss: -0.7084792852401733\n",
      "Batch 338, Loss: -0.765089750289917\n",
      "Batch 339, Loss: -0.7008240222930908\n",
      "Batch 340, Loss: -0.7611629366874695\n",
      "Batch 341, Loss: -0.18341869115829468\n",
      "Batch 342, Loss: -0.8413398265838623\n",
      "Batch 343, Loss: -0.6229703426361084\n",
      "Batch 344, Loss: -0.6042020320892334\n",
      "Batch 345, Loss: -0.8062729239463806\n",
      "Batch 346, Loss: -0.5707148313522339\n",
      "Batch 347, Loss: -0.6538852453231812\n",
      "Batch 348, Loss: -0.7921684384346008\n",
      "Batch 349, Loss: -0.4749443531036377\n",
      "Batch 350, Loss: -0.27136552333831787\n",
      "Batch 351, Loss: -0.41911596059799194\n",
      "Batch 352, Loss: -0.8082419633865356\n",
      "Batch 353, Loss: -0.1146969199180603\n",
      "Batch 354, Loss: -0.2671552300453186\n",
      "Batch 355, Loss: 0.20244669914245605\n",
      "Batch 356, Loss: -0.5005926489830017\n",
      "Batch 357, Loss: -0.7311553359031677\n",
      "Batch 358, Loss: -0.07750540971755981\n",
      "Batch 359, Loss: -0.7624515891075134\n",
      "Batch 360, Loss: -0.9713138341903687\n",
      "Batch 361, Loss: -0.7680646181106567\n",
      "Batch 362, Loss: 0.13817989826202393\n",
      "Batch 363, Loss: -0.7859185338020325\n",
      "Batch 364, Loss: 0.2883570194244385\n",
      "Batch 365, Loss: -0.6954894065856934\n",
      "Batch 366, Loss: -1.0275733470916748\n",
      "Batch 367, Loss: -1.1587040424346924\n",
      "Batch 368, Loss: -0.9787834286689758\n",
      "Batch 369, Loss: -1.0710313320159912\n",
      "Batch 370, Loss: -0.021879196166992188\n",
      "Batch 371, Loss: -0.9457074999809265\n",
      "Batch 372, Loss: 0.199773907661438\n",
      "Batch 373, Loss: -0.052047014236450195\n",
      "Batch 374, Loss: -1.0525014400482178\n",
      "Batch 375, Loss: -0.7297645807266235\n",
      "Batch 376, Loss: -1.1910881996154785\n",
      "Batch 377, Loss: -0.6570634245872498\n",
      "Batch 378, Loss: -0.46642929315567017\n",
      "Batch 379, Loss: -0.9056786298751831\n",
      "Batch 380, Loss: -1.1263420581817627\n",
      "Batch 381, Loss: -0.5046284794807434\n",
      "Batch 382, Loss: -0.2554323673248291\n",
      "Batch 383, Loss: 0.1484469175338745\n",
      "Batch 384, Loss: -0.23687011003494263\n",
      "Batch 385, Loss: -0.5605635643005371\n",
      "Batch 386, Loss: -0.10075938701629639\n",
      "Batch 387, Loss: -0.43883955478668213\n",
      "Batch 388, Loss: 0.10647821426391602\n",
      "Batch 389, Loss: -0.47991663217544556\n",
      "Batch 390, Loss: -0.1637270450592041\n",
      "Batch 391, Loss: -1.176568865776062\n",
      "Batch 392, Loss: -0.8771413564682007\n",
      "Batch 393, Loss: -0.31387555599212646\n",
      "Batch 394, Loss: -0.07552319765090942\n",
      "Batch 395, Loss: -0.2700978219509125\n",
      "Batch 396, Loss: -0.8249312043190002\n",
      "Batch 397, Loss: -0.8058995604515076\n",
      "Batch 398, Loss: -0.18279510736465454\n",
      "Batch 399, Loss: -0.8100566267967224\n",
      "Training [60%]\tLoss: -0.4969\n",
      "Batch 0, Loss: 0.2523074150085449\n",
      "Batch 1, Loss: -0.5143208503723145\n",
      "Batch 2, Loss: -0.25621241331100464\n",
      "Batch 3, Loss: -0.8360675573348999\n",
      "Batch 4, Loss: -0.011350631713867188\n",
      "Batch 5, Loss: -0.30289584398269653\n",
      "Batch 6, Loss: 0.3776165246963501\n",
      "Batch 7, Loss: -0.12402552366256714\n",
      "Batch 8, Loss: -1.0631463527679443\n",
      "Batch 9, Loss: -0.6206543445587158\n",
      "Batch 10, Loss: -0.34191107749938965\n",
      "Batch 11, Loss: 0.17460930347442627\n",
      "Batch 12, Loss: -0.14083611965179443\n",
      "Batch 13, Loss: -0.346150666475296\n",
      "Batch 14, Loss: -1.0027191638946533\n",
      "Batch 15, Loss: -0.886778712272644\n",
      "Batch 16, Loss: -0.19945096969604492\n",
      "Batch 17, Loss: -0.8089002966880798\n",
      "Batch 18, Loss: -0.16931438446044922\n",
      "Batch 19, Loss: -0.8643783926963806\n",
      "Batch 20, Loss: -0.6774514317512512\n",
      "Batch 21, Loss: 0.014833688735961914\n",
      "Batch 22, Loss: -0.08069616556167603\n",
      "Batch 23, Loss: -0.5918918251991272\n",
      "Batch 24, Loss: -0.3864711821079254\n",
      "Batch 25, Loss: -0.9875226616859436\n",
      "Batch 26, Loss: -1.2701420783996582\n",
      "Batch 27, Loss: -0.31662309169769287\n",
      "Batch 28, Loss: -0.8636892437934875\n",
      "Batch 29, Loss: -0.407656729221344\n",
      "Batch 30, Loss: -0.6963523626327515\n",
      "Batch 31, Loss: -0.7763788104057312\n",
      "Batch 32, Loss: -0.6625705361366272\n",
      "Batch 33, Loss: -0.5033038258552551\n",
      "Batch 34, Loss: -0.7770984768867493\n",
      "Batch 35, Loss: 0.026831507682800293\n",
      "Batch 36, Loss: -0.2978738844394684\n",
      "Batch 37, Loss: -0.45968806743621826\n",
      "Batch 38, Loss: -1.3569209575653076\n",
      "Batch 39, Loss: -1.3047912120819092\n",
      "Batch 40, Loss: -0.8532295823097229\n",
      "Batch 41, Loss: -0.4873592257499695\n",
      "Batch 42, Loss: -0.7802940011024475\n",
      "Batch 43, Loss: -1.0827133655548096\n",
      "Batch 44, Loss: -0.6980699300765991\n",
      "Batch 45, Loss: -0.10139745473861694\n",
      "Batch 46, Loss: -0.6002238392829895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 47, Loss: 0.000674903392791748\n",
      "Batch 48, Loss: -0.31249886751174927\n",
      "Batch 49, Loss: -0.06986922025680542\n",
      "Batch 50, Loss: -0.42862480878829956\n",
      "Batch 51, Loss: -0.9497308731079102\n",
      "Batch 52, Loss: -0.6874411106109619\n",
      "Batch 53, Loss: -0.26758062839508057\n",
      "Batch 54, Loss: -0.987114667892456\n",
      "Batch 55, Loss: -1.095263957977295\n",
      "Batch 56, Loss: -0.11610865592956543\n",
      "Batch 57, Loss: -0.5635011196136475\n",
      "Batch 58, Loss: -0.9284473657608032\n",
      "Batch 59, Loss: -0.7711907029151917\n",
      "Batch 60, Loss: -0.44882792234420776\n",
      "Batch 61, Loss: -0.47213149070739746\n",
      "Batch 62, Loss: 0.3130086660385132\n",
      "Batch 63, Loss: 0.01437997817993164\n",
      "Batch 64, Loss: -0.2158294916152954\n",
      "Batch 65, Loss: -0.24059617519378662\n",
      "Batch 66, Loss: -0.3360985219478607\n",
      "Batch 67, Loss: -0.6463869214057922\n",
      "Batch 68, Loss: -0.9558646082878113\n",
      "Batch 69, Loss: 0.1601734161376953\n",
      "Batch 70, Loss: -0.08958959579467773\n",
      "Batch 71, Loss: 0.15489602088928223\n",
      "Batch 72, Loss: -0.4650716781616211\n",
      "Batch 73, Loss: -0.3262271285057068\n",
      "Batch 74, Loss: -0.47926485538482666\n",
      "Batch 75, Loss: -0.1949787735939026\n",
      "Batch 76, Loss: -0.5400002002716064\n",
      "Batch 77, Loss: -0.19744151830673218\n",
      "Batch 78, Loss: -0.18837833404541016\n",
      "Batch 79, Loss: -0.31016868352890015\n",
      "Batch 80, Loss: -0.05981934070587158\n",
      "Batch 81, Loss: -1.358588695526123\n",
      "Batch 82, Loss: -0.2720247209072113\n",
      "Batch 83, Loss: -0.3534640073776245\n",
      "Batch 84, Loss: -0.36245793104171753\n",
      "Batch 85, Loss: -0.5189793705940247\n",
      "Batch 86, Loss: -0.8945224285125732\n",
      "Batch 87, Loss: -0.7934666872024536\n",
      "Batch 88, Loss: -0.1247747540473938\n",
      "Batch 89, Loss: -0.6229508519172668\n",
      "Batch 90, Loss: -0.5183295607566833\n",
      "Batch 91, Loss: -0.9592220783233643\n",
      "Batch 92, Loss: -0.9304022192955017\n",
      "Batch 93, Loss: -0.1276189088821411\n",
      "Batch 94, Loss: -1.0263667106628418\n",
      "Batch 95, Loss: -0.7095332741737366\n",
      "Batch 96, Loss: -0.7749709486961365\n",
      "Batch 97, Loss: -0.2145829200744629\n",
      "Batch 98, Loss: -0.3528784513473511\n",
      "Batch 99, Loss: -0.44888779520988464\n",
      "Batch 100, Loss: 0.059616684913635254\n",
      "Batch 101, Loss: -0.3830050230026245\n",
      "Batch 102, Loss: 0.3318619728088379\n",
      "Batch 103, Loss: -0.16143381595611572\n",
      "Batch 104, Loss: 0.19534027576446533\n",
      "Batch 105, Loss: -0.9472470283508301\n",
      "Batch 106, Loss: -0.5818620324134827\n",
      "Batch 107, Loss: -0.5479610562324524\n",
      "Batch 108, Loss: -0.8195157051086426\n",
      "Batch 109, Loss: -0.2045002579689026\n",
      "Batch 110, Loss: -0.7998171448707581\n",
      "Batch 111, Loss: -0.43101924657821655\n",
      "Batch 112, Loss: -0.6083170175552368\n",
      "Batch 113, Loss: -0.7729750275611877\n",
      "Batch 114, Loss: 0.2594578266143799\n",
      "Batch 115, Loss: -0.9414770603179932\n",
      "Batch 116, Loss: -0.07009530067443848\n",
      "Batch 117, Loss: -0.8861165046691895\n",
      "Batch 118, Loss: -0.1774827241897583\n",
      "Batch 119, Loss: -0.6308991312980652\n",
      "Batch 120, Loss: -0.6122134923934937\n",
      "Batch 121, Loss: -0.7617578506469727\n",
      "Batch 122, Loss: 0.0009427070617675781\n",
      "Batch 123, Loss: -0.7704734802246094\n",
      "Batch 124, Loss: -0.40504640340805054\n",
      "Batch 125, Loss: 0.05845367908477783\n",
      "Batch 126, Loss: -0.2781698703765869\n",
      "Batch 127, Loss: -0.9404550790786743\n",
      "Batch 128, Loss: -0.7270963191986084\n",
      "Batch 129, Loss: -1.0093348026275635\n",
      "Batch 130, Loss: -0.3687819540500641\n",
      "Batch 131, Loss: -1.1109470129013062\n",
      "Batch 132, Loss: -1.139379620552063\n",
      "Batch 133, Loss: -0.5230380296707153\n",
      "Batch 134, Loss: -0.279405415058136\n",
      "Batch 135, Loss: -0.4289993643760681\n",
      "Batch 136, Loss: -0.501537561416626\n",
      "Batch 137, Loss: -0.8796459436416626\n",
      "Batch 138, Loss: -1.359893798828125\n",
      "Batch 139, Loss: -1.2612760066986084\n",
      "Batch 140, Loss: -0.7546324729919434\n",
      "Batch 141, Loss: -0.673436164855957\n",
      "Batch 142, Loss: -0.2970849573612213\n",
      "Batch 143, Loss: -1.2196611166000366\n",
      "Batch 144, Loss: -1.207662582397461\n",
      "Batch 145, Loss: -0.9056774973869324\n",
      "Batch 146, Loss: 0.02924191951751709\n",
      "Batch 147, Loss: -0.8484379053115845\n",
      "Batch 148, Loss: -1.1628719568252563\n",
      "Batch 149, Loss: -0.6378720998764038\n",
      "Batch 150, Loss: -0.7413924932479858\n",
      "Batch 151, Loss: -0.13310956954956055\n",
      "Batch 152, Loss: -0.909542977809906\n",
      "Batch 153, Loss: 0.18019461631774902\n",
      "Batch 154, Loss: -0.29144179821014404\n",
      "Batch 155, Loss: -0.6974127888679504\n",
      "Batch 156, Loss: -0.20716649293899536\n",
      "Batch 157, Loss: -0.5129469633102417\n",
      "Batch 158, Loss: -1.3007240295410156\n",
      "Batch 159, Loss: -1.231148362159729\n",
      "Batch 160, Loss: -0.07774388790130615\n",
      "Batch 161, Loss: -0.6587720513343811\n",
      "Batch 162, Loss: -0.7013224363327026\n",
      "Batch 163, Loss: -0.8132716417312622\n",
      "Batch 164, Loss: -0.8196189403533936\n",
      "Batch 165, Loss: -0.3548635244369507\n",
      "Batch 166, Loss: 0.03146839141845703\n",
      "Batch 167, Loss: 0.2221759557723999\n",
      "Batch 168, Loss: -0.9063169956207275\n",
      "Batch 169, Loss: -0.3968455195426941\n",
      "Batch 170, Loss: -0.9520026445388794\n",
      "Batch 171, Loss: 0.4362525939941406\n",
      "Batch 172, Loss: -0.3048620820045471\n",
      "Batch 173, Loss: -0.06661003828048706\n",
      "Batch 174, Loss: -0.9974019527435303\n",
      "Batch 175, Loss: -0.902090311050415\n",
      "Batch 176, Loss: -0.7639638781547546\n",
      "Batch 177, Loss: -0.9704051613807678\n",
      "Batch 178, Loss: -0.7750037908554077\n",
      "Batch 179, Loss: -0.7685362100601196\n",
      "Batch 180, Loss: -0.02802366018295288\n",
      "Batch 181, Loss: -0.2643413543701172\n",
      "Batch 182, Loss: -0.09169334173202515\n",
      "Batch 183, Loss: -0.32274705171585083\n",
      "Batch 184, Loss: -0.518368124961853\n",
      "Batch 185, Loss: -0.9559499621391296\n",
      "Batch 186, Loss: -0.7359991669654846\n",
      "Batch 187, Loss: -0.5143115520477295\n",
      "Batch 188, Loss: -0.7706330418586731\n",
      "Batch 189, Loss: -0.22376155853271484\n",
      "Batch 190, Loss: 0.029033899307250977\n",
      "Batch 191, Loss: -0.9337037801742554\n",
      "Batch 192, Loss: -1.393202781677246\n",
      "Batch 193, Loss: -0.295835942029953\n",
      "Batch 194, Loss: -0.9054153561592102\n",
      "Batch 195, Loss: -0.760982096195221\n",
      "Batch 196, Loss: -0.7856838703155518\n",
      "Batch 197, Loss: 0.16312146186828613\n",
      "Batch 198, Loss: -0.6490529179573059\n",
      "Batch 199, Loss: -0.29009246826171875\n",
      "Batch 200, Loss: -0.0010129213333129883\n",
      "Batch 201, Loss: -0.8143061995506287\n",
      "Batch 202, Loss: 0.21215009689331055\n",
      "Batch 203, Loss: -0.8672641515731812\n",
      "Batch 204, Loss: -0.2667025327682495\n",
      "Batch 205, Loss: 0.23540282249450684\n",
      "Batch 206, Loss: -0.8788122534751892\n",
      "Batch 207, Loss: -0.2664499878883362\n",
      "Batch 208, Loss: -0.9676789045333862\n",
      "Batch 209, Loss: -1.0038883686065674\n",
      "Batch 210, Loss: -0.21102309226989746\n",
      "Batch 211, Loss: -0.07891732454299927\n",
      "Batch 212, Loss: -0.22645002603530884\n",
      "Batch 213, Loss: -0.8878350853919983\n",
      "Batch 214, Loss: -0.4692925214767456\n",
      "Batch 215, Loss: -0.0835452675819397\n",
      "Batch 216, Loss: -0.16757804155349731\n",
      "Batch 217, Loss: 0.13743257522583008\n",
      "Batch 218, Loss: -0.3230702877044678\n",
      "Batch 219, Loss: -0.16533887386322021\n",
      "Batch 220, Loss: 0.13249826431274414\n",
      "Batch 221, Loss: -0.14944738149642944\n",
      "Batch 222, Loss: -1.194864273071289\n",
      "Batch 223, Loss: 0.10508441925048828\n",
      "Batch 224, Loss: -0.26639124751091003\n",
      "Batch 225, Loss: 0.36920130252838135\n",
      "Batch 226, Loss: -0.8272324204444885\n",
      "Batch 227, Loss: -0.0887710452079773\n",
      "Batch 228, Loss: -0.9279964566230774\n",
      "Batch 229, Loss: -0.9031336307525635\n",
      "Batch 230, Loss: -1.1163160800933838\n",
      "Batch 231, Loss: -0.6615084409713745\n",
      "Batch 232, Loss: -0.2811391353607178\n",
      "Batch 233, Loss: -0.1391473412513733\n",
      "Batch 234, Loss: -1.0118381977081299\n",
      "Batch 235, Loss: -0.3843440115451813\n",
      "Batch 236, Loss: -0.7415847778320312\n",
      "Batch 237, Loss: -0.23306339979171753\n",
      "Batch 238, Loss: -0.3268231451511383\n",
      "Batch 239, Loss: -0.28286486864089966\n",
      "Batch 240, Loss: -0.8517673015594482\n",
      "Batch 241, Loss: -0.9600158333778381\n",
      "Batch 242, Loss: -0.3500519394874573\n",
      "Batch 243, Loss: -0.10454940795898438\n",
      "Batch 244, Loss: -1.2309010028839111\n",
      "Batch 245, Loss: -0.3182228207588196\n",
      "Batch 246, Loss: 0.32841789722442627\n",
      "Batch 247, Loss: -0.8850134611129761\n",
      "Batch 248, Loss: -0.6015017628669739\n",
      "Batch 249, Loss: -0.8973063230514526\n",
      "Batch 250, Loss: -0.4796716868877411\n",
      "Batch 251, Loss: -1.3092563152313232\n",
      "Batch 252, Loss: -0.6032422780990601\n",
      "Batch 253, Loss: -0.5300425291061401\n",
      "Batch 254, Loss: -0.6635919809341431\n",
      "Batch 255, Loss: -0.48550957441329956\n",
      "Batch 256, Loss: -0.9075220823287964\n",
      "Batch 257, Loss: -0.6141486167907715\n",
      "Batch 258, Loss: -0.36369824409484863\n",
      "Batch 259, Loss: -0.5511946678161621\n",
      "Batch 260, Loss: -0.17228680849075317\n",
      "Batch 261, Loss: -0.6222124695777893\n",
      "Batch 262, Loss: -0.7733049392700195\n",
      "Batch 263, Loss: -0.4951566457748413\n",
      "Batch 264, Loss: -0.6139976978302002\n",
      "Batch 265, Loss: -0.6097186207771301\n",
      "Batch 266, Loss: -0.636688232421875\n",
      "Batch 267, Loss: -0.2738063335418701\n",
      "Batch 268, Loss: 0.21903979778289795\n",
      "Batch 269, Loss: -0.6362574100494385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 270, Loss: -0.6253806948661804\n",
      "Batch 271, Loss: -0.3677899241447449\n",
      "Batch 272, Loss: 0.3797271251678467\n",
      "Batch 273, Loss: -1.2884275913238525\n",
      "Batch 274, Loss: 0.4014458656311035\n",
      "Batch 275, Loss: 0.27483344078063965\n",
      "Batch 276, Loss: -0.8514484763145447\n",
      "Batch 277, Loss: -0.4591240882873535\n",
      "Batch 278, Loss: -0.3367966115474701\n",
      "Batch 279, Loss: -0.1750500202178955\n",
      "Batch 280, Loss: -0.24513769149780273\n",
      "Batch 281, Loss: -0.37654340267181396\n",
      "Batch 282, Loss: -0.5960327386856079\n",
      "Batch 283, Loss: -0.8232004046440125\n",
      "Batch 284, Loss: 0.031022310256958008\n",
      "Batch 285, Loss: -0.36332112550735474\n",
      "Batch 286, Loss: 0.30631983280181885\n",
      "Batch 287, Loss: -0.11865711212158203\n",
      "Batch 288, Loss: -0.8840785026550293\n",
      "Batch 289, Loss: -0.16549444198608398\n",
      "Batch 290, Loss: 0.089272141456604\n",
      "Batch 291, Loss: -1.1502269506454468\n",
      "Batch 292, Loss: -0.20038896799087524\n",
      "Batch 293, Loss: -0.5051663517951965\n",
      "Batch 294, Loss: -0.8108303546905518\n",
      "Batch 295, Loss: -0.6722986102104187\n",
      "Batch 296, Loss: -0.7010613083839417\n",
      "Batch 297, Loss: -0.4403993487358093\n",
      "Batch 298, Loss: 0.12204623222351074\n",
      "Batch 299, Loss: -1.0306065082550049\n",
      "Batch 300, Loss: -1.811981201171875e-05\n",
      "Batch 301, Loss: -0.7487133145332336\n",
      "Batch 302, Loss: -0.2117396593093872\n",
      "Batch 303, Loss: -0.7239961624145508\n",
      "Batch 304, Loss: -0.058542609214782715\n",
      "Batch 305, Loss: -0.7691245079040527\n",
      "Batch 306, Loss: -0.9252715110778809\n",
      "Batch 307, Loss: -0.9725562930107117\n",
      "Batch 308, Loss: -0.7647467255592346\n",
      "Batch 309, Loss: -0.4519139230251312\n",
      "Batch 310, Loss: -0.05099332332611084\n",
      "Batch 311, Loss: -0.41668230295181274\n",
      "Batch 312, Loss: -0.8181515336036682\n",
      "Batch 313, Loss: 0.08072483539581299\n",
      "Batch 314, Loss: -0.724048376083374\n",
      "Batch 315, Loss: 0.14596855640411377\n",
      "Batch 316, Loss: -1.0091203451156616\n",
      "Batch 317, Loss: -0.5998247265815735\n",
      "Batch 318, Loss: 0.13135695457458496\n",
      "Batch 319, Loss: -0.5737448930740356\n",
      "Batch 320, Loss: -0.386516273021698\n",
      "Batch 321, Loss: -0.395349383354187\n",
      "Batch 322, Loss: -0.6375211477279663\n",
      "Batch 323, Loss: -0.7440135478973389\n",
      "Batch 324, Loss: -0.0830082893371582\n",
      "Batch 325, Loss: -0.6613494157791138\n",
      "Batch 326, Loss: -0.7846716046333313\n",
      "Batch 327, Loss: 0.35271644592285156\n",
      "Batch 328, Loss: -1.0762745141983032\n",
      "Batch 329, Loss: -0.44793862104415894\n",
      "Batch 330, Loss: -0.4268716871738434\n",
      "Batch 331, Loss: -0.19405150413513184\n",
      "Batch 332, Loss: -0.7825465202331543\n",
      "Batch 333, Loss: -0.8300288319587708\n",
      "Batch 334, Loss: -0.3272159695625305\n",
      "Batch 335, Loss: -0.1970738172531128\n",
      "Batch 336, Loss: -1.019369125366211\n",
      "Batch 337, Loss: -0.30277353525161743\n",
      "Batch 338, Loss: -0.3978537321090698\n",
      "Batch 339, Loss: -0.040892958641052246\n",
      "Batch 340, Loss: -0.9528882503509521\n",
      "Batch 341, Loss: -0.1585102081298828\n",
      "Batch 342, Loss: -0.15785306692123413\n",
      "Batch 343, Loss: -0.6174846291542053\n",
      "Batch 344, Loss: -0.10683953762054443\n",
      "Batch 345, Loss: -0.20373225212097168\n",
      "Batch 346, Loss: -0.2993485629558563\n",
      "Batch 347, Loss: -1.152153730392456\n",
      "Batch 348, Loss: -0.03936535120010376\n",
      "Batch 349, Loss: -0.7811896204948425\n",
      "Batch 350, Loss: -1.1795060634613037\n",
      "Batch 351, Loss: -0.7346123456954956\n",
      "Batch 352, Loss: -1.245935320854187\n",
      "Batch 353, Loss: -0.17245209217071533\n",
      "Batch 354, Loss: -0.345039963722229\n",
      "Batch 355, Loss: -0.1923958659172058\n",
      "Batch 356, Loss: -0.7697341442108154\n",
      "Batch 357, Loss: -0.5323822498321533\n",
      "Batch 358, Loss: -0.14970260858535767\n",
      "Batch 359, Loss: 0.1560811996459961\n",
      "Batch 360, Loss: -0.3579448461532593\n",
      "Batch 361, Loss: -0.5735719799995422\n",
      "Batch 362, Loss: -0.4392639398574829\n",
      "Batch 363, Loss: -0.8110710382461548\n",
      "Batch 364, Loss: -0.4480937719345093\n",
      "Batch 365, Loss: -1.256188988685608\n",
      "Batch 366, Loss: -0.7566434741020203\n",
      "Batch 367, Loss: -0.3335987627506256\n",
      "Batch 368, Loss: -1.152791142463684\n",
      "Batch 369, Loss: -0.2638815939426422\n",
      "Batch 370, Loss: -0.81678307056427\n",
      "Batch 371, Loss: -0.6812821626663208\n",
      "Batch 372, Loss: -0.8039937019348145\n",
      "Batch 373, Loss: -0.5919637680053711\n",
      "Batch 374, Loss: -0.02612239122390747\n",
      "Batch 375, Loss: -0.6267210245132446\n",
      "Batch 376, Loss: -0.6653649806976318\n",
      "Batch 377, Loss: -0.5978260040283203\n",
      "Batch 378, Loss: 0.08016335964202881\n",
      "Batch 379, Loss: -0.8694309592247009\n",
      "Batch 380, Loss: -0.42064833641052246\n",
      "Batch 381, Loss: 0.16195154190063477\n",
      "Batch 382, Loss: -0.6339886784553528\n",
      "Batch 383, Loss: -0.3073195815086365\n",
      "Batch 384, Loss: 0.15846145153045654\n",
      "Batch 385, Loss: -0.6231243014335632\n",
      "Batch 386, Loss: -0.7922024130821228\n",
      "Batch 387, Loss: -0.38353389501571655\n",
      "Batch 388, Loss: -1.3167200088500977\n",
      "Batch 389, Loss: -1.0146872997283936\n",
      "Batch 390, Loss: -0.357215940952301\n",
      "Batch 391, Loss: -0.8807114958763123\n",
      "Batch 392, Loss: -0.5388369560241699\n",
      "Batch 393, Loss: 0.08371388912200928\n",
      "Batch 394, Loss: -0.4892614185810089\n",
      "Batch 395, Loss: -0.46403729915618896\n",
      "Batch 396, Loss: -0.13764071464538574\n",
      "Batch 397, Loss: -0.801817774772644\n",
      "Batch 398, Loss: 0.13765192031860352\n",
      "Batch 399, Loss: 0.4556405544281006\n",
      "Training [70%]\tLoss: -0.4942\n",
      "Batch 0, Loss: -0.7498540282249451\n",
      "Batch 1, Loss: 0.16269254684448242\n",
      "Batch 2, Loss: 0.21234679222106934\n",
      "Batch 3, Loss: -0.2640248239040375\n",
      "Batch 4, Loss: -0.14855068922042847\n",
      "Batch 5, Loss: -0.6428975462913513\n",
      "Batch 6, Loss: -0.27951133251190186\n",
      "Batch 7, Loss: -0.5413074493408203\n",
      "Batch 8, Loss: -1.045544147491455\n",
      "Batch 9, Loss: -0.9037485122680664\n",
      "Batch 10, Loss: -0.4780747890472412\n",
      "Batch 11, Loss: -0.8727543950080872\n",
      "Batch 12, Loss: -0.21325761079788208\n",
      "Batch 13, Loss: -0.5499184131622314\n",
      "Batch 14, Loss: -0.1443766951560974\n",
      "Batch 15, Loss: -1.1060799360275269\n",
      "Batch 16, Loss: -0.7669300436973572\n",
      "Batch 17, Loss: -0.6827336549758911\n",
      "Batch 18, Loss: -0.09835833311080933\n",
      "Batch 19, Loss: -0.29982393980026245\n",
      "Batch 20, Loss: -0.3566001355648041\n",
      "Batch 21, Loss: 0.2648113965988159\n",
      "Batch 22, Loss: -0.6811376214027405\n",
      "Batch 23, Loss: -0.4760103225708008\n",
      "Batch 24, Loss: -1.1269912719726562\n",
      "Batch 25, Loss: -1.1781339645385742\n",
      "Batch 26, Loss: -1.3025062084197998\n",
      "Batch 27, Loss: -0.8005535006523132\n",
      "Batch 28, Loss: -0.41408634185791016\n",
      "Batch 29, Loss: -0.0481148362159729\n",
      "Batch 30, Loss: -0.17703688144683838\n",
      "Batch 31, Loss: -0.22632300853729248\n",
      "Batch 32, Loss: -0.3536977767944336\n",
      "Batch 33, Loss: -0.3027808666229248\n",
      "Batch 34, Loss: -0.4637320935726166\n",
      "Batch 35, Loss: -0.2265203595161438\n",
      "Batch 36, Loss: -0.7709195017814636\n",
      "Batch 37, Loss: -1.109749674797058\n",
      "Batch 38, Loss: -0.1790468692779541\n",
      "Batch 39, Loss: -0.42581260204315186\n",
      "Batch 40, Loss: -0.5113129615783691\n",
      "Batch 41, Loss: 0.3110489845275879\n",
      "Batch 42, Loss: -0.008481442928314209\n",
      "Batch 43, Loss: -0.011289596557617188\n",
      "Batch 44, Loss: -1.086675763130188\n",
      "Batch 45, Loss: -0.19280147552490234\n",
      "Batch 46, Loss: -0.581348180770874\n",
      "Batch 47, Loss: -0.5626693964004517\n",
      "Batch 48, Loss: -0.35458797216415405\n",
      "Batch 49, Loss: -0.38143450021743774\n",
      "Batch 50, Loss: -0.7290917038917542\n",
      "Batch 51, Loss: -0.7853675484657288\n",
      "Batch 52, Loss: -0.7100390791893005\n",
      "Batch 53, Loss: 0.33852481842041016\n",
      "Batch 54, Loss: -0.27140524983406067\n",
      "Batch 55, Loss: -0.3510148227214813\n",
      "Batch 56, Loss: -0.41397231817245483\n",
      "Batch 57, Loss: -0.32176098227500916\n",
      "Batch 58, Loss: -0.4129711091518402\n",
      "Batch 59, Loss: -1.102979302406311\n",
      "Batch 60, Loss: -0.5968047380447388\n",
      "Batch 61, Loss: -0.8344864845275879\n",
      "Batch 62, Loss: -0.7628937363624573\n",
      "Batch 63, Loss: -0.08456188440322876\n",
      "Batch 64, Loss: -0.3996662497520447\n",
      "Batch 65, Loss: -0.5310800075531006\n",
      "Batch 66, Loss: 0.36797356605529785\n",
      "Batch 67, Loss: -0.25730645656585693\n",
      "Batch 68, Loss: -0.3973590135574341\n",
      "Batch 69, Loss: -0.14583176374435425\n",
      "Batch 70, Loss: 0.20154500007629395\n",
      "Batch 71, Loss: -0.4516814947128296\n",
      "Batch 72, Loss: -1.0161703824996948\n",
      "Batch 73, Loss: -0.3971189856529236\n",
      "Batch 74, Loss: -1.3600382804870605\n",
      "Batch 75, Loss: 0.3638497591018677\n",
      "Batch 76, Loss: -0.034727394580841064\n",
      "Batch 77, Loss: -0.82913738489151\n",
      "Batch 78, Loss: -0.6813478469848633\n",
      "Batch 79, Loss: -0.6252353191375732\n",
      "Batch 80, Loss: -0.1606752872467041\n",
      "Batch 81, Loss: -0.31494635343551636\n",
      "Batch 82, Loss: -0.859894871711731\n",
      "Batch 83, Loss: -0.2636205554008484\n",
      "Batch 84, Loss: -0.19373559951782227\n",
      "Batch 85, Loss: -0.31420594453811646\n",
      "Batch 86, Loss: -0.7433011531829834\n",
      "Batch 87, Loss: 0.4741652011871338\n",
      "Batch 88, Loss: -0.2631232440471649\n",
      "Batch 89, Loss: -0.21882712841033936\n",
      "Batch 90, Loss: 0.1759321689605713\n",
      "Batch 91, Loss: -0.5514479279518127\n",
      "Batch 92, Loss: -0.8122803568840027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 93, Loss: -0.506847620010376\n",
      "Batch 94, Loss: -0.23798155784606934\n",
      "Batch 95, Loss: -0.7974259853363037\n",
      "Batch 96, Loss: 0.17964410781860352\n",
      "Batch 97, Loss: -0.3636305332183838\n",
      "Batch 98, Loss: -0.2907767593860626\n",
      "Batch 99, Loss: -0.6882563829421997\n",
      "Batch 100, Loss: -0.14350074529647827\n",
      "Batch 101, Loss: -0.0404740571975708\n",
      "Batch 102, Loss: -0.38219302892684937\n",
      "Batch 103, Loss: -0.24523979425430298\n",
      "Batch 104, Loss: -0.9052023887634277\n",
      "Batch 105, Loss: -0.7165948152542114\n",
      "Batch 106, Loss: -0.299083948135376\n",
      "Batch 107, Loss: -0.32308945059776306\n",
      "Batch 108, Loss: -0.24721509218215942\n",
      "Batch 109, Loss: 0.1492081880569458\n",
      "Batch 110, Loss: -0.02273029088973999\n",
      "Batch 111, Loss: -0.7175132632255554\n",
      "Batch 112, Loss: -0.4442329406738281\n",
      "Batch 113, Loss: -0.5619258880615234\n",
      "Batch 114, Loss: -1.2344295978546143\n",
      "Batch 115, Loss: -1.351054310798645\n",
      "Batch 116, Loss: -0.16265666484832764\n",
      "Batch 117, Loss: -0.6970184445381165\n",
      "Batch 118, Loss: -0.9082903265953064\n",
      "Batch 119, Loss: -0.3872988820075989\n",
      "Batch 120, Loss: -0.5196568369865417\n",
      "Batch 121, Loss: -0.3152717351913452\n",
      "Batch 122, Loss: 0.3097158670425415\n",
      "Batch 123, Loss: -0.6161351203918457\n",
      "Batch 124, Loss: 0.09253537654876709\n",
      "Batch 125, Loss: -0.07122129201889038\n",
      "Batch 126, Loss: -0.46677160263061523\n",
      "Batch 127, Loss: -0.27752530574798584\n",
      "Batch 128, Loss: -0.29531121253967285\n",
      "Batch 129, Loss: -0.5117757320404053\n",
      "Batch 130, Loss: -0.3496495485305786\n",
      "Batch 131, Loss: -0.16482901573181152\n",
      "Batch 132, Loss: -1.218639612197876\n",
      "Batch 133, Loss: -0.5170474052429199\n",
      "Batch 134, Loss: -0.264743447303772\n",
      "Batch 135, Loss: -1.3506295680999756\n",
      "Batch 136, Loss: -0.8674505352973938\n",
      "Batch 137, Loss: -0.9014903903007507\n",
      "Batch 138, Loss: -0.3983435034751892\n",
      "Batch 139, Loss: -0.9020400047302246\n",
      "Batch 140, Loss: -0.692088782787323\n",
      "Batch 141, Loss: -0.5500960350036621\n",
      "Batch 142, Loss: -0.7339946031570435\n",
      "Batch 143, Loss: -0.9866326451301575\n",
      "Batch 144, Loss: -0.8227638006210327\n",
      "Batch 145, Loss: -0.23511260747909546\n",
      "Batch 146, Loss: -0.5180537700653076\n",
      "Batch 147, Loss: -0.838086724281311\n",
      "Batch 148, Loss: -0.6344826221466064\n",
      "Batch 149, Loss: -0.5444245934486389\n",
      "Batch 150, Loss: -0.925865888595581\n",
      "Batch 151, Loss: -1.0705138444900513\n",
      "Batch 152, Loss: -0.8123246431350708\n",
      "Batch 153, Loss: -0.38820287585258484\n",
      "Batch 154, Loss: -0.33028852939605713\n",
      "Batch 155, Loss: -0.8234878778457642\n",
      "Batch 156, Loss: -0.5575453639030457\n",
      "Batch 157, Loss: -0.7540376782417297\n",
      "Batch 158, Loss: -0.252916157245636\n",
      "Batch 159, Loss: -0.11019855737686157\n",
      "Batch 160, Loss: -0.6147834062576294\n",
      "Batch 161, Loss: -0.08291471004486084\n",
      "Batch 162, Loss: -0.6724657416343689\n",
      "Batch 163, Loss: -0.1490488052368164\n",
      "Batch 164, Loss: -0.4361695349216461\n",
      "Batch 165, Loss: -0.6153355836868286\n",
      "Batch 166, Loss: -0.6053681969642639\n",
      "Batch 167, Loss: -0.721055805683136\n",
      "Batch 168, Loss: -0.4065963923931122\n",
      "Batch 169, Loss: -1.1496928930282593\n",
      "Batch 170, Loss: -0.839103639125824\n",
      "Batch 171, Loss: -0.14445436000823975\n",
      "Batch 172, Loss: -0.717261552810669\n",
      "Batch 173, Loss: -0.6322253942489624\n",
      "Batch 174, Loss: -1.3194985389709473\n",
      "Batch 175, Loss: -0.5327243804931641\n",
      "Batch 176, Loss: 0.10721886157989502\n",
      "Batch 177, Loss: -0.3113965690135956\n",
      "Batch 178, Loss: -0.18315738439559937\n",
      "Batch 179, Loss: -0.8179283738136292\n",
      "Batch 180, Loss: -1.192123293876648\n",
      "Batch 181, Loss: -0.002450704574584961\n",
      "Batch 182, Loss: -1.0159021615982056\n",
      "Batch 183, Loss: -0.9213773012161255\n",
      "Batch 184, Loss: -0.9826675653457642\n",
      "Batch 185, Loss: -0.8848221302032471\n",
      "Batch 186, Loss: -0.3872034251689911\n",
      "Batch 187, Loss: 0.35410547256469727\n",
      "Batch 188, Loss: -0.25311124324798584\n",
      "Batch 189, Loss: -0.6451326012611389\n",
      "Batch 190, Loss: -0.5213654041290283\n",
      "Batch 191, Loss: -0.6266918182373047\n",
      "Batch 192, Loss: -0.5575134754180908\n",
      "Batch 193, Loss: -1.2377707958221436\n",
      "Batch 194, Loss: -0.7340587973594666\n",
      "Batch 195, Loss: -1.0202229022979736\n",
      "Batch 196, Loss: -1.0304745435714722\n",
      "Batch 197, Loss: -0.27479827404022217\n",
      "Batch 198, Loss: -0.02616983652114868\n",
      "Batch 199, Loss: 0.33029699325561523\n",
      "Batch 200, Loss: -0.3759957551956177\n",
      "Batch 201, Loss: -0.17874836921691895\n",
      "Batch 202, Loss: -0.7119523286819458\n",
      "Batch 203, Loss: -0.6559564471244812\n",
      "Batch 204, Loss: -0.34486550092697144\n",
      "Batch 205, Loss: -0.7459786534309387\n",
      "Batch 206, Loss: -0.868323028087616\n",
      "Batch 207, Loss: -0.4395865201950073\n",
      "Batch 208, Loss: -0.689583420753479\n",
      "Batch 209, Loss: -0.5033795237541199\n",
      "Batch 210, Loss: -0.8222227096557617\n",
      "Batch 211, Loss: -0.11619633436203003\n",
      "Batch 212, Loss: -0.9609036445617676\n",
      "Batch 213, Loss: -0.47672170400619507\n",
      "Batch 214, Loss: -0.45934486389160156\n",
      "Batch 215, Loss: -1.2097046375274658\n",
      "Batch 216, Loss: -0.6123973727226257\n",
      "Batch 217, Loss: 0.26798200607299805\n",
      "Batch 218, Loss: -0.7991917729377747\n",
      "Batch 219, Loss: -0.4477677643299103\n",
      "Batch 220, Loss: -0.2416977882385254\n",
      "Batch 221, Loss: 0.13133156299591064\n",
      "Batch 222, Loss: -0.43755990266799927\n",
      "Batch 223, Loss: -0.7907223105430603\n",
      "Batch 224, Loss: -0.06533551216125488\n",
      "Batch 225, Loss: -0.7568255662918091\n",
      "Batch 226, Loss: 0.059299349784851074\n",
      "Batch 227, Loss: -0.3305329978466034\n",
      "Batch 228, Loss: -1.1799299716949463\n",
      "Batch 229, Loss: -0.0889623761177063\n",
      "Batch 230, Loss: -0.15500080585479736\n",
      "Batch 231, Loss: -0.18732255697250366\n",
      "Batch 232, Loss: -0.25407466292381287\n",
      "Batch 233, Loss: 0.02062821388244629\n",
      "Batch 234, Loss: -1.1323739290237427\n",
      "Batch 235, Loss: -0.2004360556602478\n",
      "Batch 236, Loss: -0.4742942154407501\n",
      "Batch 237, Loss: -0.6551681756973267\n",
      "Batch 238, Loss: 0.3801589012145996\n",
      "Batch 239, Loss: -0.6540816426277161\n",
      "Batch 240, Loss: -0.0780940055847168\n",
      "Batch 241, Loss: -1.1948095560073853\n",
      "Batch 242, Loss: -0.3915195167064667\n",
      "Batch 243, Loss: -1.2800328731536865\n",
      "Batch 244, Loss: -0.6890925168991089\n",
      "Batch 245, Loss: -1.0616170167922974\n",
      "Batch 246, Loss: -0.3475646674633026\n",
      "Batch 247, Loss: -0.6515964865684509\n",
      "Batch 248, Loss: -0.5233013033866882\n",
      "Batch 249, Loss: -0.7354608774185181\n",
      "Batch 250, Loss: -0.8408843278884888\n",
      "Batch 251, Loss: -0.1331455111503601\n",
      "Batch 252, Loss: -0.6572120785713196\n",
      "Batch 253, Loss: -0.08217054605484009\n",
      "Batch 254, Loss: -0.5395956039428711\n",
      "Batch 255, Loss: -0.11702138185501099\n",
      "Batch 256, Loss: -0.42269253730773926\n",
      "Batch 257, Loss: 0.010596275329589844\n",
      "Batch 258, Loss: -0.7973129153251648\n",
      "Batch 259, Loss: -0.8803024291992188\n",
      "Batch 260, Loss: -0.6795501708984375\n",
      "Batch 261, Loss: -0.2908363938331604\n",
      "Batch 262, Loss: 0.022339701652526855\n",
      "Batch 263, Loss: -0.8209472894668579\n",
      "Batch 264, Loss: -1.2621667385101318\n",
      "Batch 265, Loss: -0.7253785729408264\n",
      "Batch 266, Loss: -0.4832022786140442\n",
      "Batch 267, Loss: -0.9046733379364014\n",
      "Batch 268, Loss: -0.6236714124679565\n",
      "Batch 269, Loss: -0.7647224068641663\n",
      "Batch 270, Loss: -0.225577712059021\n",
      "Batch 271, Loss: 0.20698845386505127\n",
      "Batch 272, Loss: -0.4245741367340088\n",
      "Batch 273, Loss: -0.12788838148117065\n",
      "Batch 274, Loss: 0.019284963607788086\n",
      "Batch 275, Loss: -0.36483055353164673\n",
      "Batch 276, Loss: -0.5326673984527588\n",
      "Batch 277, Loss: -0.5519336462020874\n",
      "Batch 278, Loss: -0.761871337890625\n",
      "Batch 279, Loss: -0.2598077952861786\n",
      "Batch 280, Loss: -0.5311233997344971\n",
      "Batch 281, Loss: -0.31300127506256104\n",
      "Batch 282, Loss: -0.23604530096054077\n",
      "Batch 283, Loss: -0.6878533363342285\n",
      "Batch 284, Loss: -0.017980575561523438\n",
      "Batch 285, Loss: -0.4016742706298828\n",
      "Batch 286, Loss: -0.6430628895759583\n",
      "Batch 287, Loss: -0.3357236683368683\n",
      "Batch 288, Loss: -0.1420077681541443\n",
      "Batch 289, Loss: -0.7176743745803833\n",
      "Batch 290, Loss: -0.28356486558914185\n",
      "Batch 291, Loss: -0.7522870898246765\n",
      "Batch 292, Loss: -0.3239426612854004\n",
      "Batch 293, Loss: -0.4067288935184479\n",
      "Batch 294, Loss: -0.9081148505210876\n",
      "Batch 295, Loss: -0.3793674409389496\n",
      "Batch 296, Loss: -0.7375898361206055\n",
      "Batch 297, Loss: -0.29656943678855896\n",
      "Batch 298, Loss: 0.04616689682006836\n",
      "Batch 299, Loss: -0.35470446944236755\n",
      "Batch 300, Loss: -0.22917360067367554\n",
      "Batch 301, Loss: -0.16407722234725952\n",
      "Batch 302, Loss: -0.8144787549972534\n",
      "Batch 303, Loss: -0.5876516699790955\n",
      "Batch 304, Loss: -0.13840031623840332\n",
      "Batch 305, Loss: -0.4951823651790619\n",
      "Batch 306, Loss: -0.8043017387390137\n",
      "Batch 307, Loss: -0.604853630065918\n",
      "Batch 308, Loss: -0.0792345404624939\n",
      "Batch 309, Loss: -0.6847530603408813\n",
      "Batch 310, Loss: -0.8980503082275391\n",
      "Batch 311, Loss: -0.4606897532939911\n",
      "Batch 312, Loss: -0.6890994906425476\n",
      "Batch 313, Loss: -0.8933161497116089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 314, Loss: -0.8187737464904785\n",
      "Batch 315, Loss: -0.5589514970779419\n",
      "Batch 316, Loss: -0.4356577396392822\n",
      "Batch 317, Loss: -0.11302101612091064\n",
      "Batch 318, Loss: -0.3189268410205841\n",
      "Batch 319, Loss: -0.41210055351257324\n",
      "Batch 320, Loss: -0.6619864702224731\n",
      "Batch 321, Loss: -0.4278406500816345\n",
      "Batch 322, Loss: -1.2665138244628906\n",
      "Batch 323, Loss: -0.34267228841781616\n",
      "Batch 324, Loss: -0.32840123772621155\n",
      "Batch 325, Loss: -0.8433419466018677\n",
      "Batch 326, Loss: -0.6641770601272583\n",
      "Batch 327, Loss: -0.4268907904624939\n",
      "Batch 328, Loss: -0.5983422994613647\n",
      "Batch 329, Loss: -0.7373669147491455\n",
      "Batch 330, Loss: -1.1907968521118164\n",
      "Batch 331, Loss: -0.4912377595901489\n",
      "Batch 332, Loss: -0.07780903577804565\n",
      "Batch 333, Loss: -0.4855378270149231\n",
      "Batch 334, Loss: -1.205087423324585\n",
      "Batch 335, Loss: -0.33505886793136597\n",
      "Batch 336, Loss: -0.34437811374664307\n",
      "Batch 337, Loss: -0.3222377896308899\n",
      "Batch 338, Loss: -0.5455036163330078\n",
      "Batch 339, Loss: -1.2177281379699707\n",
      "Batch 340, Loss: -0.20943212509155273\n",
      "Batch 341, Loss: -0.7210609912872314\n",
      "Batch 342, Loss: -0.30168843269348145\n",
      "Batch 343, Loss: -1.1074029207229614\n",
      "Batch 344, Loss: -0.7166118025779724\n",
      "Batch 345, Loss: -0.7825246453285217\n",
      "Batch 346, Loss: -0.03337550163269043\n",
      "Batch 347, Loss: -0.18144714832305908\n",
      "Batch 348, Loss: -0.5711485147476196\n",
      "Batch 349, Loss: -1.2640819549560547\n",
      "Batch 350, Loss: 0.04561424255371094\n",
      "Batch 351, Loss: -0.660388708114624\n",
      "Batch 352, Loss: -1.093521237373352\n",
      "Batch 353, Loss: -0.21735072135925293\n",
      "Batch 354, Loss: -0.6655309796333313\n",
      "Batch 355, Loss: -0.917785108089447\n",
      "Batch 356, Loss: -0.20280498266220093\n",
      "Batch 357, Loss: -0.20104318857192993\n",
      "Batch 358, Loss: -0.9398611783981323\n",
      "Batch 359, Loss: -0.3187381327152252\n",
      "Batch 360, Loss: -0.35001662373542786\n",
      "Batch 361, Loss: -0.29529404640197754\n",
      "Batch 362, Loss: -0.2981730103492737\n",
      "Batch 363, Loss: -0.681948184967041\n",
      "Batch 364, Loss: -0.40806329250335693\n",
      "Batch 365, Loss: -0.8419709205627441\n",
      "Batch 366, Loss: -0.50958651304245\n",
      "Batch 367, Loss: -0.3670812249183655\n",
      "Batch 368, Loss: -0.77435702085495\n",
      "Batch 369, Loss: -0.624488115310669\n",
      "Batch 370, Loss: -1.2615983486175537\n",
      "Batch 371, Loss: -0.5120323896408081\n",
      "Batch 372, Loss: -0.27516043186187744\n",
      "Batch 373, Loss: 0.3566868305206299\n",
      "Batch 374, Loss: -0.5766189098358154\n",
      "Batch 375, Loss: -0.7473176717758179\n",
      "Batch 376, Loss: -0.8661374449729919\n",
      "Batch 377, Loss: -0.874595046043396\n",
      "Batch 378, Loss: -0.04403311014175415\n",
      "Batch 379, Loss: -0.8957535624504089\n",
      "Batch 380, Loss: -0.38523465394973755\n",
      "Batch 381, Loss: 0.22157955169677734\n",
      "Batch 382, Loss: -0.216522216796875\n",
      "Batch 383, Loss: -0.4723842740058899\n",
      "Batch 384, Loss: -0.578373908996582\n",
      "Batch 385, Loss: -0.9653084874153137\n",
      "Batch 386, Loss: -0.8993769884109497\n",
      "Batch 387, Loss: -0.26484936475753784\n",
      "Batch 388, Loss: -0.5799382925033569\n",
      "Batch 389, Loss: -0.4181249141693115\n",
      "Batch 390, Loss: 0.116965651512146\n",
      "Batch 391, Loss: 0.1275477409362793\n",
      "Batch 392, Loss: -0.6693747639656067\n",
      "Batch 393, Loss: -0.3646879196166992\n",
      "Batch 394, Loss: -0.36147117614746094\n",
      "Batch 395, Loss: -0.9112548828125\n",
      "Batch 396, Loss: 0.3306770324707031\n",
      "Batch 397, Loss: -0.6298480033874512\n",
      "Batch 398, Loss: -0.8297542929649353\n",
      "Batch 399, Loss: -0.2733802795410156\n",
      "Training [80%]\tLoss: -0.4941\n",
      "Batch 0, Loss: -0.6190118193626404\n",
      "Batch 1, Loss: -0.9473587274551392\n",
      "Batch 2, Loss: -0.6492687463760376\n",
      "Batch 3, Loss: -0.7044445872306824\n",
      "Batch 4, Loss: -0.6595677733421326\n",
      "Batch 5, Loss: 0.22175991535186768\n",
      "Batch 6, Loss: 0.21906566619873047\n",
      "Batch 7, Loss: -1.3533004522323608\n",
      "Batch 8, Loss: -0.5741499662399292\n",
      "Batch 9, Loss: -0.6449699401855469\n",
      "Batch 10, Loss: -0.6467106938362122\n",
      "Batch 11, Loss: -0.7271116971969604\n",
      "Batch 12, Loss: -0.31557291746139526\n",
      "Batch 13, Loss: -0.5237829089164734\n",
      "Batch 14, Loss: -0.5731024742126465\n",
      "Batch 15, Loss: -0.5515806674957275\n",
      "Batch 16, Loss: -0.6559593677520752\n",
      "Batch 17, Loss: -0.4254548251628876\n",
      "Batch 18, Loss: -1.2499333620071411\n",
      "Batch 19, Loss: -0.7288067936897278\n",
      "Batch 20, Loss: -1.4345054626464844\n",
      "Batch 21, Loss: -0.39870744943618774\n",
      "Batch 22, Loss: -0.06866389513015747\n",
      "Batch 23, Loss: -0.1289212703704834\n",
      "Batch 24, Loss: -1.1638474464416504\n",
      "Batch 25, Loss: 0.17466974258422852\n",
      "Batch 26, Loss: -0.2005177140235901\n",
      "Batch 27, Loss: -0.5269904136657715\n",
      "Batch 28, Loss: -0.6855119466781616\n",
      "Batch 29, Loss: -0.040733397006988525\n",
      "Batch 30, Loss: -0.3003307580947876\n",
      "Batch 31, Loss: -0.6235909461975098\n",
      "Batch 32, Loss: -0.5124824047088623\n",
      "Batch 33, Loss: -0.3161851763725281\n",
      "Batch 34, Loss: -0.8374406695365906\n",
      "Batch 35, Loss: -0.814145565032959\n",
      "Batch 36, Loss: -0.39265725016593933\n",
      "Batch 37, Loss: -0.901838481426239\n",
      "Batch 38, Loss: -0.4123210906982422\n",
      "Batch 39, Loss: -1.0617187023162842\n",
      "Batch 40, Loss: -0.5372075438499451\n",
      "Batch 41, Loss: -0.339017778635025\n",
      "Batch 42, Loss: -0.27626869082450867\n",
      "Batch 43, Loss: -0.3818434178829193\n",
      "Batch 44, Loss: -0.5602875351905823\n",
      "Batch 45, Loss: -0.41992390155792236\n",
      "Batch 46, Loss: -0.8301841616630554\n",
      "Batch 47, Loss: -0.289884090423584\n",
      "Batch 48, Loss: -0.8043966293334961\n",
      "Batch 49, Loss: -1.2556742429733276\n",
      "Batch 50, Loss: -0.1790289282798767\n",
      "Batch 51, Loss: -0.4664878845214844\n",
      "Batch 52, Loss: -0.4094547629356384\n",
      "Batch 53, Loss: -0.12633317708969116\n",
      "Batch 54, Loss: -0.06472480297088623\n",
      "Batch 55, Loss: -0.8701016902923584\n",
      "Batch 56, Loss: -0.4262324571609497\n",
      "Batch 57, Loss: -0.25117015838623047\n",
      "Batch 58, Loss: 0.30103135108947754\n",
      "Batch 59, Loss: -0.01508420705795288\n",
      "Batch 60, Loss: -0.13947325944900513\n",
      "Batch 61, Loss: -0.9800342917442322\n",
      "Batch 62, Loss: -0.36127859354019165\n",
      "Batch 63, Loss: -0.3842889666557312\n",
      "Batch 64, Loss: -0.8136948347091675\n",
      "Batch 65, Loss: -0.7931607365608215\n",
      "Batch 66, Loss: -0.48214346170425415\n",
      "Batch 67, Loss: -0.8621965050697327\n",
      "Batch 68, Loss: -0.6148116588592529\n",
      "Batch 69, Loss: 0.09270370006561279\n",
      "Batch 70, Loss: -0.5979547500610352\n",
      "Batch 71, Loss: -0.8567899465560913\n",
      "Batch 72, Loss: -0.19294685125350952\n",
      "Batch 73, Loss: -1.1737040281295776\n",
      "Batch 74, Loss: -0.24344462156295776\n",
      "Batch 75, Loss: -0.6028248071670532\n",
      "Batch 76, Loss: -1.0260494947433472\n",
      "Batch 77, Loss: -0.2272222638130188\n",
      "Batch 78, Loss: -0.4183046817779541\n",
      "Batch 79, Loss: 0.07644379138946533\n",
      "Batch 80, Loss: -0.3929087519645691\n",
      "Batch 81, Loss: -0.21164095401763916\n",
      "Batch 82, Loss: -0.44225507974624634\n",
      "Batch 83, Loss: -0.6200504302978516\n",
      "Batch 84, Loss: -0.47814929485321045\n",
      "Batch 85, Loss: 0.15595626831054688\n",
      "Batch 86, Loss: -1.2342183589935303\n",
      "Batch 87, Loss: -0.6730844378471375\n",
      "Batch 88, Loss: -0.3375887870788574\n",
      "Batch 89, Loss: -1.4653551578521729\n",
      "Batch 90, Loss: -0.3258419632911682\n",
      "Batch 91, Loss: -0.4164199233055115\n",
      "Batch 92, Loss: -0.35027480125427246\n",
      "Batch 93, Loss: -0.3715909421443939\n",
      "Batch 94, Loss: -0.4318356513977051\n",
      "Batch 95, Loss: -1.1141289472579956\n",
      "Batch 96, Loss: -0.30670347809791565\n",
      "Batch 97, Loss: -0.47055983543395996\n",
      "Batch 98, Loss: -0.7857221961021423\n",
      "Batch 99, Loss: -0.6195526123046875\n",
      "Batch 100, Loss: -0.7592800259590149\n",
      "Batch 101, Loss: -0.47642481327056885\n",
      "Batch 102, Loss: -0.9517611265182495\n",
      "Batch 103, Loss: -0.5160258412361145\n",
      "Batch 104, Loss: -0.5866348743438721\n",
      "Batch 105, Loss: -1.2787673473358154\n",
      "Batch 106, Loss: -0.6079832315444946\n",
      "Batch 107, Loss: 0.2778369188308716\n",
      "Batch 108, Loss: -1.0250446796417236\n",
      "Batch 109, Loss: -0.7357856631278992\n",
      "Batch 110, Loss: -0.47211045026779175\n",
      "Batch 111, Loss: -0.48724204301834106\n",
      "Batch 112, Loss: -0.5118668079376221\n",
      "Batch 113, Loss: -0.5022832751274109\n",
      "Batch 114, Loss: -0.3641446828842163\n",
      "Batch 115, Loss: -0.49829912185668945\n",
      "Batch 116, Loss: -0.5078848004341125\n",
      "Batch 117, Loss: -0.7677083015441895\n",
      "Batch 118, Loss: -0.7352544069290161\n",
      "Batch 119, Loss: -0.2957821190357208\n",
      "Batch 120, Loss: 0.35188305377960205\n",
      "Batch 121, Loss: -0.07604986429214478\n",
      "Batch 122, Loss: -0.4837450385093689\n",
      "Batch 123, Loss: -0.5083035826683044\n",
      "Batch 124, Loss: -0.5630615949630737\n",
      "Batch 125, Loss: -0.7203959822654724\n",
      "Batch 126, Loss: -0.20839589834213257\n",
      "Batch 127, Loss: 0.04635453224182129\n",
      "Batch 128, Loss: -0.6103143692016602\n",
      "Batch 129, Loss: -0.10640990734100342\n",
      "Batch 130, Loss: 0.4317464828491211\n",
      "Batch 131, Loss: -0.4225390553474426\n",
      "Batch 132, Loss: -0.1336517333984375\n",
      "Batch 133, Loss: -0.5719481706619263\n",
      "Batch 134, Loss: -0.1326998472213745\n",
      "Batch 135, Loss: -0.32253530621528625\n",
      "Batch 136, Loss: -0.6493443250656128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 137, Loss: -0.5779246091842651\n",
      "Batch 138, Loss: -0.6642156839370728\n",
      "Batch 139, Loss: -0.2136385440826416\n",
      "Batch 140, Loss: -0.7957953810691833\n",
      "Batch 141, Loss: 0.1826632022857666\n",
      "Batch 142, Loss: -0.6672546863555908\n",
      "Batch 143, Loss: -0.3549420237541199\n",
      "Batch 144, Loss: -0.7753753066062927\n",
      "Batch 145, Loss: -0.5508596897125244\n",
      "Batch 146, Loss: -0.4873734712600708\n",
      "Batch 147, Loss: -1.333399772644043\n",
      "Batch 148, Loss: 0.08077263832092285\n",
      "Batch 149, Loss: -0.9566566944122314\n",
      "Batch 150, Loss: 0.05910658836364746\n",
      "Batch 151, Loss: -0.08519905805587769\n",
      "Batch 152, Loss: -0.24774497747421265\n",
      "Batch 153, Loss: -0.7203917503356934\n",
      "Batch 154, Loss: -0.20665597915649414\n",
      "Batch 155, Loss: -0.5075389742851257\n",
      "Batch 156, Loss: -0.26023662090301514\n",
      "Batch 157, Loss: -0.7752406001091003\n",
      "Batch 158, Loss: -0.7395074367523193\n",
      "Batch 159, Loss: 0.1894223690032959\n",
      "Batch 160, Loss: -1.0114593505859375\n",
      "Batch 161, Loss: -0.161288321018219\n",
      "Batch 162, Loss: -0.45460695028305054\n",
      "Batch 163, Loss: -0.05035156011581421\n",
      "Batch 164, Loss: -0.6391690373420715\n",
      "Batch 165, Loss: -0.538697361946106\n",
      "Batch 166, Loss: -1.1511204242706299\n",
      "Batch 167, Loss: -0.904558539390564\n",
      "Batch 168, Loss: -1.0018205642700195\n",
      "Batch 169, Loss: -0.31058552861213684\n",
      "Batch 170, Loss: -0.7206214070320129\n",
      "Batch 171, Loss: -0.4663163423538208\n",
      "Batch 172, Loss: -0.5338867902755737\n",
      "Batch 173, Loss: -1.0667946338653564\n",
      "Batch 174, Loss: -0.7882281541824341\n",
      "Batch 175, Loss: -0.7087185978889465\n",
      "Batch 176, Loss: -0.6510779857635498\n",
      "Batch 177, Loss: -0.40077897906303406\n",
      "Batch 178, Loss: -0.5966320037841797\n",
      "Batch 179, Loss: -0.0222359299659729\n",
      "Batch 180, Loss: -0.6051970720291138\n",
      "Batch 181, Loss: 0.160241961479187\n",
      "Batch 182, Loss: -0.3937607407569885\n",
      "Batch 183, Loss: 0.09900069236755371\n",
      "Batch 184, Loss: 0.012207508087158203\n",
      "Batch 185, Loss: -0.6734868884086609\n",
      "Batch 186, Loss: -1.3227648735046387\n",
      "Batch 187, Loss: -0.48865175247192383\n",
      "Batch 188, Loss: -0.8664135336875916\n",
      "Batch 189, Loss: -1.0252429246902466\n",
      "Batch 190, Loss: 0.04579925537109375\n",
      "Batch 191, Loss: -0.08968406915664673\n",
      "Batch 192, Loss: -0.8445823192596436\n",
      "Batch 193, Loss: -0.886713445186615\n",
      "Batch 194, Loss: -0.2090889811515808\n",
      "Batch 195, Loss: -0.3274843990802765\n",
      "Batch 196, Loss: -0.947479248046875\n",
      "Batch 197, Loss: 0.2889382839202881\n",
      "Batch 198, Loss: -0.9724338054656982\n",
      "Batch 199, Loss: -0.13122916221618652\n",
      "Batch 200, Loss: -0.6052679419517517\n",
      "Batch 201, Loss: -0.7577376961708069\n",
      "Batch 202, Loss: -0.24918416142463684\n",
      "Batch 203, Loss: -1.230575680732727\n",
      "Batch 204, Loss: -0.4762569069862366\n",
      "Batch 205, Loss: -0.2014501690864563\n",
      "Batch 206, Loss: -0.6397462487220764\n",
      "Batch 207, Loss: -0.11837685108184814\n",
      "Batch 208, Loss: 0.02580249309539795\n",
      "Batch 209, Loss: -0.2018072009086609\n",
      "Batch 210, Loss: -0.156053364276886\n",
      "Batch 211, Loss: -0.15895837545394897\n",
      "Batch 212, Loss: -0.26345860958099365\n",
      "Batch 213, Loss: -0.3998141288757324\n",
      "Batch 214, Loss: 0.1261991262435913\n",
      "Batch 215, Loss: -0.22046178579330444\n",
      "Batch 216, Loss: -0.5963916182518005\n",
      "Batch 217, Loss: -0.2592085301876068\n",
      "Batch 218, Loss: -0.3911972641944885\n",
      "Batch 219, Loss: -0.36056965589523315\n",
      "Batch 220, Loss: 0.21616113185882568\n",
      "Batch 221, Loss: -0.5342565774917603\n",
      "Batch 222, Loss: -0.7879701852798462\n",
      "Batch 223, Loss: -0.957267165184021\n",
      "Batch 224, Loss: -0.4803019165992737\n",
      "Batch 225, Loss: 0.29147791862487793\n",
      "Batch 226, Loss: -1.2761194705963135\n",
      "Batch 227, Loss: -0.5126120448112488\n",
      "Batch 228, Loss: 0.09903383255004883\n",
      "Batch 229, Loss: -0.024727702140808105\n",
      "Batch 230, Loss: -0.5408604145050049\n",
      "Batch 231, Loss: -0.660498857498169\n",
      "Batch 232, Loss: -0.19929075241088867\n",
      "Batch 233, Loss: -0.24519026279449463\n",
      "Batch 234, Loss: -0.22844195365905762\n",
      "Batch 235, Loss: -0.4766029119491577\n",
      "Batch 236, Loss: 0.19885718822479248\n",
      "Batch 237, Loss: 0.013180971145629883\n",
      "Batch 238, Loss: -0.8555399179458618\n",
      "Batch 239, Loss: -0.8440147638320923\n",
      "Batch 240, Loss: -0.7093163728713989\n",
      "Batch 241, Loss: -0.46363621950149536\n",
      "Batch 242, Loss: -0.7103118896484375\n",
      "Batch 243, Loss: -1.236748456954956\n",
      "Batch 244, Loss: -0.36730438470840454\n",
      "Batch 245, Loss: -0.7202997803688049\n",
      "Batch 246, Loss: -0.6909471750259399\n",
      "Batch 247, Loss: -1.0493239164352417\n",
      "Batch 248, Loss: -0.6406363844871521\n",
      "Batch 249, Loss: -0.6259177923202515\n",
      "Batch 250, Loss: -0.05126744508743286\n",
      "Batch 251, Loss: -0.43516623973846436\n",
      "Batch 252, Loss: -0.4867496192455292\n",
      "Batch 253, Loss: -0.7391188144683838\n",
      "Batch 254, Loss: -0.5193515419960022\n",
      "Batch 255, Loss: -0.8401138186454773\n",
      "Batch 256, Loss: -0.18876028060913086\n",
      "Batch 257, Loss: -0.8536366820335388\n",
      "Batch 258, Loss: -0.5382729768753052\n",
      "Batch 259, Loss: -0.578811526298523\n",
      "Batch 260, Loss: -0.42904114723205566\n",
      "Batch 261, Loss: -0.811335563659668\n",
      "Batch 262, Loss: -0.309905469417572\n",
      "Batch 263, Loss: -0.3053985834121704\n",
      "Batch 264, Loss: -0.22414523363113403\n",
      "Batch 265, Loss: -0.06111502647399902\n",
      "Batch 266, Loss: -0.04920005798339844\n",
      "Batch 267, Loss: -0.7863366007804871\n",
      "Batch 268, Loss: -0.5864123106002808\n",
      "Batch 269, Loss: -0.0485420823097229\n",
      "Batch 270, Loss: 0.25711822509765625\n",
      "Batch 271, Loss: -0.6140681505203247\n",
      "Batch 272, Loss: -0.041660428047180176\n",
      "Batch 273, Loss: -0.4680517315864563\n",
      "Batch 274, Loss: 0.010812044143676758\n",
      "Batch 275, Loss: -0.6251100301742554\n",
      "Batch 276, Loss: -0.35628384351730347\n",
      "Batch 277, Loss: -0.7812805771827698\n",
      "Batch 278, Loss: -1.0813578367233276\n",
      "Batch 279, Loss: -0.1905289888381958\n",
      "Batch 280, Loss: -0.09139925241470337\n",
      "Batch 281, Loss: -0.7537432909011841\n",
      "Batch 282, Loss: -1.3099384307861328\n",
      "Batch 283, Loss: -0.8370637893676758\n",
      "Batch 284, Loss: -0.39859580993652344\n",
      "Batch 285, Loss: -1.0112309455871582\n",
      "Batch 286, Loss: -0.8582149744033813\n",
      "Batch 287, Loss: -0.06313216686248779\n",
      "Batch 288, Loss: -0.6858450174331665\n",
      "Batch 289, Loss: 0.051876068115234375\n",
      "Batch 290, Loss: -1.0744514465332031\n",
      "Batch 291, Loss: -0.496535986661911\n",
      "Batch 292, Loss: -0.6802744269371033\n",
      "Batch 293, Loss: -0.2727265954017639\n",
      "Batch 294, Loss: -0.9669040441513062\n",
      "Batch 295, Loss: -0.8249353766441345\n",
      "Batch 296, Loss: -0.35390087962150574\n",
      "Batch 297, Loss: -0.27755624055862427\n",
      "Batch 298, Loss: 0.2595093250274658\n",
      "Batch 299, Loss: -0.30665555596351624\n",
      "Batch 300, Loss: -0.5541077852249146\n",
      "Batch 301, Loss: -0.4280436635017395\n",
      "Batch 302, Loss: -0.7168951034545898\n",
      "Batch 303, Loss: -0.5096185207366943\n",
      "Batch 304, Loss: 0.07102000713348389\n",
      "Batch 305, Loss: -0.37501463294029236\n",
      "Batch 306, Loss: 0.0031665563583374023\n",
      "Batch 307, Loss: 0.14113259315490723\n",
      "Batch 308, Loss: -0.16473960876464844\n",
      "Batch 309, Loss: -0.6896827220916748\n",
      "Batch 310, Loss: -0.30856871604919434\n",
      "Batch 311, Loss: -1.0014896392822266\n",
      "Batch 312, Loss: -0.7286909222602844\n",
      "Batch 313, Loss: 0.20606231689453125\n",
      "Batch 314, Loss: -0.6889682412147522\n",
      "Batch 315, Loss: -0.906104564666748\n",
      "Batch 316, Loss: -0.3900976777076721\n",
      "Batch 317, Loss: -0.6881164908409119\n",
      "Batch 318, Loss: -0.6890010833740234\n",
      "Batch 319, Loss: -0.2409551739692688\n",
      "Batch 320, Loss: -0.30710673332214355\n",
      "Batch 321, Loss: -0.4062653183937073\n",
      "Batch 322, Loss: -0.235551118850708\n",
      "Batch 323, Loss: -0.7945637702941895\n",
      "Batch 324, Loss: -0.5405101776123047\n",
      "Batch 325, Loss: -0.6757837533950806\n",
      "Batch 326, Loss: -0.6441999673843384\n",
      "Batch 327, Loss: -0.44396835565567017\n",
      "Batch 328, Loss: -1.2550328969955444\n",
      "Batch 329, Loss: -0.3532160520553589\n",
      "Batch 330, Loss: -0.5870004892349243\n",
      "Batch 331, Loss: -0.9326581954956055\n",
      "Batch 332, Loss: -0.12501859664916992\n",
      "Batch 333, Loss: -0.369495689868927\n",
      "Batch 334, Loss: -0.5850964188575745\n",
      "Batch 335, Loss: -0.39151236414909363\n",
      "Batch 336, Loss: -0.685757577419281\n",
      "Batch 337, Loss: -0.7865039110183716\n",
      "Batch 338, Loss: -0.19625800848007202\n",
      "Batch 339, Loss: -0.6737166047096252\n",
      "Batch 340, Loss: -0.6986557245254517\n",
      "Batch 341, Loss: -0.1957879662513733\n",
      "Batch 342, Loss: -0.3173564672470093\n",
      "Batch 343, Loss: -0.44994571805000305\n",
      "Batch 344, Loss: -0.5234549641609192\n",
      "Batch 345, Loss: -0.3232553005218506\n",
      "Batch 346, Loss: -0.6101819276809692\n",
      "Batch 347, Loss: -0.8385921716690063\n",
      "Batch 348, Loss: -0.33303260803222656\n",
      "Batch 349, Loss: -0.7453703880310059\n",
      "Batch 350, Loss: -0.34303393959999084\n",
      "Batch 351, Loss: -0.6390825510025024\n",
      "Batch 352, Loss: 0.0797797441482544\n",
      "Batch 353, Loss: -0.376955509185791\n",
      "Batch 354, Loss: -0.1201639175415039\n",
      "Batch 355, Loss: -0.3431006073951721\n",
      "Batch 356, Loss: -0.7014933824539185\n",
      "Batch 357, Loss: -0.7256390452384949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 358, Loss: 0.030514955520629883\n",
      "Batch 359, Loss: -0.4819899797439575\n",
      "Batch 360, Loss: -0.7431691884994507\n",
      "Batch 361, Loss: 0.14418697357177734\n",
      "Batch 362, Loss: -0.35655808448791504\n",
      "Batch 363, Loss: -0.44312310218811035\n",
      "Batch 364, Loss: -0.6762717366218567\n",
      "Batch 365, Loss: -1.3969616889953613\n",
      "Batch 366, Loss: -0.09285134077072144\n",
      "Batch 367, Loss: -0.5472038388252258\n",
      "Batch 368, Loss: -0.3662158250808716\n",
      "Batch 369, Loss: -0.24662071466445923\n",
      "Batch 370, Loss: -0.8111624717712402\n",
      "Batch 371, Loss: -0.6680872440338135\n",
      "Batch 372, Loss: -1.1901302337646484\n",
      "Batch 373, Loss: -0.25159788131713867\n",
      "Batch 374, Loss: -0.11684924364089966\n",
      "Batch 375, Loss: -0.3900182247161865\n",
      "Batch 376, Loss: -0.5652875900268555\n",
      "Batch 377, Loss: -0.6034820079803467\n",
      "Batch 378, Loss: -0.7559584379196167\n",
      "Batch 379, Loss: -0.8678379654884338\n",
      "Batch 380, Loss: -0.46319031715393066\n",
      "Batch 381, Loss: -0.6046678423881531\n",
      "Batch 382, Loss: -0.9018915891647339\n",
      "Batch 383, Loss: -0.11589193344116211\n",
      "Batch 384, Loss: -0.8512827754020691\n",
      "Batch 385, Loss: -0.7718916535377502\n",
      "Batch 386, Loss: -0.5386285185813904\n",
      "Batch 387, Loss: -0.1986827254295349\n",
      "Batch 388, Loss: -0.5323187112808228\n",
      "Batch 389, Loss: -0.39144110679626465\n",
      "Batch 390, Loss: -0.9296239614486694\n",
      "Batch 391, Loss: -0.7079354524612427\n",
      "Batch 392, Loss: -0.44478824734687805\n",
      "Batch 393, Loss: -0.5067752599716187\n",
      "Batch 394, Loss: -0.23569035530090332\n",
      "Batch 395, Loss: -1.3998974561691284\n",
      "Batch 396, Loss: -0.28622981905937195\n",
      "Batch 397, Loss: -0.23192870616912842\n",
      "Batch 398, Loss: -0.4461787939071655\n",
      "Batch 399, Loss: -0.5616478323936462\n",
      "Training [90%]\tLoss: -0.4923\n",
      "Batch 0, Loss: -0.09116291999816895\n",
      "Batch 1, Loss: -0.1893019676208496\n",
      "Batch 2, Loss: -0.7022815942764282\n",
      "Batch 3, Loss: -0.08082574605941772\n",
      "Batch 4, Loss: -0.6278349161148071\n",
      "Batch 5, Loss: -0.7466410398483276\n",
      "Batch 6, Loss: -0.3622511029243469\n",
      "Batch 7, Loss: -0.607185423374176\n",
      "Batch 8, Loss: -0.6413927674293518\n",
      "Batch 9, Loss: 0.03780567646026611\n",
      "Batch 10, Loss: -0.3882138729095459\n",
      "Batch 11, Loss: -0.6397783756256104\n",
      "Batch 12, Loss: -0.1871352195739746\n",
      "Batch 13, Loss: -0.8914409875869751\n",
      "Batch 14, Loss: 0.20880866050720215\n",
      "Batch 15, Loss: -0.3385509252548218\n",
      "Batch 16, Loss: -0.2090470790863037\n",
      "Batch 17, Loss: -1.1462293863296509\n",
      "Batch 18, Loss: -0.3080422878265381\n",
      "Batch 19, Loss: -0.21417587995529175\n",
      "Batch 20, Loss: -0.8182026743888855\n",
      "Batch 21, Loss: 0.17432284355163574\n",
      "Batch 22, Loss: -0.12223643064498901\n",
      "Batch 23, Loss: -0.6587156653404236\n",
      "Batch 24, Loss: -0.6439734697341919\n",
      "Batch 25, Loss: 0.1207810640335083\n",
      "Batch 26, Loss: -0.7720986604690552\n",
      "Batch 27, Loss: -0.5545043349266052\n",
      "Batch 28, Loss: -0.5503203272819519\n",
      "Batch 29, Loss: -0.5448054075241089\n",
      "Batch 30, Loss: -0.5913461446762085\n",
      "Batch 31, Loss: 0.10255491733551025\n",
      "Batch 32, Loss: -0.0046634674072265625\n",
      "Batch 33, Loss: -0.1831362247467041\n",
      "Batch 34, Loss: -0.0626179575920105\n",
      "Batch 35, Loss: -0.37469759583473206\n",
      "Batch 36, Loss: -0.20097970962524414\n",
      "Batch 37, Loss: -1.15611732006073\n",
      "Batch 38, Loss: -0.8805471062660217\n",
      "Batch 39, Loss: -0.2841593325138092\n",
      "Batch 40, Loss: -0.7067872881889343\n",
      "Batch 41, Loss: -0.626682698726654\n",
      "Batch 42, Loss: -0.5652181506156921\n",
      "Batch 43, Loss: -0.3732897639274597\n",
      "Batch 44, Loss: -1.3659793138504028\n",
      "Batch 45, Loss: -0.6908658146858215\n",
      "Batch 46, Loss: -0.24403095245361328\n",
      "Batch 47, Loss: -1.2418729066848755\n",
      "Batch 48, Loss: -1.0873682498931885\n",
      "Batch 49, Loss: -0.4163074493408203\n",
      "Batch 50, Loss: 0.237815260887146\n",
      "Batch 51, Loss: -0.6348162889480591\n",
      "Batch 52, Loss: -0.6988776326179504\n",
      "Batch 53, Loss: -0.6247393488883972\n",
      "Batch 54, Loss: -0.7746390700340271\n",
      "Batch 55, Loss: -0.7585945129394531\n",
      "Batch 56, Loss: -0.31770068407058716\n",
      "Batch 57, Loss: -0.4053027629852295\n",
      "Batch 58, Loss: -0.47043025493621826\n",
      "Batch 59, Loss: -0.31482797861099243\n",
      "Batch 60, Loss: -1.359398603439331\n",
      "Batch 61, Loss: -0.5940964221954346\n",
      "Batch 62, Loss: -0.7120809555053711\n",
      "Batch 63, Loss: -0.7661924362182617\n",
      "Batch 64, Loss: -1.325558066368103\n",
      "Batch 65, Loss: -0.350556880235672\n",
      "Batch 66, Loss: -1.0343250036239624\n",
      "Batch 67, Loss: -0.7710580229759216\n",
      "Batch 68, Loss: -0.45879924297332764\n",
      "Batch 69, Loss: -0.4412229657173157\n",
      "Batch 70, Loss: -0.7817486524581909\n",
      "Batch 71, Loss: -0.15670466423034668\n",
      "Batch 72, Loss: -0.49078845977783203\n",
      "Batch 73, Loss: -0.40840911865234375\n",
      "Batch 74, Loss: -0.7160176038742065\n",
      "Batch 75, Loss: -0.11661028861999512\n",
      "Batch 76, Loss: -0.15853756666183472\n",
      "Batch 77, Loss: -0.8594293594360352\n",
      "Batch 78, Loss: -0.8206503391265869\n",
      "Batch 79, Loss: -1.2370892763137817\n",
      "Batch 80, Loss: -0.3748133182525635\n",
      "Batch 81, Loss: -0.057788848876953125\n",
      "Batch 82, Loss: -0.042193472385406494\n",
      "Batch 83, Loss: -0.02260303497314453\n",
      "Batch 84, Loss: -0.800540566444397\n",
      "Batch 85, Loss: -0.7474042773246765\n",
      "Batch 86, Loss: -0.8435958027839661\n",
      "Batch 87, Loss: -0.9018609523773193\n",
      "Batch 88, Loss: -0.4154391884803772\n",
      "Batch 89, Loss: -0.7987534999847412\n",
      "Batch 90, Loss: -0.8848699927330017\n",
      "Batch 91, Loss: 0.30210864543914795\n",
      "Batch 92, Loss: -0.2906426787376404\n",
      "Batch 93, Loss: -0.41453826427459717\n",
      "Batch 94, Loss: -0.15867608785629272\n",
      "Batch 95, Loss: -1.298832654953003\n",
      "Batch 96, Loss: -0.6479030847549438\n",
      "Batch 97, Loss: -0.36846989393234253\n",
      "Batch 98, Loss: -0.7192482948303223\n",
      "Batch 99, Loss: 0.07644534111022949\n",
      "Batch 100, Loss: -0.2053600549697876\n",
      "Batch 101, Loss: -0.48329412937164307\n",
      "Batch 102, Loss: -0.3403685688972473\n",
      "Batch 103, Loss: -0.22847044467926025\n",
      "Batch 104, Loss: -0.3285076320171356\n",
      "Batch 105, Loss: -1.0355989933013916\n",
      "Batch 106, Loss: -0.2920357584953308\n",
      "Batch 107, Loss: -0.6822693943977356\n",
      "Batch 108, Loss: -0.5635780692100525\n",
      "Batch 109, Loss: -0.31690096855163574\n",
      "Batch 110, Loss: -0.724696695804596\n",
      "Batch 111, Loss: -0.036537766456604004\n",
      "Batch 112, Loss: -0.8598243594169617\n",
      "Batch 113, Loss: -0.12047672271728516\n",
      "Batch 114, Loss: -0.5504980683326721\n",
      "Batch 115, Loss: -0.14828968048095703\n",
      "Batch 116, Loss: 0.19731402397155762\n",
      "Batch 117, Loss: -0.7623424530029297\n",
      "Batch 118, Loss: -0.6803746819496155\n",
      "Batch 119, Loss: -0.7851860523223877\n",
      "Batch 120, Loss: -0.833919107913971\n",
      "Batch 121, Loss: -0.5954412221908569\n",
      "Batch 122, Loss: -0.4307096302509308\n",
      "Batch 123, Loss: -0.6155052185058594\n",
      "Batch 124, Loss: -0.28721731901168823\n",
      "Batch 125, Loss: 0.1490098237991333\n",
      "Batch 126, Loss: -0.7385076284408569\n",
      "Batch 127, Loss: -1.0779584646224976\n",
      "Batch 128, Loss: -0.4676545262336731\n",
      "Batch 129, Loss: -0.3078894019126892\n",
      "Batch 130, Loss: -0.1618911623954773\n",
      "Batch 131, Loss: -0.6578173041343689\n",
      "Batch 132, Loss: -0.7306498885154724\n",
      "Batch 133, Loss: -1.2569470405578613\n",
      "Batch 134, Loss: -0.7588086724281311\n",
      "Batch 135, Loss: -0.8512853980064392\n",
      "Batch 136, Loss: -0.6033341884613037\n",
      "Batch 137, Loss: -0.8008960485458374\n",
      "Batch 138, Loss: -0.327559232711792\n",
      "Batch 139, Loss: -0.0252799391746521\n",
      "Batch 140, Loss: -0.6658151149749756\n",
      "Batch 141, Loss: -0.33629411458969116\n",
      "Batch 142, Loss: 0.3134326934814453\n",
      "Batch 143, Loss: -0.22408390045166016\n",
      "Batch 144, Loss: -0.8755650520324707\n",
      "Batch 145, Loss: -1.0723085403442383\n",
      "Batch 146, Loss: -1.2177146673202515\n",
      "Batch 147, Loss: -0.7662943005561829\n",
      "Batch 148, Loss: -0.4564746022224426\n",
      "Batch 149, Loss: -0.1870148777961731\n",
      "Batch 150, Loss: -0.4567677974700928\n",
      "Batch 151, Loss: -0.26843973994255066\n",
      "Batch 152, Loss: -1.131356120109558\n",
      "Batch 153, Loss: -1.2902686595916748\n",
      "Batch 154, Loss: -0.20321333408355713\n",
      "Batch 155, Loss: -0.3497798442840576\n",
      "Batch 156, Loss: -0.5800541639328003\n",
      "Batch 157, Loss: -0.7529075145721436\n",
      "Batch 158, Loss: -0.6452670097351074\n",
      "Batch 159, Loss: -0.37283673882484436\n",
      "Batch 160, Loss: -0.5132521390914917\n",
      "Batch 161, Loss: 0.03661906719207764\n",
      "Batch 162, Loss: -0.909697949886322\n",
      "Batch 163, Loss: -0.21875041723251343\n",
      "Batch 164, Loss: -0.21774804592132568\n",
      "Batch 165, Loss: -0.23043233156204224\n",
      "Batch 166, Loss: -1.1281335353851318\n",
      "Batch 167, Loss: -0.42888379096984863\n",
      "Batch 168, Loss: 0.04437291622161865\n",
      "Batch 169, Loss: -0.5597684383392334\n",
      "Batch 170, Loss: -1.1985142230987549\n",
      "Batch 171, Loss: -0.644225537776947\n",
      "Batch 172, Loss: -1.01082444190979\n",
      "Batch 173, Loss: -0.24259859323501587\n",
      "Batch 174, Loss: -0.525903582572937\n",
      "Batch 175, Loss: -0.1402692198753357\n",
      "Batch 176, Loss: -0.6737929582595825\n",
      "Batch 177, Loss: -1.1710205078125\n",
      "Batch 178, Loss: -0.39794230461120605\n",
      "Batch 179, Loss: -0.19771045446395874\n",
      "Batch 180, Loss: -0.4450594186782837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 181, Loss: -0.78113853931427\n",
      "Batch 182, Loss: -0.8382176756858826\n",
      "Batch 183, Loss: -0.4297272861003876\n",
      "Batch 184, Loss: -0.7377245426177979\n",
      "Batch 185, Loss: -0.5472952723503113\n",
      "Batch 186, Loss: -0.7310705184936523\n",
      "Batch 187, Loss: -0.8972862958908081\n",
      "Batch 188, Loss: -0.14547419548034668\n",
      "Batch 189, Loss: -0.6140860915184021\n",
      "Batch 190, Loss: -1.4418995380401611\n",
      "Batch 191, Loss: -0.6211787462234497\n",
      "Batch 192, Loss: -0.5785268545150757\n",
      "Batch 193, Loss: -0.8656067252159119\n",
      "Batch 194, Loss: -0.5912436842918396\n",
      "Batch 195, Loss: -0.2824699580669403\n",
      "Batch 196, Loss: -0.6813477277755737\n",
      "Batch 197, Loss: 0.02273392677307129\n",
      "Batch 198, Loss: -0.6425901651382446\n",
      "Batch 199, Loss: -0.48676466941833496\n",
      "Batch 200, Loss: -0.25502508878707886\n",
      "Batch 201, Loss: -1.2068603038787842\n",
      "Batch 202, Loss: -0.5216131806373596\n",
      "Batch 203, Loss: -0.3066217601299286\n",
      "Batch 204, Loss: 0.2721381187438965\n",
      "Batch 205, Loss: -0.01753091812133789\n",
      "Batch 206, Loss: -0.11270076036453247\n",
      "Batch 207, Loss: -0.8606237769126892\n",
      "Batch 208, Loss: -0.26799315214157104\n",
      "Batch 209, Loss: 0.30986344814300537\n",
      "Batch 210, Loss: -0.44412171840667725\n",
      "Batch 211, Loss: -1.0385009050369263\n",
      "Batch 212, Loss: -0.6900368928909302\n",
      "Batch 213, Loss: 0.16180062294006348\n",
      "Batch 214, Loss: -0.8509645462036133\n",
      "Batch 215, Loss: -0.23015648126602173\n",
      "Batch 216, Loss: -0.6134533882141113\n",
      "Batch 217, Loss: -0.7089073657989502\n",
      "Batch 218, Loss: -0.4775642454624176\n",
      "Batch 219, Loss: -0.9951416254043579\n",
      "Batch 220, Loss: -0.6767339706420898\n",
      "Batch 221, Loss: -0.09440892934799194\n",
      "Batch 222, Loss: -0.38811609148979187\n",
      "Batch 223, Loss: -0.732829749584198\n",
      "Batch 224, Loss: -0.5455033779144287\n",
      "Batch 225, Loss: 0.1390913724899292\n",
      "Batch 226, Loss: 0.1501622200012207\n",
      "Batch 227, Loss: -0.985508918762207\n",
      "Batch 228, Loss: -0.5464497208595276\n",
      "Batch 229, Loss: -0.4445926547050476\n",
      "Batch 230, Loss: -0.3204547166824341\n",
      "Batch 231, Loss: -0.15008217096328735\n",
      "Batch 232, Loss: -0.6752864122390747\n",
      "Batch 233, Loss: -0.47363975644111633\n",
      "Batch 234, Loss: 0.25358033180236816\n",
      "Batch 235, Loss: -1.4210143089294434\n",
      "Batch 236, Loss: -0.7388693690299988\n",
      "Batch 237, Loss: -0.5921055674552917\n",
      "Batch 238, Loss: -0.3339881896972656\n",
      "Batch 239, Loss: -0.396630197763443\n",
      "Batch 240, Loss: -0.15369707345962524\n",
      "Batch 241, Loss: -0.3574731945991516\n",
      "Batch 242, Loss: -0.8240618705749512\n",
      "Batch 243, Loss: -0.14041095972061157\n",
      "Batch 244, Loss: -0.04953348636627197\n",
      "Batch 245, Loss: -0.39384424686431885\n",
      "Batch 246, Loss: -0.2719283401966095\n",
      "Batch 247, Loss: -0.21936935186386108\n",
      "Batch 248, Loss: -0.6579289436340332\n",
      "Batch 249, Loss: -0.4700901508331299\n",
      "Batch 250, Loss: -0.3663915991783142\n",
      "Batch 251, Loss: -1.1911511421203613\n",
      "Batch 252, Loss: -0.8673960566520691\n",
      "Batch 253, Loss: -0.5228530764579773\n",
      "Batch 254, Loss: -0.3748319149017334\n",
      "Batch 255, Loss: -0.6412765979766846\n",
      "Batch 256, Loss: -0.4295964241027832\n",
      "Batch 257, Loss: -0.5443074703216553\n",
      "Batch 258, Loss: -0.67714524269104\n",
      "Batch 259, Loss: -0.37419503927230835\n",
      "Batch 260, Loss: -0.7195616364479065\n",
      "Batch 261, Loss: -0.8014379739761353\n",
      "Batch 262, Loss: -0.8792597651481628\n",
      "Batch 263, Loss: 0.01740729808807373\n",
      "Batch 264, Loss: -0.8489358425140381\n",
      "Batch 265, Loss: -0.8947716951370239\n",
      "Batch 266, Loss: -0.9904881119728088\n",
      "Batch 267, Loss: -0.4189378619194031\n",
      "Batch 268, Loss: -0.15494459867477417\n",
      "Batch 269, Loss: -0.6587969064712524\n",
      "Batch 270, Loss: -0.4022639989852905\n",
      "Batch 271, Loss: -0.21971625089645386\n",
      "Batch 272, Loss: -0.42779481410980225\n",
      "Batch 273, Loss: -0.9348121285438538\n",
      "Batch 274, Loss: -0.7248110771179199\n",
      "Batch 275, Loss: -0.35638830065727234\n",
      "Batch 276, Loss: 0.01128387451171875\n",
      "Batch 277, Loss: -0.30587291717529297\n",
      "Batch 278, Loss: -0.809826672077179\n",
      "Batch 279, Loss: -0.45525693893432617\n",
      "Batch 280, Loss: -0.21250677108764648\n",
      "Batch 281, Loss: -0.378448486328125\n",
      "Batch 282, Loss: -0.3327662944793701\n",
      "Batch 283, Loss: -1.1645153760910034\n",
      "Batch 284, Loss: -0.8022162318229675\n",
      "Batch 285, Loss: -0.3939642906188965\n",
      "Batch 286, Loss: -0.3269599378108978\n",
      "Batch 287, Loss: -1.1935839653015137\n",
      "Batch 288, Loss: -0.4781653583049774\n",
      "Batch 289, Loss: -0.865136444568634\n",
      "Batch 290, Loss: -0.18224704265594482\n",
      "Batch 291, Loss: -0.62654048204422\n",
      "Batch 292, Loss: -0.7760278582572937\n",
      "Batch 293, Loss: -1.2377840280532837\n",
      "Batch 294, Loss: -0.2166963815689087\n",
      "Batch 295, Loss: -0.5441644191741943\n",
      "Batch 296, Loss: -0.8523585200309753\n",
      "Batch 297, Loss: -0.9261813163757324\n",
      "Batch 298, Loss: -0.43649452924728394\n",
      "Batch 299, Loss: -0.4711436629295349\n",
      "Batch 300, Loss: -0.5366537570953369\n",
      "Batch 301, Loss: -0.4954182803630829\n",
      "Batch 302, Loss: -0.4642646908760071\n",
      "Batch 303, Loss: -0.39112356305122375\n",
      "Batch 304, Loss: -0.032312870025634766\n",
      "Batch 305, Loss: -0.030045688152313232\n",
      "Batch 306, Loss: -0.7962946891784668\n",
      "Batch 307, Loss: -0.43686234951019287\n",
      "Batch 308, Loss: -0.8834457397460938\n",
      "Batch 309, Loss: 0.09487152099609375\n",
      "Batch 310, Loss: -0.8094889521598816\n",
      "Batch 311, Loss: -1.0749694108963013\n",
      "Batch 312, Loss: -0.43981581926345825\n",
      "Batch 313, Loss: -0.8273531794548035\n",
      "Batch 314, Loss: -0.31795382499694824\n",
      "Batch 315, Loss: -0.7230477333068848\n",
      "Batch 316, Loss: -0.352547824382782\n",
      "Batch 317, Loss: -0.5040117502212524\n",
      "Batch 318, Loss: -0.8639386296272278\n",
      "Batch 319, Loss: -0.4348280727863312\n",
      "Batch 320, Loss: -0.09898656606674194\n",
      "Batch 321, Loss: -0.771718442440033\n",
      "Batch 322, Loss: -0.8030767440795898\n",
      "Batch 323, Loss: -0.4614478349685669\n",
      "Batch 324, Loss: -0.15941119194030762\n",
      "Batch 325, Loss: -0.6401374936103821\n",
      "Batch 326, Loss: -0.38991105556488037\n",
      "Batch 327, Loss: -0.5486696362495422\n",
      "Batch 328, Loss: -0.7933540344238281\n",
      "Batch 329, Loss: -0.8115596771240234\n",
      "Batch 330, Loss: -0.30612581968307495\n",
      "Batch 331, Loss: -0.2968391478061676\n",
      "Batch 332, Loss: 0.009401440620422363\n",
      "Batch 333, Loss: -0.02953416109085083\n",
      "Batch 334, Loss: 0.14685845375061035\n",
      "Batch 335, Loss: -0.8315626978874207\n",
      "Batch 336, Loss: -0.5502067804336548\n",
      "Batch 337, Loss: -0.4954257011413574\n",
      "Batch 338, Loss: -0.7456306219100952\n",
      "Batch 339, Loss: -0.3391128480434418\n",
      "Batch 340, Loss: -1.0731583833694458\n",
      "Batch 341, Loss: -0.3781491816043854\n",
      "Batch 342, Loss: -0.39055222272872925\n",
      "Batch 343, Loss: -0.4003152847290039\n",
      "Batch 344, Loss: -0.4973081052303314\n",
      "Batch 345, Loss: -0.8193603157997131\n",
      "Batch 346, Loss: -0.3834604322910309\n",
      "Batch 347, Loss: -0.22423094511032104\n",
      "Batch 348, Loss: -0.5521950721740723\n",
      "Batch 349, Loss: -0.3589397966861725\n",
      "Batch 350, Loss: -0.137473464012146\n",
      "Batch 351, Loss: -0.3997225761413574\n",
      "Batch 352, Loss: -0.3101467192173004\n",
      "Batch 353, Loss: -0.7908691167831421\n",
      "Batch 354, Loss: -0.5936538577079773\n",
      "Batch 355, Loss: -0.7312349081039429\n",
      "Batch 356, Loss: -0.3899334669113159\n",
      "Batch 357, Loss: 0.30113446712493896\n",
      "Batch 358, Loss: -0.8498733639717102\n",
      "Batch 359, Loss: -0.8562633395195007\n",
      "Batch 360, Loss: -0.3745865821838379\n",
      "Batch 361, Loss: -1.0953831672668457\n",
      "Batch 362, Loss: -0.2696067690849304\n",
      "Batch 363, Loss: -0.310400128364563\n",
      "Batch 364, Loss: -0.5502811670303345\n",
      "Batch 365, Loss: -0.08523470163345337\n",
      "Batch 366, Loss: -0.475430428981781\n",
      "Batch 367, Loss: -0.3864654004573822\n",
      "Batch 368, Loss: -0.6953596472740173\n",
      "Batch 369, Loss: -0.33949944376945496\n",
      "Batch 370, Loss: -0.5512970685958862\n",
      "Batch 371, Loss: -1.1377317905426025\n",
      "Batch 372, Loss: -0.6067036986351013\n",
      "Batch 373, Loss: -0.6845285892486572\n",
      "Batch 374, Loss: 0.4115234613418579\n",
      "Batch 375, Loss: -0.5629249811172485\n",
      "Batch 376, Loss: -0.4558407962322235\n",
      "Batch 377, Loss: -0.4804356098175049\n",
      "Batch 378, Loss: 0.017717480659484863\n",
      "Batch 379, Loss: -0.10754191875457764\n",
      "Batch 380, Loss: -0.36455392837524414\n",
      "Batch 381, Loss: -0.4441603720188141\n",
      "Batch 382, Loss: -0.14603471755981445\n",
      "Batch 383, Loss: -0.5629004836082458\n",
      "Batch 384, Loss: -0.9697181582450867\n",
      "Batch 385, Loss: 0.38213491439819336\n",
      "Batch 386, Loss: -0.5321207046508789\n",
      "Batch 387, Loss: -0.9985386729240417\n",
      "Batch 388, Loss: -0.33842384815216064\n",
      "Batch 389, Loss: 0.027702808380126953\n",
      "Batch 390, Loss: -0.7895739078521729\n",
      "Batch 391, Loss: -0.22896289825439453\n",
      "Batch 392, Loss: -0.7753391265869141\n",
      "Batch 393, Loss: -0.6610484719276428\n",
      "Batch 394, Loss: -0.08671152591705322\n",
      "Batch 395, Loss: 0.012843489646911621\n",
      "Batch 396, Loss: -0.24169331789016724\n",
      "Batch 397, Loss: -0.7090685367584229\n",
      "Batch 398, Loss: -0.5404836535453796\n",
      "Batch 399, Loss: -0.054703354835510254\n",
      "Training [100%]\tLoss: -0.5057\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: -0.0005447650328278542 s\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
