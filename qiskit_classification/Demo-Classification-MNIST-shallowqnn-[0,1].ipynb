{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"hybridqnn_shallow\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "LR = 0.001\n",
    "n_samples_train = 100 #128\n",
    "n_samples_test = 50 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"MNIST\" # CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "if network == 'hybridqnn_shallow' or network == 'QSVM':\n",
    "    # Set preprocessing transforms\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "else:\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    \n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: 0.00096525700064376 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZ),\n",
      "  CircuitStateFn(\n",
      "       ┌──────────────────────────┐┌──────────────────────────────────────┐\n",
      "  q_0: ┤0                         ├┤0                                     ├\n",
      "       │  ZZFeatureMap(x[0],x[1]) ││  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
      "  q_1: ┤1                         ├┤1                                     ├\n",
      "       └──────────────────────────┘└──────────────────────────────────────┘\n",
      "  )\n",
      "])\n",
      "HybridQNN_Shallow(\n",
      "  (conv1): Conv2d(1, 2, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (qnn): TorchConnector()\n",
      "  (fc3): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Network init elapsed time: -0.004744197998661548 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]+7-4)/2)-4)/2)**2)*16\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: -0.5855482816696167\n",
      "Batch 1, Loss: -0.6264811754226685\n",
      "Batch 2, Loss: -0.447964608669281\n",
      "Batch 3, Loss: -0.6887856721878052\n",
      "Batch 4, Loss: -0.6666295528411865\n",
      "Batch 5, Loss: -0.5564414858818054\n",
      "Batch 6, Loss: -0.5875259637832642\n",
      "Batch 7, Loss: -0.47940585017204285\n",
      "Batch 8, Loss: -0.8505805730819702\n",
      "Batch 9, Loss: -0.27159106731414795\n",
      "Batch 10, Loss: -0.8099122047424316\n",
      "Batch 11, Loss: -0.1333482563495636\n",
      "Batch 12, Loss: -0.5400454998016357\n",
      "Batch 13, Loss: -0.5060176849365234\n",
      "Batch 14, Loss: -0.5056136250495911\n",
      "Batch 15, Loss: -0.5984381437301636\n",
      "Batch 16, Loss: -0.20317748188972473\n",
      "Batch 17, Loss: -0.27078670263290405\n",
      "Batch 18, Loss: -0.49193379282951355\n",
      "Batch 19, Loss: -0.6603018045425415\n",
      "Batch 20, Loss: -0.21019399166107178\n",
      "Batch 21, Loss: -0.5801764726638794\n",
      "Batch 22, Loss: -0.3821413516998291\n",
      "Batch 23, Loss: -0.4952358603477478\n",
      "Batch 24, Loss: -0.6095492839813232\n",
      "Batch 25, Loss: -0.5359004139900208\n",
      "Batch 26, Loss: -0.4844483435153961\n",
      "Batch 27, Loss: -0.6599592566490173\n",
      "Batch 28, Loss: -0.49242645502090454\n",
      "Batch 29, Loss: -0.6526997089385986\n",
      "Batch 30, Loss: -0.6012316942214966\n",
      "Batch 31, Loss: -0.5288554430007935\n",
      "Batch 32, Loss: -0.7410094738006592\n",
      "Batch 33, Loss: -0.707855224609375\n",
      "Batch 34, Loss: -0.6375954151153564\n",
      "Batch 35, Loss: -0.8145476579666138\n",
      "Batch 36, Loss: -0.22284287214279175\n",
      "Batch 37, Loss: -0.9072792530059814\n",
      "Batch 38, Loss: -0.4177643656730652\n",
      "Batch 39, Loss: -0.21820807456970215\n",
      "Batch 40, Loss: -0.7732685208320618\n",
      "Batch 41, Loss: -0.46865811944007874\n",
      "Batch 42, Loss: -0.41400769352912903\n",
      "Batch 43, Loss: -0.6673420667648315\n",
      "Batch 44, Loss: -0.2647542357444763\n",
      "Batch 45, Loss: -0.5194355249404907\n",
      "Batch 46, Loss: -0.5076190233230591\n",
      "Batch 47, Loss: -0.4699099659919739\n",
      "Batch 48, Loss: -0.4400428235530853\n",
      "Batch 49, Loss: -0.5751510858535767\n",
      "Batch 50, Loss: -0.5411871671676636\n",
      "Batch 51, Loss: -0.4350734353065491\n",
      "Batch 52, Loss: -0.43550270795822144\n",
      "Batch 53, Loss: -0.5111710429191589\n",
      "Batch 54, Loss: -0.5220546722412109\n",
      "Batch 55, Loss: -0.48508402705192566\n",
      "Batch 56, Loss: -0.4400213956832886\n",
      "Batch 57, Loss: -0.4163879156112671\n",
      "Batch 58, Loss: -0.5221941471099854\n",
      "Batch 59, Loss: -0.5054712295532227\n",
      "Batch 60, Loss: -0.4905485212802887\n",
      "Batch 61, Loss: -0.6084210276603699\n",
      "Batch 62, Loss: -0.4736727178096771\n",
      "Batch 63, Loss: -0.49322816729545593\n",
      "Batch 64, Loss: -0.4270508885383606\n",
      "Batch 65, Loss: -0.4794646203517914\n",
      "Batch 66, Loss: -0.6165874004364014\n",
      "Batch 67, Loss: -0.6813026070594788\n",
      "Batch 68, Loss: -0.6380866169929504\n",
      "Batch 69, Loss: -0.6500096321105957\n",
      "Batch 70, Loss: -0.607768177986145\n",
      "Batch 71, Loss: -0.6751769781112671\n",
      "Batch 72, Loss: -0.6483893394470215\n",
      "Batch 73, Loss: -0.47167807817459106\n",
      "Batch 74, Loss: -0.5064285397529602\n",
      "Batch 75, Loss: -0.5479280948638916\n",
      "Batch 76, Loss: -0.14243705570697784\n",
      "Batch 77, Loss: -0.191939115524292\n",
      "Batch 78, Loss: -0.14949524402618408\n",
      "Batch 79, Loss: 0.016245514154434204\n",
      "Batch 80, Loss: -0.4121522903442383\n",
      "Batch 81, Loss: -0.2810446321964264\n",
      "Batch 82, Loss: -0.59871906042099\n",
      "Batch 83, Loss: -0.14178524911403656\n",
      "Batch 84, Loss: -0.028418898582458496\n",
      "Batch 85, Loss: -0.24157094955444336\n",
      "Batch 86, Loss: -0.18649831414222717\n",
      "Batch 87, Loss: -0.6130809783935547\n",
      "Batch 88, Loss: -0.5058094263076782\n",
      "Batch 89, Loss: -0.5976774096488953\n",
      "Batch 90, Loss: -0.5708848237991333\n",
      "Batch 91, Loss: -0.4995584785938263\n",
      "Batch 92, Loss: -0.6705666780471802\n",
      "Batch 93, Loss: -0.4965395927429199\n",
      "Batch 94, Loss: -0.7477937340736389\n",
      "Batch 95, Loss: -0.4695999026298523\n",
      "Batch 96, Loss: -0.5362914800643921\n",
      "Batch 97, Loss: -0.5063061714172363\n",
      "Batch 98, Loss: -0.5200912952423096\n",
      "Batch 99, Loss: -0.44314703345298767\n",
      "Training [10%]\tLoss: -0.4985\n",
      "Batch 0, Loss: -0.6089533567428589\n",
      "Batch 1, Loss: -0.42738890647888184\n",
      "Batch 2, Loss: -0.70856773853302\n",
      "Batch 3, Loss: -0.6919012069702148\n",
      "Batch 4, Loss: -0.669298529624939\n",
      "Batch 5, Loss: -0.7923178672790527\n",
      "Batch 6, Loss: -0.7677485942840576\n",
      "Batch 7, Loss: -0.29760655760765076\n",
      "Batch 8, Loss: -0.49243444204330444\n",
      "Batch 9, Loss: -0.6427302360534668\n",
      "Batch 10, Loss: -0.8750956654548645\n",
      "Batch 11, Loss: -0.3295454978942871\n",
      "Batch 12, Loss: -0.6729005575180054\n",
      "Batch 13, Loss: -0.6575757265090942\n",
      "Batch 14, Loss: -0.4179611802101135\n",
      "Batch 15, Loss: -0.5818526148796082\n",
      "Batch 16, Loss: -0.5902025699615479\n",
      "Batch 17, Loss: -0.612809419631958\n",
      "Batch 18, Loss: -0.5158900618553162\n",
      "Batch 19, Loss: -0.539331316947937\n",
      "Batch 20, Loss: -0.5652614831924438\n",
      "Batch 21, Loss: -0.7196975946426392\n",
      "Batch 22, Loss: -0.6436923742294312\n",
      "Batch 23, Loss: -0.6355603933334351\n",
      "Batch 24, Loss: -0.5702787041664124\n",
      "Batch 25, Loss: -0.5432804822921753\n",
      "Batch 26, Loss: -0.5764148235321045\n",
      "Batch 27, Loss: -0.6988548636436462\n",
      "Batch 28, Loss: -0.5459362864494324\n",
      "Batch 29, Loss: -0.6207895278930664\n",
      "Batch 30, Loss: -0.5849730968475342\n",
      "Batch 31, Loss: -0.5496507287025452\n",
      "Batch 32, Loss: -0.7850419282913208\n",
      "Batch 33, Loss: -0.7203383445739746\n",
      "Batch 34, Loss: -0.455549418926239\n",
      "Batch 35, Loss: -0.8465261459350586\n",
      "Batch 36, Loss: -0.5433712005615234\n",
      "Batch 37, Loss: -0.8843607902526855\n",
      "Batch 38, Loss: -0.7526061534881592\n",
      "Batch 39, Loss: -0.6191608905792236\n",
      "Batch 40, Loss: -0.41697266697883606\n",
      "Batch 41, Loss: -0.7067798972129822\n",
      "Batch 42, Loss: -0.6090540289878845\n",
      "Batch 43, Loss: -0.49584904313087463\n",
      "Batch 44, Loss: -0.6688419580459595\n",
      "Batch 45, Loss: -0.7859084010124207\n",
      "Batch 46, Loss: -0.2850794196128845\n",
      "Batch 47, Loss: -0.8228240609169006\n",
      "Batch 48, Loss: -0.5601884722709656\n",
      "Batch 49, Loss: -0.7162426710128784\n",
      "Batch 50, Loss: -0.7482701539993286\n",
      "Batch 51, Loss: -0.7207455039024353\n",
      "Batch 52, Loss: -0.8081222176551819\n",
      "Batch 53, Loss: -0.6467645168304443\n",
      "Batch 54, Loss: -0.5247708559036255\n",
      "Batch 55, Loss: -0.5651845932006836\n",
      "Batch 56, Loss: -0.7898300290107727\n",
      "Batch 57, Loss: -0.9020468592643738\n",
      "Batch 58, Loss: -0.7557753324508667\n",
      "Batch 59, Loss: -1.0407681465148926\n",
      "Batch 60, Loss: -0.8792949914932251\n",
      "Batch 61, Loss: -0.8013529777526855\n",
      "Batch 62, Loss: -0.7658458948135376\n",
      "Batch 63, Loss: -0.831610381603241\n",
      "Batch 64, Loss: -0.6529706716537476\n",
      "Batch 65, Loss: -0.7207621335983276\n",
      "Batch 66, Loss: -0.7792643308639526\n",
      "Batch 67, Loss: -0.6912573575973511\n",
      "Batch 68, Loss: -1.0663549900054932\n",
      "Batch 69, Loss: -0.9258322715759277\n",
      "Batch 70, Loss: -0.7726216316223145\n",
      "Batch 71, Loss: -0.7525523900985718\n",
      "Batch 72, Loss: -1.0136501789093018\n",
      "Batch 73, Loss: -0.9491419792175293\n",
      "Batch 74, Loss: -0.833001971244812\n",
      "Batch 75, Loss: -0.7524961233139038\n",
      "Batch 76, Loss: -0.9270601868629456\n",
      "Batch 77, Loss: -1.1346845626831055\n",
      "Batch 78, Loss: -0.6014130115509033\n",
      "Batch 79, Loss: -0.568577766418457\n",
      "Batch 80, Loss: -0.7214337587356567\n",
      "Batch 81, Loss: -0.721218466758728\n",
      "Batch 82, Loss: -0.42079660296440125\n",
      "Batch 83, Loss: -0.7567324638366699\n",
      "Batch 84, Loss: -0.9236240983009338\n",
      "Batch 85, Loss: -0.5933552980422974\n",
      "Batch 86, Loss: -0.3594575524330139\n",
      "Batch 87, Loss: -0.8492151498794556\n",
      "Batch 88, Loss: -0.6487683057785034\n",
      "Batch 89, Loss: -0.7848667502403259\n",
      "Batch 90, Loss: -0.7162913084030151\n",
      "Batch 91, Loss: -0.686042070388794\n",
      "Batch 92, Loss: -0.7497143149375916\n",
      "Batch 93, Loss: -0.6507039070129395\n",
      "Batch 94, Loss: -0.5700408816337585\n",
      "Batch 95, Loss: -0.6363047361373901\n",
      "Batch 96, Loss: -0.7469245195388794\n",
      "Batch 97, Loss: -0.7298216819763184\n",
      "Batch 98, Loss: -0.8010357022285461\n",
      "Batch 99, Loss: -0.9737817049026489\n",
      "Training [20%]\tLoss: -0.6878\n",
      "Batch 0, Loss: -0.7539923191070557\n",
      "Batch 1, Loss: -0.5969834923744202\n",
      "Batch 2, Loss: -0.7487736940383911\n",
      "Batch 3, Loss: -0.6222684383392334\n",
      "Batch 4, Loss: -0.496756374835968\n",
      "Batch 5, Loss: -0.6176572442054749\n",
      "Batch 6, Loss: -0.5551128387451172\n",
      "Batch 7, Loss: -0.9487529993057251\n",
      "Batch 8, Loss: -0.906927764415741\n",
      "Batch 9, Loss: -0.7966067790985107\n",
      "Batch 10, Loss: -1.1221904754638672\n",
      "Batch 11, Loss: -0.5020722150802612\n",
      "Batch 12, Loss: -0.7096632719039917\n",
      "Batch 13, Loss: -0.9617504477500916\n",
      "Batch 14, Loss: -0.9693382978439331\n",
      "Batch 15, Loss: -0.9606858491897583\n",
      "Batch 16, Loss: -0.8986635208129883\n",
      "Batch 17, Loss: -0.782572329044342\n",
      "Batch 18, Loss: -0.9461630582809448\n",
      "Batch 19, Loss: -0.7687724232673645\n",
      "Batch 20, Loss: -0.9670532941818237\n",
      "Batch 21, Loss: -0.9640190601348877\n",
      "Batch 22, Loss: -1.0112073421478271\n",
      "Batch 23, Loss: -0.6366809010505676\n",
      "Batch 24, Loss: -0.9681071639060974\n",
      "Batch 25, Loss: -0.7699097990989685\n",
      "Batch 26, Loss: -0.9625728130340576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 27, Loss: -0.8265467286109924\n",
      "Batch 28, Loss: -0.8746054172515869\n",
      "Batch 29, Loss: -0.809795618057251\n",
      "Batch 30, Loss: -0.731360673904419\n",
      "Batch 31, Loss: -0.7747328281402588\n",
      "Batch 32, Loss: -0.7448090314865112\n",
      "Batch 33, Loss: -0.7223209738731384\n",
      "Batch 34, Loss: -0.7245961427688599\n",
      "Batch 35, Loss: -0.9376828670501709\n",
      "Batch 36, Loss: -0.8389705419540405\n",
      "Batch 37, Loss: -0.9270395040512085\n",
      "Batch 38, Loss: -0.8366602659225464\n",
      "Batch 39, Loss: -0.8948414921760559\n",
      "Batch 40, Loss: -0.8963310718536377\n",
      "Batch 41, Loss: -0.936529815196991\n",
      "Batch 42, Loss: -0.6373792886734009\n",
      "Batch 43, Loss: -0.8000602722167969\n",
      "Batch 44, Loss: -0.7671064734458923\n",
      "Batch 45, Loss: -0.650383472442627\n",
      "Batch 46, Loss: -0.991895318031311\n",
      "Batch 47, Loss: -1.0361988544464111\n",
      "Batch 48, Loss: -0.9929186105728149\n",
      "Batch 49, Loss: -0.8043723106384277\n",
      "Batch 50, Loss: -0.8779894113540649\n",
      "Batch 51, Loss: -0.8913369178771973\n",
      "Batch 52, Loss: -0.9783854484558105\n",
      "Batch 53, Loss: -0.901833176612854\n",
      "Batch 54, Loss: -0.8146549463272095\n",
      "Batch 55, Loss: -0.9082072377204895\n",
      "Batch 56, Loss: -0.9471730589866638\n",
      "Batch 57, Loss: -0.8974215984344482\n",
      "Batch 58, Loss: -0.6279180645942688\n",
      "Batch 59, Loss: -0.7911189794540405\n",
      "Batch 60, Loss: -1.0516849756240845\n",
      "Batch 61, Loss: -0.9797123670578003\n",
      "Batch 62, Loss: -0.7512181401252747\n",
      "Batch 63, Loss: -0.7901855707168579\n",
      "Batch 64, Loss: -0.7606703042984009\n",
      "Batch 65, Loss: -0.9101085066795349\n",
      "Batch 66, Loss: -1.047549843788147\n",
      "Batch 67, Loss: -0.7744947671890259\n",
      "Batch 68, Loss: -0.9572916030883789\n",
      "Batch 69, Loss: -0.8830758333206177\n",
      "Batch 70, Loss: -0.7096216678619385\n",
      "Batch 71, Loss: -0.9192852973937988\n",
      "Batch 72, Loss: -0.9025593996047974\n",
      "Batch 73, Loss: -0.9843853116035461\n",
      "Batch 74, Loss: -1.1031911373138428\n",
      "Batch 75, Loss: -0.9789115190505981\n",
      "Batch 76, Loss: -1.1893410682678223\n",
      "Batch 77, Loss: -0.7753526568412781\n",
      "Batch 78, Loss: -0.8486307859420776\n",
      "Batch 79, Loss: -0.9735180735588074\n",
      "Batch 80, Loss: -0.9542587399482727\n",
      "Batch 81, Loss: -1.193375587463379\n",
      "Batch 82, Loss: -0.7435892224311829\n",
      "Batch 83, Loss: -0.8901130557060242\n",
      "Batch 84, Loss: -1.0238642692565918\n",
      "Batch 85, Loss: -0.9342319369316101\n",
      "Batch 86, Loss: -0.8994299173355103\n",
      "Batch 87, Loss: -1.0432040691375732\n",
      "Batch 88, Loss: -0.7746477127075195\n",
      "Batch 89, Loss: -0.8827612400054932\n",
      "Batch 90, Loss: -1.0440515279769897\n",
      "Batch 91, Loss: -1.0792351961135864\n",
      "Batch 92, Loss: -1.0843634605407715\n",
      "Batch 93, Loss: -0.7440826296806335\n",
      "Batch 94, Loss: -0.9958206415176392\n",
      "Batch 95, Loss: -1.0135244131088257\n",
      "Batch 96, Loss: -1.0662294626235962\n",
      "Batch 97, Loss: -0.9726943373680115\n",
      "Batch 98, Loss: -1.0667359828948975\n",
      "Batch 99, Loss: -0.9305990934371948\n",
      "Training [30%]\tLoss: -0.8737\n",
      "Batch 0, Loss: -1.0450365543365479\n",
      "Batch 1, Loss: -0.9400019645690918\n",
      "Batch 2, Loss: -0.8533532619476318\n",
      "Batch 3, Loss: -0.8850294351577759\n",
      "Batch 4, Loss: -0.703761637210846\n",
      "Batch 5, Loss: -1.2534781694412231\n",
      "Batch 6, Loss: -1.2031121253967285\n",
      "Batch 7, Loss: -0.9357590675354004\n",
      "Batch 8, Loss: -0.9166237115859985\n",
      "Batch 9, Loss: -1.2349457740783691\n",
      "Batch 10, Loss: -1.1808303594589233\n",
      "Batch 11, Loss: -0.6761493682861328\n",
      "Batch 12, Loss: -1.029157042503357\n",
      "Batch 13, Loss: -0.9864007234573364\n",
      "Batch 14, Loss: -0.95682692527771\n",
      "Batch 15, Loss: -0.9538899660110474\n",
      "Batch 16, Loss: -0.9531465768814087\n",
      "Batch 17, Loss: -1.1039576530456543\n",
      "Batch 18, Loss: -0.9624431729316711\n",
      "Batch 19, Loss: -1.180737853050232\n",
      "Batch 20, Loss: -1.0830423831939697\n",
      "Batch 21, Loss: -1.2676759958267212\n",
      "Batch 22, Loss: -0.974473237991333\n",
      "Batch 23, Loss: -1.1364625692367554\n",
      "Batch 24, Loss: -1.0397276878356934\n",
      "Batch 25, Loss: -0.7755215167999268\n",
      "Batch 26, Loss: -1.1176743507385254\n",
      "Batch 27, Loss: -1.0121052265167236\n",
      "Batch 28, Loss: -0.890688419342041\n",
      "Batch 29, Loss: -0.99839848279953\n",
      "Batch 30, Loss: -0.8763307332992554\n",
      "Batch 31, Loss: -0.9605430364608765\n",
      "Batch 32, Loss: -1.0071783065795898\n",
      "Batch 33, Loss: -0.532875657081604\n",
      "Batch 34, Loss: -1.142986536026001\n",
      "Batch 35, Loss: -0.8743219375610352\n",
      "Batch 36, Loss: -0.8540552258491516\n",
      "Batch 37, Loss: -1.0366414785385132\n",
      "Batch 38, Loss: -0.8823986053466797\n",
      "Batch 39, Loss: -0.9897735118865967\n",
      "Batch 40, Loss: -0.8894569873809814\n",
      "Batch 41, Loss: -0.7452571392059326\n",
      "Batch 42, Loss: -0.84096360206604\n",
      "Batch 43, Loss: -0.9465377330780029\n",
      "Batch 44, Loss: -1.0242546796798706\n",
      "Batch 45, Loss: -0.8737107515335083\n",
      "Batch 46, Loss: -0.8817689418792725\n",
      "Batch 47, Loss: -0.9929574728012085\n",
      "Batch 48, Loss: -0.9831964373588562\n",
      "Batch 49, Loss: -1.0336394309997559\n",
      "Batch 50, Loss: -0.9039992094039917\n",
      "Batch 51, Loss: -0.7123837471008301\n",
      "Batch 52, Loss: -0.950962483882904\n",
      "Batch 53, Loss: -0.8565026521682739\n",
      "Batch 54, Loss: -1.2789714336395264\n",
      "Batch 55, Loss: -1.0048034191131592\n",
      "Batch 56, Loss: -0.9345013499259949\n",
      "Batch 57, Loss: -1.0169336795806885\n",
      "Batch 58, Loss: -0.8939138650894165\n",
      "Batch 59, Loss: -0.5739154815673828\n",
      "Batch 60, Loss: -0.8891050815582275\n",
      "Batch 61, Loss: -1.0030763149261475\n",
      "Batch 62, Loss: -1.214145302772522\n",
      "Batch 63, Loss: -1.0619516372680664\n",
      "Batch 64, Loss: -0.9954020380973816\n",
      "Batch 65, Loss: -0.8162063360214233\n",
      "Batch 66, Loss: -0.8696665167808533\n",
      "Batch 67, Loss: -1.1367636919021606\n",
      "Batch 68, Loss: -1.2891465425491333\n",
      "Batch 69, Loss: -1.0413782596588135\n",
      "Batch 70, Loss: -1.2593226432800293\n",
      "Batch 71, Loss: -0.866997241973877\n",
      "Batch 72, Loss: -1.203933596611023\n",
      "Batch 73, Loss: -1.0201537609100342\n",
      "Batch 74, Loss: -1.1616113185882568\n",
      "Batch 75, Loss: -0.9870259165763855\n",
      "Batch 76, Loss: -1.042219877243042\n",
      "Batch 77, Loss: -0.9241516590118408\n",
      "Batch 78, Loss: -1.1315687894821167\n",
      "Batch 79, Loss: -1.0804239511489868\n",
      "Batch 80, Loss: -0.8309325575828552\n",
      "Batch 81, Loss: -1.1310601234436035\n",
      "Batch 82, Loss: -1.1046507358551025\n",
      "Batch 83, Loss: -1.0656460523605347\n",
      "Batch 84, Loss: -1.0228090286254883\n",
      "Batch 85, Loss: -0.9819417595863342\n",
      "Batch 86, Loss: -1.0816013813018799\n",
      "Batch 87, Loss: -1.1772007942199707\n",
      "Batch 88, Loss: -1.3751349449157715\n",
      "Batch 89, Loss: -1.0105681419372559\n",
      "Batch 90, Loss: -1.073266863822937\n",
      "Batch 91, Loss: -0.9526674747467041\n",
      "Batch 92, Loss: -0.9873336553573608\n",
      "Batch 93, Loss: -0.852392315864563\n",
      "Batch 94, Loss: -1.134485125541687\n",
      "Batch 95, Loss: -1.3020834922790527\n",
      "Batch 96, Loss: -0.8074719905853271\n",
      "Batch 97, Loss: -1.07916259765625\n",
      "Batch 98, Loss: -1.0688903331756592\n",
      "Batch 99, Loss: -0.9974373579025269\n",
      "Training [40%]\tLoss: -0.9978\n",
      "Batch 0, Loss: -1.0809831619262695\n",
      "Batch 1, Loss: -1.0428811311721802\n",
      "Batch 2, Loss: -0.8153269290924072\n",
      "Batch 3, Loss: -0.7080401182174683\n",
      "Batch 4, Loss: -1.1919902563095093\n",
      "Batch 5, Loss: -0.8767173290252686\n",
      "Batch 6, Loss: -1.118039608001709\n",
      "Batch 7, Loss: -1.1198019981384277\n",
      "Batch 8, Loss: -1.0200684070587158\n",
      "Batch 9, Loss: -1.1682655811309814\n",
      "Batch 10, Loss: -1.1960760354995728\n",
      "Batch 11, Loss: -1.1122792959213257\n",
      "Batch 12, Loss: -1.074688196182251\n",
      "Batch 13, Loss: -1.3075612783432007\n",
      "Batch 14, Loss: -1.1803269386291504\n",
      "Batch 15, Loss: -1.2704800367355347\n",
      "Batch 16, Loss: -1.1951086521148682\n",
      "Batch 17, Loss: -1.1168122291564941\n",
      "Batch 18, Loss: -1.2458959817886353\n",
      "Batch 19, Loss: -1.405259370803833\n",
      "Batch 20, Loss: -1.251031756401062\n",
      "Batch 21, Loss: -1.2137482166290283\n",
      "Batch 22, Loss: -1.2499970197677612\n",
      "Batch 23, Loss: -1.4140218496322632\n",
      "Batch 24, Loss: -1.2851848602294922\n",
      "Batch 25, Loss: -1.2986892461776733\n",
      "Batch 26, Loss: -1.1840059757232666\n",
      "Batch 27, Loss: -1.271629810333252\n",
      "Batch 28, Loss: -1.190554141998291\n",
      "Batch 29, Loss: -0.9736486077308655\n",
      "Batch 30, Loss: -1.3979582786560059\n",
      "Batch 31, Loss: -1.2391886711120605\n",
      "Batch 32, Loss: -0.8109934329986572\n",
      "Batch 33, Loss: -1.2860960960388184\n",
      "Batch 34, Loss: -0.984684407711029\n",
      "Batch 35, Loss: -1.1732579469680786\n",
      "Batch 36, Loss: -1.2031532526016235\n",
      "Batch 37, Loss: -1.1601080894470215\n",
      "Batch 38, Loss: -1.1597542762756348\n",
      "Batch 39, Loss: -1.3974796533584595\n",
      "Batch 40, Loss: -1.2955808639526367\n",
      "Batch 41, Loss: -1.1399681568145752\n",
      "Batch 42, Loss: -1.1523900032043457\n",
      "Batch 43, Loss: -1.3300929069519043\n",
      "Batch 44, Loss: -1.1250296831130981\n",
      "Batch 45, Loss: -0.9151365756988525\n",
      "Batch 46, Loss: -1.325136423110962\n",
      "Batch 47, Loss: -1.3143353462219238\n",
      "Batch 48, Loss: -1.3286076784133911\n",
      "Batch 49, Loss: -1.3783644437789917\n",
      "Batch 50, Loss: -1.170491099357605\n",
      "Batch 51, Loss: -0.742564857006073\n",
      "Batch 52, Loss: -1.3645288944244385\n",
      "Batch 53, Loss: -1.17326819896698\n",
      "Batch 54, Loss: -1.0383708477020264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 55, Loss: -1.0218830108642578\n",
      "Batch 56, Loss: -1.184117317199707\n",
      "Batch 57, Loss: -1.1400561332702637\n",
      "Batch 58, Loss: -1.204148292541504\n",
      "Batch 59, Loss: -1.3559372425079346\n",
      "Batch 60, Loss: -1.1292610168457031\n",
      "Batch 61, Loss: -1.1880909204483032\n",
      "Batch 62, Loss: -1.0893408060073853\n",
      "Batch 63, Loss: -1.2844276428222656\n",
      "Batch 64, Loss: -1.1955363750457764\n",
      "Batch 65, Loss: -0.9680509567260742\n",
      "Batch 66, Loss: -1.270398736000061\n",
      "Batch 67, Loss: -1.2136231660842896\n",
      "Batch 68, Loss: -1.4127497673034668\n",
      "Batch 69, Loss: -0.5894584059715271\n",
      "Batch 70, Loss: -1.2408497333526611\n",
      "Batch 71, Loss: -1.0258055925369263\n",
      "Batch 72, Loss: -1.1553860902786255\n",
      "Batch 73, Loss: -1.3793036937713623\n",
      "Batch 74, Loss: -1.2878403663635254\n",
      "Batch 75, Loss: -1.279905915260315\n",
      "Batch 76, Loss: -1.3725759983062744\n",
      "Batch 77, Loss: -1.4896330833435059\n",
      "Batch 78, Loss: -1.3086425065994263\n",
      "Batch 79, Loss: -1.3041605949401855\n",
      "Batch 80, Loss: -1.3116497993469238\n",
      "Batch 81, Loss: -1.3700076341629028\n",
      "Batch 82, Loss: -1.110625982284546\n",
      "Batch 83, Loss: -1.2138080596923828\n",
      "Batch 84, Loss: -1.25278902053833\n",
      "Batch 85, Loss: -1.3074761629104614\n",
      "Batch 86, Loss: -1.3725037574768066\n",
      "Batch 87, Loss: -1.1450304985046387\n",
      "Batch 88, Loss: -1.41506028175354\n",
      "Batch 89, Loss: -1.3172194957733154\n",
      "Batch 90, Loss: -1.211549162864685\n",
      "Batch 91, Loss: -1.4390933513641357\n",
      "Batch 92, Loss: -1.2498669624328613\n",
      "Batch 93, Loss: -1.2751829624176025\n",
      "Batch 94, Loss: -1.2969504594802856\n",
      "Batch 95, Loss: -1.279659628868103\n",
      "Batch 96, Loss: -1.0212609767913818\n",
      "Batch 97, Loss: -1.3286662101745605\n",
      "Batch 98, Loss: -1.3178800344467163\n",
      "Batch 99, Loss: -1.2711488008499146\n",
      "Training [50%]\tLoss: -1.1978\n",
      "Batch 0, Loss: -1.0998742580413818\n",
      "Batch 1, Loss: -1.5451754331588745\n",
      "Batch 2, Loss: -1.3065035343170166\n",
      "Batch 3, Loss: -1.3357107639312744\n",
      "Batch 4, Loss: -1.400127649307251\n",
      "Batch 5, Loss: -1.3340154886245728\n",
      "Batch 6, Loss: -1.0133715867996216\n",
      "Batch 7, Loss: -0.6437662839889526\n",
      "Batch 8, Loss: -0.5704906582832336\n",
      "Batch 9, Loss: -1.49513840675354\n",
      "Batch 10, Loss: -1.0119953155517578\n",
      "Batch 11, Loss: -1.3690128326416016\n",
      "Batch 12, Loss: -1.4464068412780762\n",
      "Batch 13, Loss: -1.3260914087295532\n",
      "Batch 14, Loss: -1.2788150310516357\n",
      "Batch 15, Loss: -1.352888584136963\n",
      "Batch 16, Loss: -1.3052399158477783\n",
      "Batch 17, Loss: -1.4315173625946045\n",
      "Batch 18, Loss: -1.3349790573120117\n",
      "Batch 19, Loss: -1.5152320861816406\n",
      "Batch 20, Loss: -1.3415555953979492\n",
      "Batch 21, Loss: -1.0032379627227783\n",
      "Batch 22, Loss: -1.4373396635055542\n",
      "Batch 23, Loss: -1.4431774616241455\n",
      "Batch 24, Loss: -1.543633222579956\n",
      "Batch 25, Loss: -1.5565778017044067\n",
      "Batch 26, Loss: -1.0330445766448975\n",
      "Batch 27, Loss: -1.4451470375061035\n",
      "Batch 28, Loss: -1.4343385696411133\n",
      "Batch 29, Loss: -1.4957950115203857\n",
      "Batch 30, Loss: -1.4831757545471191\n",
      "Batch 31, Loss: -1.3553595542907715\n",
      "Batch 32, Loss: -1.2531242370605469\n",
      "Batch 33, Loss: -1.362250566482544\n",
      "Batch 34, Loss: -1.4609719514846802\n",
      "Batch 35, Loss: -1.489973783493042\n",
      "Batch 36, Loss: -1.5479121208190918\n",
      "Batch 37, Loss: -1.330979585647583\n",
      "Batch 38, Loss: -1.2662147283554077\n",
      "Batch 39, Loss: -1.1821880340576172\n",
      "Batch 40, Loss: -1.4533281326293945\n",
      "Batch 41, Loss: -1.563624382019043\n",
      "Batch 42, Loss: -0.5281989574432373\n",
      "Batch 43, Loss: -1.399950385093689\n",
      "Batch 44, Loss: -1.557096004486084\n",
      "Batch 45, Loss: -1.5021355152130127\n",
      "Batch 46, Loss: -1.4496092796325684\n",
      "Batch 47, Loss: -1.4167702198028564\n",
      "Batch 48, Loss: -1.3284533023834229\n",
      "Batch 49, Loss: -1.5869455337524414\n",
      "Batch 50, Loss: -1.1633822917938232\n",
      "Batch 51, Loss: -1.1979310512542725\n",
      "Batch 52, Loss: -1.5469253063201904\n",
      "Batch 53, Loss: -1.5609514713287354\n",
      "Batch 54, Loss: -1.4870808124542236\n",
      "Batch 55, Loss: -1.4707417488098145\n",
      "Batch 56, Loss: -1.4612865447998047\n",
      "Batch 57, Loss: -1.214803695678711\n",
      "Batch 58, Loss: -1.2319941520690918\n",
      "Batch 59, Loss: -1.4020698070526123\n",
      "Batch 60, Loss: -1.4576222896575928\n",
      "Batch 61, Loss: -1.119394063949585\n",
      "Batch 62, Loss: -1.4349603652954102\n",
      "Batch 63, Loss: -1.1896440982818604\n",
      "Batch 64, Loss: -1.3945660591125488\n",
      "Batch 65, Loss: -0.8882895708084106\n",
      "Batch 66, Loss: -1.4271889925003052\n",
      "Batch 67, Loss: -1.5498559474945068\n",
      "Batch 68, Loss: -1.5497920513153076\n",
      "Batch 69, Loss: -1.5906733274459839\n",
      "Batch 70, Loss: -1.5076625347137451\n",
      "Batch 71, Loss: -1.393967866897583\n",
      "Batch 72, Loss: -1.3920295238494873\n",
      "Batch 73, Loss: -1.5925737619400024\n",
      "Batch 74, Loss: -1.3326176404953003\n",
      "Batch 75, Loss: -1.5243041515350342\n",
      "Batch 76, Loss: -1.42613685131073\n",
      "Batch 77, Loss: -1.3292491436004639\n",
      "Batch 78, Loss: -1.455142855644226\n",
      "Batch 79, Loss: -1.5799416303634644\n",
      "Batch 80, Loss: -1.5278902053833008\n",
      "Batch 81, Loss: -1.4627776145935059\n",
      "Batch 82, Loss: -1.3038051128387451\n",
      "Batch 83, Loss: -1.2365899085998535\n",
      "Batch 84, Loss: -1.5996922254562378\n",
      "Batch 85, Loss: -1.2906166315078735\n",
      "Batch 86, Loss: -1.533235788345337\n",
      "Batch 87, Loss: -1.420651912689209\n",
      "Batch 88, Loss: -1.609217643737793\n",
      "Batch 89, Loss: -1.3807222843170166\n",
      "Batch 90, Loss: -1.6163983345031738\n",
      "Batch 91, Loss: -1.63529634475708\n",
      "Batch 92, Loss: -1.1578073501586914\n",
      "Batch 93, Loss: -1.5823984146118164\n",
      "Batch 94, Loss: -1.4611668586730957\n",
      "Batch 95, Loss: -1.3819611072540283\n",
      "Batch 96, Loss: -1.3698393106460571\n",
      "Batch 97, Loss: -1.5907262563705444\n",
      "Batch 98, Loss: -1.601229190826416\n",
      "Batch 99, Loss: -1.52791166305542\n",
      "Training [60%]\tLoss: -1.3753\n",
      "Batch 0, Loss: -1.4834345579147339\n",
      "Batch 1, Loss: -1.620055913925171\n",
      "Batch 2, Loss: -1.5610851049423218\n",
      "Batch 3, Loss: -1.5449011325836182\n",
      "Batch 4, Loss: -1.5657328367233276\n",
      "Batch 5, Loss: -1.4277516603469849\n",
      "Batch 6, Loss: -1.6192452907562256\n",
      "Batch 7, Loss: -1.511155128479004\n",
      "Batch 8, Loss: -1.5602117776870728\n",
      "Batch 9, Loss: -1.6248241662979126\n",
      "Batch 10, Loss: -1.2136805057525635\n",
      "Batch 11, Loss: -1.5966283082962036\n",
      "Batch 12, Loss: -1.5003986358642578\n",
      "Batch 13, Loss: -0.9593406319618225\n",
      "Batch 14, Loss: -1.2785717248916626\n",
      "Batch 15, Loss: -1.3673455715179443\n",
      "Batch 16, Loss: -1.3149820566177368\n",
      "Batch 17, Loss: -1.2580223083496094\n",
      "Batch 18, Loss: -1.5650975704193115\n",
      "Batch 19, Loss: -1.631917953491211\n",
      "Batch 20, Loss: -1.4017908573150635\n",
      "Batch 21, Loss: -1.5856013298034668\n",
      "Batch 22, Loss: -1.5366978645324707\n",
      "Batch 23, Loss: -1.5627946853637695\n",
      "Batch 24, Loss: -1.6672321557998657\n",
      "Batch 25, Loss: -1.5334731340408325\n",
      "Batch 26, Loss: -1.6796000003814697\n",
      "Batch 27, Loss: -1.5446819067001343\n",
      "Batch 28, Loss: -1.596662998199463\n",
      "Batch 29, Loss: -1.5631611347198486\n",
      "Batch 30, Loss: -1.5565872192382812\n",
      "Batch 31, Loss: -0.9508758187294006\n",
      "Batch 32, Loss: -1.6176323890686035\n",
      "Batch 33, Loss: -1.1905121803283691\n",
      "Batch 34, Loss: -1.5373263359069824\n",
      "Batch 35, Loss: -1.5133157968521118\n",
      "Batch 36, Loss: -1.07993483543396\n",
      "Batch 37, Loss: -1.4477381706237793\n",
      "Batch 38, Loss: -1.511967658996582\n",
      "Batch 39, Loss: -1.6417179107666016\n",
      "Batch 40, Loss: -1.4956846237182617\n",
      "Batch 41, Loss: -1.6325645446777344\n",
      "Batch 42, Loss: -1.6946463584899902\n",
      "Batch 43, Loss: -1.6009833812713623\n",
      "Batch 44, Loss: -1.6067700386047363\n",
      "Batch 45, Loss: -1.590273380279541\n",
      "Batch 46, Loss: -1.5081703662872314\n",
      "Batch 47, Loss: -1.4094188213348389\n",
      "Batch 48, Loss: -1.4362815618515015\n",
      "Batch 49, Loss: -1.5962166786193848\n",
      "Batch 50, Loss: -1.7124965190887451\n",
      "Batch 51, Loss: -1.6098690032958984\n",
      "Batch 52, Loss: -1.5685245990753174\n",
      "Batch 53, Loss: -1.6331961154937744\n",
      "Batch 54, Loss: -1.5999850034713745\n",
      "Batch 55, Loss: -1.5762956142425537\n",
      "Batch 56, Loss: -1.388488531112671\n",
      "Batch 57, Loss: -1.691346526145935\n",
      "Batch 58, Loss: -1.6361911296844482\n",
      "Batch 59, Loss: -1.683140754699707\n",
      "Batch 60, Loss: -1.6589380502700806\n",
      "Batch 61, Loss: -1.7669041156768799\n",
      "Batch 62, Loss: -1.6363729238510132\n",
      "Batch 63, Loss: -1.7410292625427246\n",
      "Batch 64, Loss: -1.5619556903839111\n",
      "Batch 65, Loss: -1.2114163637161255\n",
      "Batch 66, Loss: -1.555363655090332\n",
      "Batch 67, Loss: -1.548724889755249\n",
      "Batch 68, Loss: -1.5816545486450195\n",
      "Batch 69, Loss: -1.773996114730835\n",
      "Batch 70, Loss: -0.77809739112854\n",
      "Batch 71, Loss: -1.3947855234146118\n",
      "Batch 72, Loss: -1.6580852270126343\n",
      "Batch 73, Loss: -1.683769702911377\n",
      "Batch 74, Loss: -1.5729522705078125\n",
      "Batch 75, Loss: -1.2640124559402466\n",
      "Batch 76, Loss: -1.4794657230377197\n",
      "Batch 77, Loss: -1.6658668518066406\n",
      "Batch 78, Loss: -1.6759556531906128\n",
      "Batch 79, Loss: -1.5903176069259644\n",
      "Batch 80, Loss: -1.4061262607574463\n",
      "Batch 81, Loss: -1.7045739889144897\n",
      "Batch 82, Loss: -1.1035065650939941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 83, Loss: -1.498187780380249\n",
      "Batch 84, Loss: -1.4522933959960938\n",
      "Batch 85, Loss: -1.6947277784347534\n",
      "Batch 86, Loss: -1.4803457260131836\n",
      "Batch 87, Loss: -1.2995810508728027\n",
      "Batch 88, Loss: -1.6314681768417358\n",
      "Batch 89, Loss: -1.554149866104126\n",
      "Batch 90, Loss: -1.5821714401245117\n",
      "Batch 91, Loss: -1.650634765625\n",
      "Batch 92, Loss: -1.6308045387268066\n",
      "Batch 93, Loss: -1.8149316310882568\n",
      "Batch 94, Loss: -1.5086324214935303\n",
      "Batch 95, Loss: -1.707378625869751\n",
      "Batch 96, Loss: -1.2762235403060913\n",
      "Batch 97, Loss: -1.7420700788497925\n",
      "Batch 98, Loss: -1.6965335607528687\n",
      "Batch 99, Loss: -1.7854520082473755\n",
      "Training [70%]\tLoss: -1.5261\n",
      "Batch 0, Loss: -1.1476043462753296\n",
      "Batch 1, Loss: -0.5879133939743042\n",
      "Batch 2, Loss: -1.6786433458328247\n",
      "Batch 3, Loss: -1.7417535781860352\n",
      "Batch 4, Loss: -1.7440073490142822\n",
      "Batch 5, Loss: -1.7936731576919556\n",
      "Batch 6, Loss: -1.788763165473938\n",
      "Batch 7, Loss: -1.7942016124725342\n",
      "Batch 8, Loss: -1.6891405582427979\n",
      "Batch 9, Loss: -1.7883400917053223\n",
      "Batch 10, Loss: -1.6349000930786133\n",
      "Batch 11, Loss: -1.8004940748214722\n",
      "Batch 12, Loss: -1.8139567375183105\n",
      "Batch 13, Loss: -1.7284663915634155\n",
      "Batch 14, Loss: -1.5304641723632812\n",
      "Batch 15, Loss: -1.731133222579956\n",
      "Batch 16, Loss: -1.7513892650604248\n",
      "Batch 17, Loss: -1.59531831741333\n",
      "Batch 18, Loss: -1.8397303819656372\n",
      "Batch 19, Loss: -1.368840217590332\n",
      "Batch 20, Loss: -1.8398938179016113\n",
      "Batch 21, Loss: -1.4757393598556519\n",
      "Batch 22, Loss: -1.8431872129440308\n",
      "Batch 23, Loss: -1.5547173023223877\n",
      "Batch 24, Loss: -1.674773931503296\n",
      "Batch 25, Loss: -1.5447571277618408\n",
      "Batch 26, Loss: -1.6791770458221436\n",
      "Batch 27, Loss: -1.6915438175201416\n",
      "Batch 28, Loss: -1.521144151687622\n",
      "Batch 29, Loss: -1.740632176399231\n",
      "Batch 30, Loss: -1.5209550857543945\n",
      "Batch 31, Loss: -1.8782081604003906\n",
      "Batch 32, Loss: -1.8055866956710815\n",
      "Batch 33, Loss: -1.5724588632583618\n",
      "Batch 34, Loss: -1.7485908269882202\n",
      "Batch 35, Loss: -1.6542688608169556\n",
      "Batch 36, Loss: -1.5597305297851562\n",
      "Batch 37, Loss: -1.413437008857727\n",
      "Batch 38, Loss: -1.5307183265686035\n",
      "Batch 39, Loss: -1.7906100749969482\n",
      "Batch 40, Loss: -1.737526774406433\n",
      "Batch 41, Loss: -1.747801423072815\n",
      "Batch 42, Loss: -1.6742167472839355\n",
      "Batch 43, Loss: -1.5910708904266357\n",
      "Batch 44, Loss: -1.6600863933563232\n",
      "Batch 45, Loss: -1.7295411825180054\n",
      "Batch 46, Loss: -1.8027775287628174\n",
      "Batch 47, Loss: -1.6186320781707764\n",
      "Batch 48, Loss: -1.5831615924835205\n",
      "Batch 49, Loss: -1.777511715888977\n",
      "Batch 50, Loss: -1.8325998783111572\n",
      "Batch 51, Loss: -1.6403087377548218\n",
      "Batch 52, Loss: -1.7683708667755127\n",
      "Batch 53, Loss: -1.749351978302002\n",
      "Batch 54, Loss: -1.801651954650879\n",
      "Batch 55, Loss: -1.8155324459075928\n",
      "Batch 56, Loss: -1.7062015533447266\n",
      "Batch 57, Loss: -1.8312979936599731\n",
      "Batch 58, Loss: -1.682311773300171\n",
      "Batch 59, Loss: -1.845689058303833\n",
      "Batch 60, Loss: -1.6557129621505737\n",
      "Batch 61, Loss: -1.1013774871826172\n",
      "Batch 62, Loss: -1.3474351167678833\n",
      "Batch 63, Loss: -1.6566565036773682\n",
      "Batch 64, Loss: -1.846545696258545\n",
      "Batch 65, Loss: -1.8822873830795288\n",
      "Batch 66, Loss: -1.800310492515564\n",
      "Batch 67, Loss: -1.5943430662155151\n",
      "Batch 68, Loss: -1.8732889890670776\n",
      "Batch 69, Loss: -1.8099461793899536\n",
      "Batch 70, Loss: -1.7564148902893066\n",
      "Batch 71, Loss: -1.711157202720642\n",
      "Batch 72, Loss: -1.847703218460083\n",
      "Batch 73, Loss: -1.728989839553833\n",
      "Batch 74, Loss: -1.7610379457473755\n",
      "Batch 75, Loss: -1.836496353149414\n",
      "Batch 76, Loss: -1.7547404766082764\n",
      "Batch 77, Loss: -1.725299596786499\n",
      "Batch 78, Loss: -1.8532860279083252\n",
      "Batch 79, Loss: -1.8502788543701172\n",
      "Batch 80, Loss: -1.8063311576843262\n",
      "Batch 81, Loss: -1.7251429557800293\n",
      "Batch 82, Loss: -1.8871071338653564\n",
      "Batch 83, Loss: -1.802330493927002\n",
      "Batch 84, Loss: -1.3351285457611084\n",
      "Batch 85, Loss: -1.8230736255645752\n",
      "Batch 86, Loss: -1.8260740041732788\n",
      "Batch 87, Loss: -1.8145577907562256\n",
      "Batch 88, Loss: -1.7720868587493896\n",
      "Batch 89, Loss: -1.7325490713119507\n",
      "Batch 90, Loss: -1.8306552171707153\n",
      "Batch 91, Loss: -1.7384251356124878\n",
      "Batch 92, Loss: -1.795570731163025\n",
      "Batch 93, Loss: -1.8168156147003174\n",
      "Batch 94, Loss: -1.8503189086914062\n",
      "Batch 95, Loss: -1.822751760482788\n",
      "Batch 96, Loss: -1.3512506484985352\n",
      "Batch 97, Loss: -1.6533682346343994\n",
      "Batch 98, Loss: -1.368079423904419\n",
      "Batch 99, Loss: -1.927294373512268\n",
      "Training [80%]\tLoss: -1.6926\n",
      "Batch 0, Loss: -1.5787155628204346\n",
      "Batch 1, Loss: -1.8702201843261719\n",
      "Batch 2, Loss: -1.8500298261642456\n",
      "Batch 3, Loss: -1.8153281211853027\n",
      "Batch 4, Loss: -1.8644585609436035\n",
      "Batch 5, Loss: -1.9282233715057373\n",
      "Batch 6, Loss: -1.762629747390747\n",
      "Batch 7, Loss: -1.863951325416565\n",
      "Batch 8, Loss: -1.9354147911071777\n",
      "Batch 9, Loss: -1.994028925895691\n",
      "Batch 10, Loss: -1.821751356124878\n",
      "Batch 11, Loss: -1.8329421281814575\n",
      "Batch 12, Loss: -1.9470264911651611\n",
      "Batch 13, Loss: -1.8325363397598267\n",
      "Batch 14, Loss: -1.9659342765808105\n",
      "Batch 15, Loss: -1.946150541305542\n",
      "Batch 16, Loss: -1.8911155462265015\n",
      "Batch 17, Loss: -1.6900968551635742\n",
      "Batch 18, Loss: -1.6466567516326904\n",
      "Batch 19, Loss: -1.9451301097869873\n",
      "Batch 20, Loss: -1.942244291305542\n",
      "Batch 21, Loss: -1.8342043161392212\n",
      "Batch 22, Loss: -2.011174201965332\n",
      "Batch 23, Loss: -1.8594528436660767\n",
      "Batch 24, Loss: -1.6980267763137817\n",
      "Batch 25, Loss: -1.9717884063720703\n",
      "Batch 26, Loss: -1.8591265678405762\n",
      "Batch 27, Loss: -1.676513671875\n",
      "Batch 28, Loss: -1.8191810846328735\n",
      "Batch 29, Loss: -1.8369824886322021\n",
      "Batch 30, Loss: -1.9034315347671509\n",
      "Batch 31, Loss: -1.9264215230941772\n",
      "Batch 32, Loss: -1.8914847373962402\n",
      "Batch 33, Loss: -1.972967505455017\n",
      "Batch 34, Loss: -1.4001740217208862\n",
      "Batch 35, Loss: -1.8971335887908936\n",
      "Batch 36, Loss: -1.6445674896240234\n",
      "Batch 37, Loss: -1.8080414533615112\n",
      "Batch 38, Loss: -1.929194450378418\n",
      "Batch 39, Loss: -1.7918833494186401\n",
      "Batch 40, Loss: -1.9142413139343262\n",
      "Batch 41, Loss: -1.9604672193527222\n",
      "Batch 42, Loss: -1.5231670141220093\n",
      "Batch 43, Loss: -1.8744789361953735\n",
      "Batch 44, Loss: -1.9090968370437622\n",
      "Batch 45, Loss: -1.926591396331787\n",
      "Batch 46, Loss: -1.8232176303863525\n",
      "Batch 47, Loss: -1.934373140335083\n",
      "Batch 48, Loss: -1.8987975120544434\n",
      "Batch 49, Loss: -1.761230707168579\n",
      "Batch 50, Loss: -1.8254120349884033\n",
      "Batch 51, Loss: -1.8914685249328613\n",
      "Batch 52, Loss: -1.8166755437850952\n",
      "Batch 53, Loss: -1.9052741527557373\n",
      "Batch 54, Loss: -1.9827324151992798\n",
      "Batch 55, Loss: -1.9668623208999634\n",
      "Batch 56, Loss: -1.887696623802185\n",
      "Batch 57, Loss: -1.3651137351989746\n",
      "Batch 58, Loss: -2.0024263858795166\n",
      "Batch 59, Loss: -1.8012230396270752\n",
      "Batch 60, Loss: -1.6803885698318481\n",
      "Batch 61, Loss: -2.011561632156372\n",
      "Batch 62, Loss: -1.9932113885879517\n",
      "Batch 63, Loss: -1.9948742389678955\n",
      "Batch 64, Loss: -1.6343334913253784\n",
      "Batch 65, Loss: -2.0208611488342285\n",
      "Batch 66, Loss: -1.5465292930603027\n",
      "Batch 67, Loss: -1.9805850982666016\n",
      "Batch 68, Loss: -1.9106241464614868\n",
      "Batch 69, Loss: -1.9934338331222534\n",
      "Batch 70, Loss: -1.7522242069244385\n",
      "Batch 71, Loss: -1.2448782920837402\n",
      "Batch 72, Loss: -1.8608462810516357\n",
      "Batch 73, Loss: -2.036917209625244\n",
      "Batch 74, Loss: -1.989796757698059\n",
      "Batch 75, Loss: -2.046112060546875\n",
      "Batch 76, Loss: -2.0321707725524902\n",
      "Batch 77, Loss: -1.951972246170044\n",
      "Batch 78, Loss: -2.030794143676758\n",
      "Batch 79, Loss: -1.9643425941467285\n",
      "Batch 80, Loss: -1.8352621793746948\n",
      "Batch 81, Loss: -2.0945918560028076\n",
      "Batch 82, Loss: -1.828063726425171\n",
      "Batch 83, Loss: -1.8857102394104004\n",
      "Batch 84, Loss: -1.2975270748138428\n",
      "Batch 85, Loss: -2.013472557067871\n",
      "Batch 86, Loss: -1.7936369180679321\n",
      "Batch 87, Loss: -2.0888495445251465\n",
      "Batch 88, Loss: -1.9213883876800537\n",
      "Batch 89, Loss: -1.9709343910217285\n",
      "Batch 90, Loss: -1.5612168312072754\n",
      "Batch 91, Loss: -1.669189453125\n",
      "Batch 92, Loss: -1.2468352317810059\n",
      "Batch 93, Loss: -0.9864844679832458\n",
      "Batch 94, Loss: -1.9048579931259155\n",
      "Batch 95, Loss: -2.0316109657287598\n",
      "Batch 96, Loss: -2.086393117904663\n",
      "Batch 97, Loss: -1.9380123615264893\n",
      "Batch 98, Loss: -1.6517949104309082\n",
      "Batch 99, Loss: -1.9550487995147705\n",
      "Training [90%]\tLoss: -1.8406\n",
      "Batch 0, Loss: -2.0680572986602783\n",
      "Batch 1, Loss: -1.8884222507476807\n",
      "Batch 2, Loss: -2.051403522491455\n",
      "Batch 3, Loss: -2.0966529846191406\n",
      "Batch 4, Loss: -2.1119189262390137\n",
      "Batch 5, Loss: -2.0175647735595703\n",
      "Batch 6, Loss: -2.0857064723968506\n",
      "Batch 7, Loss: -0.8219425678253174\n",
      "Batch 8, Loss: -2.0279617309570312\n",
      "Batch 9, Loss: -1.7028443813323975\n",
      "Batch 10, Loss: -1.812157392501831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11, Loss: -1.869231939315796\n",
      "Batch 12, Loss: -2.1126976013183594\n",
      "Batch 13, Loss: -2.1073803901672363\n",
      "Batch 14, Loss: -1.5701584815979004\n",
      "Batch 15, Loss: -2.120431900024414\n",
      "Batch 16, Loss: -2.1068170070648193\n",
      "Batch 17, Loss: -1.970274806022644\n",
      "Batch 18, Loss: -2.057112693786621\n",
      "Batch 19, Loss: -2.046637535095215\n",
      "Batch 20, Loss: -2.087515354156494\n",
      "Batch 21, Loss: -1.7961926460266113\n",
      "Batch 22, Loss: -1.7582565546035767\n",
      "Batch 23, Loss: -1.9496395587921143\n",
      "Batch 24, Loss: -1.7394492626190186\n",
      "Batch 25, Loss: -2.0501046180725098\n",
      "Batch 26, Loss: -2.087566375732422\n",
      "Batch 27, Loss: -2.1203203201293945\n",
      "Batch 28, Loss: -2.1336569786071777\n",
      "Batch 29, Loss: -1.9513134956359863\n",
      "Batch 30, Loss: -1.855086326599121\n",
      "Batch 31, Loss: -2.0360488891601562\n",
      "Batch 32, Loss: -2.083221435546875\n",
      "Batch 33, Loss: -2.0366501808166504\n",
      "Batch 34, Loss: -1.85807466506958\n",
      "Batch 35, Loss: -2.0718228816986084\n",
      "Batch 36, Loss: -2.030879497528076\n",
      "Batch 37, Loss: -2.0563926696777344\n",
      "Batch 38, Loss: -2.0384421348571777\n",
      "Batch 39, Loss: -1.981742262840271\n",
      "Batch 40, Loss: -1.9584732055664062\n",
      "Batch 41, Loss: -2.0028700828552246\n",
      "Batch 42, Loss: -2.007678508758545\n",
      "Batch 43, Loss: -2.1221587657928467\n",
      "Batch 44, Loss: -2.0323171615600586\n",
      "Batch 45, Loss: -1.9154300689697266\n",
      "Batch 46, Loss: -2.113710880279541\n",
      "Batch 47, Loss: -2.037214756011963\n",
      "Batch 48, Loss: -1.7382326126098633\n",
      "Batch 49, Loss: -2.010615110397339\n",
      "Batch 50, Loss: -1.9117838144302368\n",
      "Batch 51, Loss: -1.999253273010254\n",
      "Batch 52, Loss: -1.9875288009643555\n",
      "Batch 53, Loss: -1.0247504711151123\n",
      "Batch 54, Loss: -1.5542064905166626\n",
      "Batch 55, Loss: -1.8621222972869873\n",
      "Batch 56, Loss: -2.1308834552764893\n",
      "Batch 57, Loss: -2.053464412689209\n",
      "Batch 58, Loss: -1.979250431060791\n",
      "Batch 59, Loss: -2.0875234603881836\n",
      "Batch 60, Loss: -2.0652003288269043\n",
      "Batch 61, Loss: -1.6867038011550903\n",
      "Batch 62, Loss: -1.935196042060852\n",
      "Batch 63, Loss: -2.098275661468506\n",
      "Batch 64, Loss: -2.1356277465820312\n",
      "Batch 65, Loss: -1.8911292552947998\n",
      "Batch 66, Loss: -1.9874138832092285\n",
      "Batch 67, Loss: -2.0571541786193848\n",
      "Batch 68, Loss: -2.09444522857666\n",
      "Batch 69, Loss: -1.9490442276000977\n",
      "Batch 70, Loss: -1.6926441192626953\n",
      "Batch 71, Loss: -2.101444721221924\n",
      "Batch 72, Loss: -2.0941877365112305\n",
      "Batch 73, Loss: -1.885133147239685\n",
      "Batch 74, Loss: -2.1007533073425293\n",
      "Batch 75, Loss: -2.077204704284668\n",
      "Batch 76, Loss: -2.031982898712158\n",
      "Batch 77, Loss: -2.152097225189209\n",
      "Batch 78, Loss: -1.9340064525604248\n",
      "Batch 79, Loss: -1.0439941883087158\n",
      "Batch 80, Loss: -2.033092737197876\n",
      "Batch 81, Loss: -2.253718376159668\n",
      "Batch 82, Loss: -2.087763786315918\n",
      "Batch 83, Loss: -2.0428271293640137\n",
      "Batch 84, Loss: -2.028650999069214\n",
      "Batch 85, Loss: -1.923201084136963\n",
      "Batch 86, Loss: -2.067854642868042\n",
      "Batch 87, Loss: -1.9959818124771118\n",
      "Batch 88, Loss: -1.9282164573669434\n",
      "Batch 89, Loss: -2.1421544551849365\n",
      "Batch 90, Loss: -2.092648506164551\n",
      "Batch 91, Loss: -2.0083625316619873\n",
      "Batch 92, Loss: -2.0422050952911377\n",
      "Batch 93, Loss: -2.1513712406158447\n",
      "Batch 94, Loss: -2.20473051071167\n",
      "Batch 95, Loss: -2.0676097869873047\n",
      "Batch 96, Loss: -2.1909003257751465\n",
      "Batch 97, Loss: -2.0632288455963135\n",
      "Batch 98, Loss: -2.0752601623535156\n",
      "Batch 99, Loss: -2.1218695640563965\n",
      "Training [100%]\tLoss: -1.9743\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: -0.005425075942184776 s\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: -2.0383\n",
      "\tAccuracy: 100.0%\n",
      "Test time: 0.0002368820714764297 s\n"
     ]
    }
   ],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPEAAACHCAYAAADHsL/VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAJvklEQVR4nO3df2wW9R3A8c+1pS0WmYAFihBLwToGpgwIOowCaohbIoEhMGSbY5Ix2JItikRNwbGAMEwmDpmTDdOIC4O1I8TMbQmNxm0MmcovKVBBQOg2gdnYgtLC09sf07v7HDwPz/PwPE+fT5/366/Pp9977r7l+uHu+9zd9xzXdQWAXXmd3QEAV4ciBoyjiAHjKGLAOIoYMI4iBoyjiAHjcqqIHcfp7TjOFsdxzjmOc9xxnAc6u09IvVzbzwWd3YEMWysi7SLST0RGisgfHcfZ47ru/k7tFVItp/azkyt3bDmOUyIizSIywnXdxs9+tkFEmlzXfaxTO4eUycX9nEun05UicvHzHfuZPSIyvJP6g/TIuf2cS0XcQ0RaQj/7WESu7YS+IH1ybj/nUhGfFZGeoZ/1FJHWTugL0ifn9nMuFXGjiBQ4jnNT4GdVItIlv+zIYTm3n3Pmiy0REcdxficirojMlf9/a/mqiIzrqt9a5qpc28+5dCQWEVkgIt1F5JSIbBSR+V11x+a4nNrPOXUkBrqiXDsSA10ORQwYRxEDxlHEgHEUMWBcQk8xFTpFbrGUpKsvSMJ5OSftbpuTqvWxj7NTqzSfcV239HJtCRVxsZTIrc7dqekVUuJNtz6l62MfZ6dtbu3xaG2cTgPGUcSAcRQxYBxFDBhHEQPGUcSAcRQxYBxFDBhHEQPGUcSAcRQxYBxFDBhHEQPGUcSAcRQxYBxFDBhHEQPGUcSAcRQxYBxFDBhHEQPGUcSAcQlNWduVtTxwmxdHCpOfxrlXzT9S0R1EkT/8ZpV3FMb/J3xo/jVeXH3HK/Fv0+lQ+cpN93txxaYzqi3S0Bj3elOFIzFgHEUMGEcRA8aZGxPn9+vrxWdvK1dt7fM+UvmOkbVeHHH1uOZS7/jbcPT/bVf+bKB/T/mfrWnpq9p+vXiqynvUveUnHZG4t5FLPp0yVuXLfr5O5WMK26N+tpuTr/ILbnL/xuH1zHhotRfv/Gaxalvy8FyVd9+6M6ltJoIjMWAcRQwYRxEDxmX9mPiDn4xT+dY5T3vx4ILi8OJKxPWv93aIG/9GQ2PgWJ9943yhyicUX/Di2df+W7XNXv1LlY+o+qEXl1dzfflz3V4v8+I5Zb9XbbHGwJ1hbNF5lZ+4T//tVG5Nfx84EgPGUcSAcVl3On345S+rfNsdq1Q+sKB73Os667Z58cKme1Tba0duUnnFWj/Oa7sY9zbyjn+o8iUTh3jxj5ZtVG1TS/QlsEXTtnjx5ur+cW/Tug+W6CHS3nlrVB68pHPpZaF8ieaJD8fo9Y5KYAgVQ/CWXBGRN55eG2VJkcavvqDyO2f9wIt7btyRkv6EcSQGjKOIAeMoYsC4rBgTB8fBDRP1bXUFco3Kg5d79rbr8dLM7fNUPmRNoH3HXt0mu6P2J5GRVHjE1mPzaS+uX/gl1Ta15G8q/3bPJi/eLF17TFwwaKAXj763QbXFuh3ySrdKfvf4vV7cMrdPqPW9+DuYgERu3+z2ncB3JhujL3c1OBIDxlHEgHEUMWBc5sbEef71vSMv36KaGsevD2T6OuCxi5+o/J76H3vxsEeOqLYhzbuuro8p4I6r8uL5pXp8nyfdMt2djMq/7gte3DF4oGobvO6wF68q+2vS29hyrkzlrTP9+wYiJ9MzBl760/VXXiiKPw/f5MVTZWyMJZPHkRgwjiIGjMvY6XT+sKFefGC8Pj2JNW/GjOWPqrxynf+0TzbMhZHfp7fKb3jGP20cHpqJMfw01EstN6SvY53g0HMVXrxvwgsxlkxe9WvTVF55Mv0zZ0zsrp9UupDANchJ737Di0vk/VR1SeFIDBhHEQPGUcSAcRkbEx+Z1fvKC4lI5V/0rZPBMXC2iEwc5cVDVulbCNcM2O7FV5ojc1WtP/tluWTf75mogxN/48WJjBtjGVH/fZUPW3RQ5dnwvUgsHTXBGU8ZEwO4DIoYMI4iBozL2Jj4ZzM3xLXckJfif9tCpkQmjFL5Xc/+3Ysf7dMQWjr6GxXH7Zql8oqV/uOR2fdbJ6761GgvXtb37aTXs6vd/9corS9SbZGWlqTXm4jgjJvhN0Akwk3+BZtx40gMGEcRA8Zl7HR6fPEpL86T6JO+V6+vUfnK+/UpqHPoqBd3fKKfcEpEQf9+Km+a7s9S2Wtyk2p7ZujzKr+lMPg0kj5fCr6M7cFjE1Rb6bdOqzxy7ly83TVh3+xKL66aOi7qcj3G6X+H16t+q/JjF0q9uOjj9FxECs40IiLSsFTPrLKirM6LwzN5BPOdbfpvecGL+pJY+W7/d03X5TCOxIBxFDFgHEUMGJexMfGY2oe9+OCM6DPo3x54IZmIyBN1ery059MbvfjdcwNibjPf8e/9i4S+61/cv07l/fL9GSLyQuPcDon9SGHQ0Ff9l0wPe+yoaos0/zdmf62LNDR68aBAHHZicWi8XKXT4JsyHp+gjzM3v1ep8kiM7cTSOlr/7eyf9FyMpaNfYlr3n/EqH7R8u8ozcVsoR2LAOIoYMI4iBozL2Ji4csl+L146caRqe7J0d9TP3V6kb0j8SlHgca7rYj/aFRzbXjqOjf/timGHL/hvW5y8+RHVNmyF/6hcpLk56W10Zdfv1W+dDM9gOaOHf0/Bvum/UG07J4euy9b412XL687oDTn6u41jX/ffEPGrOfqF78lqXhB+c0fm9zlHYsA4ihgwLmOn0x2trV68674bVVtltf8S539+bbVq65WX/Glvsr534k6VH16uX4xWssO/dFRxWs/Ike0zTWSD7lv1DJUvtk5R+YwNetL9oLFFeubJt+at9uLFU/TLwAvy9FCstjQ1M2MGZxv54tHDMZbMDI7EgHEUMWAcRQwY57hu/NMS9nR6u7c6d6exOyIFFeUx2y++fyyt27fmTbdeWtyPUjZ/RCb2cSJOPq5v0fzT/FUqL8uP/p1JeEaOWC8Hrzt7vcprmvztunc1hRfPuG1u7duu6465XBtHYsA4ihgwLnPvJ44Tp8sIGrhCPxU0qXiRyt95aHXc64o1I8fzC6ervPiV9L+oLVU4EgPGUcSAcRQxYFzWjYmBWMqf0pPST/vDg1GX/ddSnQ940o+ddv0kVfEBO2PgMI7EgHEUMWAcRQwYx5gYprhtbTrfcyDqsv2n6LwrvLTucjgSA8ZRxIBxFDFgHEUMGEcRA8ZRxIBxFDFgHEUMGEcRA8ZRxIBxFDFgHEUMGEcRA8ZRxIBxFDFgHEUMGEcRA8Yl9EI1x3FOi8jx9HUHSbjRdd3SVK2MfZy1ou7nhIoYQPbhdBowjiIGjKOIAeMoYsA4ihgwjiIGjKOIAeMoYsA4ihgw7n80NlWwnQnAHwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
