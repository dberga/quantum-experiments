{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"resnet18\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 4\n",
    "epochs = 30\n",
    "LR = 0.001\n",
    "n_samples_train = 256 #128\n",
    "n_samples_test = 64 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"MNIST\" # CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "# Set preprocessing transforms\n",
    "list_preprocessing = [\n",
    "    transforms.Resize(input_resolution),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: -0.0005266517400741577 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZ),\n",
      "  CircuitStateFn(\n",
      "       ┌──────────────────────────┐┌──────────────────────────────────────┐\n",
      "  q_0: ┤0                         ├┤0                                     ├\n",
      "       │  ZZFeatureMap(x[0],x[1]) ││  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
      "  q_1: ┤1                         ├┤1                                     ├\n",
      "       └──────────────────────────┘└──────────────────────────────────────┘\n",
      "  )\n",
      "])\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
      "    (1): TorchConnector()\n",
      "    (2): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Network init elapsed time: -0.0002812035381793976 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]+7-4)/2)-4)/2)**2)*16\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: -0.4166314899921417\n",
      "Batch 1, Loss: -0.10218349099159241\n",
      "Batch 2, Loss: -0.6114848852157593\n",
      "Batch 3, Loss: -0.5028235912322998\n",
      "Batch 4, Loss: -0.42455655336380005\n",
      "Batch 5, Loss: -0.636171281337738\n",
      "Batch 6, Loss: -0.4483354091644287\n",
      "Batch 7, Loss: -0.3133126497268677\n",
      "Batch 8, Loss: -0.3969833254814148\n",
      "Batch 9, Loss: -0.39715680480003357\n",
      "Batch 10, Loss: -0.4889727830886841\n",
      "Batch 11, Loss: -0.5164608955383301\n",
      "Batch 12, Loss: -0.3789302110671997\n",
      "Batch 13, Loss: -0.5427126288414001\n",
      "Batch 14, Loss: -0.708215594291687\n",
      "Batch 15, Loss: -0.6985499262809753\n",
      "Batch 16, Loss: -0.7378766536712646\n",
      "Batch 17, Loss: -0.3623179793357849\n",
      "Batch 18, Loss: -0.27029553055763245\n",
      "Batch 19, Loss: -0.5685149431228638\n",
      "Batch 20, Loss: -0.6514142751693726\n",
      "Batch 21, Loss: -0.577258288860321\n",
      "Batch 22, Loss: -0.5802364349365234\n",
      "Batch 23, Loss: -0.35480570793151855\n",
      "Batch 24, Loss: -0.4911598265171051\n",
      "Batch 25, Loss: -0.5271931886672974\n",
      "Batch 26, Loss: -0.2926080822944641\n",
      "Batch 27, Loss: -0.843526303768158\n",
      "Batch 28, Loss: -0.697384238243103\n",
      "Batch 29, Loss: -0.7194449305534363\n",
      "Batch 30, Loss: -0.4572259485721588\n",
      "Batch 31, Loss: -0.5431433916091919\n",
      "Batch 32, Loss: -0.6141362190246582\n",
      "Batch 33, Loss: -0.36689528822898865\n",
      "Batch 34, Loss: -0.35070064663887024\n",
      "Batch 35, Loss: -0.6033852100372314\n",
      "Batch 36, Loss: -0.4548969268798828\n",
      "Batch 37, Loss: -0.44543594121932983\n",
      "Batch 38, Loss: -0.5972563028335571\n",
      "Batch 39, Loss: -0.5024383068084717\n",
      "Batch 40, Loss: -0.6514967679977417\n",
      "Batch 41, Loss: -0.6603968143463135\n",
      "Batch 42, Loss: -0.6876946091651917\n",
      "Batch 43, Loss: -0.7570350766181946\n",
      "Batch 44, Loss: -0.6001711487770081\n",
      "Batch 45, Loss: -0.5310546159744263\n",
      "Batch 46, Loss: -0.5539181232452393\n",
      "Batch 47, Loss: -0.6192642450332642\n",
      "Batch 48, Loss: -0.13466447591781616\n",
      "Batch 49, Loss: -0.7012404799461365\n",
      "Batch 50, Loss: -0.24058692157268524\n",
      "Batch 51, Loss: -0.22037746012210846\n",
      "Batch 52, Loss: -0.4487203061580658\n",
      "Batch 53, Loss: -0.7770158052444458\n",
      "Batch 54, Loss: -0.47996893525123596\n",
      "Batch 55, Loss: -0.41439592838287354\n",
      "Batch 56, Loss: -0.29728594422340393\n",
      "Batch 57, Loss: -0.5897051692008972\n",
      "Batch 58, Loss: -0.41755956411361694\n",
      "Batch 59, Loss: -0.5770999193191528\n",
      "Batch 60, Loss: -0.5432493686676025\n",
      "Batch 61, Loss: -0.49441084265708923\n",
      "Batch 62, Loss: -0.3192816376686096\n",
      "Batch 63, Loss: -0.14559967815876007\n",
      "Batch 64, Loss: -0.3495965600013733\n",
      "Batch 65, Loss: -0.9007227420806885\n",
      "Batch 66, Loss: -0.6396396160125732\n",
      "Batch 67, Loss: -0.507776141166687\n",
      "Batch 68, Loss: -0.4864543676376343\n",
      "Batch 69, Loss: -0.44255709648132324\n",
      "Batch 70, Loss: -0.521665096282959\n",
      "Batch 71, Loss: -0.305630624294281\n",
      "Batch 72, Loss: -0.2414819598197937\n",
      "Batch 73, Loss: -0.4510456621646881\n",
      "Batch 74, Loss: -0.32061195373535156\n",
      "Batch 75, Loss: -0.3133814334869385\n",
      "Batch 76, Loss: -0.5202688574790955\n",
      "Batch 77, Loss: -0.38185492157936096\n",
      "Batch 78, Loss: -0.7187079787254333\n",
      "Batch 79, Loss: -0.2646443545818329\n",
      "Batch 80, Loss: -0.703486442565918\n",
      "Batch 81, Loss: -0.6173604726791382\n",
      "Batch 82, Loss: -0.5096287131309509\n",
      "Batch 83, Loss: -0.26399898529052734\n",
      "Batch 84, Loss: -0.42917847633361816\n",
      "Batch 85, Loss: -0.5834800004959106\n",
      "Batch 86, Loss: -0.21213574707508087\n",
      "Batch 87, Loss: -0.34977924823760986\n",
      "Batch 88, Loss: -0.5146197080612183\n",
      "Batch 89, Loss: -0.5768840312957764\n",
      "Batch 90, Loss: -0.23500506579875946\n",
      "Batch 91, Loss: -0.8141757249832153\n",
      "Batch 92, Loss: -0.6613385677337646\n",
      "Batch 93, Loss: -0.5075829029083252\n",
      "Batch 94, Loss: -0.33994996547698975\n",
      "Batch 95, Loss: -0.5611007213592529\n",
      "Batch 96, Loss: -0.5476435422897339\n",
      "Batch 97, Loss: -0.5231747031211853\n",
      "Batch 98, Loss: -0.3221365809440613\n",
      "Batch 99, Loss: -0.30951839685440063\n",
      "Batch 100, Loss: -0.43463993072509766\n",
      "Batch 101, Loss: -0.5326975584030151\n",
      "Batch 102, Loss: -0.6201928853988647\n",
      "Batch 103, Loss: -0.5038163661956787\n",
      "Batch 104, Loss: -0.2706804573535919\n",
      "Batch 105, Loss: -0.1402190923690796\n",
      "Batch 106, Loss: -0.5481032133102417\n",
      "Batch 107, Loss: -0.712433934211731\n",
      "Batch 108, Loss: -0.5722306966781616\n",
      "Batch 109, Loss: -0.7396311163902283\n",
      "Batch 110, Loss: -0.8319305777549744\n",
      "Batch 111, Loss: -0.8441060781478882\n",
      "Batch 112, Loss: -0.6792032718658447\n",
      "Batch 113, Loss: -0.1884310245513916\n",
      "Batch 114, Loss: -0.5877769589424133\n",
      "Batch 115, Loss: -0.3277844190597534\n",
      "Batch 116, Loss: -0.7567984461784363\n",
      "Batch 117, Loss: -0.2980382442474365\n",
      "Batch 4, Loss: -0.22071170806884766\n",
      "Batch 5, Loss: -0.5872000455856323\n",
      "Batch 6, Loss: -0.5902178287506104\n",
      "Batch 7, Loss: -0.5903091430664062\n",
      "Batch 8, Loss: -0.17227333784103394\n",
      "Batch 9, Loss: -0.3684579133987427\n",
      "Batch 10, Loss: -0.27291300892829895\n",
      "Batch 11, Loss: -0.385062038898468\n",
      "Batch 12, Loss: -0.5211491584777832\n",
      "Batch 13, Loss: -0.5627774000167847\n",
      "Batch 14, Loss: -0.294545441865921\n",
      "Batch 15, Loss: -0.7306917905807495\n",
      "Batch 16, Loss: -0.6868337392807007\n",
      "Batch 17, Loss: -0.7156261801719666\n",
      "Batch 18, Loss: -0.266336590051651\n",
      "Batch 19, Loss: -0.5134121179580688\n",
      "Batch 20, Loss: -0.4598122239112854\n",
      "Batch 21, Loss: -0.571431040763855\n",
      "Batch 22, Loss: -0.5657428503036499\n",
      "Batch 23, Loss: -0.5863227248191833\n",
      "Batch 24, Loss: -0.42884179949760437\n",
      "Batch 25, Loss: -0.35482800006866455\n",
      "Batch 26, Loss: -0.4139378070831299\n",
      "Batch 27, Loss: -0.6279603242874146\n",
      "Batch 28, Loss: -0.5616230964660645\n",
      "Batch 29, Loss: -0.4908355474472046\n",
      "Batch 30, Loss: -0.5509587526321411\n",
      "Batch 31, Loss: -0.6217701435089111\n",
      "Batch 32, Loss: -0.5771883130073547\n",
      "Batch 33, Loss: -0.6244591474533081\n",
      "Batch 34, Loss: -0.4486633539199829\n",
      "Batch 35, Loss: -0.4955085217952728\n",
      "Batch 36, Loss: -0.5259103775024414\n",
      "Batch 37, Loss: -0.3997592329978943\n",
      "Batch 38, Loss: -0.7914242744445801\n",
      "Batch 39, Loss: -0.5872993469238281\n",
      "Batch 40, Loss: -0.22156573832035065\n",
      "Batch 41, Loss: -0.16414064168930054\n",
      "Batch 42, Loss: -0.4401835501194\n",
      "Batch 43, Loss: -0.43005216121673584\n",
      "Batch 44, Loss: -0.5276403427124023\n",
      "Batch 45, Loss: -0.7223561406135559\n",
      "Batch 46, Loss: -0.7747910022735596\n",
      "Batch 47, Loss: -0.15420250594615936\n",
      "Batch 48, Loss: -0.31684744358062744\n",
      "Batch 49, Loss: -0.7488384246826172\n",
      "Batch 50, Loss: -0.2874562740325928\n",
      "Batch 51, Loss: -0.5603455901145935\n",
      "Batch 52, Loss: -0.7034164667129517\n",
      "Batch 53, Loss: -0.6079295873641968\n",
      "Batch 54, Loss: -0.49077340960502625\n",
      "Batch 55, Loss: -0.5852739214897156\n",
      "Batch 56, Loss: -0.4319171905517578\n",
      "Batch 57, Loss: -0.6537936925888062\n",
      "Batch 58, Loss: -0.23046836256980896\n",
      "Batch 59, Loss: -0.5812817811965942\n",
      "Batch 60, Loss: -0.45288002490997314\n",
      "Batch 61, Loss: -0.019750595092773438\n",
      "Batch 62, Loss: -0.3697238862514496\n",
      "Batch 63, Loss: -0.7282193303108215\n",
      "Batch 64, Loss: -0.7113126516342163\n",
      "Batch 65, Loss: -0.5531541109085083\n",
      "Batch 66, Loss: -0.4143926501274109\n",
      "Batch 67, Loss: -0.41599446535110474\n",
      "Batch 68, Loss: -0.43017274141311646\n",
      "Batch 69, Loss: -0.18236929178237915\n",
      "Batch 70, Loss: -0.9479155540466309\n",
      "Batch 71, Loss: -0.4555608332157135\n",
      "Batch 72, Loss: -0.8020166754722595\n",
      "Batch 73, Loss: -0.3042764961719513\n",
      "Batch 74, Loss: -0.6695979833602905\n",
      "Batch 75, Loss: -0.41339555382728577\n",
      "Batch 76, Loss: -0.5009239315986633\n",
      "Batch 77, Loss: -0.5943906307220459\n",
      "Batch 78, Loss: -0.5739320516586304\n",
      "Batch 79, Loss: -0.5360389947891235\n",
      "Batch 80, Loss: -0.02708430588245392\n",
      "Batch 81, Loss: -0.4731253683567047\n",
      "Batch 82, Loss: -0.37163758277893066\n",
      "Batch 83, Loss: -0.5697301626205444\n",
      "Batch 84, Loss: -0.7304869890213013\n",
      "Batch 85, Loss: -0.5257721543312073\n",
      "Batch 86, Loss: -0.644587516784668\n",
      "Batch 87, Loss: -0.56429123878479\n",
      "Batch 88, Loss: -0.40531301498413086\n",
      "Batch 89, Loss: -0.5819928050041199\n",
      "Batch 90, Loss: -0.5625028610229492\n",
      "Batch 91, Loss: -0.8683514595031738\n",
      "Batch 92, Loss: -0.8193759322166443\n",
      "Batch 93, Loss: -0.39142245054244995\n",
      "Batch 94, Loss: -0.4654226303100586\n",
      "Batch 95, Loss: -0.5914734601974487\n",
      "Batch 96, Loss: -0.6574727892875671\n",
      "Batch 97, Loss: -0.5425189733505249\n",
      "Batch 98, Loss: -0.4191376864910126\n",
      "Batch 99, Loss: -0.6293473839759827\n",
      "Batch 100, Loss: -0.10952389240264893\n",
      "Batch 101, Loss: -0.8750913143157959\n",
      "Batch 102, Loss: -0.3286748230457306\n",
      "Batch 103, Loss: -0.63238525390625\n",
      "Batch 104, Loss: -0.5609115362167358\n",
      "Batch 105, Loss: -0.648785412311554\n",
      "Batch 106, Loss: -0.1687018871307373\n",
      "Batch 107, Loss: -0.3966595232486725\n",
      "Batch 108, Loss: -0.6456001996994019\n",
      "Batch 109, Loss: -0.33709537982940674\n",
      "Batch 110, Loss: -0.5719898343086243\n",
      "Batch 111, Loss: -0.3334963321685791\n",
      "Batch 112, Loss: -0.597367525100708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 113, Loss: -0.1482396423816681\n",
      "Batch 114, Loss: -0.7174340486526489\n",
      "Batch 115, Loss: -0.691590428352356\n",
      "Batch 116, Loss: -0.5501775741577148\n",
      "Batch 117, Loss: -0.3070457875728607\n",
      "Batch 118, Loss: -0.8195841908454895\n",
      "Batch 119, Loss: -0.5544508695602417\n",
      "Batch 120, Loss: -0.5713567733764648\n",
      "Batch 121, Loss: -0.3039945960044861\n",
      "Batch 122, Loss: -0.7676150798797607\n",
      "Batch 123, Loss: -0.34004926681518555\n",
      "Batch 124, Loss: -0.6915640830993652\n",
      "Batch 125, Loss: -0.33651190996170044\n",
      "Batch 126, Loss: -0.10793182253837585\n",
      "Batch 127, Loss: -0.1952713429927826\n",
      "Training [7%]\tLoss: -0.5028\n",
      "Batch 0, Loss: -0.6748136281967163\n",
      "Batch 1, Loss: -0.5775800943374634\n",
      "Batch 2, Loss: -0.5898263454437256\n",
      "Batch 3, Loss: -0.6121124029159546\n",
      "Batch 4, Loss: -0.8885807394981384\n",
      "Batch 5, Loss: -0.5715498328208923\n",
      "Batch 6, Loss: -0.7358931303024292\n",
      "Batch 7, Loss: -0.6382240056991577\n",
      "Batch 8, Loss: -0.19505596160888672\n",
      "Batch 9, Loss: -0.47128716111183167\n",
      "Batch 10, Loss: -0.48618897795677185\n",
      "Batch 11, Loss: -0.5504150390625\n",
      "Batch 12, Loss: -0.8593357801437378\n",
      "Batch 13, Loss: -0.7072793245315552\n",
      "Batch 14, Loss: -0.46224838495254517\n",
      "Batch 15, Loss: -0.6136142015457153\n",
      "Batch 16, Loss: -0.9044092893600464\n",
      "Batch 17, Loss: -0.7102206945419312\n",
      "Batch 18, Loss: -0.7614343762397766\n",
      "Batch 19, Loss: -0.23314492404460907\n",
      "Batch 20, Loss: -0.27160871028900146\n",
      "Batch 21, Loss: -0.9932237863540649\n",
      "Batch 22, Loss: -0.6492358446121216\n",
      "Batch 23, Loss: -0.37949442863464355\n",
      "Batch 24, Loss: -0.15029093623161316\n",
      "Batch 25, Loss: -0.4859867990016937\n",
      "Batch 26, Loss: -0.7751506567001343\n",
      "Batch 27, Loss: -0.395150750875473\n",
      "Batch 28, Loss: -0.5673195123672485\n",
      "Batch 29, Loss: -0.6086750626564026\n",
      "Batch 30, Loss: -0.7547720670700073\n",
      "Batch 31, Loss: -0.41597452759742737\n",
      "Batch 32, Loss: -0.23997271060943604\n",
      "Batch 33, Loss: -0.14102056622505188\n",
      "Batch 34, Loss: -0.7091926336288452\n",
      "Batch 35, Loss: -0.7871721982955933\n",
      "Batch 36, Loss: -0.48790356516838074\n",
      "Batch 37, Loss: -0.431124746799469\n",
      "Batch 38, Loss: -0.313060462474823\n",
      "Batch 39, Loss: -0.3245883584022522\n",
      "Batch 40, Loss: -0.4520891010761261\n",
      "Batch 41, Loss: -0.780285656452179\n",
      "Batch 42, Loss: -0.4074068069458008\n",
      "Batch 43, Loss: -0.5619336366653442\n",
      "Batch 44, Loss: -0.5223051905632019\n",
      "Batch 45, Loss: -0.2763863801956177\n",
      "Batch 46, Loss: -0.5336222648620605\n",
      "Batch 47, Loss: -0.2765224575996399\n",
      "Batch 48, Loss: -0.7385377287864685\n",
      "Batch 49, Loss: -0.6387760639190674\n",
      "Batch 50, Loss: -0.1747424155473709\n",
      "Batch 51, Loss: -0.7484164237976074\n",
      "Batch 52, Loss: -0.5078815221786499\n",
      "Batch 53, Loss: -0.7992610931396484\n",
      "Batch 54, Loss: -0.9307724237442017\n",
      "Batch 55, Loss: -0.8207242488861084\n",
      "Batch 56, Loss: -0.5787703990936279\n",
      "Batch 57, Loss: -0.32865244150161743\n",
      "Batch 58, Loss: -0.6629205346107483\n",
      "Batch 59, Loss: -0.3062974810600281\n",
      "Batch 60, Loss: -0.794263482093811\n",
      "Batch 61, Loss: -0.45985114574432373\n",
      "Batch 62, Loss: -0.5069844722747803\n",
      "Batch 63, Loss: -0.5655745267868042\n",
      "Batch 64, Loss: -0.7135231494903564\n",
      "Batch 65, Loss: -0.6773834824562073\n",
      "Batch 66, Loss: -0.3896278142929077\n",
      "Batch 67, Loss: -0.7074891328811646\n",
      "Batch 68, Loss: -0.5039716362953186\n",
      "Batch 69, Loss: -0.6571375131607056\n",
      "Batch 70, Loss: -0.24567362666130066\n",
      "Batch 71, Loss: -0.779258131980896\n",
      "Batch 72, Loss: -0.4351491928100586\n",
      "Batch 73, Loss: -0.22053663432598114\n",
      "Batch 74, Loss: -0.713407039642334\n",
      "Batch 75, Loss: -0.7270873785018921\n",
      "Batch 76, Loss: -0.6070973873138428\n",
      "Batch 77, Loss: -0.5710572004318237\n",
      "Batch 78, Loss: -0.14593535661697388\n",
      "Batch 79, Loss: -0.5033316612243652\n",
      "Batch 80, Loss: -0.5027605891227722\n",
      "Batch 81, Loss: -0.48820677399635315\n",
      "Batch 82, Loss: -0.620137095451355\n",
      "Batch 83, Loss: -0.15039759874343872\n",
      "Batch 84, Loss: -0.20926347374916077\n",
      "Batch 85, Loss: -0.5736764669418335\n",
      "Batch 86, Loss: -0.5318577289581299\n",
      "Batch 87, Loss: -0.6952130794525146\n",
      "Batch 88, Loss: -0.08803106844425201\n",
      "Batch 89, Loss: -0.659213662147522\n",
      "Batch 90, Loss: -0.5444126725196838\n",
      "Batch 91, Loss: -0.6900081634521484\n",
      "Batch 92, Loss: -0.3279050886631012\n",
      "Batch 93, Loss: 0.028827190399169922\n",
      "Batch 94, Loss: -0.5042569637298584\n",
      "Batch 95, Loss: -0.4477148652076721\n",
      "Batch 96, Loss: -0.41621658205986023\n",
      "Batch 97, Loss: -0.6162934899330139\n",
      "Batch 98, Loss: -0.35677045583724976\n",
      "Batch 99, Loss: -0.26220494508743286\n",
      "Batch 100, Loss: -0.7122769355773926\n",
      "Batch 101, Loss: -0.6588883996009827\n",
      "Batch 102, Loss: -0.4547920227050781\n",
      "Batch 103, Loss: -0.3950934112071991\n",
      "Batch 104, Loss: -0.8848795294761658\n",
      "Batch 105, Loss: -0.3030027747154236\n",
      "Batch 106, Loss: -0.5470244288444519\n",
      "Batch 107, Loss: -0.5273570418357849\n",
      "Batch 108, Loss: -0.4905988872051239\n",
      "Batch 109, Loss: -0.5674397349357605\n",
      "Batch 110, Loss: -0.6233683824539185\n",
      "Batch 111, Loss: -0.610383927822113\n",
      "Batch 112, Loss: -0.34486454725265503\n",
      "Batch 113, Loss: -0.5398903489112854\n",
      "Batch 114, Loss: -0.7646020650863647\n",
      "Batch 115, Loss: -0.5370909571647644\n",
      "Batch 116, Loss: -0.6987878084182739\n",
      "Batch 117, Loss: -0.8675961494445801\n",
      "Batch 118, Loss: -0.33264052867889404\n",
      "Batch 119, Loss: -0.41530144214630127\n",
      "Batch 120, Loss: -0.7326713800430298\n",
      "Batch 121, Loss: -0.6638317108154297\n",
      "Batch 122, Loss: -0.3939746618270874\n",
      "Batch 123, Loss: -0.6600403189659119\n",
      "Batch 124, Loss: -0.5149340629577637\n",
      "Batch 125, Loss: -0.9563544392585754\n",
      "Batch 126, Loss: -0.9963124394416809\n",
      "Batch 127, Loss: -0.30125027894973755\n",
      "Training [10%]\tLoss: -0.5410\n",
      "Batch 0, Loss: -0.12627477943897247\n",
      "Batch 1, Loss: -0.7466992735862732\n",
      "Batch 2, Loss: -0.2788011133670807\n",
      "Batch 3, Loss: -0.718131422996521\n",
      "Batch 4, Loss: -0.45422816276550293\n",
      "Batch 5, Loss: -0.5275536775588989\n",
      "Batch 6, Loss: -0.4786252975463867\n",
      "Batch 7, Loss: 0.02428358793258667\n",
      "Batch 8, Loss: -0.5112941265106201\n",
      "Batch 9, Loss: -0.4600134789943695\n",
      "Batch 10, Loss: -0.3348449766635895\n",
      "Batch 11, Loss: -0.6777009963989258\n",
      "Batch 12, Loss: -0.37951985001564026\n",
      "Batch 13, Loss: -0.5970848798751831\n",
      "Batch 14, Loss: -0.8115739226341248\n",
      "Batch 15, Loss: -0.716081976890564\n",
      "Batch 16, Loss: -0.7436776161193848\n",
      "Batch 17, Loss: -0.2728346288204193\n",
      "Batch 18, Loss: -0.5107285380363464\n",
      "Batch 19, Loss: -0.549407958984375\n",
      "Batch 20, Loss: -0.4132491648197174\n",
      "Batch 21, Loss: -0.43850773572921753\n",
      "Batch 22, Loss: -0.2726161777973175\n",
      "Batch 23, Loss: -0.6768847703933716\n",
      "Batch 24, Loss: -0.950737476348877\n",
      "Batch 25, Loss: -0.1352473795413971\n",
      "Batch 26, Loss: -0.5705083012580872\n",
      "Batch 27, Loss: -0.4697704613208771\n",
      "Batch 28, Loss: -0.5889118909835815\n",
      "Batch 29, Loss: -0.5896139740943909\n",
      "Batch 30, Loss: -0.5168905854225159\n",
      "Batch 31, Loss: -0.2998822331428528\n",
      "Batch 32, Loss: -0.7954150438308716\n",
      "Batch 33, Loss: -0.31216394901275635\n",
      "Batch 34, Loss: -0.701972246170044\n",
      "Batch 35, Loss: -0.4958820939064026\n",
      "Batch 36, Loss: -0.659199595451355\n",
      "Batch 37, Loss: -0.41419291496276855\n",
      "Batch 38, Loss: -0.5946739315986633\n",
      "Batch 39, Loss: -0.45919710397720337\n",
      "Batch 40, Loss: -0.4147722125053406\n",
      "Batch 41, Loss: -0.4416351318359375\n",
      "Batch 42, Loss: -0.3185085952281952\n",
      "Batch 43, Loss: -0.08560553193092346\n",
      "Batch 44, Loss: -0.6760090589523315\n",
      "Batch 45, Loss: -0.4465245008468628\n",
      "Batch 46, Loss: -0.5392665863037109\n",
      "Batch 47, Loss: -0.4401733875274658\n",
      "Batch 48, Loss: -0.3458693027496338\n",
      "Batch 49, Loss: -0.38178080320358276\n",
      "Batch 50, Loss: -0.5549625158309937\n",
      "Batch 51, Loss: -0.5831407308578491\n",
      "Batch 52, Loss: -0.43308568000793457\n",
      "Batch 53, Loss: -0.4295618236064911\n",
      "Batch 54, Loss: -0.36687588691711426\n",
      "Batch 55, Loss: -0.3646464943885803\n",
      "Batch 56, Loss: -0.2996894419193268\n",
      "Batch 57, Loss: -0.5825217962265015\n",
      "Batch 58, Loss: -0.30528467893600464\n",
      "Batch 59, Loss: -0.34678757190704346\n",
      "Batch 60, Loss: -0.26842784881591797\n",
      "Batch 61, Loss: -0.4676358699798584\n",
      "Batch 62, Loss: -0.6358733177185059\n",
      "Batch 63, Loss: -0.33705201745033264\n",
      "Batch 64, Loss: -0.12853850424289703\n",
      "Batch 65, Loss: -0.5371130108833313\n",
      "Batch 66, Loss: -0.4344574809074402\n",
      "Batch 67, Loss: -0.6486724019050598\n",
      "Batch 68, Loss: -0.432473361492157\n",
      "Batch 69, Loss: -0.4350193440914154\n",
      "Batch 70, Loss: -0.15812931954860687\n",
      "Batch 71, Loss: -0.5811995267868042\n",
      "Batch 72, Loss: -0.5709952712059021\n",
      "Batch 73, Loss: -0.37842151522636414\n",
      "Batch 74, Loss: -0.41667091846466064\n",
      "Batch 75, Loss: -0.47554731369018555\n",
      "Batch 76, Loss: -0.6858000755310059\n",
      "Batch 77, Loss: -0.9926400184631348\n",
      "Batch 78, Loss: -0.6223287582397461\n",
      "Batch 79, Loss: -0.5970814228057861\n",
      "Batch 80, Loss: -0.48466140031814575\n",
      "Batch 81, Loss: -0.47486987709999084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 82, Loss: -0.2534886598587036\n",
      "Batch 83, Loss: -0.8142794370651245\n",
      "Batch 84, Loss: -0.6910189390182495\n",
      "Batch 85, Loss: -0.40203291177749634\n",
      "Batch 86, Loss: -0.5937817096710205\n",
      "Batch 87, Loss: -0.5151182413101196\n",
      "Batch 88, Loss: -0.2561958134174347\n",
      "Batch 89, Loss: -0.5347195267677307\n",
      "Batch 90, Loss: -0.4451332092285156\n",
      "Batch 91, Loss: -0.45695531368255615\n",
      "Batch 92, Loss: -0.46514344215393066\n",
      "Batch 93, Loss: -0.7831294536590576\n",
      "Batch 94, Loss: -0.6098117232322693\n",
      "Batch 95, Loss: -0.8277483582496643\n",
      "Batch 96, Loss: -0.5348843336105347\n",
      "Batch 97, Loss: -0.5818649530410767\n",
      "Batch 98, Loss: -0.4227563142776489\n",
      "Batch 99, Loss: -0.3933160901069641\n",
      "Batch 100, Loss: -0.530808687210083\n",
      "Batch 101, Loss: -0.9238342046737671\n",
      "Batch 102, Loss: -0.33896347880363464\n",
      "Batch 103, Loss: -0.4428390562534332\n",
      "Batch 104, Loss: -0.7551522254943848\n",
      "Batch 105, Loss: -0.670065701007843\n",
      "Batch 106, Loss: -0.9104184508323669\n",
      "Batch 107, Loss: -0.3385244309902191\n",
      "Batch 108, Loss: -0.7850823402404785\n",
      "Batch 109, Loss: -0.17945386469364166\n",
      "Batch 110, Loss: -0.3637332618236542\n",
      "Batch 111, Loss: -0.3501282036304474\n",
      "Batch 112, Loss: -0.5276930332183838\n",
      "Batch 113, Loss: -0.7242733836174011\n",
      "Batch 114, Loss: -0.527803897857666\n",
      "Batch 115, Loss: -0.6488012075424194\n",
      "Batch 116, Loss: -0.5541943311691284\n",
      "Batch 117, Loss: -0.4077872037887573\n",
      "Batch 118, Loss: -0.23060055077075958\n",
      "Batch 119, Loss: -0.6486563682556152\n",
      "Batch 120, Loss: -0.8085579872131348\n",
      "Batch 121, Loss: -0.7417882084846497\n",
      "Batch 122, Loss: -0.2819652557373047\n",
      "Batch 123, Loss: -0.9142209887504578\n",
      "Batch 124, Loss: -0.022362522780895233\n",
      "Batch 125, Loss: -0.05731368064880371\n",
      "Batch 126, Loss: -0.7079277634620667\n",
      "Batch 127, Loss: -0.7881320118904114\n",
      "Training [13%]\tLoss: -0.5009\n",
      "Batch 0, Loss: -0.42772477865219116\n",
      "Batch 1, Loss: -0.6163159608840942\n",
      "Batch 2, Loss: -0.7207132577896118\n",
      "Batch 3, Loss: -0.39126068353652954\n",
      "Batch 4, Loss: -0.6021311283111572\n",
      "Batch 5, Loss: -0.5573564767837524\n",
      "Batch 6, Loss: -0.7698342800140381\n",
      "Batch 7, Loss: -0.4612043797969818\n",
      "Batch 8, Loss: -0.7658296823501587\n",
      "Batch 9, Loss: -0.4637475907802582\n",
      "Batch 10, Loss: -0.45058608055114746\n",
      "Batch 11, Loss: -0.5048819780349731\n",
      "Batch 12, Loss: -0.3514936566352844\n",
      "Batch 13, Loss: -0.3950239419937134\n",
      "Batch 14, Loss: -0.4594973921775818\n",
      "Batch 15, Loss: -0.36993715167045593\n",
      "Batch 16, Loss: -0.7473536133766174\n",
      "Batch 17, Loss: -0.2733612060546875\n",
      "Batch 18, Loss: -0.3381950259208679\n",
      "Batch 19, Loss: -0.850213348865509\n",
      "Batch 20, Loss: -0.8958311676979065\n",
      "Batch 21, Loss: -0.060330867767333984\n",
      "Batch 22, Loss: -0.6814823150634766\n",
      "Batch 23, Loss: -0.5430791974067688\n",
      "Batch 24, Loss: -0.5093377232551575\n",
      "Batch 25, Loss: -0.5323587656021118\n",
      "Batch 26, Loss: -0.5765951871871948\n",
      "Batch 27, Loss: -0.8574056625366211\n",
      "Batch 28, Loss: -0.5219477415084839\n",
      "Batch 29, Loss: -0.62508225440979\n",
      "Batch 30, Loss: -0.8923178911209106\n",
      "Batch 31, Loss: -0.7998303174972534\n",
      "Batch 32, Loss: -0.8108823895454407\n",
      "Batch 33, Loss: -0.62950199842453\n",
      "Batch 34, Loss: -0.7858835458755493\n",
      "Batch 35, Loss: -0.7387089729309082\n",
      "Batch 36, Loss: -0.14681559801101685\n",
      "Batch 37, Loss: -0.5864741802215576\n",
      "Batch 38, Loss: -0.18718913197517395\n",
      "Batch 39, Loss: -0.1996673047542572\n",
      "Batch 40, Loss: -0.7359673380851746\n",
      "Batch 41, Loss: -0.5145730376243591\n",
      "Batch 42, Loss: -0.5329529047012329\n",
      "Batch 43, Loss: -0.9482709169387817\n",
      "Batch 44, Loss: -0.9210336208343506\n",
      "Batch 45, Loss: -0.961895763874054\n",
      "Batch 46, Loss: -0.5399425029754639\n",
      "Batch 47, Loss: -0.3696943521499634\n",
      "Batch 48, Loss: -0.8914089202880859\n",
      "Batch 49, Loss: -0.35001063346862793\n",
      "Batch 50, Loss: -0.2843821942806244\n",
      "Batch 51, Loss: -0.5033940076828003\n",
      "Batch 52, Loss: -0.6655357480049133\n",
      "Batch 53, Loss: -0.20891478657722473\n",
      "Batch 54, Loss: -0.5515968799591064\n",
      "Batch 55, Loss: -0.579248309135437\n",
      "Batch 56, Loss: -0.8132100701332092\n",
      "Batch 57, Loss: -0.5882642865180969\n",
      "Batch 58, Loss: -0.6652866005897522\n",
      "Batch 59, Loss: -0.5757414102554321\n",
      "Batch 60, Loss: -0.27010220289230347\n",
      "Batch 61, Loss: -0.5354131460189819\n",
      "Batch 62, Loss: -0.8460378050804138\n",
      "Batch 63, Loss: -0.614701509475708\n",
      "Batch 64, Loss: -0.6693131327629089\n",
      "Batch 65, Loss: -0.15069453418254852\n",
      "Batch 66, Loss: -0.8018019795417786\n",
      "Batch 67, Loss: -0.6651071310043335\n",
      "Batch 68, Loss: -0.48610353469848633\n",
      "Batch 69, Loss: -0.6647445559501648\n",
      "Batch 70, Loss: -0.37432658672332764\n",
      "Batch 71, Loss: -0.7207792401313782\n",
      "Batch 72, Loss: -0.9587137699127197\n",
      "Batch 73, Loss: -0.904876172542572\n",
      "Batch 74, Loss: -0.9756777286529541\n",
      "Batch 75, Loss: -0.21717870235443115\n",
      "Batch 76, Loss: -1.0417556762695312\n",
      "Batch 77, Loss: -0.5105003118515015\n",
      "Batch 78, Loss: -0.6346368789672852\n",
      "Batch 79, Loss: -0.6761037111282349\n",
      "Batch 80, Loss: -0.28715160489082336\n",
      "Batch 81, Loss: -0.38225847482681274\n",
      "Batch 82, Loss: -0.24658337235450745\n",
      "Batch 83, Loss: -0.6127445101737976\n",
      "Batch 84, Loss: -0.5285186767578125\n",
      "Batch 85, Loss: -0.7164499759674072\n",
      "Batch 86, Loss: -0.39456698298454285\n",
      "Batch 87, Loss: -1.0590697526931763\n",
      "Batch 88, Loss: -0.10526411235332489\n",
      "Batch 89, Loss: -0.6816890835762024\n",
      "Batch 90, Loss: -0.1445302665233612\n",
      "Batch 91, Loss: -0.23694969713687897\n",
      "Batch 92, Loss: -0.719300389289856\n",
      "Batch 93, Loss: -0.6508711576461792\n",
      "Batch 94, Loss: -0.8208293914794922\n",
      "Batch 95, Loss: -0.20327623188495636\n",
      "Batch 96, Loss: -0.45867082476615906\n",
      "Batch 97, Loss: -0.9758641123771667\n",
      "Batch 98, Loss: -0.7937902212142944\n",
      "Batch 99, Loss: -0.9247120022773743\n",
      "Batch 100, Loss: -0.5556640028953552\n",
      "Batch 101, Loss: -0.7683330178260803\n",
      "Batch 102, Loss: -0.6590331792831421\n",
      "Batch 103, Loss: -0.5762708783149719\n",
      "Batch 104, Loss: -0.582229495048523\n",
      "Batch 105, Loss: -0.5773358345031738\n",
      "Batch 106, Loss: -0.07295572757720947\n",
      "Batch 107, Loss: -0.9786871671676636\n",
      "Batch 108, Loss: -1.1658921241760254\n",
      "Batch 109, Loss: -1.030045509338379\n",
      "Batch 110, Loss: -0.9177380800247192\n",
      "Batch 111, Loss: -0.40498021245002747\n",
      "Batch 112, Loss: -0.557368278503418\n",
      "Batch 113, Loss: -0.4653255343437195\n",
      "Batch 114, Loss: -0.2924044728279114\n",
      "Batch 115, Loss: -0.3870311975479126\n",
      "Batch 116, Loss: -0.1279720962047577\n",
      "Batch 117, Loss: -0.05505748093128204\n",
      "Batch 118, Loss: -0.7109389901161194\n",
      "Batch 119, Loss: -0.6699768304824829\n",
      "Batch 120, Loss: -1.0024237632751465\n",
      "Batch 121, Loss: -0.777397871017456\n",
      "Batch 122, Loss: -0.6671380400657654\n",
      "Batch 123, Loss: -0.13605628907680511\n",
      "Batch 124, Loss: -0.3343088626861572\n",
      "Batch 125, Loss: -0.5254634618759155\n",
      "Batch 126, Loss: -0.8452099561691284\n",
      "Batch 127, Loss: -0.6101298928260803\n",
      "Training [17%]\tLoss: -0.5814\n",
      "Batch 0, Loss: -0.017837107181549072\n",
      "Batch 1, Loss: -0.68951016664505\n",
      "Batch 2, Loss: -0.25087082386016846\n",
      "Batch 3, Loss: -1.042800784111023\n",
      "Batch 4, Loss: -0.38792842626571655\n",
      "Batch 5, Loss: -0.8116469383239746\n",
      "Batch 6, Loss: -0.6100431084632874\n",
      "Batch 7, Loss: -0.6317749619483948\n",
      "Batch 8, Loss: -0.4093964397907257\n",
      "Batch 9, Loss: -0.5757789611816406\n",
      "Batch 10, Loss: -0.2968986928462982\n",
      "Batch 11, Loss: -0.7534728050231934\n",
      "Batch 12, Loss: -0.38913601636886597\n",
      "Batch 13, Loss: -0.48622599244117737\n",
      "Batch 14, Loss: -0.3359641134738922\n",
      "Batch 15, Loss: -0.2837579846382141\n",
      "Batch 16, Loss: -0.5544970631599426\n",
      "Batch 17, Loss: -0.441135048866272\n",
      "Batch 18, Loss: -0.4528541564941406\n",
      "Batch 19, Loss: -0.4629198908805847\n",
      "Batch 20, Loss: -0.9608219861984253\n",
      "Batch 21, Loss: -0.5412643551826477\n",
      "Batch 22, Loss: -0.6341898441314697\n",
      "Batch 23, Loss: -0.7318198680877686\n",
      "Batch 24, Loss: -0.5282717943191528\n",
      "Batch 25, Loss: -0.6734801530838013\n",
      "Batch 26, Loss: -0.7842854261398315\n",
      "Batch 27, Loss: -0.8546451330184937\n",
      "Batch 28, Loss: -0.8954954147338867\n",
      "Batch 29, Loss: -0.5339628458023071\n",
      "Batch 30, Loss: -0.5277919173240662\n",
      "Batch 31, Loss: -0.5376424193382263\n",
      "Batch 32, Loss: -0.2437405288219452\n",
      "Batch 33, Loss: -0.7116712331771851\n",
      "Batch 34, Loss: -0.3786279559135437\n",
      "Batch 35, Loss: -0.45296892523765564\n",
      "Batch 36, Loss: -0.8523250818252563\n",
      "Batch 37, Loss: -0.8900542855262756\n",
      "Batch 38, Loss: -0.8869554996490479\n",
      "Batch 39, Loss: -0.20877543091773987\n",
      "Batch 40, Loss: -0.641403317451477\n",
      "Batch 41, Loss: -0.29137739539146423\n",
      "Batch 42, Loss: -0.6477768421173096\n",
      "Batch 43, Loss: -0.611812949180603\n",
      "Batch 44, Loss: 0.12887689471244812\n",
      "Batch 45, Loss: -0.47869810461997986\n",
      "Batch 46, Loss: -0.6659948825836182\n",
      "Batch 47, Loss: -0.6167211532592773\n",
      "Batch 48, Loss: -0.30268409848213196\n",
      "Batch 49, Loss: -0.7273693084716797\n",
      "Batch 50, Loss: -0.9302780628204346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 51, Loss: -0.6725867390632629\n",
      "Batch 52, Loss: -0.7286524772644043\n",
      "Batch 53, Loss: -0.39106810092926025\n",
      "Batch 54, Loss: -0.7725070118904114\n",
      "Batch 55, Loss: -0.7600772976875305\n",
      "Batch 56, Loss: -0.3178309202194214\n",
      "Batch 57, Loss: -1.157450556755066\n",
      "Batch 58, Loss: -0.4001496434211731\n",
      "Batch 59, Loss: -0.24238459765911102\n",
      "Batch 60, Loss: -1.1380767822265625\n",
      "Batch 61, Loss: -1.0870593786239624\n",
      "Batch 62, Loss: -0.4060516357421875\n",
      "Batch 63, Loss: -0.6530207395553589\n",
      "Batch 64, Loss: -0.30477994680404663\n",
      "Batch 65, Loss: -0.03139546513557434\n",
      "Batch 66, Loss: -1.1161043643951416\n",
      "Batch 67, Loss: -1.0703120231628418\n",
      "Batch 68, Loss: -0.38148418068885803\n",
      "Batch 69, Loss: -0.664427638053894\n",
      "Batch 70, Loss: -0.26320987939834595\n",
      "Batch 71, Loss: -0.7414629459381104\n",
      "Batch 72, Loss: -0.3447725176811218\n",
      "Batch 73, Loss: -0.8242151141166687\n",
      "Batch 74, Loss: -0.8179298043251038\n",
      "Batch 75, Loss: -0.7317405939102173\n",
      "Batch 76, Loss: -0.4664440155029297\n",
      "Batch 77, Loss: -0.6517561674118042\n",
      "Batch 78, Loss: -0.35157305002212524\n",
      "Batch 79, Loss: -0.7724304795265198\n",
      "Batch 80, Loss: -0.23404160141944885\n",
      "Batch 81, Loss: -0.7651771903038025\n",
      "Batch 82, Loss: -1.0035967826843262\n",
      "Batch 83, Loss: -0.3319018483161926\n",
      "Batch 84, Loss: -1.0505120754241943\n",
      "Batch 85, Loss: -0.2658042907714844\n",
      "Batch 86, Loss: -0.7250640988349915\n",
      "Batch 87, Loss: -0.7149393558502197\n",
      "Batch 88, Loss: -0.7483133673667908\n",
      "Batch 89, Loss: -1.154522180557251\n",
      "Batch 90, Loss: -0.35859113931655884\n",
      "Batch 91, Loss: -0.7487220168113708\n",
      "Batch 92, Loss: -1.1467596292495728\n",
      "Batch 93, Loss: -1.0552359819412231\n",
      "Batch 94, Loss: -1.0672199726104736\n",
      "Batch 95, Loss: -0.5271528363227844\n",
      "Batch 96, Loss: -0.6486647129058838\n",
      "Batch 97, Loss: -0.7541702389717102\n",
      "Batch 98, Loss: -1.0644587278366089\n",
      "Batch 99, Loss: -1.0679984092712402\n",
      "Batch 100, Loss: -1.1670799255371094\n",
      "Batch 101, Loss: -1.154285192489624\n",
      "Batch 102, Loss: -0.3855959177017212\n",
      "Batch 103, Loss: -0.1713663935661316\n",
      "Batch 104, Loss: -0.7258431315422058\n",
      "Batch 105, Loss: -0.762290358543396\n",
      "Batch 106, Loss: -1.1695759296417236\n",
      "Batch 107, Loss: -0.3951985239982605\n",
      "Batch 108, Loss: -0.33499687910079956\n",
      "Batch 109, Loss: -0.7585982084274292\n",
      "Batch 110, Loss: -0.8600186705589294\n",
      "Batch 111, Loss: -0.9559093713760376\n",
      "Batch 112, Loss: -1.160099744796753\n",
      "Batch 113, Loss: -0.3604327142238617\n",
      "Batch 114, Loss: -0.37477388978004456\n",
      "Batch 115, Loss: -0.4716726541519165\n",
      "Batch 116, Loss: -0.26210930943489075\n",
      "Batch 117, Loss: -0.8340120315551758\n",
      "Batch 118, Loss: -0.5268809199333191\n",
      "Batch 119, Loss: -0.22023288905620575\n",
      "Batch 120, Loss: 0.08545549213886261\n",
      "Batch 121, Loss: -0.8359441757202148\n",
      "Batch 122, Loss: -0.4279146194458008\n",
      "Batch 123, Loss: -0.03631551191210747\n",
      "Batch 124, Loss: -1.0646013021469116\n",
      "Batch 125, Loss: -1.1970268487930298\n",
      "Batch 126, Loss: -0.22523969411849976\n",
      "Batch 127, Loss: -0.21770766377449036\n",
      "Training [20%]\tLoss: -0.6185\n",
      "Batch 0, Loss: -0.6950663328170776\n",
      "Batch 1, Loss: -0.8599240779876709\n",
      "Batch 2, Loss: -0.8220317363739014\n",
      "Batch 3, Loss: -0.4598309397697449\n",
      "Batch 4, Loss: -0.35136789083480835\n",
      "Batch 5, Loss: -0.6788148283958435\n",
      "Batch 6, Loss: -0.714012086391449\n",
      "Batch 7, Loss: -0.8509207963943481\n",
      "Batch 8, Loss: -1.1373929977416992\n",
      "Batch 9, Loss: -0.028511013835668564\n",
      "Batch 10, Loss: -1.1807496547698975\n",
      "Batch 11, Loss: -1.0746235847473145\n",
      "Batch 12, Loss: -0.5936416983604431\n",
      "Batch 13, Loss: -0.431496798992157\n",
      "Batch 14, Loss: -0.4973233938217163\n",
      "Batch 15, Loss: -1.0262763500213623\n",
      "Batch 16, Loss: 0.021036352962255478\n",
      "Batch 17, Loss: -0.33221858739852905\n",
      "Batch 18, Loss: -1.0523806810379028\n",
      "Batch 19, Loss: -0.2732909917831421\n",
      "Batch 20, Loss: -0.30213889479637146\n",
      "Batch 21, Loss: -0.2179376482963562\n",
      "Batch 22, Loss: -0.45253658294677734\n",
      "Batch 23, Loss: -0.7897277474403381\n",
      "Batch 24, Loss: -0.8055230975151062\n",
      "Batch 25, Loss: -0.5321969985961914\n",
      "Batch 26, Loss: -0.3635103404521942\n",
      "Batch 27, Loss: -0.40049225091934204\n",
      "Batch 28, Loss: -0.4977667033672333\n",
      "Batch 29, Loss: -0.40618520975112915\n",
      "Batch 30, Loss: -0.1441161334514618\n",
      "Batch 31, Loss: -0.9908681511878967\n",
      "Batch 32, Loss: -0.44529324769973755\n",
      "Batch 33, Loss: -0.5972403287887573\n",
      "Batch 34, Loss: -0.41153231263160706\n",
      "Batch 35, Loss: -0.678280234336853\n",
      "Batch 36, Loss: -0.06185474619269371\n",
      "Batch 37, Loss: -0.14745841920375824\n",
      "Batch 38, Loss: -0.764391303062439\n",
      "Batch 39, Loss: -0.5745398998260498\n",
      "Batch 40, Loss: -0.8386125564575195\n",
      "Batch 41, Loss: -0.30208295583724976\n",
      "Batch 42, Loss: -0.4732652008533478\n",
      "Batch 43, Loss: -0.7407917976379395\n",
      "Batch 44, Loss: -0.9250341057777405\n",
      "Batch 45, Loss: -0.41240978240966797\n",
      "Batch 46, Loss: -0.8331355452537537\n",
      "Batch 47, Loss: -0.42749619483947754\n",
      "Batch 48, Loss: -0.4837658703327179\n",
      "Batch 49, Loss: -0.717792272567749\n",
      "Batch 50, Loss: -0.22426392138004303\n",
      "Batch 51, Loss: -0.7813369035720825\n",
      "Batch 52, Loss: -0.6460009217262268\n",
      "Batch 53, Loss: -0.737126886844635\n",
      "Batch 54, Loss: -0.810221254825592\n",
      "Batch 55, Loss: -0.37289488315582275\n",
      "Batch 56, Loss: -1.0191214084625244\n",
      "Batch 57, Loss: -0.14810368418693542\n",
      "Batch 58, Loss: -0.5106879472732544\n",
      "Batch 59, Loss: -0.7530885934829712\n",
      "Batch 60, Loss: -0.9629711508750916\n",
      "Batch 61, Loss: -0.7861881256103516\n",
      "Batch 62, Loss: -1.1776390075683594\n",
      "Batch 63, Loss: -0.4412785768508911\n",
      "Batch 64, Loss: -0.8040748238563538\n",
      "Batch 65, Loss: -0.7574435472488403\n",
      "Batch 66, Loss: -0.6419239640235901\n",
      "Batch 67, Loss: -0.9593605995178223\n",
      "Batch 68, Loss: -0.4305129647254944\n",
      "Batch 69, Loss: -0.9895209074020386\n",
      "Batch 70, Loss: -0.8193029165267944\n",
      "Batch 71, Loss: -1.0005959272384644\n",
      "Batch 72, Loss: -0.8418784737586975\n",
      "Batch 73, Loss: -0.2532579302787781\n",
      "Batch 74, Loss: -0.5161522626876831\n",
      "Batch 75, Loss: -0.36049574613571167\n",
      "Batch 76, Loss: -0.8393582701683044\n",
      "Batch 77, Loss: -0.9236618876457214\n",
      "Batch 78, Loss: -0.7305684685707092\n",
      "Batch 79, Loss: -0.6349202394485474\n",
      "Batch 80, Loss: -0.7529202699661255\n",
      "Batch 81, Loss: -0.8899856805801392\n",
      "Batch 82, Loss: -0.5582448840141296\n",
      "Batch 83, Loss: -0.7469491362571716\n",
      "Batch 84, Loss: -0.5939112901687622\n",
      "Batch 85, Loss: -1.185809850692749\n",
      "Batch 86, Loss: -0.45607322454452515\n",
      "Batch 87, Loss: -1.0005617141723633\n",
      "Batch 88, Loss: -1.1549158096313477\n",
      "Batch 89, Loss: -0.3686566948890686\n",
      "Batch 90, Loss: -0.1609637588262558\n",
      "Batch 91, Loss: -0.43037670850753784\n",
      "Batch 92, Loss: -0.42314329743385315\n",
      "Batch 93, Loss: -0.5950660705566406\n",
      "Batch 94, Loss: -0.7886704206466675\n",
      "Batch 95, Loss: -0.8442354202270508\n",
      "Batch 96, Loss: -0.5710633993148804\n",
      "Batch 97, Loss: -0.8299943208694458\n",
      "Batch 98, Loss: -0.7207672595977783\n",
      "Batch 99, Loss: -0.5891621112823486\n",
      "Batch 100, Loss: -0.7116641998291016\n",
      "Batch 101, Loss: -0.23554475605487823\n",
      "Batch 102, Loss: -0.8439611792564392\n",
      "Batch 103, Loss: -0.6309566497802734\n",
      "Batch 104, Loss: -0.32011282444000244\n",
      "Batch 105, Loss: -0.5629530549049377\n",
      "Batch 106, Loss: -0.5232902765274048\n",
      "Batch 107, Loss: -0.19397898018360138\n",
      "Batch 108, Loss: -0.5115733742713928\n",
      "Batch 109, Loss: -0.5974559783935547\n",
      "Batch 110, Loss: -0.9036416411399841\n",
      "Batch 111, Loss: -0.61943519115448\n",
      "Batch 112, Loss: -0.8899211287498474\n",
      "Batch 113, Loss: -0.7677212357521057\n",
      "Batch 114, Loss: -0.9336047172546387\n",
      "Batch 115, Loss: -0.6442720890045166\n",
      "Batch 116, Loss: -0.8313925266265869\n",
      "Batch 117, Loss: -1.1372485160827637\n",
      "Batch 118, Loss: -0.7852720618247986\n",
      "Batch 119, Loss: -0.9573050737380981\n",
      "Batch 120, Loss: -0.8926135301589966\n",
      "Batch 121, Loss: -0.7882832884788513\n",
      "Batch 122, Loss: -1.1873364448547363\n",
      "Batch 123, Loss: -1.1740072965621948\n",
      "Batch 124, Loss: -0.9403121471405029\n",
      "Batch 125, Loss: -1.1886507272720337\n",
      "Batch 126, Loss: -1.2132868766784668\n",
      "Batch 127, Loss: -0.8993282318115234\n",
      "Training [23%]\tLoss: -0.6656\n",
      "Batch 0, Loss: -0.7423381805419922\n",
      "Batch 1, Loss: -0.7009484767913818\n",
      "Batch 2, Loss: -0.7509774565696716\n",
      "Batch 3, Loss: -0.7470349073410034\n",
      "Batch 4, Loss: -1.2159581184387207\n",
      "Batch 5, Loss: -0.34649360179901123\n",
      "Batch 6, Loss: -0.971278190612793\n",
      "Batch 7, Loss: -1.0226582288742065\n",
      "Batch 8, Loss: -1.10597825050354\n",
      "Batch 9, Loss: -1.0784573554992676\n",
      "Batch 10, Loss: -1.0526801347732544\n",
      "Batch 11, Loss: -0.7354490756988525\n",
      "Batch 12, Loss: -1.1322567462921143\n",
      "Batch 13, Loss: -1.1196397542953491\n",
      "Batch 14, Loss: -0.6887050867080688\n",
      "Batch 15, Loss: -1.1579241752624512\n",
      "Batch 16, Loss: -1.1766629219055176\n",
      "Batch 17, Loss: -1.1956546306610107\n",
      "Batch 18, Loss: -1.1907628774642944\n",
      "Batch 19, Loss: -1.0717766284942627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20, Loss: -1.0141232013702393\n",
      "Batch 21, Loss: -1.0651086568832397\n",
      "Batch 22, Loss: -0.7434109449386597\n",
      "Batch 23, Loss: -0.7731097936630249\n",
      "Batch 24, Loss: -0.7240459322929382\n",
      "Batch 25, Loss: -1.0444295406341553\n",
      "Batch 26, Loss: -1.1149481534957886\n",
      "Batch 27, Loss: -1.152788758277893\n",
      "Batch 28, Loss: -0.7203387022018433\n",
      "Batch 29, Loss: -0.6891164779663086\n",
      "Batch 30, Loss: -0.7136101126670837\n",
      "Batch 31, Loss: -1.2263816595077515\n",
      "Batch 32, Loss: -0.759697437286377\n",
      "Batch 33, Loss: -1.2597377300262451\n",
      "Batch 34, Loss: -1.2614613771438599\n",
      "Batch 35, Loss: -0.8826995491981506\n",
      "Batch 36, Loss: -0.8549783825874329\n",
      "Batch 37, Loss: -1.226097583770752\n",
      "Batch 38, Loss: -0.9453562498092651\n",
      "Batch 39, Loss: -0.9268449544906616\n",
      "Batch 40, Loss: -0.745486855506897\n",
      "Batch 41, Loss: -0.280791312456131\n",
      "Batch 42, Loss: -0.903540313243866\n",
      "Batch 43, Loss: -1.0569478273391724\n",
      "Batch 44, Loss: -1.2414264678955078\n",
      "Batch 45, Loss: -1.2485935688018799\n",
      "Batch 46, Loss: -0.21878460049629211\n",
      "Batch 47, Loss: -0.8764840364456177\n",
      "Batch 48, Loss: -0.8437861800193787\n",
      "Batch 49, Loss: -0.9028204083442688\n",
      "Batch 50, Loss: -0.9103459715843201\n",
      "Batch 51, Loss: -0.7356362342834473\n",
      "Batch 52, Loss: -0.8483434915542603\n",
      "Batch 53, Loss: -1.1207866668701172\n",
      "Batch 54, Loss: -1.0149312019348145\n",
      "Batch 55, Loss: -1.0561673641204834\n",
      "Batch 56, Loss: -1.159301996231079\n",
      "Batch 57, Loss: -0.9875007271766663\n",
      "Batch 58, Loss: -0.23969300091266632\n",
      "Batch 59, Loss: -0.8401629328727722\n",
      "Batch 60, Loss: -0.7706186175346375\n",
      "Batch 61, Loss: -0.86248779296875\n",
      "Batch 62, Loss: -1.103273868560791\n",
      "Batch 63, Loss: -1.0316270589828491\n",
      "Batch 64, Loss: -1.2441755533218384\n",
      "Batch 65, Loss: -0.7406216859817505\n",
      "Batch 66, Loss: -0.9012538194656372\n",
      "Batch 67, Loss: -0.2317868322134018\n",
      "Batch 68, Loss: -1.263906717300415\n",
      "Batch 69, Loss: -0.7084661722183228\n",
      "Batch 70, Loss: -1.2294232845306396\n",
      "Batch 71, Loss: -1.1579368114471436\n",
      "Batch 72, Loss: -0.2884220480918884\n",
      "Batch 73, Loss: -0.9042549729347229\n",
      "Batch 74, Loss: -0.7945809364318848\n",
      "Batch 75, Loss: -1.1270027160644531\n",
      "Batch 76, Loss: -1.3625057935714722\n",
      "Batch 77, Loss: -0.42141711711883545\n",
      "Batch 78, Loss: -1.0298479795455933\n",
      "Batch 79, Loss: -0.656243622303009\n",
      "Batch 80, Loss: -1.1711992025375366\n",
      "Batch 81, Loss: -1.3765829801559448\n",
      "Batch 82, Loss: -0.41402196884155273\n",
      "Batch 83, Loss: -0.9995143413543701\n",
      "Batch 84, Loss: -0.5610791444778442\n",
      "Batch 85, Loss: -0.4063129127025604\n",
      "Batch 86, Loss: -0.4514918625354767\n",
      "Batch 87, Loss: -0.22942039370536804\n",
      "Batch 88, Loss: -0.8782126307487488\n",
      "Batch 89, Loss: -0.9541572332382202\n",
      "Batch 90, Loss: -0.8633061647415161\n",
      "Batch 91, Loss: -0.5371851921081543\n",
      "Batch 92, Loss: -0.40015533566474915\n",
      "Batch 93, Loss: -0.14922937750816345\n",
      "Batch 94, Loss: -0.6268897652626038\n",
      "Batch 95, Loss: -1.1314293146133423\n",
      "Batch 96, Loss: -1.1020877361297607\n",
      "Batch 97, Loss: -0.6996753215789795\n",
      "Batch 98, Loss: -1.1333434581756592\n",
      "Batch 99, Loss: -1.1929597854614258\n",
      "Batch 100, Loss: -0.1348220556974411\n",
      "Batch 101, Loss: -0.7172619104385376\n",
      "Batch 102, Loss: -0.9485307335853577\n",
      "Batch 103, Loss: -1.2300127744674683\n",
      "Batch 104, Loss: -0.8203719854354858\n",
      "Batch 105, Loss: -0.7984293699264526\n",
      "Batch 106, Loss: -0.875438928604126\n",
      "Batch 107, Loss: -1.1817294359207153\n",
      "Batch 108, Loss: -1.2936203479766846\n",
      "Batch 109, Loss: -1.3311057090759277\n",
      "Batch 110, Loss: -0.760289192199707\n",
      "Batch 111, Loss: -0.6946955919265747\n",
      "Batch 112, Loss: -0.9576739072799683\n",
      "Batch 113, Loss: -1.149855613708496\n",
      "Batch 114, Loss: -0.8384444713592529\n",
      "Batch 115, Loss: -1.2262418270111084\n",
      "Batch 116, Loss: -1.117659568786621\n",
      "Batch 117, Loss: -0.9823494553565979\n",
      "Batch 118, Loss: -1.0468146800994873\n",
      "Batch 119, Loss: -1.2873598337173462\n",
      "Batch 120, Loss: -1.3287606239318848\n",
      "Batch 121, Loss: -1.3096251487731934\n",
      "Batch 122, Loss: -1.2088305950164795\n",
      "Batch 123, Loss: -0.45723581314086914\n",
      "Batch 124, Loss: -1.1885042190551758\n",
      "Batch 125, Loss: -0.6771454811096191\n",
      "Batch 126, Loss: -1.260137915611267\n",
      "Batch 127, Loss: -0.7970647811889648\n",
      "Training [27%]\tLoss: -0.9083\n",
      "Batch 0, Loss: -0.7147713899612427\n",
      "Batch 1, Loss: -1.207669973373413\n",
      "Batch 2, Loss: -0.9999717473983765\n",
      "Batch 3, Loss: -1.441019058227539\n",
      "Batch 4, Loss: -1.0300918817520142\n",
      "Batch 5, Loss: -0.6706390976905823\n",
      "Batch 6, Loss: -0.8994567394256592\n",
      "Batch 7, Loss: -0.17353880405426025\n",
      "Batch 8, Loss: -1.1411761045455933\n",
      "Batch 9, Loss: -1.0571013689041138\n",
      "Batch 10, Loss: -0.940547525882721\n",
      "Batch 11, Loss: -0.7869900465011597\n",
      "Batch 12, Loss: -1.1793805360794067\n",
      "Batch 13, Loss: -0.8181098103523254\n",
      "Batch 14, Loss: -0.9221484065055847\n",
      "Batch 15, Loss: -0.7785122394561768\n",
      "Batch 16, Loss: -0.9855641722679138\n",
      "Batch 17, Loss: -1.0824636220932007\n",
      "Batch 18, Loss: -0.9128910899162292\n",
      "Batch 19, Loss: -1.094875693321228\n",
      "Batch 20, Loss: -0.9625760912895203\n",
      "Batch 21, Loss: -0.3982616364955902\n",
      "Batch 22, Loss: -0.7285334467887878\n",
      "Batch 23, Loss: -0.849937379360199\n",
      "Batch 24, Loss: -0.685549259185791\n",
      "Batch 25, Loss: -1.0848828554153442\n",
      "Batch 26, Loss: -0.8394784927368164\n",
      "Batch 27, Loss: -1.325251579284668\n",
      "Batch 28, Loss: -0.6233139634132385\n",
      "Batch 29, Loss: -0.28758615255355835\n",
      "Batch 30, Loss: -0.8796721696853638\n",
      "Batch 31, Loss: -1.1334335803985596\n",
      "Batch 32, Loss: -0.8811860084533691\n",
      "Batch 33, Loss: -0.37988752126693726\n",
      "Batch 34, Loss: -0.6984301805496216\n",
      "Batch 35, Loss: -1.049957036972046\n",
      "Batch 36, Loss: -0.4303458333015442\n",
      "Batch 37, Loss: -0.8343140482902527\n",
      "Batch 38, Loss: -0.6033431887626648\n",
      "Batch 39, Loss: -0.7845993041992188\n",
      "Batch 40, Loss: -0.9495762586593628\n",
      "Batch 41, Loss: -1.1691433191299438\n",
      "Batch 42, Loss: -0.7168850898742676\n",
      "Batch 43, Loss: -1.3811792135238647\n",
      "Batch 44, Loss: -1.1941094398498535\n",
      "Batch 45, Loss: -0.8473718166351318\n",
      "Batch 46, Loss: 0.06232099235057831\n",
      "Batch 47, Loss: -0.7203702926635742\n",
      "Batch 48, Loss: -1.0766870975494385\n",
      "Batch 49, Loss: -0.6624553203582764\n",
      "Batch 50, Loss: -1.1024470329284668\n",
      "Batch 51, Loss: -1.1746379137039185\n",
      "Batch 52, Loss: -1.338233232498169\n",
      "Batch 53, Loss: -1.026302456855774\n",
      "Batch 54, Loss: -1.3532154560089111\n",
      "Batch 55, Loss: -1.0607335567474365\n",
      "Batch 56, Loss: -0.9263523817062378\n",
      "Batch 57, Loss: -0.7622315883636475\n",
      "Batch 58, Loss: -1.0741872787475586\n",
      "Batch 59, Loss: 0.0036765336990356445\n",
      "Batch 60, Loss: -1.2205784320831299\n",
      "Batch 61, Loss: -0.7800579071044922\n",
      "Batch 62, Loss: -0.9885637164115906\n",
      "Batch 63, Loss: -1.0402017831802368\n",
      "Batch 64, Loss: -1.2032305002212524\n",
      "Batch 65, Loss: -0.49318426847457886\n",
      "Batch 66, Loss: -1.3263827562332153\n",
      "Batch 67, Loss: -0.6030457019805908\n",
      "Batch 68, Loss: -0.8512406349182129\n",
      "Batch 69, Loss: -1.4324332475662231\n",
      "Batch 70, Loss: -0.983845055103302\n",
      "Batch 71, Loss: -1.0231759548187256\n",
      "Batch 72, Loss: -1.427375078201294\n",
      "Batch 73, Loss: -0.7584270238876343\n",
      "Batch 74, Loss: -1.0267767906188965\n",
      "Batch 75, Loss: -0.5806853175163269\n",
      "Batch 76, Loss: -1.120934247970581\n",
      "Batch 77, Loss: -1.3499751091003418\n",
      "Batch 78, Loss: -1.0859582424163818\n",
      "Batch 79, Loss: -1.131702184677124\n",
      "Batch 80, Loss: -1.3406915664672852\n",
      "Batch 81, Loss: -1.0944634675979614\n",
      "Batch 82, Loss: -1.222434401512146\n",
      "Batch 83, Loss: -0.5976636409759521\n",
      "Batch 84, Loss: -0.9979501962661743\n",
      "Batch 85, Loss: -0.927798867225647\n",
      "Batch 86, Loss: -0.9789495468139648\n",
      "Batch 87, Loss: -1.10927414894104\n",
      "Batch 88, Loss: -0.0008872151374816895\n",
      "Batch 89, Loss: -0.501197338104248\n",
      "Batch 90, Loss: -0.7058365345001221\n",
      "Batch 91, Loss: -1.1423414945602417\n",
      "Batch 92, Loss: 0.2801375687122345\n",
      "Batch 93, Loss: -1.0642025470733643\n",
      "Batch 94, Loss: -0.9034186601638794\n",
      "Batch 95, Loss: -0.8638904094696045\n",
      "Batch 96, Loss: -1.2669804096221924\n",
      "Batch 97, Loss: -0.6610622406005859\n",
      "Batch 98, Loss: -0.597482442855835\n",
      "Batch 99, Loss: -0.9454168677330017\n",
      "Batch 100, Loss: -1.284027099609375\n",
      "Batch 101, Loss: -1.0984463691711426\n",
      "Batch 102, Loss: -0.7547324895858765\n",
      "Batch 103, Loss: -0.2654002606868744\n",
      "Batch 104, Loss: -0.036038994789123535\n",
      "Batch 105, Loss: -1.392306923866272\n",
      "Batch 106, Loss: -0.7126854062080383\n",
      "Batch 107, Loss: -0.8525028228759766\n",
      "Batch 108, Loss: -0.8870401382446289\n",
      "Batch 109, Loss: -0.595263659954071\n",
      "Batch 110, Loss: -0.21985360980033875\n",
      "Batch 111, Loss: -1.1394951343536377\n",
      "Batch 112, Loss: -0.9170215725898743\n",
      "Batch 113, Loss: -1.0338006019592285\n",
      "Batch 114, Loss: 0.25109028816223145\n",
      "Batch 115, Loss: -0.20339885354042053\n",
      "Batch 116, Loss: -1.1549161672592163\n",
      "Batch 117, Loss: -0.04350283741950989\n",
      "Batch 118, Loss: -1.2183772325515747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 119, Loss: -0.9599918723106384\n",
      "Batch 120, Loss: -1.3477214574813843\n",
      "Batch 121, Loss: -0.5058302283287048\n",
      "Batch 122, Loss: -0.5169326066970825\n",
      "Batch 123, Loss: -1.4677510261535645\n",
      "Batch 124, Loss: -0.3212987184524536\n",
      "Batch 125, Loss: -1.1450920104980469\n",
      "Batch 126, Loss: -0.17790070176124573\n",
      "Batch 127, Loss: -1.3360579013824463\n",
      "Training [30%]\tLoss: -0.8685\n",
      "Batch 0, Loss: -0.49251145124435425\n",
      "Batch 1, Loss: -1.2882521152496338\n",
      "Batch 2, Loss: -1.5236196517944336\n",
      "Batch 3, Loss: 0.0063816457986831665\n",
      "Batch 4, Loss: -1.2343175411224365\n",
      "Batch 5, Loss: -0.9745916128158569\n",
      "Batch 6, Loss: -0.8389434218406677\n",
      "Batch 7, Loss: -0.993798017501831\n",
      "Batch 8, Loss: -1.29555082321167\n",
      "Batch 9, Loss: -0.5119680762290955\n",
      "Batch 10, Loss: -0.4302597939968109\n",
      "Batch 11, Loss: -1.4083819389343262\n",
      "Batch 12, Loss: -0.9877331852912903\n",
      "Batch 13, Loss: -0.1288444846868515\n",
      "Batch 14, Loss: -0.5548493266105652\n",
      "Batch 15, Loss: -0.9021363854408264\n",
      "Batch 16, Loss: -0.2748590409755707\n",
      "Batch 17, Loss: 0.006153777241706848\n",
      "Batch 18, Loss: -1.229893445968628\n",
      "Batch 19, Loss: -0.5291229486465454\n",
      "Batch 20, Loss: -0.5779997110366821\n",
      "Batch 21, Loss: -0.7513575553894043\n",
      "Batch 22, Loss: -1.339714765548706\n",
      "Batch 23, Loss: -1.2332549095153809\n",
      "Batch 24, Loss: -0.6542161703109741\n",
      "Batch 25, Loss: -1.4153907299041748\n",
      "Batch 26, Loss: -0.7244102954864502\n",
      "Batch 27, Loss: -0.868745744228363\n",
      "Batch 28, Loss: -1.296428918838501\n",
      "Batch 29, Loss: -1.119256615638733\n",
      "Batch 30, Loss: -1.4314993619918823\n",
      "Batch 31, Loss: -1.4806296825408936\n",
      "Batch 32, Loss: -0.2719269096851349\n",
      "Batch 33, Loss: -0.9518675208091736\n",
      "Batch 34, Loss: -1.0668187141418457\n",
      "Batch 35, Loss: -1.0137683153152466\n",
      "Batch 36, Loss: -1.4928092956542969\n",
      "Batch 37, Loss: -0.5954618453979492\n",
      "Batch 38, Loss: -0.37401479482650757\n",
      "Batch 39, Loss: -0.8405187129974365\n",
      "Batch 40, Loss: -1.4404665231704712\n",
      "Batch 41, Loss: -1.4024085998535156\n",
      "Batch 42, Loss: -1.2397408485412598\n",
      "Batch 43, Loss: -1.0388402938842773\n",
      "Batch 44, Loss: -0.24322913587093353\n",
      "Batch 45, Loss: -1.2064530849456787\n",
      "Batch 46, Loss: -0.5293613076210022\n",
      "Batch 47, Loss: -0.39815616607666016\n",
      "Batch 48, Loss: -1.2886691093444824\n",
      "Batch 49, Loss: -1.2668155431747437\n",
      "Batch 50, Loss: -0.2540661096572876\n",
      "Batch 51, Loss: -1.3609349727630615\n",
      "Batch 52, Loss: -0.570434033870697\n",
      "Batch 53, Loss: -1.5020747184753418\n",
      "Batch 54, Loss: -0.47130638360977173\n",
      "Batch 55, Loss: -1.4446473121643066\n",
      "Batch 56, Loss: -0.6136037111282349\n",
      "Batch 57, Loss: -1.0101947784423828\n",
      "Batch 58, Loss: -1.0915648937225342\n",
      "Batch 59, Loss: -1.3943967819213867\n",
      "Batch 60, Loss: -1.4840335845947266\n",
      "Batch 61, Loss: -1.579641342163086\n",
      "Batch 62, Loss: -1.2141863107681274\n",
      "Batch 63, Loss: -0.8605620861053467\n",
      "Batch 64, Loss: -1.1925044059753418\n",
      "Batch 65, Loss: -1.0490704774856567\n",
      "Batch 66, Loss: -1.0204923152923584\n",
      "Batch 67, Loss: -0.8304725885391235\n",
      "Batch 68, Loss: -1.300437092781067\n",
      "Batch 69, Loss: -1.5724825859069824\n",
      "Batch 70, Loss: -0.8053916692733765\n",
      "Batch 71, Loss: -1.5414187908172607\n",
      "Batch 72, Loss: -1.4977459907531738\n",
      "Batch 73, Loss: -1.3908472061157227\n",
      "Batch 74, Loss: -1.4987030029296875\n",
      "Batch 75, Loss: -0.8640439510345459\n",
      "Batch 76, Loss: -0.8666203022003174\n",
      "Batch 77, Loss: -0.8956801891326904\n",
      "Batch 78, Loss: -1.4466935396194458\n",
      "Batch 79, Loss: -0.8476445078849792\n",
      "Batch 80, Loss: 0.1616104394197464\n",
      "Batch 81, Loss: -1.227777361869812\n",
      "Batch 82, Loss: -0.3648669719696045\n",
      "Batch 83, Loss: -1.3828849792480469\n",
      "Batch 84, Loss: -1.3643600940704346\n",
      "Batch 85, Loss: -0.8484290838241577\n",
      "Batch 86, Loss: -0.791263222694397\n",
      "Batch 87, Loss: -1.3971214294433594\n",
      "Batch 88, Loss: -1.0023713111877441\n",
      "Batch 89, Loss: -1.381887435913086\n",
      "Batch 90, Loss: -0.7784600257873535\n",
      "Batch 91, Loss: -1.0243158340454102\n",
      "Batch 92, Loss: -1.1961073875427246\n",
      "Batch 93, Loss: -0.8235403299331665\n",
      "Batch 94, Loss: -0.8113304972648621\n",
      "Batch 95, Loss: -1.027126431465149\n",
      "Batch 96, Loss: -0.8906013369560242\n",
      "Batch 97, Loss: -1.265418529510498\n",
      "Batch 98, Loss: -0.9942775964736938\n",
      "Batch 99, Loss: -0.3357400894165039\n",
      "Batch 100, Loss: -0.5054070949554443\n",
      "Batch 101, Loss: -0.7020301818847656\n",
      "Batch 102, Loss: -1.5556986331939697\n",
      "Batch 103, Loss: -0.6073684692382812\n",
      "Batch 104, Loss: -0.9570872187614441\n",
      "Batch 105, Loss: -1.3435969352722168\n",
      "Batch 106, Loss: -1.3164441585540771\n",
      "Batch 107, Loss: -0.6761829853057861\n",
      "Batch 108, Loss: -0.6873622536659241\n",
      "Batch 109, Loss: -1.099670171737671\n",
      "Batch 110, Loss: -0.6749777793884277\n",
      "Batch 111, Loss: -1.7146660089492798\n",
      "Batch 112, Loss: -1.1761503219604492\n",
      "Batch 113, Loss: 0.7596139907836914\n",
      "Batch 114, Loss: -0.7389054298400879\n",
      "Batch 115, Loss: -1.0377346277236938\n",
      "Batch 116, Loss: -1.0273313522338867\n",
      "Batch 117, Loss: -1.0872278213500977\n",
      "Batch 118, Loss: -0.5053974390029907\n",
      "Batch 119, Loss: -1.4343690872192383\n",
      "Batch 120, Loss: -1.0715566873550415\n",
      "Batch 121, Loss: -0.27168774604797363\n",
      "Batch 122, Loss: -1.1941571235656738\n",
      "Batch 123, Loss: -0.6052299737930298\n",
      "Batch 124, Loss: -0.7694641351699829\n",
      "Batch 125, Loss: -0.8736608624458313\n",
      "Batch 126, Loss: -0.8459166884422302\n",
      "Batch 127, Loss: -0.7117124199867249\n",
      "Training [33%]\tLoss: -0.9515\n",
      "Batch 0, Loss: -0.9095778465270996\n",
      "Batch 1, Loss: -0.9766027331352234\n",
      "Batch 2, Loss: -1.485628366470337\n",
      "Batch 3, Loss: -0.8768737316131592\n",
      "Batch 4, Loss: -1.2452725172042847\n",
      "Batch 5, Loss: -0.12269335240125656\n",
      "Batch 6, Loss: -1.520518183708191\n",
      "Batch 7, Loss: -1.2386239767074585\n",
      "Batch 8, Loss: -0.7948142290115356\n",
      "Batch 9, Loss: -0.5730072855949402\n",
      "Batch 10, Loss: -1.2719236612319946\n",
      "Batch 11, Loss: -1.3834776878356934\n",
      "Batch 12, Loss: -1.5395255088806152\n",
      "Batch 13, Loss: -1.3925697803497314\n",
      "Batch 14, Loss: -1.0883018970489502\n",
      "Batch 15, Loss: -0.9677040576934814\n",
      "Batch 16, Loss: -1.4618334770202637\n",
      "Batch 17, Loss: -1.5189793109893799\n",
      "Batch 18, Loss: -1.5108797550201416\n",
      "Batch 19, Loss: -0.873968243598938\n",
      "Batch 20, Loss: -1.45920729637146\n",
      "Batch 21, Loss: -0.02601851522922516\n",
      "Batch 22, Loss: -1.1618517637252808\n",
      "Batch 23, Loss: -1.2381983995437622\n",
      "Batch 24, Loss: -0.9409006237983704\n",
      "Batch 25, Loss: -1.376033067703247\n",
      "Batch 26, Loss: -0.6883592009544373\n",
      "Batch 27, Loss: -1.4077157974243164\n",
      "Batch 28, Loss: -0.669960618019104\n",
      "Batch 29, Loss: -0.7966997027397156\n",
      "Batch 30, Loss: -1.4685323238372803\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
