{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bicycle', 'motorcycle']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"hybridqnn_shallow\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "LR = 0.001\n",
    "n_samples_train = 200 #128\n",
    "n_samples_test = 50 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"CIFAR100\" # MNIST / CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = [\"bicycle\",\"motorcycle\"] # ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e591776b1ad431089d9e42454cb4ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Using downloaded and verified file: ./data/cifar-100-python.tar.gz\n",
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "if network == 'hybridqnn_shallow' or network == 'QSVM':\n",
    "    # Set preprocessing transforms\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "else:\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    \n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: 0.0025852309772744775 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZ),\n",
      "  CircuitStateFn(\n",
      "       ┌──────────────────────────┐┌──────────────────────────────────────┐\n",
      "  q_0: ┤0                         ├┤0                                     ├\n",
      "       │  ZZFeatureMap(x[0],x[1]) ││  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
      "  q_1: ┤1                         ├┤1                                     ├\n",
      "       └──────────────────────────┘└──────────────────────────────────────┘\n",
      "  )\n",
      "])\n",
      "HybridQNN_Shallow(\n",
      "  (conv1): Conv2d(3, 2, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (qnn): TorchConnector()\n",
      "  (fc3): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Network init elapsed time: 1.6599660739302635e-06 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        #n_filts_fc1 = int(((((input_resolution[0])/2)-4)/2)**2)*16 # +7\n",
    "        n_filts_fc1 = 256\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: -1.212534785270691\n",
      "Batch 1, Loss: -1.204883337020874\n",
      "Batch 2, Loss: -1.2001348733901978\n",
      "Batch 3, Loss: -1.3078863620758057\n",
      "Batch 4, Loss: -1.286938190460205\n",
      "Batch 5, Loss: 0.2696937620639801\n",
      "Batch 6, Loss: 0.35107195377349854\n",
      "Batch 7, Loss: 0.348338782787323\n",
      "Batch 8, Loss: -1.3499846458435059\n",
      "Batch 9, Loss: 0.31803587079048157\n",
      "Batch 10, Loss: -1.3334612846374512\n",
      "Batch 11, Loss: -1.3357789516448975\n",
      "Batch 12, Loss: -1.3301243782043457\n",
      "Batch 13, Loss: -1.319766640663147\n",
      "Batch 14, Loss: -1.3508172035217285\n",
      "Batch 15, Loss: -1.3367314338684082\n",
      "Batch 16, Loss: 0.3498566150665283\n",
      "Batch 17, Loss: 0.34401512145996094\n",
      "Batch 18, Loss: -1.3395390510559082\n",
      "Batch 19, Loss: -1.3268282413482666\n",
      "Batch 20, Loss: -1.3329232931137085\n",
      "Batch 21, Loss: 0.2781914174556732\n",
      "Batch 22, Loss: -1.3619310855865479\n",
      "Batch 23, Loss: -1.3608784675598145\n",
      "Batch 24, Loss: 0.36144834756851196\n",
      "Batch 25, Loss: -1.3524246215820312\n",
      "Batch 26, Loss: -1.357405424118042\n",
      "Batch 27, Loss: -1.3542943000793457\n",
      "Batch 28, Loss: 0.3669761121273041\n",
      "Batch 29, Loss: -1.3566279411315918\n",
      "Batch 30, Loss: -1.3353869915008545\n",
      "Batch 31, Loss: 0.34775835275650024\n",
      "Batch 32, Loss: 0.36804598569869995\n",
      "Batch 33, Loss: 0.2888370156288147\n",
      "Batch 34, Loss: 0.37002032995224\n",
      "Batch 35, Loss: 0.3522190451622009\n",
      "Batch 36, Loss: 0.32196712493896484\n",
      "Batch 37, Loss: -1.3363640308380127\n",
      "Batch 38, Loss: 0.3439940810203552\n",
      "Batch 39, Loss: -1.2671188116073608\n",
      "Batch 40, Loss: -1.2993026971817017\n",
      "Batch 41, Loss: -1.3286960124969482\n",
      "Batch 42, Loss: 0.3001895844936371\n",
      "Batch 43, Loss: -1.2696112394332886\n",
      "Batch 44, Loss: -1.3048628568649292\n",
      "Batch 45, Loss: -1.3476462364196777\n",
      "Batch 46, Loss: 0.34058791399002075\n",
      "Batch 47, Loss: -1.2727057933807373\n",
      "Batch 48, Loss: -1.332862377166748\n",
      "Batch 49, Loss: -1.3478503227233887\n",
      "Batch 50, Loss: 0.2849220931529999\n",
      "Batch 51, Loss: 0.32436978816986084\n",
      "Batch 52, Loss: -1.2900737524032593\n",
      "Batch 53, Loss: 0.3498897850513458\n",
      "Batch 54, Loss: -1.3013519048690796\n",
      "Batch 55, Loss: -1.3171769380569458\n",
      "Batch 56, Loss: 0.34479114413261414\n",
      "Batch 57, Loss: -1.3151648044586182\n",
      "Batch 58, Loss: -1.368628978729248\n",
      "Batch 59, Loss: 0.30980777740478516\n",
      "Batch 60, Loss: -1.334820032119751\n",
      "Batch 61, Loss: -1.339423418045044\n",
      "Batch 62, Loss: 0.35686761140823364\n",
      "Batch 63, Loss: -1.3204116821289062\n",
      "Batch 64, Loss: -1.3508459329605103\n",
      "Batch 65, Loss: -1.3437622785568237\n",
      "Batch 66, Loss: -1.3624664545059204\n",
      "Batch 67, Loss: 0.3235534429550171\n",
      "Batch 68, Loss: 0.38084250688552856\n",
      "Batch 69, Loss: -1.3545840978622437\n",
      "Batch 70, Loss: -1.3222492933273315\n",
      "Batch 71, Loss: -1.365136742591858\n",
      "Batch 72, Loss: 0.37681156396865845\n",
      "Batch 73, Loss: -1.3547892570495605\n",
      "Batch 74, Loss: -1.356957197189331\n",
      "Batch 75, Loss: 0.3490614891052246\n",
      "Batch 76, Loss: 0.3575206398963928\n",
      "Batch 77, Loss: -1.3902171850204468\n",
      "Batch 78, Loss: -1.3905049562454224\n",
      "Batch 79, Loss: 0.38678598403930664\n",
      "Batch 80, Loss: -1.3880592584609985\n",
      "Batch 81, Loss: 0.3689606189727783\n",
      "Batch 82, Loss: 0.3900302052497864\n",
      "Batch 83, Loss: -1.3891267776489258\n",
      "Batch 84, Loss: -1.373335361480713\n",
      "Batch 85, Loss: -1.390364646911621\n",
      "Batch 86, Loss: 0.3814584016799927\n",
      "Batch 87, Loss: 0.38768696784973145\n",
      "Batch 88, Loss: 0.3553311824798584\n",
      "Batch 89, Loss: -1.3338626623153687\n",
      "Batch 90, Loss: 0.36613866686820984\n",
      "Batch 91, Loss: -1.3793847560882568\n",
      "Batch 92, Loss: -1.3926523923873901\n",
      "Batch 93, Loss: -1.391817569732666\n",
      "Batch 94, Loss: 0.391615629196167\n",
      "Batch 95, Loss: -1.391903042793274\n",
      "Batch 96, Loss: 0.3515424430370331\n",
      "Batch 97, Loss: 0.395361989736557\n",
      "Batch 98, Loss: -1.3471468687057495\n",
      "Batch 99, Loss: 0.39416563510894775\n",
      "Batch 100, Loss: -1.3810513019561768\n",
      "Batch 101, Loss: -1.3958675861358643\n",
      "Batch 102, Loss: 0.38501882553100586\n",
      "Batch 103, Loss: 0.3889804482460022\n",
      "Batch 104, Loss: 0.39366331696510315\n",
      "Batch 105, Loss: 0.3770741820335388\n",
      "Batch 106, Loss: -1.3935363292694092\n",
      "Batch 107, Loss: -1.3945155143737793\n",
      "Batch 108, Loss: -1.3945704698562622\n",
      "Batch 109, Loss: 0.3671116828918457\n",
      "Batch 110, Loss: 0.38667553663253784\n",
      "Batch 111, Loss: -1.353049397468567\n",
      "Batch 112, Loss: 0.344146728515625\n",
      "Batch 113, Loss: -1.373429298400879\n",
      "Batch 114, Loss: 0.3344775438308716\n",
      "Batch 115, Loss: 0.36218512058258057\n",
      "Batch 116, Loss: -1.3553776741027832\n",
      "Batch 117, Loss: 0.335610032081604\n",
      "Batch 118, Loss: 0.34434497356414795\n",
      "Batch 119, Loss: 0.3313254117965698\n",
      "Batch 120, Loss: -1.3228133916854858\n",
      "Batch 121, Loss: 0.3212937116622925\n",
      "Batch 122, Loss: 0.25715184211730957\n",
      "Batch 123, Loss: -1.3381843566894531\n",
      "Batch 124, Loss: 0.34978875517845154\n",
      "Batch 125, Loss: 0.26975083351135254\n",
      "Batch 126, Loss: 0.2685135006904602\n",
      "Batch 127, Loss: -1.304309606552124\n",
      "Batch 128, Loss: -1.3323745727539062\n",
      "Batch 129, Loss: -1.3154023885726929\n",
      "Batch 130, Loss: 0.3145442008972168\n",
      "Batch 131, Loss: -1.1977282762527466\n",
      "Batch 132, Loss: 0.19864121079444885\n",
      "Batch 133, Loss: 0.2994046211242676\n",
      "Batch 134, Loss: 0.19140851497650146\n",
      "Batch 135, Loss: 0.30786484479904175\n",
      "Batch 136, Loss: -1.1877909898757935\n",
      "Batch 137, Loss: 0.185674250125885\n",
      "Batch 138, Loss: 0.1866372972726822\n",
      "Batch 139, Loss: -1.3188629150390625\n",
      "Batch 140, Loss: 0.17340050637722015\n",
      "Batch 141, Loss: -1.3082870244979858\n",
      "Batch 142, Loss: 0.16419829428195953\n",
      "Batch 143, Loss: -1.3058539628982544\n",
      "Batch 144, Loss: -1.3075566291809082\n",
      "Batch 145, Loss: 0.3137553334236145\n",
      "Batch 146, Loss: -1.3614997863769531\n",
      "Batch 147, Loss: -1.373481035232544\n",
      "Batch 148, Loss: 0.1500919610261917\n",
      "Batch 149, Loss: -1.1572802066802979\n",
      "Batch 150, Loss: 0.33702000975608826\n",
      "Batch 151, Loss: 0.17241449654102325\n",
      "Batch 152, Loss: 0.36159271001815796\n",
      "Batch 153, Loss: 0.3576847314834595\n",
      "Batch 154, Loss: 0.17438824474811554\n",
      "Batch 155, Loss: 0.14171843230724335\n",
      "Batch 156, Loss: 0.1349305510520935\n",
      "Batch 157, Loss: 0.1338542103767395\n",
      "Batch 158, Loss: 0.26239675283432007\n",
      "Batch 159, Loss: 0.1293637901544571\n",
      "Batch 160, Loss: 0.1282251477241516\n",
      "Batch 161, Loss: -1.1265506744384766\n",
      "Batch 162, Loss: 0.1256447583436966\n",
      "Batch 163, Loss: 0.2357768416404724\n",
      "Batch 164, Loss: 0.12652409076690674\n",
      "Batch 165, Loss: -1.1287362575531006\n",
      "Batch 166, Loss: -1.2837839126586914\n",
      "Batch 167, Loss: 0.1273115873336792\n",
      "Batch 168, Loss: -1.1241974830627441\n",
      "Batch 169, Loss: 0.12402970343828201\n",
      "Batch 170, Loss: 0.12396544218063354\n",
      "Batch 171, Loss: -1.126331090927124\n",
      "Batch 172, Loss: -1.20046067237854\n",
      "Batch 173, Loss: -1.1335632801055908\n",
      "Batch 174, Loss: 0.2062024176120758\n",
      "Batch 175, Loss: -1.2380281686782837\n",
      "Batch 176, Loss: 0.12785062193870544\n",
      "Batch 177, Loss: -1.1268596649169922\n",
      "Batch 178, Loss: 0.22973328828811646\n",
      "Batch 179, Loss: -1.1449544429779053\n",
      "Batch 180, Loss: -1.131140112876892\n",
      "Batch 181, Loss: -1.2635176181793213\n",
      "Batch 182, Loss: 0.15046042203903198\n",
      "Batch 183, Loss: 0.2629440724849701\n",
      "Batch 184, Loss: 0.21105742454528809\n",
      "Batch 185, Loss: 0.18143273890018463\n",
      "Batch 186, Loss: -1.2572252750396729\n",
      "Batch 187, Loss: -1.2112051248550415\n",
      "Batch 188, Loss: -1.1452817916870117\n",
      "Batch 189, Loss: -1.1973642110824585\n",
      "Batch 190, Loss: -1.1653496026992798\n",
      "Batch 191, Loss: 0.19056865572929382\n",
      "Batch 192, Loss: 0.272136390209198\n",
      "Batch 193, Loss: 0.2641857862472534\n",
      "Batch 194, Loss: -1.270058035850525\n",
      "Batch 195, Loss: 0.21327251195907593\n",
      "Batch 196, Loss: -1.2197095155715942\n",
      "Batch 197, Loss: 0.22778362035751343\n",
      "Batch 198, Loss: -1.259392499923706\n",
      "Batch 199, Loss: 0.13493648171424866\n",
      "Batch 200, Loss: 0.13132430613040924\n",
      "Batch 201, Loss: -1.3140969276428223\n",
      "Batch 202, Loss: -1.3276946544647217\n",
      "Batch 203, Loss: 0.14279097318649292\n",
      "Batch 204, Loss: 0.3480357527732849\n",
      "Batch 205, Loss: 0.1230163648724556\n",
      "Batch 206, Loss: 0.39649707078933716\n",
      "Batch 207, Loss: 0.22292998433113098\n",
      "Batch 208, Loss: 0.11560602486133575\n",
      "Batch 209, Loss: -1.1178661584854126\n",
      "Batch 210, Loss: 0.3557750880718231\n",
      "Batch 211, Loss: 0.20855265855789185\n",
      "Batch 212, Loss: -1.3760279417037964\n",
      "Batch 213, Loss: -1.168810248374939\n",
      "Batch 214, Loss: -1.1118804216384888\n",
      "Batch 215, Loss: 0.165073961019516\n",
      "Batch 216, Loss: 0.2644604444503784\n",
      "Batch 217, Loss: -1.4029855728149414\n",
      "Batch 218, Loss: 0.24024254083633423\n",
      "Batch 219, Loss: -1.3678045272827148\n",
      "Batch 220, Loss: 0.23708686232566833\n",
      "Batch 221, Loss: -1.1176331043243408\n",
      "Batch 222, Loss: -1.2469395399093628\n",
      "Batch 223, Loss: 0.23222112655639648\n",
      "Batch 224, Loss: -1.176297664642334\n",
      "Batch 225, Loss: -1.2357573509216309\n",
      "Batch 226, Loss: -1.2459131479263306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 227, Loss: -1.1458320617675781\n",
      "Batch 228, Loss: 0.24862298369407654\n",
      "Batch 229, Loss: 0.1562686264514923\n",
      "Batch 230, Loss: 0.1591666340827942\n",
      "Batch 231, Loss: -1.2845388650894165\n",
      "Batch 232, Loss: 0.2716577351093292\n",
      "Batch 233, Loss: -1.428446650505066\n",
      "Batch 234, Loss: -1.4273293018341064\n",
      "Batch 235, Loss: 0.14747804403305054\n",
      "Batch 236, Loss: -1.3231582641601562\n",
      "Batch 237, Loss: 0.3110962212085724\n",
      "Batch 238, Loss: 0.18045935034751892\n",
      "Batch 239, Loss: -1.4220941066741943\n",
      "Batch 240, Loss: -1.3017195463180542\n",
      "Batch 241, Loss: 0.14135940372943878\n",
      "Batch 242, Loss: 0.26609286665916443\n",
      "Batch 243, Loss: 0.13939563930034637\n",
      "Batch 244, Loss: 0.1876448392868042\n",
      "Batch 245, Loss: 0.16585339605808258\n",
      "Batch 246, Loss: 0.26796942949295044\n",
      "Batch 247, Loss: -1.2773778438568115\n",
      "Batch 248, Loss: 0.40692949295043945\n",
      "Batch 249, Loss: -1.133301854133606\n",
      "Batch 250, Loss: 0.3989039361476898\n",
      "Batch 251, Loss: 0.246526300907135\n",
      "Batch 252, Loss: -1.1718372106552124\n",
      "Batch 253, Loss: 0.1386527568101883\n",
      "Batch 254, Loss: 0.2042372226715088\n",
      "Batch 255, Loss: -1.2159234285354614\n",
      "Batch 256, Loss: 0.18971024453639984\n",
      "Batch 257, Loss: 0.19743803143501282\n",
      "Batch 258, Loss: -1.176866888999939\n",
      "Batch 259, Loss: 0.2851361632347107\n",
      "Batch 260, Loss: 0.14265643060207367\n",
      "Batch 261, Loss: 0.2735428214073181\n",
      "Batch 262, Loss: 0.2498929798603058\n",
      "Batch 263, Loss: -1.1754412651062012\n",
      "Batch 264, Loss: 0.10226861387491226\n",
      "Batch 265, Loss: -1.1474021673202515\n",
      "Batch 266, Loss: 0.19414010643959045\n",
      "Batch 267, Loss: 0.1498757302761078\n",
      "Batch 268, Loss: 0.10124122351408005\n",
      "Batch 269, Loss: 0.15892790257930756\n",
      "Batch 270, Loss: -1.1497513055801392\n",
      "Batch 271, Loss: -1.1504920721054077\n",
      "Batch 272, Loss: 0.11424022912979126\n",
      "Batch 273, Loss: -1.1089966297149658\n",
      "Batch 274, Loss: -1.1308127641677856\n",
      "Batch 275, Loss: 0.10693734884262085\n",
      "Batch 276, Loss: -1.1436518430709839\n",
      "Batch 277, Loss: -1.1034516096115112\n",
      "Batch 278, Loss: -1.2071950435638428\n",
      "Batch 279, Loss: -1.2050504684448242\n",
      "Batch 280, Loss: -1.2083977460861206\n",
      "Batch 281, Loss: -1.1240674257278442\n",
      "Batch 282, Loss: -1.167087435722351\n",
      "Batch 283, Loss: -1.102491021156311\n",
      "Batch 284, Loss: -1.1735069751739502\n",
      "Batch 285, Loss: -1.1231920719146729\n",
      "Batch 286, Loss: 0.14350758492946625\n",
      "Batch 287, Loss: 0.10813217610120773\n",
      "Batch 288, Loss: -1.1443160772323608\n",
      "Batch 289, Loss: 0.10821405053138733\n",
      "Batch 290, Loss: 0.1532352715730667\n",
      "Batch 291, Loss: 0.12025828659534454\n",
      "Batch 292, Loss: 0.19714272022247314\n",
      "Batch 293, Loss: 0.13319866359233856\n",
      "Batch 294, Loss: 0.11961548775434494\n",
      "Batch 295, Loss: -1.2767879962921143\n",
      "Batch 296, Loss: 0.10450282692909241\n",
      "Batch 297, Loss: -1.2857136726379395\n",
      "Batch 298, Loss: -1.1055940389633179\n",
      "Batch 299, Loss: -1.2981637716293335\n",
      "Batch 300, Loss: -1.1452158689498901\n",
      "Batch 301, Loss: -1.116241693496704\n",
      "Batch 302, Loss: -1.1155991554260254\n",
      "Batch 303, Loss: -1.182401180267334\n",
      "Batch 304, Loss: -1.1195048093795776\n",
      "Batch 305, Loss: -1.1249903440475464\n",
      "Batch 306, Loss: -1.1067019701004028\n",
      "Batch 307, Loss: -1.126213788986206\n",
      "Batch 308, Loss: 0.1637324094772339\n",
      "Batch 309, Loss: -1.1095677614212036\n",
      "Batch 310, Loss: -1.3646912574768066\n",
      "Batch 311, Loss: 0.11111678928136826\n",
      "Batch 312, Loss: 0.13082385063171387\n",
      "Batch 313, Loss: 0.2960834503173828\n",
      "Batch 314, Loss: 0.11244655400514603\n",
      "Batch 315, Loss: 0.1213264986872673\n",
      "Batch 316, Loss: -1.2032192945480347\n",
      "Batch 317, Loss: -1.1119672060012817\n",
      "Batch 318, Loss: 0.15055763721466064\n",
      "Batch 319, Loss: -1.136852741241455\n",
      "Batch 320, Loss: 0.1792290210723877\n",
      "Batch 321, Loss: 0.12882383167743683\n",
      "Batch 322, Loss: -1.1107903718948364\n",
      "Batch 323, Loss: 0.2307126224040985\n",
      "Batch 324, Loss: -1.1116293668746948\n",
      "Batch 325, Loss: 0.11481257528066635\n",
      "Batch 326, Loss: -1.361143708229065\n",
      "Batch 327, Loss: -1.1645381450653076\n",
      "Batch 328, Loss: 0.19079196453094482\n",
      "Batch 329, Loss: 0.11936400085687637\n",
      "Batch 330, Loss: 0.11647526919841766\n",
      "Batch 331, Loss: 0.22343561053276062\n",
      "Batch 332, Loss: 0.12060444802045822\n",
      "Batch 333, Loss: -1.1125129461288452\n",
      "Batch 334, Loss: 0.1896229386329651\n",
      "Batch 335, Loss: 0.11656968295574188\n",
      "Batch 336, Loss: 0.3488304018974304\n",
      "Batch 337, Loss: 0.2132495641708374\n",
      "Batch 338, Loss: 0.11294102668762207\n",
      "Batch 339, Loss: 0.11163993924856186\n",
      "Batch 340, Loss: 0.19777673482894897\n",
      "Batch 341, Loss: -1.1118885278701782\n",
      "Batch 342, Loss: 0.1087469756603241\n",
      "Batch 343, Loss: 0.17850027978420258\n",
      "Batch 344, Loss: -1.1802641153335571\n",
      "Batch 345, Loss: -1.1060644388198853\n",
      "Batch 346, Loss: -1.1099321842193604\n",
      "Batch 347, Loss: 0.10834012180566788\n",
      "Batch 348, Loss: -1.1833728551864624\n",
      "Batch 349, Loss: 0.1357504427433014\n",
      "Batch 350, Loss: 0.15549057722091675\n",
      "Batch 351, Loss: 0.13888593018054962\n",
      "Batch 352, Loss: -1.1294450759887695\n",
      "Batch 353, Loss: 0.14527849853038788\n",
      "Batch 354, Loss: 0.13174575567245483\n",
      "Batch 355, Loss: 0.18308140337467194\n",
      "Batch 356, Loss: 0.12857262790203094\n",
      "Batch 357, Loss: -1.135486364364624\n",
      "Batch 358, Loss: -1.1302754878997803\n",
      "Batch 359, Loss: 0.1776244044303894\n",
      "Batch 360, Loss: 0.2701883912086487\n",
      "Batch 361, Loss: 0.09902873635292053\n",
      "Batch 362, Loss: -1.1053757667541504\n",
      "Batch 363, Loss: -1.1666793823242188\n",
      "Batch 364, Loss: 0.09589040279388428\n",
      "Batch 365, Loss: -1.1212284564971924\n",
      "Batch 366, Loss: -1.0948671102523804\n",
      "Batch 367, Loss: -1.249648928642273\n",
      "Batch 368, Loss: -1.0955240726470947\n",
      "Batch 369, Loss: -1.0957320928573608\n",
      "Batch 370, Loss: -1.1686972379684448\n",
      "Batch 371, Loss: 0.1703585982322693\n",
      "Batch 372, Loss: 0.25705164670944214\n",
      "Batch 373, Loss: -1.1708498001098633\n",
      "Batch 374, Loss: 0.17114821076393127\n",
      "Batch 375, Loss: -1.0984193086624146\n",
      "Batch 376, Loss: -1.1023820638656616\n",
      "Batch 377, Loss: -1.0980528593063354\n",
      "Batch 378, Loss: -1.1715260744094849\n",
      "Batch 379, Loss: -1.1301326751708984\n",
      "Batch 380, Loss: -1.1088604927062988\n",
      "Batch 381, Loss: 0.17459578812122345\n",
      "Batch 382, Loss: 0.1020888239145279\n",
      "Batch 383, Loss: -1.1022838354110718\n",
      "Batch 384, Loss: 0.2581101953983307\n",
      "Batch 385, Loss: -1.11618971824646\n",
      "Batch 386, Loss: 0.10286448895931244\n",
      "Batch 387, Loss: -1.1030853986740112\n",
      "Batch 388, Loss: -1.103402018547058\n",
      "Batch 389, Loss: 0.136692613363266\n",
      "Batch 390, Loss: 0.24687494337558746\n",
      "Batch 391, Loss: -1.2554385662078857\n",
      "Batch 392, Loss: 0.1373230218887329\n",
      "Batch 393, Loss: -1.1096965074539185\n",
      "Batch 394, Loss: -1.1389304399490356\n",
      "Batch 395, Loss: 0.1313929706811905\n",
      "Batch 396, Loss: -1.1075615882873535\n",
      "Batch 397, Loss: -1.126318335533142\n",
      "Batch 398, Loss: -1.1288444995880127\n",
      "Batch 399, Loss: -1.125196933746338\n",
      "Training [10%]\tLoss: -0.5046\n",
      "Batch 0, Loss: -1.1426540613174438\n",
      "Batch 1, Loss: -1.2816581726074219\n",
      "Batch 2, Loss: -1.111566185951233\n",
      "Batch 3, Loss: -1.2812647819519043\n",
      "Batch 4, Loss: -1.1135873794555664\n",
      "Batch 5, Loss: -1.114936113357544\n",
      "Batch 6, Loss: -1.1766637563705444\n",
      "Batch 7, Loss: -1.1987384557724\n",
      "Batch 8, Loss: -1.1868807077407837\n",
      "Batch 9, Loss: -1.1338660717010498\n",
      "Batch 10, Loss: 0.1180977001786232\n",
      "Batch 11, Loss: -1.2148699760437012\n",
      "Batch 12, Loss: 0.12099733203649521\n",
      "Batch 13, Loss: -1.1702473163604736\n",
      "Batch 14, Loss: 0.3733474016189575\n",
      "Batch 15, Loss: 0.18526136875152588\n",
      "Batch 16, Loss: 0.1291506290435791\n",
      "Batch 17, Loss: -1.1242661476135254\n",
      "Batch 18, Loss: -1.200787901878357\n",
      "Batch 19, Loss: -1.192604422569275\n",
      "Batch 20, Loss: -1.1306540966033936\n",
      "Batch 21, Loss: -1.1265321969985962\n",
      "Batch 22, Loss: -1.4015313386917114\n",
      "Batch 23, Loss: 0.20632803440093994\n",
      "Batch 24, Loss: -1.3895984888076782\n",
      "Batch 25, Loss: -1.1877009868621826\n",
      "Batch 26, Loss: 0.39752668142318726\n",
      "Batch 27, Loss: 0.3802952766418457\n",
      "Batch 28, Loss: 0.1600794643163681\n",
      "Batch 29, Loss: 0.12439406663179398\n",
      "Batch 30, Loss: 0.44101619720458984\n",
      "Batch 31, Loss: 0.13141395151615143\n",
      "Batch 32, Loss: -1.4011075496673584\n",
      "Batch 33, Loss: 0.4400486648082733\n",
      "Batch 34, Loss: -1.1330870389938354\n",
      "Batch 35, Loss: 0.2946023643016815\n",
      "Batch 36, Loss: -1.2716352939605713\n",
      "Batch 37, Loss: 0.3807813227176666\n",
      "Batch 38, Loss: 0.2510559856891632\n",
      "Batch 39, Loss: -1.4326685667037964\n",
      "Batch 40, Loss: 0.12272165715694427\n",
      "Batch 41, Loss: -1.2295764684677124\n",
      "Batch 42, Loss: -1.3741521835327148\n",
      "Batch 43, Loss: 0.14377056062221527\n",
      "Batch 44, Loss: -1.4213159084320068\n",
      "Batch 45, Loss: 0.1947993040084839\n",
      "Batch 46, Loss: -1.1907618045806885\n",
      "Batch 47, Loss: -1.3166158199310303\n",
      "Batch 48, Loss: -1.4171746969223022\n",
      "Batch 49, Loss: -1.362356424331665\n",
      "Batch 50, Loss: -1.356792688369751\n",
      "Batch 51, Loss: 0.13928930461406708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 52, Loss: 0.13365285098552704\n",
      "Batch 53, Loss: 0.3839511275291443\n",
      "Batch 54, Loss: -1.2580293416976929\n",
      "Batch 55, Loss: -1.3887332677841187\n",
      "Batch 56, Loss: 0.19573716819286346\n",
      "Batch 57, Loss: 0.38619837164878845\n",
      "Batch 58, Loss: -1.4323257207870483\n",
      "Batch 59, Loss: -1.126522421836853\n",
      "Batch 60, Loss: -1.4368884563446045\n",
      "Batch 61, Loss: 0.40238475799560547\n",
      "Batch 62, Loss: -1.126842975616455\n",
      "Batch 63, Loss: 0.44617703557014465\n",
      "Batch 64, Loss: 0.3881749212741852\n",
      "Batch 65, Loss: 0.44745099544525146\n",
      "Batch 66, Loss: -1.1365275382995605\n",
      "Batch 67, Loss: -1.3870792388916016\n",
      "Batch 68, Loss: -1.1551655530929565\n",
      "Batch 69, Loss: 0.4199081063270569\n",
      "Batch 70, Loss: 0.4422171711921692\n",
      "Batch 71, Loss: -1.1302642822265625\n",
      "Batch 72, Loss: 0.39761796593666077\n",
      "Batch 73, Loss: 0.12867675721645355\n",
      "Batch 74, Loss: -1.4424341917037964\n",
      "Batch 75, Loss: 0.4417378306388855\n",
      "Batch 76, Loss: -1.2210413217544556\n",
      "Batch 77, Loss: 0.3932170569896698\n",
      "Batch 78, Loss: 0.40859103202819824\n",
      "Batch 79, Loss: 0.19278883934020996\n",
      "Batch 80, Loss: -1.3952726125717163\n",
      "Batch 81, Loss: -1.4111459255218506\n",
      "Batch 82, Loss: 0.36669281125068665\n",
      "Batch 83, Loss: 0.18232449889183044\n",
      "Batch 84, Loss: 0.2996314764022827\n",
      "Batch 85, Loss: -1.1268824338912964\n",
      "Batch 86, Loss: -1.396397590637207\n",
      "Batch 87, Loss: 0.37225884199142456\n",
      "Batch 88, Loss: 0.296078622341156\n",
      "Batch 89, Loss: -1.3434233665466309\n",
      "Batch 90, Loss: 0.17395076155662537\n",
      "Batch 91, Loss: 0.17947852611541748\n",
      "Batch 92, Loss: 0.19004607200622559\n",
      "Batch 93, Loss: 0.3067993223667145\n",
      "Batch 94, Loss: -1.1431576013565063\n",
      "Batch 95, Loss: -1.369798183441162\n",
      "Batch 96, Loss: 0.12402930110692978\n",
      "Batch 97, Loss: 0.1279536485671997\n",
      "Batch 98, Loss: 0.3892545998096466\n",
      "Batch 99, Loss: 0.14374388754367828\n",
      "Batch 100, Loss: -1.2087757587432861\n",
      "Batch 101, Loss: -1.1417388916015625\n",
      "Batch 102, Loss: -1.427220344543457\n",
      "Batch 103, Loss: 0.42611926794052124\n",
      "Batch 104, Loss: -1.3925671577453613\n",
      "Batch 105, Loss: -1.1781463623046875\n",
      "Batch 106, Loss: -1.2686543464660645\n",
      "Batch 107, Loss: -1.2870268821716309\n",
      "Batch 108, Loss: 0.26912710070610046\n",
      "Batch 109, Loss: 0.21335263550281525\n",
      "Batch 110, Loss: -1.2076940536499023\n",
      "Batch 111, Loss: -1.3492486476898193\n",
      "Batch 112, Loss: 0.2974050045013428\n",
      "Batch 113, Loss: -1.153861403465271\n",
      "Batch 114, Loss: 0.15141183137893677\n",
      "Batch 115, Loss: -1.139708399772644\n",
      "Batch 116, Loss: -1.2107479572296143\n",
      "Batch 117, Loss: 0.13724437355995178\n",
      "Batch 118, Loss: 0.16588342189788818\n",
      "Batch 119, Loss: 0.24815204739570618\n",
      "Batch 120, Loss: 0.1689544916152954\n",
      "Batch 121, Loss: 0.24777859449386597\n",
      "Batch 122, Loss: 0.2177577018737793\n",
      "Batch 123, Loss: 0.14098505675792694\n",
      "Batch 124, Loss: -1.208509087562561\n",
      "Batch 125, Loss: 0.13627125322818756\n",
      "Batch 126, Loss: 0.1344294250011444\n",
      "Batch 127, Loss: 0.1990659534931183\n",
      "Batch 128, Loss: 0.1281765103340149\n",
      "Batch 129, Loss: -1.1566981077194214\n",
      "Batch 130, Loss: 0.12328381836414337\n",
      "Batch 131, Loss: -1.2020697593688965\n",
      "Batch 132, Loss: -1.1555771827697754\n",
      "Batch 133, Loss: 0.12535881996154785\n",
      "Batch 134, Loss: 0.1248253881931305\n",
      "Batch 135, Loss: 0.11759558320045471\n",
      "Batch 136, Loss: -1.1233506202697754\n",
      "Batch 137, Loss: 0.19804826378822327\n",
      "Batch 138, Loss: 0.1838303506374359\n",
      "Batch 139, Loss: 0.11778711527585983\n",
      "Batch 140, Loss: 0.11591082066297531\n",
      "Batch 141, Loss: -1.1153067350387573\n",
      "Batch 142, Loss: -1.2673648595809937\n",
      "Batch 143, Loss: 0.27615535259246826\n",
      "Batch 144, Loss: 0.17334899306297302\n",
      "Batch 145, Loss: -1.111260175704956\n",
      "Batch 146, Loss: 0.11410768330097198\n",
      "Batch 147, Loss: -1.1517765522003174\n",
      "Batch 148, Loss: -1.1881777048110962\n",
      "Batch 149, Loss: -1.1112922430038452\n",
      "Batch 150, Loss: 0.15238338708877563\n",
      "Batch 151, Loss: -1.111108660697937\n",
      "Batch 152, Loss: -1.1110916137695312\n",
      "Batch 153, Loss: -1.1518884897232056\n",
      "Batch 154, Loss: 0.11162074655294418\n",
      "Batch 155, Loss: -1.1126123666763306\n",
      "Batch 156, Loss: -1.1523869037628174\n",
      "Batch 157, Loss: -1.2658557891845703\n",
      "Batch 158, Loss: 0.15642377734184265\n",
      "Batch 159, Loss: -1.1131280660629272\n",
      "Batch 160, Loss: 0.11271824687719345\n",
      "Batch 161, Loss: 0.11552397161722183\n",
      "Batch 162, Loss: 0.11366350203752518\n",
      "Batch 163, Loss: -1.115970253944397\n",
      "Batch 164, Loss: -1.1991209983825684\n",
      "Batch 165, Loss: -1.157601237297058\n",
      "Batch 166, Loss: -1.2724510431289673\n",
      "Batch 167, Loss: 0.20383641123771667\n",
      "Batch 168, Loss: -1.2827255725860596\n",
      "Batch 169, Loss: 0.11700968444347382\n",
      "Batch 170, Loss: 0.11411838978528976\n",
      "Batch 171, Loss: -1.1162792444229126\n",
      "Batch 172, Loss: 0.11550386250019073\n",
      "Batch 173, Loss: 0.17116621136665344\n",
      "Batch 174, Loss: -1.3100464344024658\n",
      "Batch 175, Loss: 0.11390134692192078\n",
      "Batch 176, Loss: -1.3178038597106934\n",
      "Batch 177, Loss: 0.22262461483478546\n",
      "Batch 178, Loss: 0.17276740074157715\n",
      "Batch 179, Loss: -1.1162525415420532\n",
      "Batch 180, Loss: -1.1704233884811401\n",
      "Batch 181, Loss: -1.2275261878967285\n",
      "Batch 182, Loss: -1.1733496189117432\n",
      "Batch 183, Loss: -1.1134430170059204\n",
      "Batch 184, Loss: -1.2444484233856201\n",
      "Batch 185, Loss: 0.24888260662555695\n",
      "Batch 186, Loss: -1.1148325204849243\n",
      "Batch 187, Loss: 0.18588677048683167\n",
      "Batch 188, Loss: 0.3618637025356293\n",
      "Batch 189, Loss: 0.37225764989852905\n",
      "Batch 190, Loss: -1.2883235216140747\n",
      "Batch 191, Loss: -1.374617099761963\n",
      "Batch 192, Loss: -1.116331934928894\n",
      "Batch 193, Loss: -1.1198710203170776\n",
      "Batch 194, Loss: 0.1180104985833168\n",
      "Batch 195, Loss: 0.30618056654930115\n",
      "Batch 196, Loss: -1.1935101747512817\n",
      "Batch 197, Loss: -1.3384500741958618\n",
      "Batch 198, Loss: -1.1189990043640137\n",
      "Batch 199, Loss: 0.37638211250305176\n",
      "Batch 200, Loss: 0.12210063636302948\n",
      "Batch 201, Loss: 0.1192638948559761\n",
      "Batch 202, Loss: 0.11829765141010284\n",
      "Batch 203, Loss: 0.12103191763162613\n",
      "Batch 204, Loss: 0.11906609684228897\n",
      "Batch 205, Loss: 0.27064353227615356\n",
      "Batch 206, Loss: 0.11811186373233795\n",
      "Batch 207, Loss: -1.2117705345153809\n",
      "Batch 208, Loss: -1.1987688541412354\n",
      "Batch 209, Loss: -1.1150946617126465\n",
      "Batch 210, Loss: -1.1176317930221558\n",
      "Batch 211, Loss: -1.1188002824783325\n",
      "Batch 212, Loss: 0.11719806492328644\n",
      "Batch 213, Loss: -1.1176247596740723\n",
      "Batch 214, Loss: 0.362029105424881\n",
      "Batch 215, Loss: -1.3650569915771484\n",
      "Batch 216, Loss: -1.3662015199661255\n",
      "Batch 217, Loss: 0.11647558957338333\n",
      "Batch 218, Loss: 0.27135252952575684\n",
      "Batch 219, Loss: 0.11772379279136658\n",
      "Batch 220, Loss: 0.24642133712768555\n",
      "Batch 221, Loss: 0.3748951554298401\n",
      "Batch 222, Loss: 0.11749564856290817\n",
      "Batch 223, Loss: 0.25371384620666504\n",
      "Batch 224, Loss: 0.12309640645980835\n",
      "Batch 225, Loss: 0.11406907439231873\n",
      "Batch 226, Loss: 0.36786139011383057\n",
      "Batch 227, Loss: -1.3713583946228027\n",
      "Batch 228, Loss: 0.368503600358963\n",
      "Batch 229, Loss: -1.2321048974990845\n",
      "Batch 230, Loss: 0.2073608785867691\n",
      "Batch 231, Loss: -1.109886884689331\n",
      "Batch 232, Loss: -1.2148841619491577\n",
      "Batch 233, Loss: -1.1103260517120361\n",
      "Batch 234, Loss: 0.11090276390314102\n",
      "Batch 235, Loss: -1.3670796155929565\n",
      "Batch 236, Loss: -1.1109521389007568\n",
      "Batch 237, Loss: 0.11064288765192032\n",
      "Batch 238, Loss: -1.2021949291229248\n",
      "Batch 239, Loss: 0.36751800775527954\n",
      "Batch 240, Loss: -1.1119133234024048\n",
      "Batch 241, Loss: 0.10960166901350021\n",
      "Batch 242, Loss: 0.11101115494966507\n",
      "Batch 243, Loss: 0.35124343633651733\n",
      "Batch 244, Loss: -1.3164758682250977\n",
      "Batch 245, Loss: -1.1092876195907593\n",
      "Batch 246, Loss: 0.34964480996131897\n",
      "Batch 247, Loss: -1.1089578866958618\n",
      "Batch 248, Loss: -1.1091680526733398\n",
      "Batch 249, Loss: 0.11099333316087723\n",
      "Batch 250, Loss: 0.11031854897737503\n",
      "Batch 251, Loss: -1.2362338304519653\n",
      "Batch 252, Loss: -1.3624751567840576\n",
      "Batch 253, Loss: -1.1098973751068115\n",
      "Batch 254, Loss: 0.11029993742704391\n",
      "Batch 255, Loss: -1.1098289489746094\n",
      "Batch 256, Loss: 0.11821984499692917\n",
      "Batch 257, Loss: -1.1099555492401123\n",
      "Batch 258, Loss: 0.3513163924217224\n",
      "Batch 259, Loss: 0.3571980595588684\n",
      "Batch 260, Loss: -1.2475109100341797\n",
      "Batch 261, Loss: -1.367066502571106\n",
      "Batch 262, Loss: -1.111615777015686\n",
      "Batch 263, Loss: -1.3503968715667725\n",
      "Batch 264, Loss: -1.1116864681243896\n",
      "Batch 265, Loss: 0.1176847517490387\n",
      "Batch 266, Loss: -1.2748231887817383\n",
      "Batch 267, Loss: 0.11272179335355759\n",
      "Batch 268, Loss: -1.2214521169662476\n",
      "Batch 269, Loss: 0.11436496675014496\n",
      "Batch 270, Loss: -1.361914038658142\n",
      "Batch 271, Loss: 0.11656556278467178\n",
      "Batch 272, Loss: -1.3720242977142334\n",
      "Batch 273, Loss: -1.2679235935211182\n",
      "Batch 274, Loss: -1.3710600137710571\n",
      "Batch 275, Loss: 0.35657554864883423\n",
      "Batch 276, Loss: 0.2941749095916748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 277, Loss: -1.3404908180236816\n",
      "Batch 278, Loss: 0.11478281766176224\n",
      "Batch 279, Loss: -1.1212589740753174\n",
      "Batch 280, Loss: 0.13019247353076935\n",
      "Batch 281, Loss: -1.1145234107971191\n",
      "Batch 282, Loss: 0.1146162673830986\n",
      "Batch 283, Loss: 0.26045069098472595\n",
      "Batch 284, Loss: 0.3452807366847992\n",
      "Batch 285, Loss: 0.1288938820362091\n",
      "Batch 286, Loss: -1.1148033142089844\n",
      "Batch 287, Loss: 0.3525395393371582\n",
      "Batch 288, Loss: -1.1159069538116455\n",
      "Batch 289, Loss: 0.3404200077056885\n",
      "Batch 290, Loss: 0.12704837322235107\n",
      "Batch 291, Loss: 0.11620983481407166\n",
      "Batch 292, Loss: 0.11854082345962524\n",
      "Batch 293, Loss: -1.37288236618042\n",
      "Batch 294, Loss: -1.3094364404678345\n",
      "Batch 295, Loss: -1.124942421913147\n",
      "Batch 296, Loss: -1.2479255199432373\n",
      "Batch 297, Loss: 0.36562085151672363\n",
      "Batch 298, Loss: -1.2496775388717651\n",
      "Batch 299, Loss: -1.1210980415344238\n",
      "Batch 300, Loss: -1.1114078760147095\n",
      "Batch 301, Loss: 0.3544827997684479\n",
      "Batch 302, Loss: 0.2756757140159607\n",
      "Batch 303, Loss: 0.1135706678032875\n",
      "Batch 304, Loss: 0.1126178577542305\n",
      "Batch 305, Loss: 0.24469754099845886\n",
      "Batch 306, Loss: 0.26268303394317627\n",
      "Batch 307, Loss: 0.11708320677280426\n",
      "Batch 308, Loss: 0.27754858136177063\n",
      "Batch 309, Loss: -1.122678518295288\n",
      "Batch 310, Loss: 0.36370593309402466\n",
      "Batch 311, Loss: -1.3624333143234253\n",
      "Batch 312, Loss: -1.2305210828781128\n",
      "Batch 313, Loss: 0.2260698676109314\n",
      "Batch 314, Loss: -1.1196104288101196\n",
      "Batch 315, Loss: 0.3644842505455017\n",
      "Batch 316, Loss: 0.22374752163887024\n",
      "Batch 317, Loss: 0.1320330947637558\n",
      "Batch 318, Loss: 0.22968056797981262\n",
      "Batch 319, Loss: -1.1510236263275146\n",
      "Batch 320, Loss: -1.1278526782989502\n",
      "Batch 321, Loss: -1.2274422645568848\n",
      "Batch 322, Loss: 0.11126561462879181\n",
      "Batch 323, Loss: -1.156531810760498\n",
      "Batch 324, Loss: -1.1251224279403687\n",
      "Batch 325, Loss: 0.355466365814209\n",
      "Batch 326, Loss: 0.219133198261261\n",
      "Batch 327, Loss: -1.3612393140792847\n",
      "Batch 328, Loss: -1.3493893146514893\n",
      "Batch 329, Loss: -1.1589956283569336\n",
      "Batch 330, Loss: -1.1343532800674438\n",
      "Batch 331, Loss: 0.13049833476543427\n",
      "Batch 332, Loss: -1.14595365524292\n",
      "Batch 333, Loss: -1.200090765953064\n",
      "Batch 334, Loss: -1.1662589311599731\n",
      "Batch 335, Loss: -1.280776858329773\n",
      "Batch 336, Loss: 0.2049681693315506\n",
      "Batch 337, Loss: 0.2062796950340271\n",
      "Batch 338, Loss: -1.1463898420333862\n",
      "Batch 339, Loss: -1.3212995529174805\n",
      "Batch 340, Loss: -1.3659377098083496\n",
      "Batch 341, Loss: -1.2468500137329102\n",
      "Batch 342, Loss: -1.2497787475585938\n",
      "Batch 343, Loss: -1.166513442993164\n",
      "Batch 344, Loss: 0.35029667615890503\n",
      "Batch 345, Loss: -1.1823803186416626\n",
      "Batch 346, Loss: 0.27292588353157043\n",
      "Batch 347, Loss: 0.3375149667263031\n",
      "Batch 348, Loss: 0.2769155502319336\n",
      "Batch 349, Loss: 0.2904047966003418\n",
      "Batch 350, Loss: -1.370389461517334\n",
      "Batch 351, Loss: 0.29661810398101807\n",
      "Batch 352, Loss: -1.3738137483596802\n",
      "Batch 353, Loss: -1.2533388137817383\n",
      "Batch 354, Loss: 0.2016473412513733\n",
      "Batch 355, Loss: 0.2128833830356598\n",
      "Batch 356, Loss: -1.2514994144439697\n",
      "Batch 357, Loss: -1.221567988395691\n",
      "Batch 358, Loss: 0.2324981391429901\n",
      "Batch 359, Loss: -1.190119981765747\n",
      "Batch 360, Loss: 0.22669650614261627\n",
      "Batch 361, Loss: 0.18220868706703186\n",
      "Batch 362, Loss: 0.18002426624298096\n",
      "Batch 363, Loss: -1.1682415008544922\n",
      "Batch 364, Loss: -1.233964443206787\n",
      "Batch 365, Loss: -1.27540922164917\n",
      "Batch 366, Loss: 0.24724367260932922\n",
      "Batch 367, Loss: 0.19644352793693542\n",
      "Batch 368, Loss: 0.2327370047569275\n",
      "Batch 369, Loss: -1.1628779172897339\n",
      "Batch 370, Loss: 0.21631528437137604\n",
      "Batch 371, Loss: -1.237953782081604\n",
      "Batch 372, Loss: 0.20357386767864227\n",
      "Batch 373, Loss: -1.2074730396270752\n",
      "Batch 374, Loss: -1.1626890897750854\n",
      "Batch 375, Loss: 0.2049938589334488\n",
      "Batch 376, Loss: -1.179266333580017\n",
      "Batch 377, Loss: 0.27419185638427734\n",
      "Batch 378, Loss: -1.1557660102844238\n",
      "Batch 379, Loss: -1.308344841003418\n",
      "Batch 380, Loss: 0.16197991371154785\n",
      "Batch 381, Loss: 0.2311112880706787\n",
      "Batch 382, Loss: 0.20328876376152039\n",
      "Batch 383, Loss: 0.22583381831645966\n",
      "Batch 384, Loss: 0.22668975591659546\n",
      "Batch 385, Loss: 0.19545122981071472\n",
      "Batch 386, Loss: -1.1954162120819092\n",
      "Batch 387, Loss: -1.1445870399475098\n",
      "Batch 388, Loss: -1.2206987142562866\n",
      "Batch 389, Loss: -1.2137279510498047\n",
      "Batch 390, Loss: 0.15247169137001038\n",
      "Batch 391, Loss: -1.1559494733810425\n",
      "Batch 392, Loss: 0.1616712510585785\n",
      "Batch 393, Loss: 0.14299944043159485\n",
      "Batch 394, Loss: 0.16490337252616882\n",
      "Batch 395, Loss: -1.1976544857025146\n",
      "Batch 396, Loss: -1.178745985031128\n",
      "Batch 397, Loss: -1.1755300760269165\n",
      "Batch 398, Loss: 0.1467413604259491\n",
      "Batch 399, Loss: 0.18521448969841003\n",
      "Training [20%]\tLoss: -0.5000\n",
      "Batch 0, Loss: 0.14025616645812988\n",
      "Batch 1, Loss: 0.20797626674175262\n",
      "Batch 2, Loss: -1.2181077003479004\n",
      "Batch 3, Loss: -1.2085797786712646\n",
      "Batch 4, Loss: -1.1421477794647217\n",
      "Batch 5, Loss: -1.2078863382339478\n",
      "Batch 6, Loss: 0.18302702903747559\n",
      "Batch 7, Loss: 0.17777326703071594\n",
      "Batch 8, Loss: -1.2086248397827148\n",
      "Batch 9, Loss: -1.1453421115875244\n",
      "Batch 10, Loss: -1.1219178438186646\n",
      "Batch 11, Loss: 0.1843540370464325\n",
      "Batch 12, Loss: -1.1299641132354736\n",
      "Batch 13, Loss: -1.2367336750030518\n",
      "Batch 14, Loss: -1.215999960899353\n",
      "Batch 15, Loss: 0.22252257168293\n",
      "Batch 16, Loss: 0.11696896702051163\n",
      "Batch 17, Loss: 0.14418992400169373\n",
      "Batch 18, Loss: -1.123237133026123\n",
      "Batch 19, Loss: 0.1977619230747223\n",
      "Batch 20, Loss: 0.2794508635997772\n",
      "Batch 21, Loss: -1.1374331712722778\n",
      "Batch 22, Loss: -1.1366456747055054\n",
      "Batch 23, Loss: 0.12501351535320282\n",
      "Batch 24, Loss: -1.1895803213119507\n",
      "Batch 25, Loss: -1.1546626091003418\n",
      "Batch 26, Loss: 0.1311367005109787\n",
      "Batch 27, Loss: -1.2125742435455322\n",
      "Batch 28, Loss: 0.1916622519493103\n",
      "Batch 29, Loss: -1.192038655281067\n",
      "Batch 30, Loss: -1.1335194110870361\n",
      "Batch 31, Loss: 0.20201730728149414\n",
      "Batch 32, Loss: -1.1930500268936157\n",
      "Batch 33, Loss: 0.22069886326789856\n",
      "Batch 34, Loss: -1.1670188903808594\n",
      "Batch 35, Loss: 0.22200274467468262\n",
      "Batch 36, Loss: 0.13113310933113098\n",
      "Batch 37, Loss: 0.20523399114608765\n",
      "Batch 38, Loss: 0.14964407682418823\n",
      "Batch 39, Loss: 0.13085612654685974\n",
      "Batch 40, Loss: -1.132167100906372\n",
      "Batch 41, Loss: -1.175285816192627\n",
      "Batch 42, Loss: -1.130689024925232\n",
      "Batch 43, Loss: -1.1297261714935303\n",
      "Batch 44, Loss: -1.202716588973999\n",
      "Batch 45, Loss: 0.1341266632080078\n",
      "Batch 46, Loss: 0.18310010433197021\n",
      "Batch 47, Loss: 0.13509033620357513\n",
      "Batch 48, Loss: -1.138995885848999\n",
      "Batch 49, Loss: 0.16246628761291504\n",
      "Batch 50, Loss: 0.14859750866889954\n",
      "Batch 51, Loss: -1.2251074314117432\n",
      "Batch 52, Loss: 0.2190159261226654\n",
      "Batch 53, Loss: -1.1323885917663574\n",
      "Batch 54, Loss: 0.19591768085956573\n",
      "Batch 55, Loss: -1.1261018514633179\n",
      "Batch 56, Loss: 0.19784238934516907\n",
      "Batch 57, Loss: 0.20520633459091187\n",
      "Batch 58, Loss: 0.1745539903640747\n",
      "Batch 59, Loss: -1.1782512664794922\n",
      "Batch 60, Loss: -1.1579556465148926\n",
      "Batch 61, Loss: 0.18840275704860687\n",
      "Batch 62, Loss: 0.17335698008537292\n",
      "Batch 63, Loss: -1.161649227142334\n",
      "Batch 64, Loss: -1.205424189567566\n",
      "Batch 65, Loss: 0.13613998889923096\n",
      "Batch 66, Loss: 0.13617220520973206\n",
      "Batch 67, Loss: 0.13954339921474457\n",
      "Batch 68, Loss: -1.1855921745300293\n",
      "Batch 69, Loss: 0.136574387550354\n",
      "Batch 70, Loss: 0.1556553989648819\n",
      "Batch 71, Loss: 0.15902838110923767\n",
      "Batch 72, Loss: 0.13130046427249908\n",
      "Batch 73, Loss: -1.1303563117980957\n",
      "Batch 74, Loss: 0.12731009721755981\n",
      "Batch 75, Loss: -1.131350040435791\n",
      "Batch 76, Loss: -1.1277447938919067\n",
      "Batch 77, Loss: -1.1234989166259766\n",
      "Batch 78, Loss: -1.208632469177246\n",
      "Batch 79, Loss: -1.1984269618988037\n",
      "Batch 80, Loss: -1.1324995756149292\n",
      "Batch 81, Loss: -1.1883702278137207\n",
      "Batch 82, Loss: 0.16806921362876892\n",
      "Batch 83, Loss: 0.15585248172283173\n",
      "Batch 84, Loss: 0.12350669503211975\n",
      "Batch 85, Loss: -1.1261098384857178\n",
      "Batch 86, Loss: 0.17708727717399597\n",
      "Batch 87, Loss: -1.2024180889129639\n",
      "Batch 88, Loss: 0.17185348272323608\n",
      "Batch 89, Loss: -1.2269041538238525\n",
      "Batch 90, Loss: 0.14444592595100403\n",
      "Batch 91, Loss: -1.124955177307129\n",
      "Batch 92, Loss: -1.208519458770752\n",
      "Batch 93, Loss: 0.19794541597366333\n",
      "Batch 94, Loss: 0.20089778304100037\n",
      "Batch 95, Loss: -1.179567813873291\n",
      "Batch 96, Loss: 0.18966488540172577\n",
      "Batch 97, Loss: -1.2387229204177856\n",
      "Batch 98, Loss: 0.12483541667461395\n",
      "Batch 99, Loss: -1.121719479560852\n",
      "Batch 100, Loss: -1.2223094701766968\n",
      "Batch 101, Loss: -1.2294681072235107\n",
      "Batch 102, Loss: 0.13687606155872345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 103, Loss: 0.23197901248931885\n",
      "Batch 104, Loss: -1.1282145977020264\n",
      "Batch 105, Loss: 0.13091714680194855\n",
      "Batch 106, Loss: 0.2406211793422699\n",
      "Batch 107, Loss: 0.21779870986938477\n",
      "Batch 108, Loss: -1.1209585666656494\n",
      "Batch 109, Loss: 0.12753860652446747\n",
      "Batch 110, Loss: 0.22556188702583313\n",
      "Batch 111, Loss: 0.19534078240394592\n",
      "Batch 112, Loss: 0.18347668647766113\n",
      "Batch 113, Loss: -1.135793685913086\n",
      "Batch 114, Loss: 0.1312498301267624\n",
      "Batch 115, Loss: 0.12333772331476212\n",
      "Batch 116, Loss: 0.15948225557804108\n",
      "Batch 117, Loss: -1.1242719888687134\n",
      "Batch 118, Loss: -1.1489896774291992\n",
      "Batch 119, Loss: -1.1227765083312988\n",
      "Batch 120, Loss: -1.1558995246887207\n",
      "Batch 121, Loss: -1.1710214614868164\n",
      "Batch 122, Loss: -1.166448950767517\n",
      "Batch 123, Loss: -1.1743807792663574\n",
      "Batch 124, Loss: -1.1309890747070312\n",
      "Batch 125, Loss: 0.12644730508327484\n",
      "Batch 126, Loss: 0.1311635822057724\n",
      "Batch 127, Loss: -1.1811727285385132\n",
      "Batch 128, Loss: 0.15793026983737946\n",
      "Batch 129, Loss: -1.12513267993927\n",
      "Batch 130, Loss: -1.1430381536483765\n",
      "Batch 131, Loss: 0.12725497782230377\n",
      "Batch 132, Loss: -1.129062533378601\n",
      "Batch 133, Loss: -1.1262930631637573\n",
      "Batch 134, Loss: 0.15334437787532806\n",
      "Batch 135, Loss: 0.168649360537529\n",
      "Batch 136, Loss: 0.131546288728714\n",
      "Batch 137, Loss: 0.18219801783561707\n",
      "Batch 138, Loss: 0.13106121122837067\n",
      "Batch 139, Loss: 0.13931168615818024\n",
      "Batch 140, Loss: 0.15593470633029938\n",
      "Batch 141, Loss: -1.1730684041976929\n",
      "Batch 142, Loss: -1.1265056133270264\n",
      "Batch 143, Loss: 0.12704992294311523\n",
      "Batch 144, Loss: -1.1324386596679688\n",
      "Batch 145, Loss: -1.1441022157669067\n",
      "Batch 146, Loss: 0.14938394725322723\n",
      "Batch 147, Loss: -1.1278859376907349\n",
      "Batch 148, Loss: 0.1608906090259552\n",
      "Batch 149, Loss: -1.167197585105896\n",
      "Batch 150, Loss: 0.15461735427379608\n",
      "Batch 151, Loss: -1.1379122734069824\n",
      "Batch 152, Loss: -1.1423879861831665\n",
      "Batch 153, Loss: -1.1446621417999268\n",
      "Batch 154, Loss: -1.1469197273254395\n",
      "Batch 155, Loss: -1.149833083152771\n",
      "Batch 156, Loss: 0.14342322945594788\n",
      "Batch 157, Loss: 0.15317118167877197\n",
      "Batch 158, Loss: -1.1492276191711426\n",
      "Batch 159, Loss: -1.1470454931259155\n",
      "Batch 160, Loss: -1.167362928390503\n",
      "Batch 161, Loss: 0.14989319443702698\n",
      "Batch 162, Loss: -1.1643117666244507\n",
      "Batch 163, Loss: -1.1616624593734741\n",
      "Batch 164, Loss: -1.1565792560577393\n",
      "Batch 165, Loss: 0.1515364944934845\n",
      "Batch 166, Loss: -1.1808760166168213\n",
      "Batch 167, Loss: 0.14827126264572144\n",
      "Batch 168, Loss: -1.1545016765594482\n",
      "Batch 169, Loss: -1.1716575622558594\n",
      "Batch 170, Loss: -1.1947602033615112\n",
      "Batch 171, Loss: -1.1646311283111572\n",
      "Batch 172, Loss: -1.1766941547393799\n",
      "Batch 173, Loss: 0.15843021869659424\n",
      "Batch 174, Loss: 0.19370192289352417\n",
      "Batch 175, Loss: -1.2195699214935303\n",
      "Batch 176, Loss: 0.15535280108451843\n",
      "Batch 177, Loss: 0.17575129866600037\n",
      "Batch 178, Loss: 0.17773081362247467\n",
      "Batch 179, Loss: 0.1581791192293167\n",
      "Batch 180, Loss: 0.2603316009044647\n",
      "Batch 181, Loss: 0.16620486974716187\n",
      "Batch 182, Loss: 0.18096467852592468\n",
      "Batch 183, Loss: 0.15034323930740356\n",
      "Batch 184, Loss: -1.1719236373901367\n",
      "Batch 185, Loss: -1.255781650543213\n",
      "Batch 186, Loss: -1.2152272462844849\n",
      "Batch 187, Loss: -1.2044312953948975\n",
      "Batch 188, Loss: -1.1595546007156372\n",
      "Batch 189, Loss: -1.1726895570755005\n",
      "Batch 190, Loss: 0.2122730016708374\n",
      "Batch 191, Loss: -1.1979470252990723\n",
      "Batch 192, Loss: 0.21253123879432678\n",
      "Batch 193, Loss: 0.16695991158485413\n",
      "Batch 194, Loss: -1.2525272369384766\n",
      "Batch 195, Loss: -1.1594772338867188\n",
      "Batch 196, Loss: -1.2013394832611084\n",
      "Batch 197, Loss: -1.1781971454620361\n",
      "Batch 198, Loss: 0.20313379168510437\n",
      "Batch 199, Loss: 0.1953541338443756\n",
      "Batch 200, Loss: 0.16867277026176453\n",
      "Batch 201, Loss: -1.2069517374038696\n",
      "Batch 202, Loss: 0.1755080223083496\n",
      "Batch 203, Loss: 0.2787083387374878\n",
      "Batch 204, Loss: -1.245740294456482\n",
      "Batch 205, Loss: 0.2352384477853775\n",
      "Batch 206, Loss: 0.2068655639886856\n",
      "Batch 207, Loss: -1.2584552764892578\n",
      "Batch 208, Loss: -1.3138463497161865\n",
      "Batch 209, Loss: 0.2415703535079956\n",
      "Batch 210, Loss: -1.17751145362854\n",
      "Batch 211, Loss: 0.15610945224761963\n",
      "Batch 212, Loss: -1.3783910274505615\n",
      "Batch 213, Loss: 0.39510515332221985\n",
      "Batch 214, Loss: 0.328329861164093\n",
      "Batch 215, Loss: -1.1615012884140015\n",
      "Batch 216, Loss: 0.1640465259552002\n",
      "Batch 217, Loss: -1.377669095993042\n",
      "Batch 218, Loss: 0.3970515727996826\n",
      "Batch 219, Loss: 0.4077971279621124\n",
      "Batch 220, Loss: 0.17076702415943146\n",
      "Batch 221, Loss: 0.15594357252120972\n",
      "Batch 222, Loss: 0.3160850703716278\n",
      "Batch 223, Loss: 0.3335917592048645\n",
      "Batch 224, Loss: -1.1651172637939453\n",
      "Batch 225, Loss: -1.2650495767593384\n",
      "Batch 226, Loss: -1.3376938104629517\n",
      "Batch 227, Loss: -1.178307056427002\n",
      "Batch 228, Loss: -1.3381316661834717\n",
      "Batch 229, Loss: 0.270934134721756\n",
      "Batch 230, Loss: 0.1735881119966507\n",
      "Batch 231, Loss: 0.3216346800327301\n",
      "Batch 232, Loss: 0.23129037022590637\n",
      "Batch 233, Loss: 0.3062533140182495\n",
      "Batch 234, Loss: 0.1352364420890808\n",
      "Batch 235, Loss: 0.16918492317199707\n",
      "Batch 236, Loss: -1.1430613994598389\n",
      "Batch 237, Loss: 0.1340520977973938\n",
      "Batch 238, Loss: -1.1494039297103882\n",
      "Batch 239, Loss: -1.158555030822754\n",
      "Batch 240, Loss: -1.1815900802612305\n",
      "Batch 241, Loss: -1.1869480609893799\n",
      "Batch 242, Loss: -1.1342582702636719\n",
      "Batch 243, Loss: 0.14748671650886536\n",
      "Batch 244, Loss: -1.201476812362671\n",
      "Batch 245, Loss: -1.190293550491333\n",
      "Batch 246, Loss: -1.1730774641036987\n",
      "Batch 247, Loss: 0.16138815879821777\n",
      "Batch 248, Loss: 0.14833152294158936\n",
      "Batch 249, Loss: -1.2172019481658936\n",
      "Batch 250, Loss: 0.1641596108675003\n",
      "Batch 251, Loss: -1.1681313514709473\n",
      "Batch 252, Loss: 0.15125524997711182\n",
      "Batch 253, Loss: 0.2031629979610443\n",
      "Batch 254, Loss: -1.167986273765564\n",
      "Batch 255, Loss: 0.22959250211715698\n",
      "Batch 256, Loss: 0.21113352477550507\n",
      "Batch 257, Loss: -1.2632019519805908\n",
      "Batch 258, Loss: -1.2830383777618408\n",
      "Batch 259, Loss: 0.1273030787706375\n",
      "Batch 260, Loss: -1.1254247426986694\n",
      "Batch 261, Loss: 0.1934797465801239\n",
      "Batch 262, Loss: -1.3058122396469116\n",
      "Batch 263, Loss: -1.2887928485870361\n",
      "Batch 264, Loss: -1.1233997344970703\n",
      "Batch 265, Loss: 0.2437686324119568\n",
      "Batch 266, Loss: 0.12476897984743118\n",
      "Batch 267, Loss: 0.1258508563041687\n",
      "Batch 268, Loss: -1.3426175117492676\n",
      "Batch 269, Loss: -1.1212750673294067\n",
      "Batch 270, Loss: 0.12804466485977173\n",
      "Batch 271, Loss: 0.27996230125427246\n",
      "Batch 272, Loss: 0.2729085385799408\n",
      "Batch 273, Loss: 0.2869531810283661\n",
      "Batch 274, Loss: 0.22697561979293823\n",
      "Batch 275, Loss: -1.122923731803894\n",
      "Batch 276, Loss: 0.12036611139774323\n",
      "Batch 277, Loss: 0.25515037775039673\n",
      "Batch 278, Loss: -1.3033806085586548\n",
      "Batch 279, Loss: 0.25481247901916504\n",
      "Batch 280, Loss: -1.1198792457580566\n",
      "Batch 281, Loss: 0.11110646277666092\n",
      "Batch 282, Loss: -1.2664905786514282\n",
      "Batch 283, Loss: -1.125580906867981\n",
      "Batch 284, Loss: 0.11651961505413055\n",
      "Batch 285, Loss: -1.1166514158248901\n",
      "Batch 286, Loss: 0.11511239409446716\n",
      "Batch 287, Loss: -1.1984272003173828\n",
      "Batch 288, Loss: 0.18209347128868103\n",
      "Batch 289, Loss: -1.1967225074768066\n",
      "Batch 290, Loss: 0.18405365943908691\n",
      "Batch 291, Loss: 0.23107153177261353\n",
      "Batch 292, Loss: 0.12124431133270264\n",
      "Batch 293, Loss: 0.117774099111557\n",
      "Batch 294, Loss: 0.18422040343284607\n",
      "Batch 295, Loss: -1.1166200637817383\n",
      "Batch 296, Loss: 0.11752098798751831\n",
      "Batch 297, Loss: 0.20659182965755463\n",
      "Batch 298, Loss: 0.11947721242904663\n",
      "Batch 299, Loss: 0.1499537229537964\n",
      "Batch 300, Loss: 0.1804230809211731\n",
      "Batch 301, Loss: -1.1840848922729492\n",
      "Batch 302, Loss: -1.191345453262329\n",
      "Batch 303, Loss: 0.19506123661994934\n",
      "Batch 304, Loss: -1.1160321235656738\n",
      "Batch 305, Loss: 0.11081154644489288\n",
      "Batch 306, Loss: 0.1103210598230362\n",
      "Batch 307, Loss: 0.14563477039337158\n",
      "Batch 308, Loss: 0.10267583280801773\n",
      "Batch 309, Loss: 0.10230386257171631\n",
      "Batch 310, Loss: -1.1678663492202759\n",
      "Batch 311, Loss: -1.1038169860839844\n",
      "Batch 312, Loss: -1.1118276119232178\n",
      "Batch 313, Loss: 0.1008542850613594\n",
      "Batch 314, Loss: -1.1033917665481567\n",
      "Batch 315, Loss: -1.0967661142349243\n",
      "Batch 316, Loss: -1.1153886318206787\n",
      "Batch 317, Loss: -1.1313990354537964\n",
      "Batch 318, Loss: -1.1120707988739014\n",
      "Batch 319, Loss: -1.111444354057312\n",
      "Batch 320, Loss: 0.10849881172180176\n",
      "Batch 321, Loss: 0.11523939669132233\n",
      "Batch 322, Loss: 0.1084318608045578\n",
      "Batch 323, Loss: 0.16730880737304688\n",
      "Batch 324, Loss: -1.1032440662384033\n",
      "Batch 325, Loss: 0.13172510266304016\n",
      "Batch 326, Loss: 0.14696992933750153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 327, Loss: -1.1081643104553223\n",
      "Batch 328, Loss: 0.17851658165454865\n",
      "Batch 329, Loss: 0.11850392073392868\n",
      "Batch 330, Loss: -1.1787753105163574\n",
      "Batch 331, Loss: -1.120384693145752\n",
      "Batch 332, Loss: -1.1355246305465698\n",
      "Batch 333, Loss: -1.1174275875091553\n",
      "Batch 334, Loss: -1.1504454612731934\n",
      "Batch 335, Loss: 0.12338875979185104\n",
      "Batch 336, Loss: 0.1245412528514862\n",
      "Batch 337, Loss: -1.1300371885299683\n",
      "Batch 338, Loss: -1.1085561513900757\n",
      "Batch 339, Loss: 0.13114142417907715\n",
      "Batch 340, Loss: -1.125254511833191\n",
      "Batch 341, Loss: -1.1583701372146606\n",
      "Batch 342, Loss: -1.2287156581878662\n",
      "Batch 343, Loss: 0.13307832181453705\n",
      "Batch 344, Loss: -1.1536545753479004\n",
      "Batch 345, Loss: -1.1240208148956299\n",
      "Batch 346, Loss: 0.16343018412590027\n",
      "Batch 347, Loss: -1.1222307682037354\n",
      "Batch 348, Loss: 0.16258504986763\n",
      "Batch 349, Loss: -1.199780821800232\n",
      "Batch 350, Loss: 0.1327463984489441\n",
      "Batch 351, Loss: 0.15263065695762634\n",
      "Batch 352, Loss: -1.1392544507980347\n",
      "Batch 353, Loss: 0.13215571641921997\n",
      "Batch 354, Loss: 0.13747499883174896\n",
      "Batch 355, Loss: -1.2110365629196167\n",
      "Batch 356, Loss: 0.1899832785129547\n",
      "Batch 357, Loss: 0.12432932108640671\n",
      "Batch 358, Loss: 0.1268080472946167\n",
      "Batch 359, Loss: -1.1444461345672607\n",
      "Batch 360, Loss: -1.1611442565917969\n",
      "Batch 361, Loss: -1.1248557567596436\n",
      "Batch 362, Loss: -1.1496267318725586\n",
      "Batch 363, Loss: -1.1303706169128418\n",
      "Batch 364, Loss: -1.1635618209838867\n",
      "Batch 365, Loss: -1.168131709098816\n",
      "Batch 366, Loss: 0.15213671326637268\n",
      "Batch 367, Loss: 0.13672968745231628\n",
      "Batch 368, Loss: 0.21230119466781616\n",
      "Batch 369, Loss: -1.1809115409851074\n",
      "Batch 370, Loss: 0.16704274713993073\n",
      "Batch 371, Loss: 0.1320909857749939\n",
      "Batch 372, Loss: -1.1414177417755127\n",
      "Batch 373, Loss: 0.18996655941009521\n",
      "Batch 374, Loss: 0.20101474225521088\n",
      "Batch 375, Loss: -1.1772961616516113\n",
      "Batch 376, Loss: 0.14683261513710022\n",
      "Batch 377, Loss: -1.1264355182647705\n",
      "Batch 378, Loss: -1.2106175422668457\n",
      "Batch 379, Loss: -1.1696584224700928\n",
      "Batch 380, Loss: -1.2031208276748657\n",
      "Batch 381, Loss: 0.19275963306427002\n",
      "Batch 382, Loss: -1.3096647262573242\n",
      "Batch 383, Loss: -1.1429026126861572\n",
      "Batch 384, Loss: -1.2088603973388672\n",
      "Batch 385, Loss: -1.2082048654556274\n",
      "Batch 386, Loss: 0.13991831243038177\n",
      "Batch 387, Loss: 0.16129784286022186\n",
      "Batch 388, Loss: -1.3767821788787842\n",
      "Batch 389, Loss: -1.383047103881836\n",
      "Batch 390, Loss: -1.412973165512085\n",
      "Batch 391, Loss: 0.12677325308322906\n",
      "Batch 392, Loss: -1.1312801837921143\n",
      "Batch 393, Loss: 0.12132091075181961\n",
      "Batch 394, Loss: 0.38500359654426575\n",
      "Batch 395, Loss: -1.3783855438232422\n",
      "Batch 396, Loss: -1.3847501277923584\n",
      "Batch 397, Loss: 0.3864942193031311\n",
      "Batch 398, Loss: 0.2878924310207367\n",
      "Batch 399, Loss: -1.1511740684509277\n",
      "Training [30%]\tLoss: -0.5007\n",
      "Batch 0, Loss: 0.2213726043701172\n",
      "Batch 1, Loss: 0.13170070946216583\n",
      "Batch 2, Loss: -1.1690033674240112\n",
      "Batch 3, Loss: 0.14056476950645447\n",
      "Batch 4, Loss: -1.1207952499389648\n",
      "Batch 5, Loss: -1.1272244453430176\n",
      "Batch 6, Loss: -1.1298097372055054\n",
      "Batch 7, Loss: 0.19223150610923767\n",
      "Batch 8, Loss: -1.1254037618637085\n",
      "Batch 9, Loss: -1.4097647666931152\n",
      "Batch 10, Loss: 0.2848013639450073\n",
      "Batch 11, Loss: 0.3843272924423218\n",
      "Batch 12, Loss: 0.14268966019153595\n",
      "Batch 13, Loss: 0.40073633193969727\n",
      "Batch 14, Loss: 0.12642796337604523\n",
      "Batch 15, Loss: -1.1705601215362549\n",
      "Batch 16, Loss: -1.0907338857650757\n",
      "Batch 17, Loss: 0.12972532212734222\n",
      "Batch 18, Loss: 0.38428765535354614\n",
      "Batch 19, Loss: -1.1080052852630615\n",
      "Batch 20, Loss: 0.14218400418758392\n",
      "Batch 21, Loss: -1.3449217081069946\n",
      "Batch 22, Loss: -1.1623313426971436\n",
      "Batch 23, Loss: 0.19889883697032928\n",
      "Batch 24, Loss: -1.257836937904358\n",
      "Batch 25, Loss: -1.279613971710205\n",
      "Batch 26, Loss: 0.33702051639556885\n",
      "Batch 27, Loss: -1.3751451969146729\n",
      "Batch 28, Loss: -1.1209336519241333\n",
      "Batch 29, Loss: 0.1148550882935524\n",
      "Batch 30, Loss: 0.11112038791179657\n",
      "Batch 31, Loss: -1.3836522102355957\n",
      "Batch 32, Loss: 0.1063520610332489\n",
      "Batch 33, Loss: -1.2051477432250977\n",
      "Batch 34, Loss: 0.11060383170843124\n",
      "Batch 35, Loss: -1.4125664234161377\n",
      "Batch 36, Loss: 0.42670583724975586\n",
      "Batch 37, Loss: 0.1144891232252121\n",
      "Batch 38, Loss: 0.14532119035720825\n",
      "Batch 39, Loss: 0.10618272423744202\n",
      "Batch 40, Loss: -1.1057770252227783\n",
      "Batch 41, Loss: 0.11608811467885971\n",
      "Batch 42, Loss: 0.42866218090057373\n",
      "Batch 43, Loss: -1.4146032333374023\n",
      "Batch 44, Loss: -1.2528713941574097\n",
      "Batch 45, Loss: 0.10637873411178589\n",
      "Batch 46, Loss: -1.144514799118042\n",
      "Batch 47, Loss: 0.16227887570858002\n",
      "Batch 48, Loss: 0.42063817381858826\n",
      "Batch 49, Loss: -1.1215330362319946\n",
      "Batch 50, Loss: 0.42850831151008606\n",
      "Batch 51, Loss: 0.41612571477890015\n",
      "Batch 52, Loss: -1.41416597366333\n",
      "Batch 53, Loss: 0.1336134970188141\n",
      "Batch 54, Loss: -1.370666265487671\n",
      "Batch 55, Loss: 0.4248175621032715\n",
      "Batch 56, Loss: 0.35770130157470703\n",
      "Batch 57, Loss: 0.22710521519184113\n",
      "Batch 58, Loss: -1.4241024255752563\n",
      "Batch 59, Loss: 0.34740307927131653\n",
      "Batch 60, Loss: -1.0987993478775024\n",
      "Batch 61, Loss: -1.1283069849014282\n",
      "Batch 62, Loss: -1.3815813064575195\n",
      "Batch 63, Loss: 0.0966501384973526\n",
      "Batch 64, Loss: 0.40990960597991943\n",
      "Batch 65, Loss: 0.34296610951423645\n",
      "Batch 66, Loss: -1.1434015035629272\n",
      "Batch 67, Loss: -1.2499008178710938\n",
      "Batch 68, Loss: -1.1430246829986572\n",
      "Batch 69, Loss: 0.2721646726131439\n",
      "Batch 70, Loss: 0.11889863759279251\n",
      "Batch 71, Loss: 0.11436277627944946\n",
      "Batch 72, Loss: 0.11772720515727997\n",
      "Batch 73, Loss: -1.241446614265442\n",
      "Batch 74, Loss: -1.220949649810791\n",
      "Batch 75, Loss: -1.138240098953247\n",
      "Batch 76, Loss: 0.10961926728487015\n",
      "Batch 77, Loss: -1.101353645324707\n",
      "Batch 78, Loss: 0.1406182497739792\n",
      "Batch 79, Loss: -1.1001948118209839\n",
      "Batch 80, Loss: 0.11619524657726288\n",
      "Batch 81, Loss: 0.10325992107391357\n",
      "Batch 82, Loss: 0.11518452316522598\n",
      "Batch 83, Loss: -1.2902129888534546\n",
      "Batch 84, Loss: -1.307992696762085\n",
      "Batch 85, Loss: -1.329988718032837\n",
      "Batch 86, Loss: -1.0957342386245728\n",
      "Batch 87, Loss: -1.097508192062378\n",
      "Batch 88, Loss: 0.09692416340112686\n",
      "Batch 89, Loss: -1.115169644355774\n",
      "Batch 90, Loss: 0.4146271347999573\n",
      "Batch 91, Loss: -1.4140316247940063\n",
      "Batch 92, Loss: -1.3823376893997192\n",
      "Batch 93, Loss: -1.4126484394073486\n",
      "Batch 94, Loss: 0.09577371925115585\n",
      "Batch 95, Loss: -1.0979136228561401\n",
      "Batch 96, Loss: -1.4137812852859497\n",
      "Batch 97, Loss: 0.3779580295085907\n",
      "Batch 98, Loss: 0.42279767990112305\n",
      "Batch 99, Loss: -1.1003742218017578\n",
      "Batch 100, Loss: 0.10136532783508301\n",
      "Batch 101, Loss: -1.3899009227752686\n",
      "Batch 102, Loss: -1.1004468202590942\n",
      "Batch 103, Loss: 0.37594151496887207\n",
      "Batch 104, Loss: -1.4260625839233398\n",
      "Batch 105, Loss: 0.11307767778635025\n",
      "Batch 106, Loss: 0.3789818286895752\n",
      "Batch 107, Loss: 0.37104612588882446\n",
      "Batch 108, Loss: 0.3454342782497406\n",
      "Batch 109, Loss: 0.39744552969932556\n",
      "Batch 110, Loss: 0.31126630306243896\n",
      "Batch 111, Loss: 0.2228216528892517\n",
      "Batch 112, Loss: -1.3845670223236084\n",
      "Batch 113, Loss: 0.37159085273742676\n",
      "Batch 114, Loss: -1.103348731994629\n",
      "Batch 115, Loss: -1.1451772451400757\n",
      "Batch 116, Loss: 0.39436453580856323\n",
      "Batch 117, Loss: 0.35748574137687683\n",
      "Batch 118, Loss: -1.111721158027649\n",
      "Batch 119, Loss: 0.10660906881093979\n",
      "Batch 120, Loss: -1.3837494850158691\n",
      "Batch 121, Loss: 0.40616437792778015\n",
      "Batch 122, Loss: 0.39581993222236633\n",
      "Batch 123, Loss: 0.33170998096466064\n",
      "Batch 124, Loss: 0.18097424507141113\n",
      "Batch 125, Loss: 0.15575271844863892\n",
      "Batch 126, Loss: -1.1102452278137207\n",
      "Batch 127, Loss: 0.16703945398330688\n",
      "Batch 128, Loss: -1.1184886693954468\n",
      "Batch 129, Loss: -1.0543001890182495\n",
      "Batch 130, Loss: -1.140331506729126\n",
      "Batch 131, Loss: 0.3586846590042114\n",
      "Batch 132, Loss: -1.1721773147583008\n",
      "Batch 133, Loss: 0.35865986347198486\n",
      "Batch 134, Loss: 0.27874836325645447\n",
      "Batch 135, Loss: -1.3952202796936035\n",
      "Batch 136, Loss: 0.13667920231819153\n",
      "Batch 137, Loss: 0.22060824930667877\n",
      "Batch 138, Loss: -1.2586140632629395\n",
      "Batch 139, Loss: 0.20137688517570496\n",
      "Batch 140, Loss: 0.3780871629714966\n",
      "Batch 141, Loss: 0.07646714150905609\n",
      "Batch 142, Loss: -1.1164541244506836\n",
      "Batch 143, Loss: -1.2658365964889526\n",
      "Batch 144, Loss: -0.9780782461166382\n",
      "Batch 145, Loss: -1.1425212621688843\n",
      "Batch 146, Loss: -0.01761416345834732\n",
      "Batch 147, Loss: -1.3170005083084106\n",
      "Batch 148, Loss: 0.15065056085586548\n",
      "Batch 149, Loss: 0.3575955629348755\n",
      "Batch 150, Loss: -1.336768627166748\n",
      "Batch 151, Loss: 0.3370404839515686\n",
      "Batch 152, Loss: -0.09580400586128235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 153, Loss: 0.3481502830982208\n",
      "Batch 154, Loss: 0.2539401650428772\n",
      "Batch 155, Loss: 0.24776160717010498\n",
      "Batch 156, Loss: 0.316328763961792\n",
      "Batch 157, Loss: -0.90087890625\n",
      "Batch 158, Loss: -1.3996893167495728\n",
      "Batch 159, Loss: -1.1658744812011719\n",
      "Batch 160, Loss: -1.2931501865386963\n",
      "Batch 161, Loss: 0.1424119919538498\n",
      "Batch 162, Loss: -1.1195645332336426\n",
      "Batch 163, Loss: -1.1166967153549194\n",
      "Batch 164, Loss: 0.21887314319610596\n",
      "Batch 165, Loss: 0.33665066957473755\n",
      "Batch 166, Loss: -1.099471092224121\n",
      "Batch 167, Loss: 0.28109902143478394\n",
      "Batch 168, Loss: 0.16472332179546356\n",
      "Batch 169, Loss: 0.11398059129714966\n",
      "Batch 170, Loss: 0.15927404165267944\n",
      "Batch 171, Loss: -1.1654114723205566\n",
      "Batch 172, Loss: 0.109820157289505\n",
      "Batch 173, Loss: -1.1155939102172852\n",
      "Batch 174, Loss: -1.1120195388793945\n",
      "Batch 175, Loss: -1.1007033586502075\n",
      "Batch 176, Loss: -1.094788908958435\n",
      "Batch 177, Loss: -1.1255600452423096\n",
      "Batch 178, Loss: -1.1514010429382324\n",
      "Batch 179, Loss: 0.10803577303886414\n",
      "Batch 180, Loss: 0.10730831325054169\n",
      "Batch 181, Loss: -1.1666756868362427\n",
      "Batch 182, Loss: 0.09489558637142181\n",
      "Batch 183, Loss: -1.1061748266220093\n",
      "Batch 184, Loss: 0.16942156851291656\n",
      "Batch 185, Loss: -1.1326638460159302\n",
      "Batch 186, Loss: -1.1716322898864746\n",
      "Batch 187, Loss: -1.1627464294433594\n",
      "Batch 188, Loss: -1.097156286239624\n",
      "Batch 189, Loss: 0.10808739066123962\n",
      "Batch 190, Loss: -1.1106035709381104\n",
      "Batch 191, Loss: 0.1087949350476265\n",
      "Batch 192, Loss: -1.176483154296875\n",
      "Batch 193, Loss: -1.186921238899231\n",
      "Batch 194, Loss: -1.1564738750457764\n",
      "Batch 195, Loss: -1.3129528760910034\n",
      "Batch 196, Loss: 0.10576379299163818\n",
      "Batch 197, Loss: 0.20284152030944824\n",
      "Batch 198, Loss: -1.098890781402588\n",
      "Batch 199, Loss: -1.2070989608764648\n",
      "Batch 200, Loss: 0.09959432482719421\n",
      "Batch 201, Loss: -1.1975483894348145\n",
      "Batch 202, Loss: -1.2767345905303955\n",
      "Batch 203, Loss: 0.24418102204799652\n",
      "Batch 204, Loss: 0.2068127691745758\n",
      "Batch 205, Loss: -1.225292444229126\n",
      "Batch 206, Loss: -1.1049919128417969\n",
      "Batch 207, Loss: 0.10038569569587708\n",
      "Batch 208, Loss: 0.10689389705657959\n",
      "Batch 209, Loss: -1.2400132417678833\n",
      "Batch 210, Loss: -1.099414348602295\n",
      "Batch 211, Loss: 0.09805542230606079\n",
      "Batch 212, Loss: -1.23784601688385\n",
      "Batch 213, Loss: -1.098544716835022\n",
      "Batch 214, Loss: -1.2961407899856567\n",
      "Batch 215, Loss: 0.2602173686027527\n",
      "Batch 216, Loss: -1.0992262363433838\n",
      "Batch 217, Loss: 0.10417959839105606\n",
      "Batch 218, Loss: 0.27811378240585327\n",
      "Batch 219, Loss: 0.10397472977638245\n",
      "Batch 220, Loss: -1.3040101528167725\n",
      "Batch 221, Loss: -1.1052424907684326\n",
      "Batch 222, Loss: 0.3231252133846283\n",
      "Batch 223, Loss: 0.2829127013683319\n",
      "Batch 224, Loss: -1.2615866661071777\n",
      "Batch 225, Loss: 0.3101605772972107\n",
      "Batch 226, Loss: -1.3405598402023315\n",
      "Batch 227, Loss: -1.1072182655334473\n",
      "Batch 228, Loss: 0.31600093841552734\n",
      "Batch 229, Loss: 0.2681316137313843\n",
      "Batch 230, Loss: -1.1052013635635376\n",
      "Batch 231, Loss: -1.2676947116851807\n",
      "Batch 232, Loss: -1.1465246677398682\n",
      "Batch 233, Loss: -1.1144136190414429\n",
      "Batch 234, Loss: -1.2607285976409912\n",
      "Batch 235, Loss: -1.1074622869491577\n",
      "Batch 236, Loss: -1.3015387058258057\n",
      "Batch 237, Loss: -1.2771141529083252\n",
      "Batch 238, Loss: -1.1370848417282104\n",
      "Batch 239, Loss: -1.3164379596710205\n",
      "Batch 240, Loss: -1.143792748451233\n",
      "Batch 241, Loss: -1.1075706481933594\n",
      "Batch 242, Loss: 0.29994773864746094\n",
      "Batch 243, Loss: 0.10952191799879074\n",
      "Batch 244, Loss: -1.2936354875564575\n",
      "Batch 245, Loss: -1.333727478981018\n",
      "Batch 246, Loss: 0.1112537831068039\n",
      "Batch 247, Loss: 0.11752447485923767\n",
      "Batch 248, Loss: 0.11634843796491623\n",
      "Batch 249, Loss: -1.275836706161499\n",
      "Batch 250, Loss: 0.11618272215127945\n",
      "Batch 251, Loss: -1.167431116104126\n",
      "Batch 252, Loss: -1.1265143156051636\n",
      "Batch 253, Loss: -1.1104439496994019\n",
      "Batch 254, Loss: 0.11518798023462296\n",
      "Batch 255, Loss: 0.12002363801002502\n",
      "Batch 256, Loss: -1.3377811908721924\n",
      "Batch 257, Loss: 0.3630710244178772\n",
      "Batch 258, Loss: -1.3417317867279053\n",
      "Batch 259, Loss: -1.3754091262817383\n",
      "Batch 260, Loss: 0.14865142107009888\n",
      "Batch 261, Loss: 0.3765685558319092\n",
      "Batch 262, Loss: 0.3959333896636963\n",
      "Batch 263, Loss: -1.3767496347427368\n",
      "Batch 264, Loss: 0.12190549075603485\n",
      "Batch 265, Loss: 0.3836129307746887\n",
      "Batch 266, Loss: 0.11107169091701508\n",
      "Batch 267, Loss: -1.1166229248046875\n",
      "Batch 268, Loss: -1.3732129335403442\n",
      "Batch 269, Loss: -1.2428849935531616\n",
      "Batch 270, Loss: 0.3842198848724365\n",
      "Batch 271, Loss: 0.3862176537513733\n",
      "Batch 272, Loss: 0.3699782192707062\n",
      "Batch 273, Loss: -1.3423738479614258\n",
      "Batch 274, Loss: 0.11723702400922775\n",
      "Batch 275, Loss: -1.3822728395462036\n",
      "Batch 276, Loss: -1.3042340278625488\n",
      "Batch 277, Loss: 0.12828388810157776\n",
      "Batch 278, Loss: 0.11837124079465866\n",
      "Batch 279, Loss: 0.35566720366477966\n",
      "Batch 280, Loss: -1.1144208908081055\n",
      "Batch 281, Loss: -1.3683316707611084\n",
      "Batch 282, Loss: 0.11372418701648712\n",
      "Batch 283, Loss: -1.1092005968093872\n",
      "Batch 284, Loss: 0.10849428176879883\n",
      "Batch 285, Loss: -1.386379361152649\n",
      "Batch 286, Loss: -1.108295202255249\n",
      "Batch 287, Loss: -1.3804287910461426\n",
      "Batch 288, Loss: 0.3907650411128998\n",
      "Batch 289, Loss: 0.3436482846736908\n",
      "Batch 290, Loss: -1.1106427907943726\n",
      "Batch 291, Loss: 0.3804958462715149\n",
      "Batch 292, Loss: -1.248514175415039\n",
      "Batch 293, Loss: 0.36530083417892456\n",
      "Batch 294, Loss: 0.3524235486984253\n",
      "Batch 295, Loss: -1.3857126235961914\n",
      "Batch 296, Loss: 0.2116357684135437\n",
      "Batch 297, Loss: 0.10851925611495972\n",
      "Batch 298, Loss: -1.3869988918304443\n",
      "Batch 299, Loss: 0.3503511846065521\n",
      "Batch 300, Loss: -1.379860758781433\n",
      "Batch 301, Loss: 0.33688387274742126\n",
      "Batch 302, Loss: 0.12443537265062332\n",
      "Batch 303, Loss: -1.3134156465530396\n",
      "Batch 304, Loss: 0.3312433063983917\n",
      "Batch 305, Loss: -1.3163909912109375\n",
      "Batch 306, Loss: -1.327668309211731\n",
      "Batch 307, Loss: -1.1060893535614014\n",
      "Batch 308, Loss: 0.3062170743942261\n",
      "Batch 309, Loss: 0.3085346221923828\n",
      "Batch 310, Loss: -1.1719962358474731\n",
      "Batch 311, Loss: -1.2924377918243408\n",
      "Batch 312, Loss: -1.334409475326538\n",
      "Batch 313, Loss: 0.11389505863189697\n",
      "Batch 314, Loss: 0.3168415427207947\n",
      "Batch 315, Loss: -1.1126108169555664\n",
      "Batch 316, Loss: -1.357210636138916\n",
      "Batch 317, Loss: -1.3769900798797607\n",
      "Batch 318, Loss: -1.3100095987319946\n",
      "Batch 319, Loss: -1.3727591037750244\n",
      "Batch 320, Loss: 0.10801099240779877\n",
      "Batch 321, Loss: 0.336427241563797\n",
      "Batch 322, Loss: 0.31813180446624756\n",
      "Batch 323, Loss: -1.36358642578125\n",
      "Batch 324, Loss: -1.2808270454406738\n",
      "Batch 325, Loss: -1.249300241470337\n",
      "Batch 326, Loss: 0.11392789334058762\n",
      "Batch 327, Loss: 0.3713756203651428\n",
      "Batch 328, Loss: 0.10916808247566223\n",
      "Batch 329, Loss: 0.3644302785396576\n",
      "Batch 330, Loss: 0.34443098306655884\n",
      "Batch 331, Loss: -1.184942603111267\n",
      "Batch 332, Loss: 0.3584732711315155\n",
      "Batch 333, Loss: -1.1172676086425781\n",
      "Batch 334, Loss: 0.10805504769086838\n",
      "Batch 335, Loss: 0.303200900554657\n",
      "Batch 336, Loss: 0.10886434465646744\n",
      "Batch 337, Loss: 0.2875787615776062\n",
      "Batch 338, Loss: -1.3166933059692383\n",
      "Batch 339, Loss: -1.2622730731964111\n",
      "Batch 340, Loss: 0.2684488296508789\n",
      "Batch 341, Loss: 0.11216683685779572\n",
      "Batch 342, Loss: 0.12478276342153549\n",
      "Batch 343, Loss: -1.3181049823760986\n",
      "Batch 344, Loss: 0.25242260098457336\n",
      "Batch 345, Loss: -1.1161041259765625\n",
      "Batch 346, Loss: -1.35135018825531\n",
      "Batch 347, Loss: 0.26889756321907043\n",
      "Batch 348, Loss: -1.2280614376068115\n",
      "Batch 349, Loss: -1.1271783113479614\n",
      "Batch 350, Loss: 0.1186111643910408\n",
      "Batch 351, Loss: -1.1452561616897583\n",
      "Batch 352, Loss: 0.26292723417282104\n",
      "Batch 353, Loss: -1.3684821128845215\n",
      "Batch 354, Loss: -1.344923973083496\n",
      "Batch 355, Loss: 0.12007360905408859\n",
      "Batch 356, Loss: -1.2222301959991455\n",
      "Batch 357, Loss: -1.1280466318130493\n",
      "Batch 358, Loss: -1.272341251373291\n",
      "Batch 359, Loss: 0.2851988673210144\n",
      "Batch 360, Loss: -1.3388190269470215\n",
      "Batch 361, Loss: 0.13031169772148132\n",
      "Batch 362, Loss: 0.29627731442451477\n",
      "Batch 363, Loss: 0.2959424555301666\n",
      "Batch 364, Loss: -1.3701722621917725\n",
      "Batch 365, Loss: 0.1164870411157608\n",
      "Batch 366, Loss: -1.3871583938598633\n",
      "Batch 367, Loss: 0.29675257205963135\n",
      "Batch 368, Loss: 0.11442762613296509\n",
      "Batch 369, Loss: 0.12252706289291382\n",
      "Batch 370, Loss: -1.242945909500122\n",
      "Batch 371, Loss: 0.20492590963840485\n",
      "Batch 372, Loss: -1.2828036546707153\n",
      "Batch 373, Loss: -1.2837855815887451\n",
      "Batch 374, Loss: -1.3748548030853271\n",
      "Batch 375, Loss: -1.3469115495681763\n",
      "Batch 376, Loss: 0.2519207000732422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 377, Loss: -1.2393720149993896\n",
      "Batch 378, Loss: -1.1042604446411133\n",
      "Batch 379, Loss: 0.1367911696434021\n",
      "Batch 380, Loss: -1.3770726919174194\n",
      "Batch 381, Loss: 0.31526169180870056\n",
      "Batch 382, Loss: -1.103752613067627\n",
      "Batch 383, Loss: 0.26419487595558167\n",
      "Batch 384, Loss: -1.2063677310943604\n",
      "Batch 385, Loss: -1.2530441284179688\n",
      "Batch 386, Loss: 0.2672528326511383\n",
      "Batch 387, Loss: 0.3770941197872162\n",
      "Batch 388, Loss: 0.156741663813591\n",
      "Batch 389, Loss: -1.2825772762298584\n",
      "Batch 390, Loss: 0.31168586015701294\n",
      "Batch 391, Loss: -1.2020987272262573\n",
      "Batch 392, Loss: 0.3163706362247467\n",
      "Batch 393, Loss: -1.104441523551941\n",
      "Batch 394, Loss: -1.307072639465332\n",
      "Batch 395, Loss: -1.3096725940704346\n",
      "Batch 396, Loss: 0.25057584047317505\n",
      "Batch 397, Loss: 0.11474383622407913\n",
      "Batch 398, Loss: 0.12433023005723953\n",
      "Batch 399, Loss: 0.10561929643154144\n",
      "Training [40%]\tLoss: -0.4996\n",
      "Batch 0, Loss: 0.10412917286157608\n",
      "Batch 1, Loss: 0.2845531404018402\n",
      "Batch 2, Loss: -1.120989441871643\n",
      "Batch 3, Loss: 0.17530158162117004\n",
      "Batch 4, Loss: -1.102488398551941\n",
      "Batch 5, Loss: -1.351630687713623\n",
      "Batch 6, Loss: -1.1638201475143433\n",
      "Batch 7, Loss: -1.3354458808898926\n",
      "Batch 8, Loss: -1.3785061836242676\n",
      "Batch 9, Loss: -1.1126961708068848\n",
      "Batch 10, Loss: -1.2409100532531738\n",
      "Batch 11, Loss: 0.294536292552948\n",
      "Batch 12, Loss: 0.10297979414463043\n",
      "Batch 13, Loss: -1.381883978843689\n",
      "Batch 14, Loss: 0.32624930143356323\n",
      "Batch 15, Loss: 0.26039019227027893\n",
      "Batch 16, Loss: -1.116626501083374\n",
      "Batch 17, Loss: 0.10521692782640457\n",
      "Batch 18, Loss: -1.1058928966522217\n",
      "Batch 19, Loss: -1.2111988067626953\n",
      "Batch 20, Loss: 0.3277537226676941\n",
      "Batch 21, Loss: 0.13666561245918274\n",
      "Batch 22, Loss: -1.3842499256134033\n",
      "Batch 23, Loss: -1.1027758121490479\n",
      "Batch 24, Loss: -1.1068857908248901\n",
      "Batch 25, Loss: -1.2168306112289429\n",
      "Batch 26, Loss: -1.2236851453781128\n",
      "Batch 27, Loss: 0.22321388125419617\n",
      "Batch 28, Loss: 0.11126995831727982\n",
      "Batch 29, Loss: -1.1244683265686035\n",
      "Batch 30, Loss: 0.2597009539604187\n",
      "Batch 31, Loss: -1.1109391450881958\n",
      "Batch 32, Loss: 0.12841087579727173\n",
      "Batch 33, Loss: 0.11274229735136032\n",
      "Batch 34, Loss: 0.23967605829238892\n",
      "Batch 35, Loss: 0.16409021615982056\n",
      "Batch 36, Loss: 0.1131589487195015\n",
      "Batch 37, Loss: -1.2336243391036987\n",
      "Batch 38, Loss: 0.12401951104402542\n",
      "Batch 39, Loss: 0.22620651125907898\n",
      "Batch 40, Loss: 0.12276439368724823\n",
      "Batch 41, Loss: 0.1919233798980713\n",
      "Batch 42, Loss: -1.1246261596679688\n",
      "Batch 43, Loss: -1.2261403799057007\n",
      "Batch 44, Loss: -1.2960786819458008\n",
      "Batch 45, Loss: 0.2368137538433075\n",
      "Batch 46, Loss: 0.19755518436431885\n",
      "Batch 47, Loss: 0.24886447191238403\n",
      "Batch 48, Loss: 0.20603817701339722\n",
      "Batch 49, Loss: -1.108647346496582\n",
      "Batch 50, Loss: 0.11582731455564499\n",
      "Batch 51, Loss: -1.180617332458496\n",
      "Batch 52, Loss: 0.17337454855442047\n",
      "Batch 53, Loss: -1.1475880146026611\n",
      "Batch 54, Loss: -1.1157549619674683\n",
      "Batch 55, Loss: -1.2164613008499146\n",
      "Batch 56, Loss: -1.1669855117797852\n",
      "Batch 57, Loss: 0.11669836938381195\n",
      "Batch 58, Loss: -1.1724612712860107\n",
      "Batch 59, Loss: 0.1876119077205658\n",
      "Batch 60, Loss: -1.2864365577697754\n",
      "Batch 61, Loss: -1.1076136827468872\n",
      "Batch 62, Loss: -1.1067767143249512\n",
      "Batch 63, Loss: 0.11790540814399719\n",
      "Batch 64, Loss: -1.1154577732086182\n",
      "Batch 65, Loss: -1.1188278198242188\n",
      "Batch 66, Loss: 0.16426265239715576\n",
      "Batch 67, Loss: 0.20740723609924316\n",
      "Batch 68, Loss: -1.3395847082138062\n",
      "Batch 69, Loss: -1.2449841499328613\n",
      "Batch 70, Loss: 0.11484862118959427\n",
      "Batch 71, Loss: -1.1094003915786743\n",
      "Batch 72, Loss: -1.2558813095092773\n",
      "Batch 73, Loss: -1.1191390752792358\n",
      "Batch 74, Loss: -1.2267476320266724\n",
      "Batch 75, Loss: -1.258906364440918\n",
      "Batch 76, Loss: -1.2569262981414795\n",
      "Batch 77, Loss: 0.11275268346071243\n",
      "Batch 78, Loss: 0.11109843850135803\n",
      "Batch 79, Loss: 0.11449065804481506\n",
      "Batch 80, Loss: -1.3375115394592285\n",
      "Batch 81, Loss: 0.10602302849292755\n",
      "Batch 82, Loss: -1.1068156957626343\n",
      "Batch 83, Loss: 0.3231702446937561\n",
      "Batch 84, Loss: 0.10768965631723404\n",
      "Batch 85, Loss: -1.1060584783554077\n",
      "Batch 86, Loss: 0.10564430058002472\n",
      "Batch 87, Loss: 0.10671626031398773\n",
      "Batch 88, Loss: 0.37061232328414917\n",
      "Batch 89, Loss: -1.3837014436721802\n",
      "Batch 90, Loss: -1.1060346364974976\n",
      "Batch 91, Loss: -1.3661153316497803\n",
      "Batch 92, Loss: -1.2608914375305176\n",
      "Batch 93, Loss: 0.4116576910018921\n",
      "Batch 94, Loss: -1.117460012435913\n",
      "Batch 95, Loss: -1.107800006866455\n",
      "Batch 96, Loss: -1.3748486042022705\n",
      "Batch 97, Loss: -1.1064820289611816\n",
      "Batch 98, Loss: -1.4277358055114746\n",
      "Batch 99, Loss: 0.42615100741386414\n",
      "Batch 100, Loss: -1.109175443649292\n",
      "Batch 101, Loss: -1.1082500219345093\n",
      "Batch 102, Loss: 0.3878811001777649\n",
      "Batch 103, Loss: -1.4196364879608154\n",
      "Batch 104, Loss: -1.1116020679473877\n",
      "Batch 105, Loss: 0.11136909574270248\n",
      "Batch 106, Loss: 0.3896822929382324\n",
      "Batch 107, Loss: -1.1124818325042725\n",
      "Batch 108, Loss: 0.112515389919281\n",
      "Batch 109, Loss: 0.3929539620876312\n",
      "Batch 110, Loss: -1.1153035163879395\n",
      "Batch 111, Loss: 0.3719872534275055\n",
      "Batch 112, Loss: 0.11370187252759933\n",
      "Batch 113, Loss: 0.3700932562351227\n",
      "Batch 114, Loss: -1.383409023284912\n",
      "Batch 115, Loss: -1.291879415512085\n",
      "Batch 116, Loss: 0.36598560214042664\n",
      "Batch 117, Loss: 0.11438103020191193\n",
      "Batch 118, Loss: -1.3772844076156616\n",
      "Batch 119, Loss: -1.3493090867996216\n",
      "Batch 120, Loss: -1.359339714050293\n",
      "Batch 121, Loss: 0.24029824137687683\n",
      "Batch 122, Loss: 0.3766796886920929\n",
      "Batch 123, Loss: 0.1133800595998764\n",
      "Batch 124, Loss: -1.410438060760498\n",
      "Batch 125, Loss: -1.112686276435852\n",
      "Batch 126, Loss: -1.1186312437057495\n",
      "Batch 127, Loss: -1.1126071214675903\n",
      "Batch 128, Loss: 0.11255335807800293\n",
      "Batch 129, Loss: 0.11337382346391678\n",
      "Batch 130, Loss: 0.4196602702140808\n",
      "Batch 131, Loss: -1.4394354820251465\n",
      "Batch 132, Loss: -1.1125624179840088\n",
      "Batch 133, Loss: 0.11346086114645004\n",
      "Batch 134, Loss: -1.1135493516921997\n",
      "Batch 135, Loss: 0.11270106583833694\n",
      "Batch 136, Loss: -1.4246281385421753\n",
      "Batch 137, Loss: 0.11479246616363525\n",
      "Batch 138, Loss: -1.428772211074829\n",
      "Batch 139, Loss: 0.4192127585411072\n",
      "Batch 140, Loss: 0.404460072517395\n",
      "Batch 141, Loss: 0.40052950382232666\n",
      "Batch 142, Loss: 0.11221645772457123\n",
      "Batch 143, Loss: 0.41159844398498535\n",
      "Batch 144, Loss: 0.1109859049320221\n",
      "Batch 145, Loss: -1.3974997997283936\n",
      "Batch 146, Loss: -1.40152907371521\n",
      "Batch 147, Loss: -1.3795366287231445\n",
      "Batch 148, Loss: 0.11039741337299347\n",
      "Batch 149, Loss: 0.39528340101242065\n",
      "Batch 150, Loss: 0.4012550413608551\n",
      "Batch 151, Loss: -1.1087226867675781\n",
      "Batch 152, Loss: 0.3920137286186218\n",
      "Batch 153, Loss: 0.10802076011896133\n",
      "Batch 154, Loss: -1.1101698875427246\n",
      "Batch 155, Loss: -1.1081421375274658\n",
      "Batch 156, Loss: 0.37440240383148193\n",
      "Batch 157, Loss: -1.3809475898742676\n",
      "Batch 158, Loss: 0.10801242291927338\n",
      "Batch 159, Loss: -1.1139662265777588\n",
      "Batch 160, Loss: 0.37572771310806274\n",
      "Batch 161, Loss: 0.1081802248954773\n",
      "Batch 162, Loss: 0.41067835688591003\n",
      "Batch 163, Loss: -1.123108148574829\n",
      "Batch 164, Loss: -1.1076997518539429\n",
      "Batch 165, Loss: 0.3301175832748413\n",
      "Batch 166, Loss: 0.10855870693922043\n",
      "Batch 167, Loss: 0.10869583487510681\n",
      "Batch 168, Loss: 0.10867461562156677\n",
      "Batch 169, Loss: -1.1495505571365356\n",
      "Batch 170, Loss: -1.3497358560562134\n",
      "Batch 171, Loss: -1.1086113452911377\n",
      "Batch 172, Loss: 0.10831459611654282\n",
      "Batch 173, Loss: -1.1086958646774292\n",
      "Batch 174, Loss: 0.3706841766834259\n",
      "Batch 175, Loss: 0.12956345081329346\n",
      "Batch 176, Loss: -1.1084015369415283\n",
      "Batch 177, Loss: 0.10909082740545273\n",
      "Batch 178, Loss: 0.10906955599784851\n",
      "Batch 179, Loss: -1.3287266492843628\n",
      "Batch 180, Loss: 0.27624309062957764\n",
      "Batch 181, Loss: 0.1076190173625946\n",
      "Batch 182, Loss: 0.27584215998649597\n",
      "Batch 183, Loss: 0.10600634664297104\n",
      "Batch 184, Loss: -1.328292965888977\n",
      "Batch 185, Loss: 0.10499177128076553\n",
      "Batch 186, Loss: -1.3805036544799805\n",
      "Batch 187, Loss: 0.25422340631484985\n",
      "Batch 188, Loss: -1.2863292694091797\n",
      "Batch 189, Loss: -1.406050443649292\n",
      "Batch 190, Loss: -1.3278162479400635\n",
      "Batch 191, Loss: -1.3195663690567017\n",
      "Batch 192, Loss: -1.2502799034118652\n",
      "Batch 193, Loss: 0.28544026613235474\n",
      "Batch 194, Loss: -1.317253589630127\n",
      "Batch 195, Loss: -1.1018500328063965\n",
      "Batch 196, Loss: -1.3754651546478271\n",
      "Batch 197, Loss: -1.1043353080749512\n",
      "Batch 198, Loss: -1.4313459396362305\n",
      "Batch 199, Loss: 0.3310467004776001\n",
      "Batch 200, Loss: 0.10302942246198654\n",
      "Batch 201, Loss: -1.3613046407699585\n",
      "Batch 202, Loss: -1.3804845809936523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 203, Loss: 0.4107500910758972\n",
      "Batch 204, Loss: 0.22022393345832825\n",
      "Batch 205, Loss: -1.1044127941131592\n",
      "Batch 206, Loss: -1.4267523288726807\n",
      "Batch 207, Loss: 0.10355360060930252\n",
      "Batch 208, Loss: 0.10441187769174576\n",
      "Batch 209, Loss: 0.41552427411079407\n",
      "Batch 210, Loss: 0.10334119200706482\n",
      "Batch 211, Loss: -1.1038703918457031\n",
      "Batch 212, Loss: -1.1025313138961792\n",
      "Batch 213, Loss: 0.10288546234369278\n",
      "Batch 214, Loss: -1.4324930906295776\n",
      "Batch 215, Loss: 0.430205762386322\n",
      "Batch 216, Loss: -1.1027806997299194\n",
      "Batch 217, Loss: 0.10311771929264069\n",
      "Batch 218, Loss: 0.4321293830871582\n",
      "Batch 219, Loss: -1.4305317401885986\n",
      "Batch 220, Loss: -1.427520751953125\n",
      "Batch 221, Loss: 0.14384230971336365\n",
      "Batch 222, Loss: 0.10276301950216293\n",
      "Batch 223, Loss: 0.10871665924787521\n",
      "Batch 224, Loss: 0.4087919294834137\n",
      "Batch 225, Loss: -1.1150323152542114\n",
      "Batch 226, Loss: 0.36493176221847534\n",
      "Batch 227, Loss: -1.2377275228500366\n",
      "Batch 228, Loss: 0.09996466338634491\n",
      "Batch 229, Loss: -1.4309738874435425\n",
      "Batch 230, Loss: -1.4157040119171143\n",
      "Batch 231, Loss: 0.09929235279560089\n",
      "Batch 232, Loss: 0.35213279724121094\n",
      "Batch 233, Loss: 0.09921760857105255\n",
      "Batch 234, Loss: 0.09903797507286072\n",
      "Batch 235, Loss: 0.09871672093868256\n",
      "Batch 236, Loss: -1.0973695516586304\n",
      "Batch 237, Loss: -1.1364144086837769\n",
      "Batch 238, Loss: 0.37592869997024536\n",
      "Batch 239, Loss: -1.4385178089141846\n",
      "Batch 240, Loss: -1.3944482803344727\n",
      "Batch 241, Loss: 0.30696025490760803\n",
      "Batch 242, Loss: -1.0971664190292358\n",
      "Batch 243, Loss: 0.3417090177536011\n",
      "Batch 244, Loss: -1.1168030500411987\n",
      "Batch 245, Loss: 0.36720985174179077\n",
      "Batch 246, Loss: -1.099825143814087\n",
      "Batch 247, Loss: 0.3421907424926758\n",
      "Batch 248, Loss: 0.2916340231895447\n",
      "Batch 249, Loss: 0.10217176377773285\n",
      "Batch 250, Loss: -1.1092768907546997\n",
      "Batch 251, Loss: 0.25664055347442627\n",
      "Batch 252, Loss: 0.25825557112693787\n",
      "Batch 253, Loss: -1.2693454027175903\n",
      "Batch 254, Loss: -1.2449870109558105\n",
      "Batch 255, Loss: 0.1052708849310875\n",
      "Batch 256, Loss: -1.3902091979980469\n",
      "Batch 257, Loss: -1.2972354888916016\n",
      "Batch 258, Loss: 0.10349354147911072\n",
      "Batch 259, Loss: 0.10520447790622711\n",
      "Batch 260, Loss: 0.11318899691104889\n",
      "Batch 261, Loss: -1.1011850833892822\n",
      "Batch 262, Loss: 0.2772672772407532\n",
      "Batch 263, Loss: -1.2456529140472412\n",
      "Batch 264, Loss: -1.3805716037750244\n",
      "Batch 265, Loss: -1.3065612316131592\n",
      "Batch 266, Loss: -1.2649328708648682\n",
      "Batch 267, Loss: 0.1043536588549614\n",
      "Batch 268, Loss: 0.22248946130275726\n",
      "Batch 269, Loss: -1.2787280082702637\n",
      "Batch 270, Loss: 0.24118982255458832\n",
      "Batch 271, Loss: 0.10176987200975418\n",
      "Batch 272, Loss: -1.3107424974441528\n",
      "Batch 273, Loss: -1.3119966983795166\n",
      "Batch 274, Loss: -1.2450810670852661\n",
      "Batch 275, Loss: 0.23118655383586884\n",
      "Batch 276, Loss: 0.0959530770778656\n",
      "Batch 277, Loss: -1.298095941543579\n",
      "Batch 278, Loss: 0.2415318787097931\n",
      "Batch 279, Loss: -1.3674851655960083\n",
      "Batch 280, Loss: 0.27721917629241943\n",
      "Batch 281, Loss: -1.3161442279815674\n",
      "Batch 282, Loss: 0.0940866768360138\n",
      "Batch 283, Loss: -1.3824106454849243\n",
      "Batch 284, Loss: 0.28225308656692505\n",
      "Batch 285, Loss: 0.15473799407482147\n",
      "Batch 286, Loss: 0.0916823297739029\n",
      "Batch 287, Loss: -1.3947138786315918\n",
      "Batch 288, Loss: -1.4071807861328125\n",
      "Batch 289, Loss: -1.4010013341903687\n",
      "Batch 290, Loss: -1.0919578075408936\n",
      "Batch 291, Loss: -1.0902323722839355\n",
      "Batch 292, Loss: -1.088350772857666\n",
      "Batch 293, Loss: 0.41793662309646606\n",
      "Batch 294, Loss: 0.09016583114862442\n",
      "Batch 295, Loss: 0.08904111385345459\n",
      "Batch 296, Loss: 0.09034741669893265\n",
      "Batch 297, Loss: 0.09013140201568604\n",
      "Batch 298, Loss: 0.09271187335252762\n",
      "Batch 299, Loss: -1.453186273574829\n",
      "Batch 300, Loss: -1.4018726348876953\n",
      "Batch 301, Loss: 0.36706534028053284\n",
      "Batch 302, Loss: 0.08659004420042038\n",
      "Batch 303, Loss: -1.405893087387085\n",
      "Batch 304, Loss: 0.42483729124069214\n",
      "Batch 305, Loss: 0.0953521654009819\n",
      "Batch 306, Loss: -1.0899085998535156\n",
      "Batch 307, Loss: -1.0842211246490479\n",
      "Batch 308, Loss: 0.08509574085474014\n",
      "Batch 309, Loss: -1.4421510696411133\n",
      "Batch 310, Loss: -1.4417426586151123\n",
      "Batch 311, Loss: -1.4480079412460327\n",
      "Batch 312, Loss: -1.2162679433822632\n",
      "Batch 313, Loss: -1.4577237367630005\n",
      "Batch 314, Loss: -1.0839239358901978\n",
      "Batch 315, Loss: -1.1064367294311523\n",
      "Batch 316, Loss: -1.0869407653808594\n",
      "Batch 317, Loss: 0.08565211296081543\n",
      "Batch 318, Loss: -1.0865478515625\n",
      "Batch 319, Loss: -1.4519387483596802\n",
      "Batch 320, Loss: 0.09455334395170212\n",
      "Batch 321, Loss: -1.274867057800293\n",
      "Batch 322, Loss: 0.435931921005249\n",
      "Batch 323, Loss: 0.4615003168582916\n",
      "Batch 324, Loss: -1.4610154628753662\n",
      "Batch 325, Loss: 0.4602535665035248\n",
      "Batch 326, Loss: 0.311468243598938\n",
      "Batch 327, Loss: 0.45727312564849854\n",
      "Batch 328, Loss: 0.4668942987918854\n",
      "Batch 329, Loss: -1.1324228048324585\n",
      "Batch 330, Loss: -1.406975269317627\n",
      "Batch 331, Loss: 0.08945631980895996\n",
      "Batch 332, Loss: 0.4246862530708313\n",
      "Batch 333, Loss: 0.4410982131958008\n",
      "Batch 334, Loss: 0.09164309501647949\n",
      "Batch 335, Loss: 0.4497483968734741\n",
      "Batch 336, Loss: -1.442723274230957\n",
      "Batch 337, Loss: 0.44768863916397095\n",
      "Batch 338, Loss: 0.43021485209465027\n",
      "Batch 339, Loss: -1.4065732955932617\n",
      "Batch 340, Loss: -1.2885828018188477\n",
      "Batch 341, Loss: 0.4058215618133545\n",
      "Batch 342, Loss: -1.0908852815628052\n",
      "Batch 343, Loss: 0.41151267290115356\n",
      "Batch 344, Loss: 0.398717999458313\n",
      "Batch 345, Loss: -1.4105677604675293\n",
      "Batch 346, Loss: 0.4245116710662842\n",
      "Batch 347, Loss: -1.4536553621292114\n",
      "Batch 348, Loss: 0.4090159833431244\n",
      "Batch 349, Loss: -1.0894830226898193\n",
      "Batch 350, Loss: -1.0900518894195557\n",
      "Batch 351, Loss: -1.085538625717163\n",
      "Batch 352, Loss: -1.4028668403625488\n",
      "Batch 353, Loss: 0.09998936206102371\n",
      "Batch 354, Loss: -1.438473105430603\n",
      "Batch 355, Loss: -1.4619646072387695\n",
      "Batch 356, Loss: -1.4490163326263428\n",
      "Batch 357, Loss: -1.4540412425994873\n",
      "Batch 358, Loss: -1.0882298946380615\n",
      "Batch 359, Loss: 0.44949203729629517\n",
      "Batch 360, Loss: 0.23384292423725128\n",
      "Batch 361, Loss: -1.4538586139678955\n",
      "Batch 362, Loss: 0.46068233251571655\n",
      "Batch 363, Loss: 0.4123072028160095\n",
      "Batch 364, Loss: -1.0913432836532593\n",
      "Batch 365, Loss: -1.4529507160186768\n",
      "Batch 366, Loss: -1.0910212993621826\n",
      "Batch 367, Loss: 0.4540683925151825\n",
      "Batch 368, Loss: 0.45499035716056824\n",
      "Batch 369, Loss: -1.4554868936538696\n",
      "Batch 370, Loss: 0.25702059268951416\n",
      "Batch 371, Loss: 0.3266920745372772\n",
      "Batch 372, Loss: 0.44779491424560547\n",
      "Batch 373, Loss: 0.44905269145965576\n",
      "Batch 374, Loss: 0.3456982374191284\n",
      "Batch 375, Loss: 0.27665263414382935\n",
      "Batch 376, Loss: -1.1069867610931396\n",
      "Batch 377, Loss: 0.40064936876296997\n",
      "Batch 378, Loss: 0.19851918518543243\n",
      "Batch 379, Loss: -1.431295394897461\n",
      "Batch 380, Loss: -1.2380744218826294\n",
      "Batch 381, Loss: -1.2337908744812012\n",
      "Batch 382, Loss: 0.3927614986896515\n",
      "Batch 383, Loss: 0.19698630273342133\n",
      "Batch 384, Loss: -1.1769777536392212\n",
      "Batch 385, Loss: -1.4429824352264404\n",
      "Batch 386, Loss: 0.4398653507232666\n",
      "Batch 387, Loss: -1.194991111755371\n",
      "Batch 388, Loss: 0.09231694787740707\n",
      "Batch 389, Loss: -1.1805670261383057\n",
      "Batch 390, Loss: -1.0929796695709229\n",
      "Batch 391, Loss: 0.43612730503082275\n",
      "Batch 392, Loss: 0.2924128472805023\n",
      "Batch 393, Loss: -1.174520492553711\n",
      "Batch 394, Loss: 0.3764258623123169\n",
      "Batch 395, Loss: 0.26841744780540466\n",
      "Batch 396, Loss: -1.4192698001861572\n",
      "Batch 397, Loss: -1.418015718460083\n",
      "Batch 398, Loss: 0.4028818607330322\n",
      "Batch 399, Loss: -1.3088611364364624\n",
      "Training [50%]\tLoss: -0.5058\n",
      "Batch 0, Loss: -1.0269320011138916\n",
      "Batch 1, Loss: -1.307044267654419\n",
      "Batch 2, Loss: -1.0955023765563965\n",
      "Batch 3, Loss: -1.37909734249115\n",
      "Batch 4, Loss: -1.268728256225586\n",
      "Batch 5, Loss: -1.4324581623077393\n",
      "Batch 6, Loss: -1.333195686340332\n",
      "Batch 7, Loss: 0.24376153945922852\n",
      "Batch 8, Loss: 0.23627594113349915\n",
      "Batch 9, Loss: 0.16305460035800934\n",
      "Batch 10, Loss: 0.40071341395378113\n",
      "Batch 11, Loss: -1.2090847492218018\n",
      "Batch 12, Loss: -1.1381022930145264\n",
      "Batch 13, Loss: -1.4256701469421387\n",
      "Batch 14, Loss: 0.3212873935699463\n",
      "Batch 15, Loss: 0.3284702003002167\n",
      "Batch 16, Loss: -1.1392394304275513\n",
      "Batch 17, Loss: 0.27303293347358704\n",
      "Batch 18, Loss: -1.0924285650253296\n",
      "Batch 19, Loss: -1.125172734260559\n",
      "Batch 20, Loss: 0.29466044902801514\n",
      "Batch 21, Loss: -1.0264253616333008\n",
      "Batch 22, Loss: 0.06007231026887894\n",
      "Batch 23, Loss: 0.18039357662200928\n",
      "Batch 24, Loss: -1.1698708534240723\n",
      "Batch 25, Loss: -1.4434479475021362\n",
      "Batch 26, Loss: -1.3218638896942139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 27, Loss: 0.1917511522769928\n",
      "Batch 28, Loss: 0.43819379806518555\n",
      "Batch 29, Loss: -1.3919941186904907\n",
      "Batch 30, Loss: 0.13217246532440186\n",
      "Batch 31, Loss: 0.12062330543994904\n",
      "Batch 32, Loss: 0.4580234885215759\n",
      "Batch 33, Loss: -1.459869623184204\n",
      "Batch 34, Loss: -1.1089328527450562\n",
      "Batch 35, Loss: 0.45751291513442993\n",
      "Batch 36, Loss: 0.10647265613079071\n",
      "Batch 37, Loss: -1.105507254600525\n",
      "Batch 38, Loss: 0.10500821471214294\n",
      "Batch 39, Loss: 0.10621803253889084\n",
      "Batch 40, Loss: -1.1051950454711914\n",
      "Batch 41, Loss: -1.4480303525924683\n",
      "Batch 42, Loss: 0.4385669529438019\n",
      "Batch 43, Loss: 0.43716901540756226\n",
      "Batch 44, Loss: -1.4333912134170532\n",
      "Batch 45, Loss: -1.4330273866653442\n",
      "Batch 46, Loss: -1.1027991771697998\n",
      "Batch 47, Loss: -1.4337308406829834\n",
      "Batch 48, Loss: -1.4351357221603394\n",
      "Batch 49, Loss: -1.4363791942596436\n",
      "Batch 50, Loss: -1.436627745628357\n",
      "Batch 51, Loss: 0.4459129571914673\n",
      "Batch 52, Loss: 0.10481996089220047\n",
      "Batch 53, Loss: 0.44922471046447754\n",
      "Batch 54, Loss: 0.45009392499923706\n",
      "Batch 55, Loss: -1.4482965469360352\n",
      "Batch 56, Loss: -1.4521397352218628\n",
      "Batch 57, Loss: -1.1074891090393066\n",
      "Batch 58, Loss: -1.4509150981903076\n",
      "Batch 59, Loss: -1.1090275049209595\n",
      "Batch 60, Loss: 0.45720794796943665\n",
      "Batch 61, Loss: 0.11047714948654175\n",
      "Batch 62, Loss: -1.4592640399932861\n",
      "Batch 63, Loss: -1.4626868963241577\n",
      "Batch 64, Loss: 0.11204756051301956\n",
      "Batch 65, Loss: -1.1094448566436768\n",
      "Batch 66, Loss: 0.46370041370391846\n",
      "Batch 67, Loss: 0.4671822786331177\n",
      "Batch 68, Loss: -1.1103084087371826\n",
      "Batch 69, Loss: 0.46746718883514404\n",
      "Batch 70, Loss: -1.1102691888809204\n",
      "Batch 71, Loss: -1.464577317237854\n",
      "Batch 72, Loss: -1.4662034511566162\n",
      "Batch 73, Loss: -1.1167266368865967\n",
      "Batch 74, Loss: 0.46792253851890564\n",
      "Batch 75, Loss: -1.11508047580719\n",
      "Batch 76, Loss: -1.4687750339508057\n",
      "Batch 77, Loss: -1.4700216054916382\n",
      "Batch 78, Loss: -1.4705710411071777\n",
      "Batch 79, Loss: -1.1203364133834839\n",
      "Batch 80, Loss: -1.1256568431854248\n",
      "Batch 81, Loss: -1.1278564929962158\n",
      "Batch 82, Loss: 0.13085365295410156\n",
      "Batch 83, Loss: 0.12856712937355042\n",
      "Batch 84, Loss: -1.12953782081604\n",
      "Batch 85, Loss: -1.137656331062317\n",
      "Batch 86, Loss: -1.4773478507995605\n",
      "Batch 87, Loss: 0.4787956476211548\n",
      "Batch 88, Loss: 0.47922801971435547\n",
      "Batch 89, Loss: 0.4798291325569153\n",
      "Batch 90, Loss: 0.14609543979167938\n",
      "Batch 91, Loss: 0.47873175144195557\n",
      "Batch 92, Loss: 0.14039243757724762\n",
      "Batch 93, Loss: -1.4762639999389648\n",
      "Batch 94, Loss: 0.4781317114830017\n",
      "Batch 95, Loss: 0.47765442728996277\n",
      "Batch 96, Loss: -1.1499406099319458\n",
      "Batch 97, Loss: 0.4755859375\n",
      "Batch 98, Loss: 0.47486257553100586\n",
      "Batch 99, Loss: -1.472128987312317\n",
      "Batch 100, Loss: -1.4729033708572388\n",
      "Batch 101, Loss: -1.4728420972824097\n",
      "Batch 102, Loss: -1.1501575708389282\n",
      "Batch 103, Loss: 0.4738925099372864\n",
      "Batch 104, Loss: 0.472907155752182\n",
      "Batch 105, Loss: -1.4732816219329834\n",
      "Batch 106, Loss: -1.1520401239395142\n",
      "Batch 107, Loss: -1.1421113014221191\n",
      "Batch 108, Loss: -1.4731687307357788\n",
      "Batch 109, Loss: 0.4716845750808716\n",
      "Batch 110, Loss: 0.473291277885437\n",
      "Batch 111, Loss: 0.46680760383605957\n",
      "Batch 112, Loss: 0.17012305557727814\n",
      "Batch 113, Loss: -1.1527642011642456\n",
      "Batch 114, Loss: -1.4631094932556152\n",
      "Batch 115, Loss: 0.46577996015548706\n",
      "Batch 116, Loss: 0.18070030212402344\n",
      "Batch 117, Loss: -1.1600850820541382\n",
      "Batch 118, Loss: 0.4579285979270935\n",
      "Batch 119, Loss: 0.1803981363773346\n",
      "Batch 120, Loss: 0.17859341204166412\n",
      "Batch 121, Loss: -1.1566218137741089\n",
      "Batch 122, Loss: 0.4466782212257385\n",
      "Batch 123, Loss: -1.1565436124801636\n",
      "Batch 124, Loss: 0.4434967339038849\n",
      "Batch 125, Loss: 0.4337003231048584\n",
      "Batch 126, Loss: 0.4260469079017639\n",
      "Batch 127, Loss: 0.42077362537384033\n",
      "Batch 128, Loss: 0.40488141775131226\n",
      "Batch 129, Loss: 0.17196574807167053\n",
      "Batch 130, Loss: 0.3955981135368347\n",
      "Batch 131, Loss: 0.3905235230922699\n",
      "Batch 132, Loss: 0.3868509531021118\n",
      "Batch 133, Loss: -1.386469841003418\n",
      "Batch 134, Loss: 0.17811602354049683\n",
      "Batch 135, Loss: 0.38783085346221924\n",
      "Batch 136, Loss: -1.3884477615356445\n",
      "Batch 137, Loss: -1.176223874092102\n",
      "Batch 138, Loss: 0.3921244144439697\n",
      "Batch 139, Loss: 0.17825038731098175\n",
      "Batch 140, Loss: -1.3937263488769531\n",
      "Batch 141, Loss: 0.3947129249572754\n",
      "Batch 142, Loss: -1.1757087707519531\n",
      "Batch 143, Loss: -1.3941067457199097\n",
      "Batch 144, Loss: 0.3969382643699646\n",
      "Batch 145, Loss: -1.3972971439361572\n",
      "Batch 146, Loss: -1.3988487720489502\n",
      "Batch 147, Loss: -1.1810708045959473\n",
      "Batch 148, Loss: 0.18220514059066772\n",
      "Batch 149, Loss: -1.403548002243042\n",
      "Batch 150, Loss: -1.1875005960464478\n",
      "Batch 151, Loss: -1.1872929334640503\n",
      "Batch 152, Loss: -1.1943467855453491\n",
      "Batch 153, Loss: -1.4007017612457275\n",
      "Batch 154, Loss: 0.4072449207305908\n",
      "Batch 155, Loss: 0.40730637311935425\n",
      "Batch 156, Loss: -1.211130976676941\n",
      "Batch 157, Loss: 0.2170625776052475\n",
      "Batch 158, Loss: -1.220266342163086\n",
      "Batch 159, Loss: -1.4083837270736694\n",
      "Batch 160, Loss: -1.403670072555542\n",
      "Batch 161, Loss: -1.4100359678268433\n",
      "Batch 162, Loss: 0.24350059032440186\n",
      "Batch 163, Loss: 0.4096091389656067\n",
      "Batch 164, Loss: 0.4146617352962494\n",
      "Batch 165, Loss: 0.23997712135314941\n",
      "Batch 166, Loss: 0.4101601243019104\n",
      "Batch 167, Loss: 0.41344597935676575\n",
      "Batch 168, Loss: -1.4059929847717285\n",
      "Batch 169, Loss: -1.408743143081665\n",
      "Batch 170, Loss: -1.4099583625793457\n",
      "Batch 171, Loss: 0.41120296716690063\n",
      "Batch 172, Loss: 0.4114755690097809\n",
      "Batch 173, Loss: 0.4124496579170227\n",
      "Batch 174, Loss: -1.2366238832473755\n",
      "Batch 175, Loss: 0.4058799147605896\n",
      "Batch 176, Loss: 0.4067928194999695\n",
      "Batch 177, Loss: -1.2371010780334473\n",
      "Batch 178, Loss: -1.4000489711761475\n",
      "Batch 179, Loss: -1.2570446729660034\n",
      "Batch 180, Loss: -1.398111343383789\n",
      "Batch 181, Loss: -1.2474064826965332\n",
      "Batch 182, Loss: -1.398127555847168\n",
      "Batch 183, Loss: -1.2761154174804688\n",
      "Batch 184, Loss: 0.40091443061828613\n",
      "Batch 185, Loss: -1.3989856243133545\n",
      "Batch 186, Loss: 0.3021903336048126\n",
      "Batch 187, Loss: -1.3986945152282715\n",
      "Batch 188, Loss: 0.4058140814304352\n",
      "Batch 189, Loss: -1.405795931816101\n",
      "Batch 190, Loss: 0.298770010471344\n",
      "Batch 191, Loss: 0.4072437286376953\n",
      "Batch 192, Loss: 0.4104519784450531\n",
      "Batch 193, Loss: 0.410230427980423\n",
      "Batch 194, Loss: 0.4077623784542084\n",
      "Batch 195, Loss: -1.3096853494644165\n",
      "Batch 196, Loss: 0.4029938876628876\n",
      "Batch 197, Loss: 0.3158259689807892\n",
      "Batch 198, Loss: 0.2966066002845764\n",
      "Batch 199, Loss: 0.29280170798301697\n",
      "Batch 200, Loss: -1.2865843772888184\n",
      "Batch 201, Loss: -1.3817013502120972\n",
      "Batch 202, Loss: -1.3094897270202637\n",
      "Batch 203, Loss: -1.3868049383163452\n",
      "Batch 204, Loss: 0.27602365612983704\n",
      "Batch 205, Loss: -1.3892700672149658\n",
      "Batch 206, Loss: -1.3957853317260742\n",
      "Batch 207, Loss: -1.3892635107040405\n",
      "Batch 208, Loss: -1.3071444034576416\n",
      "Batch 209, Loss: 0.40323832631111145\n",
      "Batch 210, Loss: 0.2692980170249939\n",
      "Batch 211, Loss: 0.27384382486343384\n",
      "Batch 212, Loss: -1.40645170211792\n",
      "Batch 213, Loss: 0.41158613562583923\n",
      "Batch 214, Loss: -1.2978966236114502\n",
      "Batch 215, Loss: -1.308670163154602\n",
      "Batch 216, Loss: -1.4111920595169067\n",
      "Batch 217, Loss: 0.4144291281700134\n",
      "Batch 218, Loss: 0.31699612736701965\n",
      "Batch 219, Loss: -1.4150564670562744\n",
      "Batch 220, Loss: -1.2588781118392944\n",
      "Batch 221, Loss: 0.3195013999938965\n",
      "Batch 222, Loss: -1.2594209909439087\n",
      "Batch 223, Loss: 0.41704973578453064\n",
      "Batch 224, Loss: 0.4168612062931061\n",
      "Batch 225, Loss: -1.4165213108062744\n",
      "Batch 226, Loss: -1.4159317016601562\n",
      "Batch 227, Loss: -1.2715232372283936\n",
      "Batch 228, Loss: -1.2669572830200195\n",
      "Batch 229, Loss: -1.4187322854995728\n",
      "Batch 230, Loss: -1.4193271398544312\n",
      "Batch 231, Loss: -1.2803940773010254\n",
      "Batch 232, Loss: 0.4216221570968628\n",
      "Batch 233, Loss: 0.3385370373725891\n",
      "Batch 234, Loss: -1.3049278259277344\n",
      "Batch 235, Loss: 0.34103792905807495\n",
      "Batch 236, Loss: 0.42493027448654175\n",
      "Batch 237, Loss: -1.425796389579773\n",
      "Batch 238, Loss: 0.4266487956047058\n",
      "Batch 239, Loss: -1.3362722396850586\n",
      "Batch 240, Loss: -1.3084352016448975\n",
      "Batch 241, Loss: -1.4249547719955444\n",
      "Batch 242, Loss: 0.4270390272140503\n",
      "Batch 243, Loss: -1.4277456998825073\n",
      "Batch 244, Loss: 0.3425672650337219\n",
      "Batch 245, Loss: 0.3534885346889496\n",
      "Batch 246, Loss: -1.4273850917816162\n",
      "Batch 247, Loss: 0.36755627393722534\n",
      "Batch 248, Loss: -1.3378578424453735\n",
      "Batch 249, Loss: -1.43043851852417\n",
      "Batch 250, Loss: -1.4319180250167847\n",
      "Batch 251, Loss: 0.43288564682006836\n",
      "Batch 252, Loss: 0.34040185809135437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 253, Loss: 0.33696603775024414\n",
      "Batch 254, Loss: -1.4340224266052246\n",
      "Batch 255, Loss: 0.31529709696769714\n",
      "Batch 256, Loss: 0.3105313777923584\n",
      "Batch 257, Loss: 0.4337483048439026\n",
      "Batch 258, Loss: 0.4326961934566498\n",
      "Batch 259, Loss: -1.299720048904419\n",
      "Batch 260, Loss: -1.431962490081787\n",
      "Batch 261, Loss: -1.2931690216064453\n",
      "Batch 262, Loss: 0.2927711606025696\n",
      "Batch 263, Loss: 0.42979931831359863\n",
      "Batch 264, Loss: 0.42878013849258423\n",
      "Batch 265, Loss: 0.4292523264884949\n",
      "Batch 266, Loss: 0.27600568532943726\n",
      "Batch 267, Loss: 0.27696356177330017\n",
      "Batch 268, Loss: 0.42004624009132385\n",
      "Batch 269, Loss: 0.2648286521434784\n",
      "Batch 270, Loss: 0.4158574342727661\n",
      "Batch 271, Loss: 0.4118729829788208\n",
      "Batch 272, Loss: 0.4078965485095978\n",
      "Batch 273, Loss: 0.23408690094947815\n",
      "Batch 274, Loss: -1.3969401121139526\n",
      "Batch 275, Loss: 0.23219892382621765\n",
      "Batch 276, Loss: 0.39128077030181885\n",
      "Batch 277, Loss: -1.3853765726089478\n",
      "Batch 278, Loss: 0.38297387957572937\n",
      "Batch 279, Loss: -1.1979161500930786\n",
      "Batch 280, Loss: -1.196506381034851\n",
      "Batch 281, Loss: 0.1934058666229248\n",
      "Batch 282, Loss: -1.1977425813674927\n",
      "Batch 283, Loss: -1.373948097229004\n",
      "Batch 284, Loss: 0.1980365514755249\n",
      "Batch 285, Loss: 0.3727547824382782\n",
      "Batch 286, Loss: 0.18552005290985107\n",
      "Batch 287, Loss: 0.1903052031993866\n",
      "Batch 288, Loss: 0.3701232373714447\n",
      "Batch 289, Loss: -1.181921124458313\n",
      "Batch 290, Loss: -1.3689777851104736\n",
      "Batch 291, Loss: 0.368854820728302\n",
      "Batch 292, Loss: -1.3685795068740845\n",
      "Batch 293, Loss: 0.16913560032844543\n",
      "Batch 294, Loss: -1.1714283227920532\n",
      "Batch 295, Loss: 0.17164106667041779\n",
      "Batch 296, Loss: -1.1695988178253174\n",
      "Batch 297, Loss: -1.3677382469177246\n",
      "Batch 298, Loss: 0.16850173473358154\n",
      "Batch 299, Loss: -1.3684786558151245\n",
      "Batch 300, Loss: 0.3705874979496002\n",
      "Batch 301, Loss: 0.15896238386631012\n",
      "Batch 302, Loss: -1.3711495399475098\n",
      "Batch 303, Loss: -1.1591719388961792\n",
      "Batch 304, Loss: 0.372820645570755\n",
      "Batch 305, Loss: -1.1593270301818848\n",
      "Batch 306, Loss: 0.15941448509693146\n",
      "Batch 307, Loss: -1.1582552194595337\n",
      "Batch 308, Loss: -1.3710377216339111\n",
      "Batch 309, Loss: -1.1552176475524902\n",
      "Batch 310, Loss: -1.1607190370559692\n",
      "Batch 311, Loss: -1.3720353841781616\n",
      "Batch 312, Loss: -1.1601064205169678\n",
      "Batch 313, Loss: 0.3752773106098175\n",
      "Batch 314, Loss: 0.3733210563659668\n",
      "Batch 315, Loss: -1.3728597164154053\n",
      "Batch 316, Loss: -1.374576449394226\n",
      "Batch 317, Loss: -1.1779447793960571\n",
      "Batch 318, Loss: -1.1802451610565186\n",
      "Batch 319, Loss: -1.1853629350662231\n",
      "Batch 320, Loss: 0.3759978115558624\n",
      "Batch 321, Loss: -1.195879340171814\n",
      "Batch 322, Loss: 0.3743392825126648\n",
      "Batch 323, Loss: -1.375294804573059\n",
      "Batch 324, Loss: -1.375307321548462\n",
      "Batch 325, Loss: 0.20294860005378723\n",
      "Batch 326, Loss: 0.3757244944572449\n",
      "Batch 327, Loss: 0.37593045830726624\n",
      "Batch 328, Loss: -1.3761035203933716\n",
      "Batch 329, Loss: 0.22899490594863892\n",
      "Batch 330, Loss: -1.22854483127594\n",
      "Batch 331, Loss: 0.37637564539909363\n",
      "Batch 332, Loss: -1.3762931823730469\n",
      "Batch 333, Loss: 0.376169890165329\n",
      "Batch 334, Loss: 0.37600967288017273\n",
      "Batch 335, Loss: 0.21224021911621094\n",
      "Batch 336, Loss: 0.37572404742240906\n",
      "Batch 337, Loss: 0.37508612871170044\n",
      "Batch 338, Loss: 0.20954486727714539\n",
      "Batch 339, Loss: -1.2087593078613281\n",
      "Batch 340, Loss: 0.37297558784484863\n",
      "Batch 341, Loss: -1.3723527193069458\n",
      "Batch 342, Loss: -1.3710169792175293\n",
      "Batch 343, Loss: 0.23220041394233704\n",
      "Batch 344, Loss: 0.20468106865882874\n",
      "Batch 345, Loss: 0.2020927220582962\n",
      "Batch 346, Loss: 0.36951103806495667\n",
      "Batch 347, Loss: 0.19518029689788818\n",
      "Batch 348, Loss: 0.19244596362113953\n",
      "Batch 349, Loss: -1.369093894958496\n",
      "Batch 350, Loss: 0.2018922120332718\n",
      "Batch 351, Loss: 0.17803612351417542\n",
      "Batch 352, Loss: 0.3701263666152954\n",
      "Batch 353, Loss: 0.16677378118038177\n",
      "Batch 354, Loss: 0.36672985553741455\n",
      "Batch 355, Loss: -1.1581742763519287\n",
      "Batch 356, Loss: -1.369583249092102\n",
      "Batch 357, Loss: -1.1636459827423096\n",
      "Batch 358, Loss: 0.3663480579853058\n",
      "Batch 359, Loss: 0.16108156740665436\n",
      "Batch 360, Loss: -1.3695273399353027\n",
      "Batch 361, Loss: 0.3701639771461487\n",
      "Batch 362, Loss: 0.14371564984321594\n",
      "Batch 363, Loss: 0.36975258588790894\n",
      "Batch 364, Loss: 0.13978001475334167\n",
      "Batch 365, Loss: -1.3679680824279785\n",
      "Batch 366, Loss: 0.3681717813014984\n",
      "Batch 367, Loss: 0.14393994212150574\n",
      "Batch 368, Loss: 0.3670473098754883\n",
      "Batch 369, Loss: -1.137303352355957\n",
      "Batch 370, Loss: -1.3642311096191406\n",
      "Batch 371, Loss: 0.1274053454399109\n",
      "Batch 372, Loss: 0.13347399234771729\n",
      "Batch 373, Loss: -1.1314116716384888\n",
      "Batch 374, Loss: 0.35969918966293335\n",
      "Batch 375, Loss: -1.1219104528427124\n",
      "Batch 376, Loss: -1.3579928874969482\n",
      "Batch 377, Loss: 0.3579462468624115\n",
      "Batch 378, Loss: 0.35717031359672546\n",
      "Batch 379, Loss: -1.1281373500823975\n",
      "Batch 380, Loss: -1.3580518960952759\n",
      "Batch 381, Loss: -1.3575575351715088\n",
      "Batch 382, Loss: 0.12179101258516312\n",
      "Batch 383, Loss: -1.1300266981124878\n",
      "Batch 384, Loss: -1.1217235326766968\n",
      "Batch 385, Loss: 0.35478582978248596\n",
      "Batch 386, Loss: 0.3580392897129059\n",
      "Batch 387, Loss: -1.1342805624008179\n",
      "Batch 388, Loss: -1.136631965637207\n",
      "Batch 389, Loss: -1.3551340103149414\n",
      "Batch 390, Loss: -1.1421186923980713\n",
      "Batch 391, Loss: -1.354651689529419\n",
      "Batch 392, Loss: -1.1324987411499023\n",
      "Batch 393, Loss: -1.135166883468628\n",
      "Batch 394, Loss: -1.3531181812286377\n",
      "Batch 395, Loss: -1.161439061164856\n",
      "Batch 396, Loss: 0.16902339458465576\n",
      "Batch 397, Loss: 0.35544508695602417\n",
      "Batch 398, Loss: -1.1732910871505737\n",
      "Batch 399, Loss: 0.35698002576828003\n",
      "Training [60%]\tLoss: -0.4855\n",
      "Batch 0, Loss: 0.35691359639167786\n",
      "Batch 1, Loss: 0.18490447103977203\n",
      "Batch 2, Loss: 0.3570445775985718\n",
      "Batch 3, Loss: 0.35668593645095825\n",
      "Batch 4, Loss: -1.156479001045227\n",
      "Batch 5, Loss: -1.15763521194458\n",
      "Batch 6, Loss: 0.15954354405403137\n",
      "Batch 7, Loss: -1.160398006439209\n",
      "Batch 8, Loss: -1.1624559164047241\n",
      "Batch 9, Loss: 0.1644594371318817\n",
      "Batch 10, Loss: -1.197880744934082\n",
      "Batch 11, Loss: -1.3556478023529053\n",
      "Batch 12, Loss: 0.20534446835517883\n",
      "Batch 13, Loss: 0.35678040981292725\n",
      "Batch 14, Loss: 0.17172303795814514\n",
      "Batch 15, Loss: 0.35289645195007324\n",
      "Batch 16, Loss: 0.17124716937541962\n",
      "Batch 17, Loss: 0.16929024457931519\n",
      "Batch 18, Loss: -1.1993062496185303\n",
      "Batch 19, Loss: 0.1661188304424286\n",
      "Batch 20, Loss: 0.19883091747760773\n",
      "Batch 21, Loss: -1.1621990203857422\n",
      "Batch 22, Loss: -1.3496754169464111\n",
      "Batch 23, Loss: 0.19091448187828064\n",
      "Batch 24, Loss: 0.157017320394516\n",
      "Batch 25, Loss: -1.1834737062454224\n",
      "Batch 26, Loss: -1.1817179918289185\n",
      "Batch 27, Loss: 0.34860390424728394\n",
      "Batch 28, Loss: -1.1524966955184937\n",
      "Batch 29, Loss: 0.3495115041732788\n",
      "Batch 30, Loss: 0.34795090556144714\n",
      "Batch 31, Loss: 0.18439336121082306\n",
      "Batch 32, Loss: -1.152014970779419\n",
      "Batch 33, Loss: -1.3475229740142822\n",
      "Batch 34, Loss: 0.3473649322986603\n",
      "Batch 35, Loss: -1.3456757068634033\n",
      "Batch 36, Loss: 0.346876859664917\n",
      "Batch 37, Loss: 0.18251261115074158\n",
      "Batch 38, Loss: -1.179335355758667\n",
      "Batch 39, Loss: -1.1527007818222046\n",
      "Batch 40, Loss: 0.1530102789402008\n",
      "Batch 41, Loss: -1.3444573879241943\n",
      "Batch 42, Loss: -1.3448518514633179\n",
      "Batch 43, Loss: 0.3448305130004883\n",
      "Batch 44, Loss: 0.180892676115036\n",
      "Batch 45, Loss: 0.15283702313899994\n",
      "Batch 46, Loss: -1.3442384004592896\n",
      "Batch 47, Loss: -1.1500072479248047\n",
      "Batch 48, Loss: -1.3453892469406128\n",
      "Batch 49, Loss: 0.34459057450294495\n",
      "Batch 50, Loss: 0.1724163293838501\n",
      "Batch 51, Loss: -1.1698713302612305\n",
      "Batch 52, Loss: 0.16812527179718018\n",
      "Batch 53, Loss: -1.3445920944213867\n",
      "Batch 54, Loss: 0.3470798134803772\n",
      "Batch 55, Loss: 0.34722042083740234\n",
      "Batch 56, Loss: -1.1625840663909912\n",
      "Batch 57, Loss: -1.3443872928619385\n",
      "Batch 58, Loss: 0.14564485847949982\n",
      "Batch 59, Loss: -1.3479821681976318\n",
      "Batch 60, Loss: 0.14502866566181183\n",
      "Batch 61, Loss: -1.143270492553711\n",
      "Batch 62, Loss: -1.1615158319473267\n",
      "Batch 63, Loss: 0.34629660844802856\n",
      "Batch 64, Loss: 0.3460615277290344\n",
      "Batch 65, Loss: -1.345307469367981\n",
      "Batch 66, Loss: 0.3454418182373047\n",
      "Batch 67, Loss: -1.3450920581817627\n",
      "Batch 68, Loss: -1.3450510501861572\n",
      "Batch 69, Loss: -1.1484147310256958\n",
      "Batch 70, Loss: -1.3471789360046387\n",
      "Batch 71, Loss: 0.1733548939228058\n",
      "Batch 72, Loss: -1.348853588104248\n",
      "Batch 73, Loss: -1.3473341464996338\n",
      "Batch 74, Loss: -1.1728168725967407\n",
      "Batch 75, Loss: -1.1749533414840698\n",
      "Batch 76, Loss: -1.3545396327972412\n",
      "Batch 77, Loss: 0.15887685120105743\n",
      "Batch 78, Loss: -1.1907743215560913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 79, Loss: 0.36081087589263916\n",
      "Batch 80, Loss: 0.16287006437778473\n",
      "Batch 81, Loss: 0.19498616456985474\n",
      "Batch 82, Loss: 0.1947360336780548\n",
      "Batch 83, Loss: -1.3561570644378662\n",
      "Batch 84, Loss: -1.1615562438964844\n",
      "Batch 85, Loss: 0.3636150658130646\n",
      "Batch 86, Loss: 0.1611672043800354\n",
      "Batch 87, Loss: -1.3564454317092896\n",
      "Batch 88, Loss: -1.1868226528167725\n",
      "Batch 89, Loss: 0.18559443950653076\n",
      "Batch 90, Loss: -1.3631064891815186\n",
      "Batch 91, Loss: -1.156935691833496\n",
      "Batch 92, Loss: -1.3590173721313477\n",
      "Batch 93, Loss: 0.3659708499908447\n",
      "Batch 94, Loss: 0.3608434796333313\n",
      "Batch 95, Loss: 0.1838914155960083\n",
      "Batch 96, Loss: 0.18395480513572693\n",
      "Batch 97, Loss: 0.36483490467071533\n",
      "Batch 98, Loss: 0.1764037013053894\n",
      "Batch 99, Loss: -1.3572230339050293\n",
      "Batch 100, Loss: 0.16744250059127808\n",
      "Batch 101, Loss: -1.355918526649475\n",
      "Batch 102, Loss: -1.1570452451705933\n",
      "Batch 103, Loss: -1.3574600219726562\n",
      "Batch 104, Loss: 0.355872243642807\n",
      "Batch 105, Loss: -1.3558546304702759\n",
      "Batch 106, Loss: -1.1417341232299805\n",
      "Batch 107, Loss: 0.3565084934234619\n",
      "Batch 108, Loss: -1.356594443321228\n",
      "Batch 109, Loss: -1.1418136358261108\n",
      "Batch 110, Loss: 0.3587789237499237\n",
      "Batch 111, Loss: 0.15098677575588226\n",
      "Batch 112, Loss: 0.35744720697402954\n",
      "Batch 113, Loss: -1.142288327217102\n",
      "Batch 114, Loss: -1.3570064306259155\n",
      "Batch 115, Loss: -1.1422935724258423\n",
      "Batch 116, Loss: 0.14403441548347473\n",
      "Batch 117, Loss: -1.3575594425201416\n",
      "Batch 118, Loss: -1.3579752445220947\n",
      "Batch 119, Loss: 0.14454792439937592\n",
      "Batch 120, Loss: 0.35897505283355713\n",
      "Batch 121, Loss: -1.3599961996078491\n",
      "Batch 122, Loss: 0.3603610396385193\n",
      "Batch 123, Loss: 0.1481907069683075\n",
      "Batch 124, Loss: -1.3600914478302002\n",
      "Batch 125, Loss: -1.1469107866287231\n",
      "Batch 126, Loss: -1.3605855703353882\n",
      "Batch 127, Loss: -1.3611133098602295\n",
      "Batch 128, Loss: -1.1433790922164917\n",
      "Batch 129, Loss: 0.36310791969299316\n",
      "Batch 130, Loss: -1.362636923789978\n",
      "Batch 131, Loss: -1.3634710311889648\n",
      "Batch 132, Loss: 0.3658044934272766\n",
      "Batch 133, Loss: 0.14708277583122253\n",
      "Batch 134, Loss: 0.366360068321228\n",
      "Batch 135, Loss: -1.365486741065979\n",
      "Batch 136, Loss: -1.1505153179168701\n",
      "Batch 137, Loss: -1.3676698207855225\n",
      "Batch 138, Loss: 0.367144912481308\n",
      "Batch 139, Loss: 0.14855124056339264\n",
      "Batch 140, Loss: 0.15265390276908875\n",
      "Batch 141, Loss: 0.3690529465675354\n",
      "Batch 142, Loss: 0.36843135952949524\n",
      "Batch 143, Loss: 0.3674372136592865\n",
      "Batch 144, Loss: -1.1488442420959473\n",
      "Batch 145, Loss: -1.1471387147903442\n",
      "Batch 146, Loss: 0.3648029565811157\n",
      "Batch 147, Loss: -1.3638991117477417\n",
      "Batch 148, Loss: -1.1436206102371216\n",
      "Batch 149, Loss: -1.144458293914795\n",
      "Batch 150, Loss: 0.1504099816083908\n",
      "Batch 151, Loss: 0.3626430332660675\n",
      "Batch 152, Loss: 0.3629017472267151\n",
      "Batch 153, Loss: -1.1514190435409546\n",
      "Batch 154, Loss: 0.152361661195755\n",
      "Batch 155, Loss: 0.3606868386268616\n",
      "Batch 156, Loss: -1.360030174255371\n",
      "Batch 157, Loss: -1.360132098197937\n",
      "Batch 158, Loss: 0.35955291986465454\n",
      "Batch 159, Loss: 0.3592471480369568\n",
      "Batch 160, Loss: -1.1536849737167358\n",
      "Batch 161, Loss: -1.149732232093811\n",
      "Batch 162, Loss: -1.1499180793762207\n",
      "Batch 163, Loss: -1.3588312864303589\n",
      "Batch 164, Loss: -1.3592212200164795\n",
      "Batch 165, Loss: -1.155730128288269\n",
      "Batch 166, Loss: 0.15829183161258698\n",
      "Batch 167, Loss: -1.16508150100708\n",
      "Batch 168, Loss: -1.162048101425171\n",
      "Batch 169, Loss: -1.3618817329406738\n",
      "Batch 170, Loss: -1.1760518550872803\n",
      "Batch 171, Loss: 0.17312213778495789\n",
      "Batch 172, Loss: 0.36739277839660645\n",
      "Batch 173, Loss: 0.17530278861522675\n",
      "Batch 174, Loss: -1.3681567907333374\n",
      "Batch 175, Loss: 0.18721258640289307\n",
      "Batch 176, Loss: -1.3669360876083374\n",
      "Batch 177, Loss: 0.3673431873321533\n",
      "Batch 178, Loss: 0.3697706162929535\n",
      "Batch 179, Loss: 0.3693344295024872\n",
      "Batch 180, Loss: -1.185388445854187\n",
      "Batch 181, Loss: -1.3679091930389404\n",
      "Batch 182, Loss: 0.36592620611190796\n",
      "Batch 183, Loss: -1.1765472888946533\n",
      "Batch 184, Loss: 0.17771172523498535\n",
      "Batch 185, Loss: 0.36549070477485657\n",
      "Batch 186, Loss: 0.3669324517250061\n",
      "Batch 187, Loss: -1.185772180557251\n",
      "Batch 188, Loss: -1.1866569519042969\n",
      "Batch 189, Loss: 0.36274027824401855\n",
      "Batch 190, Loss: -1.3618810176849365\n",
      "Batch 191, Loss: -1.3616479635238647\n",
      "Batch 192, Loss: -1.194071650505066\n",
      "Batch 193, Loss: 0.18510231375694275\n",
      "Batch 194, Loss: 0.18613767623901367\n",
      "Batch 195, Loss: -1.3645527362823486\n",
      "Batch 196, Loss: 0.18829095363616943\n",
      "Batch 197, Loss: -1.3649946451187134\n",
      "Batch 198, Loss: 0.20280106365680695\n",
      "Batch 199, Loss: 0.18539269268512726\n",
      "Batch 200, Loss: 0.18134571611881256\n",
      "Batch 201, Loss: -1.3648200035095215\n",
      "Batch 202, Loss: -1.3650507926940918\n",
      "Batch 203, Loss: -1.3653315305709839\n",
      "Batch 204, Loss: -1.3633729219436646\n",
      "Batch 205, Loss: -1.3644063472747803\n",
      "Batch 206, Loss: -1.3691117763519287\n",
      "Batch 207, Loss: 0.3675522208213806\n",
      "Batch 208, Loss: 0.37271448969841003\n",
      "Batch 209, Loss: -1.1864972114562988\n",
      "Batch 210, Loss: -1.1700152158737183\n",
      "Batch 211, Loss: 0.17111864686012268\n",
      "Batch 212, Loss: 0.17123141884803772\n",
      "Batch 213, Loss: 0.19021272659301758\n",
      "Batch 214, Loss: -1.3766162395477295\n",
      "Batch 215, Loss: 0.18687430024147034\n",
      "Batch 216, Loss: 0.3771020174026489\n",
      "Batch 217, Loss: -1.3719465732574463\n",
      "Batch 218, Loss: 0.3762773275375366\n",
      "Batch 219, Loss: 0.3755338490009308\n",
      "Batch 220, Loss: -1.1732032299041748\n",
      "Batch 221, Loss: -1.3734709024429321\n",
      "Batch 222, Loss: -1.1720486879348755\n",
      "Batch 223, Loss: -1.1725634336471558\n",
      "Batch 224, Loss: 0.17503516376018524\n",
      "Batch 225, Loss: -1.371718406677246\n",
      "Batch 226, Loss: 0.3717014789581299\n",
      "Batch 227, Loss: -1.1602879762649536\n",
      "Batch 228, Loss: 0.1777777373790741\n",
      "Batch 229, Loss: 0.37225353717803955\n",
      "Batch 230, Loss: 0.3718383312225342\n",
      "Batch 231, Loss: 0.374833881855011\n",
      "Batch 232, Loss: -1.1594462394714355\n",
      "Batch 233, Loss: 0.37234872579574585\n",
      "Batch 234, Loss: 0.3679291605949402\n",
      "Batch 235, Loss: 0.1591029167175293\n",
      "Batch 236, Loss: -1.1718114614486694\n",
      "Batch 237, Loss: -1.3665587902069092\n",
      "Batch 238, Loss: 0.3659982681274414\n",
      "Batch 239, Loss: -1.1719542741775513\n",
      "Batch 240, Loss: 0.3646557033061981\n",
      "Batch 241, Loss: -1.3638827800750732\n",
      "Batch 242, Loss: 0.17511461675167084\n",
      "Batch 243, Loss: 0.1748102307319641\n",
      "Batch 244, Loss: 0.36217665672302246\n",
      "Batch 245, Loss: 0.17121091485023499\n",
      "Batch 246, Loss: -1.168198823928833\n",
      "Batch 247, Loss: 0.15365895628929138\n",
      "Batch 248, Loss: 0.3591797351837158\n",
      "Batch 249, Loss: -1.358991265296936\n",
      "Batch 250, Loss: -1.3582757711410522\n",
      "Batch 251, Loss: 0.358280748128891\n",
      "Batch 252, Loss: -1.147823452949524\n",
      "Batch 253, Loss: 0.3578170835971832\n",
      "Batch 254, Loss: -1.1475549936294556\n",
      "Batch 255, Loss: 0.3561815917491913\n",
      "Batch 256, Loss: -1.3563507795333862\n",
      "Batch 257, Loss: 0.14896295964717865\n",
      "Batch 258, Loss: 0.15978851914405823\n",
      "Batch 259, Loss: -1.1583205461502075\n",
      "Batch 260, Loss: 0.1474253237247467\n",
      "Batch 261, Loss: 0.35515546798706055\n",
      "Batch 262, Loss: 0.35380539298057556\n",
      "Batch 263, Loss: -1.3531098365783691\n",
      "Batch 264, Loss: -1.1443569660186768\n",
      "Batch 265, Loss: -1.1547584533691406\n",
      "Batch 266, Loss: 0.15589022636413574\n",
      "Batch 267, Loss: -1.352140188217163\n",
      "Batch 268, Loss: -1.145325779914856\n",
      "Batch 269, Loss: -1.3523280620574951\n",
      "Batch 270, Loss: -1.1574503183364868\n",
      "Batch 271, Loss: -1.148327112197876\n",
      "Batch 272, Loss: -1.3538925647735596\n",
      "Batch 273, Loss: 0.15223805606365204\n",
      "Batch 274, Loss: -1.354975938796997\n",
      "Batch 275, Loss: 0.15421123802661896\n",
      "Batch 276, Loss: -1.1666613817214966\n",
      "Batch 277, Loss: 0.16813218593597412\n",
      "Batch 278, Loss: -1.1682567596435547\n",
      "Batch 279, Loss: 0.15616212785243988\n",
      "Batch 280, Loss: -1.3581089973449707\n",
      "Batch 281, Loss: 0.15633255243301392\n",
      "Batch 282, Loss: -1.359158992767334\n",
      "Batch 283, Loss: 0.17021584510803223\n",
      "Batch 284, Loss: -1.3597955703735352\n",
      "Batch 285, Loss: 0.153324156999588\n",
      "Batch 286, Loss: -1.3609663248062134\n",
      "Batch 287, Loss: -1.361222505569458\n",
      "Batch 288, Loss: -1.3619580268859863\n",
      "Batch 289, Loss: -1.3628686666488647\n",
      "Batch 290, Loss: 0.16158422827720642\n",
      "Batch 291, Loss: 0.3659549355506897\n",
      "Batch 292, Loss: -1.3653590679168701\n",
      "Batch 293, Loss: -1.1447402238845825\n",
      "Batch 294, Loss: -1.1563081741333008\n",
      "Batch 295, Loss: 0.14502522349357605\n",
      "Batch 296, Loss: 0.3693329989910126\n",
      "Batch 297, Loss: -1.3700164556503296\n",
      "Batch 298, Loss: 0.14421315491199493\n",
      "Batch 299, Loss: -1.371422529220581\n",
      "Batch 300, Loss: -1.3698408603668213\n",
      "Batch 301, Loss: -1.153456687927246\n",
      "Batch 302, Loss: 0.15378151834011078\n",
      "Batch 303, Loss: -1.3757433891296387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 304, Loss: -1.3772039413452148\n",
      "Batch 305, Loss: 0.3757012188434601\n",
      "Batch 306, Loss: -1.1400917768478394\n",
      "Batch 307, Loss: 0.38164180517196655\n",
      "Batch 308, Loss: 0.37809574604034424\n",
      "Batch 309, Loss: 0.38181743025779724\n",
      "Batch 310, Loss: -1.3809863328933716\n",
      "Batch 311, Loss: -1.380885124206543\n",
      "Batch 312, Loss: -1.141189455986023\n",
      "Batch 313, Loss: -1.141789436340332\n",
      "Batch 314, Loss: -1.1429544687271118\n",
      "Batch 315, Loss: 0.14454911649227142\n",
      "Batch 316, Loss: -1.1453769207000732\n",
      "Batch 317, Loss: 0.37934044003486633\n",
      "Batch 318, Loss: 0.37923020124435425\n",
      "Batch 319, Loss: 0.38188642263412476\n",
      "Batch 320, Loss: 0.1501992642879486\n",
      "Batch 321, Loss: -1.377440333366394\n",
      "Batch 322, Loss: -1.3771791458129883\n",
      "Batch 323, Loss: -1.3794384002685547\n",
      "Batch 324, Loss: 0.15156377851963043\n",
      "Batch 325, Loss: -1.1646933555603027\n",
      "Batch 326, Loss: 0.37807098031044006\n",
      "Batch 327, Loss: -1.1661972999572754\n",
      "Batch 328, Loss: -1.1680331230163574\n",
      "Batch 329, Loss: -1.380143165588379\n",
      "Batch 330, Loss: 0.3785317540168762\n",
      "Batch 331, Loss: -1.1760631799697876\n",
      "Batch 332, Loss: -1.3789234161376953\n",
      "Batch 333, Loss: 0.18329043686389923\n",
      "Batch 334, Loss: 0.16177412867546082\n",
      "Batch 335, Loss: -1.3799574375152588\n",
      "Batch 336, Loss: -1.1619887351989746\n",
      "Batch 337, Loss: 0.3819519877433777\n",
      "Batch 338, Loss: 0.16365638375282288\n",
      "Batch 339, Loss: -1.3819795846939087\n",
      "Batch 340, Loss: 0.18971477448940277\n",
      "Batch 341, Loss: 0.16275593638420105\n",
      "Batch 342, Loss: -1.1612567901611328\n",
      "Batch 343, Loss: 0.18497294187545776\n",
      "Batch 344, Loss: -1.1823421716690063\n",
      "Batch 345, Loss: 0.158835306763649\n",
      "Batch 346, Loss: -1.3815561532974243\n",
      "Batch 347, Loss: -1.156887412071228\n",
      "Batch 348, Loss: 0.3844943046569824\n",
      "Batch 349, Loss: -1.3821333646774292\n",
      "Batch 350, Loss: -1.158776044845581\n",
      "Batch 351, Loss: 0.17908522486686707\n",
      "Batch 352, Loss: 0.17836591601371765\n",
      "Batch 353, Loss: -1.383043646812439\n",
      "Batch 354, Loss: 0.38553959131240845\n",
      "Batch 355, Loss: 0.17269599437713623\n",
      "Batch 356, Loss: 0.15390759706497192\n",
      "Batch 357, Loss: -1.166838526725769\n",
      "Batch 358, Loss: 0.38557425141334534\n",
      "Batch 359, Loss: -1.3829996585845947\n",
      "Batch 360, Loss: 0.16299062967300415\n",
      "Batch 361, Loss: 0.3829265534877777\n",
      "Batch 362, Loss: 0.15907500684261322\n",
      "Batch 363, Loss: -1.1458089351654053\n",
      "Batch 364, Loss: -1.144887924194336\n",
      "Batch 365, Loss: -1.3839389085769653\n",
      "Batch 366, Loss: 0.3841370940208435\n",
      "Batch 367, Loss: 0.38202524185180664\n",
      "Batch 368, Loss: -1.1439322233200073\n",
      "Batch 369, Loss: 0.1442648023366928\n",
      "Batch 370, Loss: 0.38249123096466064\n",
      "Batch 371, Loss: -1.3803805112838745\n",
      "Batch 372, Loss: 0.38138580322265625\n",
      "Batch 373, Loss: 0.14347895979881287\n",
      "Batch 374, Loss: 0.1491909921169281\n",
      "Batch 375, Loss: -1.1476507186889648\n",
      "Batch 376, Loss: 0.37906527519226074\n",
      "Batch 377, Loss: 0.3774808645248413\n",
      "Batch 378, Loss: -1.1401036977767944\n",
      "Batch 379, Loss: 0.14017444849014282\n",
      "Batch 380, Loss: 0.13970406353473663\n",
      "Batch 381, Loss: -1.3747506141662598\n",
      "Batch 382, Loss: -1.1433111429214478\n",
      "Batch 383, Loss: 0.1432247757911682\n",
      "Batch 384, Loss: -1.3738465309143066\n",
      "Batch 385, Loss: 0.3738018870353699\n",
      "Batch 386, Loss: 0.3738994300365448\n",
      "Batch 387, Loss: -1.1406495571136475\n",
      "Batch 388, Loss: 0.37273508310317993\n",
      "Batch 389, Loss: 0.3722231686115265\n",
      "Batch 390, Loss: -1.3716790676116943\n",
      "Batch 391, Loss: 0.140621617436409\n",
      "Batch 392, Loss: 0.37066584825515747\n",
      "Batch 393, Loss: 0.3700026273727417\n",
      "Batch 394, Loss: -1.138326644897461\n",
      "Batch 395, Loss: 0.13316690921783447\n",
      "Batch 396, Loss: 0.3680724799633026\n",
      "Batch 397, Loss: -1.13710355758667\n",
      "Batch 398, Loss: -1.3665549755096436\n",
      "Batch 399, Loss: 0.3661898970603943\n",
      "Training [70%]\tLoss: -0.4997\n",
      "Batch 0, Loss: -1.365511178970337\n",
      "Batch 1, Loss: -1.1320204734802246\n",
      "Batch 2, Loss: -1.1326024532318115\n",
      "Batch 3, Loss: -1.139728307723999\n",
      "Batch 4, Loss: -1.1351232528686523\n",
      "Batch 5, Loss: 0.3658612370491028\n",
      "Batch 6, Loss: -1.3659694194793701\n",
      "Batch 7, Loss: 0.3655405640602112\n",
      "Batch 8, Loss: -1.149243950843811\n",
      "Batch 9, Loss: 0.3668867349624634\n",
      "Batch 10, Loss: 0.14486360549926758\n",
      "Batch 11, Loss: 0.15452702343463898\n",
      "Batch 12, Loss: -1.145585536956787\n",
      "Batch 13, Loss: -1.1462032794952393\n",
      "Batch 14, Loss: -1.3647280931472778\n",
      "Batch 15, Loss: -1.364988923072815\n",
      "Batch 16, Loss: -1.149923324584961\n",
      "Batch 17, Loss: -1.366180419921875\n",
      "Batch 18, Loss: -1.1536107063293457\n",
      "Batch 19, Loss: -1.165830373764038\n",
      "Batch 20, Loss: -1.372056245803833\n",
      "Batch 21, Loss: -1.3740603923797607\n",
      "Batch 22, Loss: -1.1649105548858643\n",
      "Batch 23, Loss: -1.1803386211395264\n",
      "Batch 24, Loss: 0.3780713975429535\n",
      "Batch 25, Loss: -1.3841736316680908\n",
      "Batch 26, Loss: 0.3821565508842468\n",
      "Batch 27, Loss: 0.38360828161239624\n",
      "Batch 28, Loss: 0.20074424147605896\n",
      "Batch 29, Loss: -1.3843446969985962\n",
      "Batch 30, Loss: -1.2032880783081055\n",
      "Batch 31, Loss: -1.3916358947753906\n",
      "Batch 32, Loss: 0.19149385392665863\n",
      "Batch 33, Loss: -1.2111479043960571\n",
      "Batch 34, Loss: 0.39107733964920044\n",
      "Batch 35, Loss: 0.19633005559444427\n",
      "Batch 36, Loss: 0.39805659651756287\n",
      "Batch 37, Loss: -1.3913209438323975\n",
      "Batch 38, Loss: 0.19691988825798035\n",
      "Batch 39, Loss: -1.3979520797729492\n",
      "Batch 40, Loss: -1.22065269947052\n",
      "Batch 41, Loss: -1.4026166200637817\n",
      "Batch 42, Loss: 0.19801628589630127\n",
      "Batch 43, Loss: 0.39544594287872314\n",
      "Batch 44, Loss: 0.4028073847293854\n",
      "Batch 45, Loss: -1.4024226665496826\n",
      "Batch 46, Loss: 0.2285476177930832\n",
      "Batch 47, Loss: -1.4023765325546265\n",
      "Batch 48, Loss: 0.39506101608276367\n",
      "Batch 49, Loss: -1.1929670572280884\n",
      "Batch 50, Loss: 0.19287866353988647\n",
      "Batch 51, Loss: -1.2232627868652344\n",
      "Batch 52, Loss: -1.4022796154022217\n",
      "Batch 53, Loss: 0.4031856656074524\n",
      "Batch 54, Loss: -1.192516803741455\n",
      "Batch 55, Loss: 0.3955176770687103\n",
      "Batch 56, Loss: 0.1941414475440979\n",
      "Batch 57, Loss: -1.3943488597869873\n",
      "Batch 58, Loss: 0.3944432735443115\n",
      "Batch 59, Loss: 0.39374372363090515\n",
      "Batch 60, Loss: -1.192278265953064\n",
      "Batch 61, Loss: 0.19262713193893433\n",
      "Batch 62, Loss: 0.1918196976184845\n",
      "Batch 63, Loss: -1.2294657230377197\n",
      "Batch 64, Loss: 0.3880181312561035\n",
      "Batch 65, Loss: 0.23126283288002014\n",
      "Batch 66, Loss: 0.3847792148590088\n",
      "Batch 67, Loss: 0.3930348753929138\n",
      "Batch 68, Loss: 0.2239404320716858\n",
      "Batch 69, Loss: -1.2189528942108154\n",
      "Batch 70, Loss: 0.21690672636032104\n",
      "Batch 71, Loss: -1.3817832469940186\n",
      "Batch 72, Loss: 0.37273135781288147\n",
      "Batch 73, Loss: -1.3718433380126953\n",
      "Batch 74, Loss: -1.3770159482955933\n",
      "Batch 75, Loss: 0.1702638864517212\n",
      "Batch 76, Loss: 0.16862580180168152\n",
      "Batch 77, Loss: -1.1972591876983643\n",
      "Batch 78, Loss: -1.1652861833572388\n",
      "Batch 79, Loss: 0.1650540679693222\n",
      "Batch 80, Loss: -1.3692898750305176\n",
      "Batch 81, Loss: 0.37450534105300903\n",
      "Batch 82, Loss: -1.162488341331482\n",
      "Batch 83, Loss: -1.3738794326782227\n",
      "Batch 84, Loss: -1.3693844079971313\n",
      "Batch 85, Loss: 0.16295206546783447\n",
      "Batch 86, Loss: -1.3752835988998413\n",
      "Batch 87, Loss: -1.3707306385040283\n",
      "Batch 88, Loss: -1.3772258758544922\n",
      "Batch 89, Loss: -1.378800392150879\n",
      "Batch 90, Loss: -1.373932123184204\n",
      "Batch 91, Loss: 0.38321882486343384\n",
      "Batch 92, Loss: 0.3766135573387146\n",
      "Batch 93, Loss: 0.38547220826148987\n",
      "Batch 94, Loss: -1.1644800901412964\n",
      "Batch 95, Loss: 0.38587531447410583\n",
      "Batch 96, Loss: 0.20264491438865662\n",
      "Batch 97, Loss: -1.164381504058838\n",
      "Batch 98, Loss: 0.19922222197055817\n",
      "Batch 99, Loss: 0.3765840530395508\n",
      "Batch 100, Loss: 0.3759955167770386\n",
      "Batch 101, Loss: 0.16160592436790466\n",
      "Batch 102, Loss: 0.3785710632801056\n",
      "Batch 103, Loss: -1.158278226852417\n",
      "Batch 104, Loss: 0.3754782974720001\n",
      "Batch 105, Loss: 0.3740043044090271\n",
      "Batch 106, Loss: 0.15567997097969055\n",
      "Batch 107, Loss: 0.3709471523761749\n",
      "Batch 108, Loss: -1.3698281049728394\n",
      "Batch 109, Loss: -1.3695889711380005\n",
      "Batch 110, Loss: 0.36981794238090515\n",
      "Batch 111, Loss: 0.14836452901363373\n",
      "Batch 112, Loss: 0.1467469334602356\n",
      "Batch 113, Loss: -1.369940996170044\n",
      "Batch 114, Loss: 0.3706851303577423\n",
      "Batch 115, Loss: -1.3708693981170654\n",
      "Batch 116, Loss: -1.1400063037872314\n",
      "Batch 117, Loss: -1.367382526397705\n",
      "Batch 118, Loss: 0.13862138986587524\n",
      "Batch 119, Loss: 0.36924469470977783\n",
      "Batch 120, Loss: 0.14947424829006195\n",
      "Batch 121, Loss: -1.376349925994873\n",
      "Batch 122, Loss: -1.1338144540786743\n",
      "Batch 123, Loss: 0.1330786645412445\n",
      "Batch 124, Loss: 0.13194218277931213\n",
      "Batch 125, Loss: 0.38230687379837036\n",
      "Batch 126, Loss: 0.13938099145889282\n",
      "Batch 127, Loss: -1.37568998336792\n",
      "Batch 128, Loss: -1.3770760297775269\n",
      "Batch 129, Loss: 0.1251126229763031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 130, Loss: 0.38189560174942017\n",
      "Batch 131, Loss: -1.3897291421890259\n",
      "Batch 132, Loss: -1.3849982023239136\n",
      "Batch 133, Loss: 0.12636205554008484\n",
      "Batch 134, Loss: 0.3982517421245575\n",
      "Batch 135, Loss: -1.12276291847229\n",
      "Batch 136, Loss: -1.1218558549880981\n",
      "Batch 137, Loss: -1.1215825080871582\n",
      "Batch 138, Loss: -1.40227472782135\n",
      "Batch 139, Loss: 0.12167283147573471\n",
      "Batch 140, Loss: 0.406009316444397\n",
      "Batch 141, Loss: -1.1159428358078003\n",
      "Batch 142, Loss: -1.406607985496521\n",
      "Batch 143, Loss: 0.11610953509807587\n",
      "Batch 144, Loss: 0.409435898065567\n",
      "Batch 145, Loss: -1.4094648361206055\n",
      "Batch 146, Loss: 0.4052225351333618\n",
      "Batch 147, Loss: 0.12001744657754898\n",
      "Batch 148, Loss: -1.4104729890823364\n",
      "Batch 149, Loss: -1.4043149948120117\n",
      "Batch 150, Loss: 0.11837151646614075\n",
      "Batch 151, Loss: 0.11330024898052216\n",
      "Batch 152, Loss: -1.412818193435669\n",
      "Batch 153, Loss: 0.11154193431138992\n",
      "Batch 154, Loss: 0.4180293679237366\n",
      "Batch 155, Loss: 0.4225187301635742\n",
      "Batch 156, Loss: -1.1119577884674072\n",
      "Batch 157, Loss: 0.11146794259548187\n",
      "Batch 158, Loss: -1.4197463989257812\n",
      "Batch 159, Loss: -1.110042929649353\n",
      "Batch 160, Loss: 0.10972042381763458\n",
      "Batch 161, Loss: -1.4245442152023315\n",
      "Batch 162, Loss: -1.1086106300354004\n",
      "Batch 163, Loss: 0.10849831253290176\n",
      "Batch 164, Loss: -1.4250609874725342\n",
      "Batch 165, Loss: -1.1077336072921753\n",
      "Batch 166, Loss: 0.4287112057209015\n",
      "Batch 167, Loss: -1.1075838804244995\n",
      "Batch 168, Loss: 0.10564637184143066\n",
      "Batch 169, Loss: 0.10772833973169327\n",
      "Batch 170, Loss: -1.4290552139282227\n",
      "Batch 171, Loss: 0.10713325440883636\n",
      "Batch 172, Loss: 0.4301033616065979\n",
      "Batch 173, Loss: -1.427437424659729\n",
      "Batch 174, Loss: 0.43053048849105835\n",
      "Batch 175, Loss: -1.1053639650344849\n",
      "Batch 176, Loss: -1.4306354522705078\n",
      "Batch 177, Loss: 0.10519950836896896\n",
      "Batch 178, Loss: -1.43111252784729\n",
      "Batch 179, Loss: 0.43154701590538025\n",
      "Batch 180, Loss: 0.4316543936729431\n",
      "Batch 181, Loss: 0.102753184735775\n",
      "Batch 182, Loss: 0.43118709325790405\n",
      "Batch 183, Loss: 0.10326656699180603\n",
      "Batch 184, Loss: 0.4295147657394409\n",
      "Batch 185, Loss: 0.101829893887043\n",
      "Batch 186, Loss: 0.42839276790618896\n",
      "Batch 187, Loss: -1.4273128509521484\n",
      "Batch 188, Loss: 0.42573294043540955\n",
      "Batch 189, Loss: -1.425661325454712\n",
      "Batch 190, Loss: 0.09700077772140503\n",
      "Batch 191, Loss: 0.42440640926361084\n",
      "Batch 192, Loss: -1.0956604480743408\n",
      "Batch 193, Loss: -1.421588659286499\n",
      "Batch 194, Loss: 0.4225733280181885\n",
      "Batch 195, Loss: 0.421974241733551\n",
      "Batch 196, Loss: -1.419696569442749\n",
      "Batch 197, Loss: 0.09393275529146194\n",
      "Batch 198, Loss: 0.09351285547018051\n",
      "Batch 199, Loss: -1.4196161031723022\n",
      "Batch 200, Loss: 0.4195094108581543\n",
      "Batch 201, Loss: 0.4190290868282318\n",
      "Batch 202, Loss: -1.4167729616165161\n",
      "Batch 203, Loss: -1.4178922176361084\n",
      "Batch 204, Loss: -1.4166502952575684\n",
      "Batch 205, Loss: 0.41853445768356323\n",
      "Batch 206, Loss: 0.0914732962846756\n",
      "Batch 207, Loss: -1.417417287826538\n",
      "Batch 208, Loss: -1.09091317653656\n",
      "Batch 209, Loss: -1.418330430984497\n",
      "Batch 210, Loss: -1.4200103282928467\n",
      "Batch 211, Loss: -1.0905332565307617\n",
      "Batch 212, Loss: 0.42178845405578613\n",
      "Batch 213, Loss: 0.09191557765007019\n",
      "Batch 214, Loss: -1.0919406414031982\n",
      "Batch 215, Loss: -1.422532320022583\n",
      "Batch 216, Loss: -1.092472791671753\n",
      "Batch 217, Loss: 0.09294835478067398\n",
      "Batch 218, Loss: 0.0931771993637085\n",
      "Batch 219, Loss: 0.4248253405094147\n",
      "Batch 220, Loss: -1.0930993556976318\n",
      "Batch 221, Loss: 0.4248709976673126\n",
      "Batch 222, Loss: 0.09240923821926117\n",
      "Batch 223, Loss: 0.09224055707454681\n",
      "Batch 224, Loss: 0.42398321628570557\n",
      "Batch 225, Loss: -1.4235275983810425\n",
      "Batch 226, Loss: -1.091216802597046\n",
      "Batch 227, Loss: -1.4229357242584229\n",
      "Batch 228, Loss: 0.09120900928974152\n",
      "Batch 229, Loss: 0.09187125414609909\n",
      "Batch 230, Loss: 0.42333129048347473\n",
      "Batch 231, Loss: -1.4230211973190308\n",
      "Batch 232, Loss: 0.4230201840400696\n",
      "Batch 233, Loss: 0.4227437674999237\n",
      "Batch 234, Loss: 0.4219930171966553\n",
      "Batch 235, Loss: 0.4212111234664917\n",
      "Batch 236, Loss: 0.4202005863189697\n",
      "Batch 237, Loss: -1.088741421699524\n",
      "Batch 238, Loss: 0.4179329574108124\n",
      "Batch 239, Loss: 0.08822330087423325\n",
      "Batch 240, Loss: 0.415961354970932\n",
      "Batch 241, Loss: -1.4139883518218994\n",
      "Batch 242, Loss: 0.08607978373765945\n",
      "Batch 243, Loss: 0.41212669014930725\n",
      "Batch 244, Loss: 0.0858229249715805\n",
      "Batch 245, Loss: 0.08433805406093597\n",
      "Batch 246, Loss: -1.0843113660812378\n",
      "Batch 247, Loss: 0.4077155590057373\n",
      "Batch 248, Loss: 0.08324997872114182\n",
      "Batch 249, Loss: 0.40524810552597046\n",
      "Batch 250, Loss: -1.4037127494812012\n",
      "Batch 251, Loss: -1.4038057327270508\n",
      "Batch 252, Loss: -1.080929160118103\n",
      "Batch 253, Loss: -1.079964518547058\n",
      "Batch 254, Loss: -1.0807617902755737\n",
      "Batch 255, Loss: -1.399553894996643\n",
      "Batch 256, Loss: -1.4030687808990479\n",
      "Batch 257, Loss: -1.0807015895843506\n",
      "Batch 258, Loss: 0.08193705976009369\n",
      "Batch 259, Loss: -1.0821284055709839\n",
      "Batch 260, Loss: 0.08171765506267548\n",
      "Batch 261, Loss: 0.0818723738193512\n",
      "Batch 262, Loss: -1.0818244218826294\n",
      "Batch 263, Loss: -1.4065165519714355\n",
      "Batch 264, Loss: 0.40651723742485046\n",
      "Batch 265, Loss: 0.406741201877594\n",
      "Batch 266, Loss: -1.406585693359375\n",
      "Batch 267, Loss: -1.0824108123779297\n",
      "Batch 268, Loss: -1.4076365232467651\n",
      "Batch 269, Loss: -1.0830072164535522\n",
      "Batch 270, Loss: -1.4087305068969727\n",
      "Batch 271, Loss: -1.409529685974121\n",
      "Batch 272, Loss: 0.41053861379623413\n",
      "Batch 273, Loss: 0.08575104176998138\n",
      "Batch 274, Loss: 0.08591616153717041\n",
      "Batch 275, Loss: -1.085598111152649\n",
      "Batch 276, Loss: -1.412024974822998\n",
      "Batch 277, Loss: 0.08545820415019989\n",
      "Batch 278, Loss: 0.08621425926685333\n",
      "Batch 279, Loss: -1.413609504699707\n",
      "Batch 280, Loss: -1.4141550064086914\n",
      "Batch 281, Loss: 0.4150936007499695\n",
      "Batch 282, Loss: -1.41549551486969\n",
      "Batch 283, Loss: 0.4162321090698242\n",
      "Batch 284, Loss: -1.4164049625396729\n",
      "Batch 285, Loss: 0.08515436947345734\n",
      "Batch 286, Loss: 0.4175350069999695\n",
      "Batch 287, Loss: -1.0847818851470947\n",
      "Batch 288, Loss: 0.08476195484399796\n",
      "Batch 289, Loss: -1.0851067304611206\n",
      "Batch 290, Loss: 0.41695359349250793\n",
      "Batch 291, Loss: -1.4178850650787354\n",
      "Batch 292, Loss: 0.418040931224823\n",
      "Batch 293, Loss: 0.4179069399833679\n",
      "Batch 294, Loss: -1.0848716497421265\n",
      "Batch 295, Loss: -1.084368109703064\n",
      "Batch 296, Loss: 0.0851711556315422\n",
      "Batch 297, Loss: -1.084614634513855\n",
      "Batch 298, Loss: 0.4167481064796448\n",
      "Batch 299, Loss: 0.08549539744853973\n",
      "Batch 300, Loss: 0.41593584418296814\n",
      "Batch 301, Loss: -1.4151642322540283\n",
      "Batch 302, Loss: -1.4149458408355713\n",
      "Batch 303, Loss: -1.4148485660552979\n",
      "Batch 304, Loss: -1.0852739810943604\n",
      "Batch 305, Loss: -1.415449619293213\n",
      "Batch 306, Loss: 0.4160219430923462\n",
      "Batch 307, Loss: 0.41626182198524475\n",
      "Batch 308, Loss: -1.4162038564682007\n",
      "Batch 309, Loss: -1.4163923263549805\n",
      "Batch 310, Loss: -1.4168572425842285\n",
      "Batch 311, Loss: -1.0864300727844238\n",
      "Batch 312, Loss: 0.4182961583137512\n",
      "Batch 313, Loss: -1.4186415672302246\n",
      "Batch 314, Loss: -1.0882469415664673\n",
      "Batch 315, Loss: 0.419900119304657\n",
      "Batch 316, Loss: -1.4201940298080444\n",
      "Batch 317, Loss: -1.0896333456039429\n",
      "Batch 318, Loss: 0.42130929231643677\n",
      "Batch 319, Loss: -1.0906788110733032\n",
      "Batch 320, Loss: 0.42188844084739685\n",
      "Batch 321, Loss: -1.0910085439682007\n",
      "Batch 322, Loss: -1.09159255027771\n",
      "Batch 323, Loss: 0.09230852872133255\n",
      "Batch 324, Loss: 0.09356790781021118\n",
      "Batch 325, Loss: -1.0937881469726562\n",
      "Batch 326, Loss: 0.09336778521537781\n",
      "Batch 327, Loss: 0.09352686256170273\n",
      "Batch 328, Loss: -1.4221466779708862\n",
      "Batch 329, Loss: -1.0935269594192505\n",
      "Batch 330, Loss: 0.0937623381614685\n",
      "Batch 331, Loss: -1.4226741790771484\n",
      "Batch 332, Loss: 0.0947551503777504\n",
      "Batch 333, Loss: 0.42330673336982727\n",
      "Batch 334, Loss: -1.4232362508773804\n",
      "Batch 335, Loss: -1.4234848022460938\n",
      "Batch 336, Loss: -1.094376564025879\n",
      "Batch 337, Loss: 0.0945897251367569\n",
      "Batch 338, Loss: 0.42490819096565247\n",
      "Batch 339, Loss: 0.4249861240386963\n",
      "Batch 340, Loss: -1.4246522188186646\n",
      "Batch 341, Loss: 0.09342095255851746\n",
      "Batch 342, Loss: -1.4248692989349365\n",
      "Batch 343, Loss: 0.4249497056007385\n",
      "Batch 344, Loss: -1.0935652256011963\n",
      "Batch 345, Loss: -1.0927926301956177\n",
      "Batch 346, Loss: -1.4252928495407104\n",
      "Batch 347, Loss: 0.09400817006826401\n",
      "Batch 348, Loss: -1.4259498119354248\n",
      "Batch 349, Loss: -1.0941598415374756\n",
      "Batch 350, Loss: 0.4270004332065582\n",
      "Batch 351, Loss: 0.0938856452703476\n",
      "Batch 352, Loss: -1.426980972290039\n",
      "Batch 353, Loss: 0.4273547828197479\n",
      "Batch 354, Loss: -1.093909740447998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 355, Loss: 0.09485434740781784\n",
      "Batch 356, Loss: -1.0940240621566772\n",
      "Batch 357, Loss: 0.42768028378486633\n",
      "Batch 358, Loss: -1.4274909496307373\n",
      "Batch 359, Loss: 0.4276188611984253\n",
      "Batch 360, Loss: 0.09510550647974014\n",
      "Batch 361, Loss: 0.09416236728429794\n",
      "Batch 362, Loss: -1.4268076419830322\n",
      "Batch 363, Loss: 0.09360072761774063\n",
      "Batch 364, Loss: 0.42668959498405457\n",
      "Batch 365, Loss: 0.09278189390897751\n",
      "Batch 366, Loss: 0.4257941246032715\n",
      "Batch 367, Loss: -1.4250280857086182\n",
      "Batch 368, Loss: 0.09174403548240662\n",
      "Batch 369, Loss: -1.091124176979065\n",
      "Batch 370, Loss: 0.09016941487789154\n",
      "Batch 371, Loss: -1.424323320388794\n",
      "Batch 372, Loss: -1.0898972749710083\n",
      "Batch 373, Loss: 0.08919733762741089\n",
      "Batch 374, Loss: -1.4244153499603271\n",
      "Batch 375, Loss: 0.42399245500564575\n",
      "Batch 376, Loss: -1.0890506505966187\n",
      "Batch 377, Loss: -1.0884326696395874\n",
      "Batch 378, Loss: -1.08857262134552\n",
      "Batch 379, Loss: -1.0894800424575806\n",
      "Batch 380, Loss: -1.425398349761963\n",
      "Batch 381, Loss: 0.090497687458992\n",
      "Batch 382, Loss: -1.0901308059692383\n",
      "Batch 383, Loss: 0.4261736273765564\n",
      "Batch 384, Loss: -1.4269635677337646\n",
      "Batch 385, Loss: 0.4273545444011688\n",
      "Batch 386, Loss: 0.09142518043518066\n",
      "Batch 387, Loss: 0.09144165366888046\n",
      "Batch 388, Loss: -1.0912731885910034\n",
      "Batch 389, Loss: -1.4272209405899048\n",
      "Batch 390, Loss: -1.427492380142212\n",
      "Batch 391, Loss: -1.0923612117767334\n",
      "Batch 392, Loss: -1.42860746383667\n",
      "Batch 393, Loss: -1.092459797859192\n",
      "Batch 394, Loss: -1.0930405855178833\n",
      "Batch 395, Loss: 0.09375080466270447\n",
      "Batch 396, Loss: -1.0950489044189453\n",
      "Batch 397, Loss: -1.4323420524597168\n",
      "Batch 398, Loss: 0.0954439640045166\n",
      "Batch 399, Loss: 0.09582896530628204\n",
      "Training [80%]\tLoss: -0.5060\n",
      "Batch 0, Loss: -1.0959856510162354\n",
      "Batch 1, Loss: -1.0965116024017334\n",
      "Batch 2, Loss: 0.43553799390792847\n",
      "Batch 3, Loss: -1.435794472694397\n",
      "Batch 4, Loss: -1.436076045036316\n",
      "Batch 5, Loss: 0.09803563356399536\n",
      "Batch 6, Loss: 0.4374021291732788\n",
      "Batch 7, Loss: 0.09945543855428696\n",
      "Batch 8, Loss: 0.09934089332818985\n",
      "Batch 9, Loss: -1.437766671180725\n",
      "Batch 10, Loss: 0.0977957546710968\n",
      "Batch 11, Loss: 0.43835511803627014\n",
      "Batch 12, Loss: 0.43823379278182983\n",
      "Batch 13, Loss: -1.4377100467681885\n",
      "Batch 14, Loss: -1.4376511573791504\n",
      "Batch 15, Loss: 0.09612716734409332\n",
      "Batch 16, Loss: -1.437972068786621\n",
      "Batch 17, Loss: 0.09555515646934509\n",
      "Batch 18, Loss: 0.0951654389500618\n",
      "Batch 19, Loss: -1.0946249961853027\n",
      "Batch 20, Loss: 0.4388794004917145\n",
      "Batch 21, Loss: 0.4387871026992798\n",
      "Batch 22, Loss: 0.09443563222885132\n",
      "Batch 23, Loss: -1.0938537120819092\n",
      "Batch 24, Loss: 0.43776026368141174\n",
      "Batch 25, Loss: -1.093159556388855\n",
      "Batch 26, Loss: -1.0930253267288208\n",
      "Batch 27, Loss: 0.09311012923717499\n",
      "Batch 28, Loss: 0.43618932366371155\n",
      "Batch 29, Loss: 0.4357743561267853\n",
      "Batch 30, Loss: 0.43501338362693787\n",
      "Batch 31, Loss: -1.091314673423767\n",
      "Batch 32, Loss: 0.43328776955604553\n",
      "Batch 33, Loss: -1.4323227405548096\n",
      "Batch 34, Loss: 0.09183164685964584\n",
      "Batch 35, Loss: -1.4307658672332764\n",
      "Batch 36, Loss: 0.09049928188323975\n",
      "Batch 37, Loss: 0.09103496372699738\n",
      "Batch 38, Loss: -1.0905182361602783\n",
      "Batch 39, Loss: 0.4295532703399658\n",
      "Batch 40, Loss: 0.4290275275707245\n",
      "Batch 41, Loss: -1.4284989833831787\n",
      "Batch 42, Loss: 0.42745351791381836\n",
      "Batch 43, Loss: -1.4271490573883057\n",
      "Batch 44, Loss: -1.088073492050171\n",
      "Batch 45, Loss: 0.08892890065908432\n",
      "Batch 46, Loss: -1.4267432689666748\n",
      "Batch 47, Loss: -1.426401972770691\n",
      "Batch 48, Loss: -1.0878850221633911\n",
      "Batch 49, Loss: -1.427488923072815\n",
      "Batch 50, Loss: -1.428096890449524\n",
      "Batch 51, Loss: -1.088683009147644\n",
      "Batch 52, Loss: 0.08916261792182922\n",
      "Batch 53, Loss: -1.0894101858139038\n",
      "Batch 54, Loss: 0.43083393573760986\n",
      "Batch 55, Loss: -1.4313843250274658\n",
      "Batch 56, Loss: 0.09044864773750305\n",
      "Batch 57, Loss: -1.4321424961090088\n",
      "Batch 58, Loss: -1.090768814086914\n",
      "Batch 59, Loss: -1.0911290645599365\n",
      "Batch 60, Loss: -1.0916390419006348\n",
      "Batch 61, Loss: 0.09228445589542389\n",
      "Batch 62, Loss: -1.0931037664413452\n",
      "Batch 63, Loss: 0.43606290221214294\n",
      "Batch 64, Loss: 0.4362996518611908\n",
      "Batch 65, Loss: -1.0947520732879639\n",
      "Batch 66, Loss: 0.43621698021888733\n",
      "Batch 67, Loss: -1.436281442642212\n",
      "Batch 68, Loss: -1.0950839519500732\n",
      "Batch 69, Loss: 0.43595340847969055\n",
      "Batch 70, Loss: 0.4356660842895508\n",
      "Batch 71, Loss: -1.4350405931472778\n",
      "Batch 72, Loss: 0.09772689640522003\n",
      "Batch 73, Loss: -1.0967803001403809\n",
      "Batch 74, Loss: 0.09814445674419403\n",
      "Batch 75, Loss: -1.435106873512268\n",
      "Batch 76, Loss: 0.09831739962100983\n",
      "Batch 77, Loss: -1.0971710681915283\n",
      "Batch 78, Loss: 0.09830599278211594\n",
      "Batch 79, Loss: -1.097191333770752\n",
      "Batch 80, Loss: -1.098264455795288\n",
      "Batch 81, Loss: -1.0975985527038574\n",
      "Batch 82, Loss: 0.09806337207555771\n",
      "Batch 83, Loss: -1.0992509126663208\n",
      "Batch 84, Loss: -1.0986894369125366\n",
      "Batch 85, Loss: -1.100247859954834\n",
      "Batch 86, Loss: 0.4359973669052124\n",
      "Batch 87, Loss: -1.4368171691894531\n",
      "Batch 88, Loss: 0.10113703459501266\n",
      "Batch 89, Loss: 0.43719911575317383\n",
      "Batch 90, Loss: -1.4358065128326416\n",
      "Batch 91, Loss: 0.10314178466796875\n",
      "Batch 92, Loss: 0.10199800133705139\n",
      "Batch 93, Loss: -1.4373135566711426\n",
      "Batch 94, Loss: 0.10172662138938904\n",
      "Batch 95, Loss: -1.437937617301941\n",
      "Batch 96, Loss: -1.436432957649231\n",
      "Batch 97, Loss: -1.1011571884155273\n",
      "Batch 98, Loss: -1.4400560855865479\n",
      "Batch 99, Loss: -1.1024253368377686\n",
      "Batch 100, Loss: 0.10278810560703278\n",
      "Batch 101, Loss: -1.1019400358200073\n",
      "Batch 102, Loss: 0.10319579392671585\n",
      "Batch 103, Loss: 0.4437483549118042\n",
      "Batch 104, Loss: -1.4440178871154785\n",
      "Batch 105, Loss: -1.1023752689361572\n",
      "Batch 106, Loss: 0.44501209259033203\n",
      "Batch 107, Loss: -1.1027686595916748\n",
      "Batch 108, Loss: -1.1030863523483276\n",
      "Batch 109, Loss: -1.4453121423721313\n",
      "Batch 110, Loss: 0.10410087555646896\n",
      "Batch 111, Loss: 0.44624823331832886\n",
      "Batch 112, Loss: 0.4467948079109192\n",
      "Batch 113, Loss: 0.44659727811813354\n",
      "Batch 114, Loss: 0.4461419880390167\n",
      "Batch 115, Loss: -1.1051716804504395\n",
      "Batch 116, Loss: 0.10444049537181854\n",
      "Batch 117, Loss: -1.1050913333892822\n",
      "Batch 118, Loss: -1.4439232349395752\n",
      "Batch 119, Loss: 0.10454796254634857\n",
      "Batch 120, Loss: -1.105323076248169\n",
      "Batch 121, Loss: -1.1055045127868652\n",
      "Batch 122, Loss: 0.4428398907184601\n",
      "Batch 123, Loss: -1.4425336122512817\n",
      "Batch 124, Loss: 0.10550526529550552\n",
      "Batch 125, Loss: 0.10650703310966492\n",
      "Batch 126, Loss: -1.106369972229004\n",
      "Batch 127, Loss: -1.4424426555633545\n",
      "Batch 128, Loss: 0.1056949719786644\n",
      "Batch 129, Loss: -1.1056649684906006\n",
      "Batch 130, Loss: -1.4432969093322754\n",
      "Batch 131, Loss: 0.1071297898888588\n",
      "Batch 132, Loss: 0.44491255283355713\n",
      "Batch 133, Loss: -1.4449498653411865\n",
      "Batch 134, Loss: 0.4452475607395172\n",
      "Batch 135, Loss: -1.1070817708969116\n",
      "Batch 136, Loss: 0.4451380670070648\n",
      "Batch 137, Loss: 0.10622170567512512\n",
      "Batch 138, Loss: 0.10712836682796478\n",
      "Batch 139, Loss: -1.4445594549179077\n",
      "Batch 140, Loss: -1.4441280364990234\n",
      "Batch 141, Loss: 0.44475987553596497\n",
      "Batch 142, Loss: 0.10518094897270203\n",
      "Batch 143, Loss: 0.44422483444213867\n",
      "Batch 144, Loss: 0.10540015250444412\n",
      "Batch 145, Loss: 0.10380091518163681\n",
      "Batch 146, Loss: -1.4428014755249023\n",
      "Batch 147, Loss: 0.10345586389303207\n",
      "Batch 148, Loss: -1.1027315855026245\n",
      "Batch 149, Loss: 0.4426497519016266\n",
      "Batch 150, Loss: 0.10095999389886856\n",
      "Batch 151, Loss: -1.1011687517166138\n",
      "Batch 152, Loss: 0.09999316185712814\n",
      "Batch 153, Loss: 0.09948588162660599\n",
      "Batch 154, Loss: 0.0996268019080162\n",
      "Batch 155, Loss: 0.43969592452049255\n",
      "Batch 156, Loss: 0.09800861775875092\n",
      "Batch 157, Loss: 0.0970720425248146\n",
      "Batch 158, Loss: 0.4372466206550598\n",
      "Batch 159, Loss: -1.0950000286102295\n",
      "Batch 160, Loss: 0.4360426068305969\n",
      "Batch 161, Loss: -1.4350085258483887\n",
      "Batch 162, Loss: -1.4343371391296387\n",
      "Batch 163, Loss: 0.09252424538135529\n",
      "Batch 164, Loss: 0.09196925908327103\n",
      "Batch 165, Loss: -1.0908212661743164\n",
      "Batch 166, Loss: -1.090848445892334\n",
      "Batch 167, Loss: -1.090641736984253\n",
      "Batch 168, Loss: -1.4316160678863525\n",
      "Batch 169, Loss: -1.4320287704467773\n",
      "Batch 170, Loss: -1.4338210821151733\n",
      "Batch 171, Loss: -1.4347116947174072\n",
      "Batch 172, Loss: -1.4353257417678833\n",
      "Batch 173, Loss: -1.0915268659591675\n",
      "Batch 174, Loss: 0.43805304169654846\n",
      "Batch 175, Loss: 0.438728392124176\n",
      "Batch 176, Loss: -1.4389357566833496\n",
      "Batch 177, Loss: 0.09310077875852585\n",
      "Batch 178, Loss: 0.43985238671302795\n",
      "Batch 179, Loss: -1.4398314952850342\n",
      "Batch 180, Loss: -1.0933769941329956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 181, Loss: 0.4403875470161438\n",
      "Batch 182, Loss: -1.0938414335250854\n",
      "Batch 183, Loss: -1.0941886901855469\n",
      "Batch 184, Loss: 0.09545598179101944\n",
      "Batch 185, Loss: 0.4402906894683838\n",
      "Batch 186, Loss: -1.0951119661331177\n",
      "Batch 187, Loss: -1.4405791759490967\n",
      "Batch 188, Loss: 0.09667259454727173\n",
      "Batch 189, Loss: 0.4407014548778534\n",
      "Batch 190, Loss: -1.4403339624404907\n",
      "Batch 191, Loss: 0.09615597873926163\n",
      "Batch 192, Loss: 0.09607657790184021\n",
      "Batch 193, Loss: -1.4402796030044556\n",
      "Batch 194, Loss: 0.09564820677042007\n",
      "Batch 195, Loss: -1.4395309686660767\n",
      "Batch 196, Loss: 0.09506992995738983\n",
      "Batch 197, Loss: -1.4408409595489502\n",
      "Batch 198, Loss: -1.095059871673584\n",
      "Batch 199, Loss: 0.09431388974189758\n",
      "Batch 200, Loss: -1.0946918725967407\n",
      "Batch 201, Loss: 0.09404349327087402\n",
      "Batch 202, Loss: 0.09442052990198135\n",
      "Batch 203, Loss: -1.4448524713516235\n",
      "Batch 204, Loss: 0.09322236478328705\n",
      "Batch 205, Loss: 0.09282414615154266\n",
      "Batch 206, Loss: 0.09276928007602692\n",
      "Batch 207, Loss: -1.0916248559951782\n",
      "Batch 208, Loss: -1.091209053993225\n",
      "Batch 209, Loss: 0.09101419150829315\n",
      "Batch 210, Loss: 0.09104838967323303\n",
      "Batch 211, Loss: 0.4463062584400177\n",
      "Batch 212, Loss: -1.0899724960327148\n",
      "Batch 213, Loss: -1.4458599090576172\n",
      "Batch 214, Loss: -1.0894672870635986\n",
      "Batch 215, Loss: 0.4455873668193817\n",
      "Batch 216, Loss: -1.089392900466919\n",
      "Batch 217, Loss: 0.08949823677539825\n",
      "Batch 218, Loss: -1.0891200304031372\n",
      "Batch 219, Loss: 0.08950865268707275\n",
      "Batch 220, Loss: -1.089415192604065\n",
      "Batch 221, Loss: 0.4457700550556183\n",
      "Batch 222, Loss: -1.0895051956176758\n",
      "Batch 223, Loss: 0.08941847085952759\n",
      "Batch 224, Loss: -1.444368600845337\n",
      "Batch 225, Loss: -1.4450815916061401\n",
      "Batch 226, Loss: 0.08991501480340958\n",
      "Batch 227, Loss: -1.4457662105560303\n",
      "Batch 228, Loss: -1.4454585313796997\n",
      "Batch 229, Loss: 0.08983433246612549\n",
      "Batch 230, Loss: -1.0901044607162476\n",
      "Batch 231, Loss: -1.4477993249893188\n",
      "Batch 232, Loss: 0.0901646763086319\n",
      "Batch 233, Loss: 0.44978010654449463\n",
      "Batch 234, Loss: 0.45004817843437195\n",
      "Batch 235, Loss: -1.0903671979904175\n",
      "Batch 236, Loss: 0.4500119686126709\n",
      "Batch 237, Loss: 0.44979530572891235\n",
      "Batch 238, Loss: -1.4492886066436768\n",
      "Batch 239, Loss: 0.08984139561653137\n",
      "Batch 240, Loss: -1.0896391868591309\n",
      "Batch 241, Loss: -1.448976755142212\n",
      "Batch 242, Loss: 0.0901932418346405\n",
      "Batch 243, Loss: -1.0900863409042358\n",
      "Batch 244, Loss: -1.449347972869873\n",
      "Batch 245, Loss: 0.09035177528858185\n",
      "Batch 246, Loss: -1.089829921722412\n",
      "Batch 247, Loss: 0.44993856549263\n",
      "Batch 248, Loss: -1.0905226469039917\n",
      "Batch 249, Loss: 0.4504389464855194\n",
      "Batch 250, Loss: 0.45024266839027405\n",
      "Batch 251, Loss: -1.090415358543396\n",
      "Batch 252, Loss: 0.44872865080833435\n",
      "Batch 253, Loss: -1.448794960975647\n",
      "Batch 254, Loss: -1.0915173292160034\n",
      "Batch 255, Loss: 0.09126688539981842\n",
      "Batch 256, Loss: 0.09140071272850037\n",
      "Batch 257, Loss: -1.0913382768630981\n",
      "Batch 258, Loss: 0.0914657711982727\n",
      "Batch 259, Loss: 0.09209491312503815\n",
      "Batch 260, Loss: -1.439481258392334\n",
      "Batch 261, Loss: -1.0909720659255981\n",
      "Batch 262, Loss: -1.0916621685028076\n",
      "Batch 263, Loss: 0.4477183222770691\n",
      "Batch 264, Loss: 0.44766801595687866\n",
      "Batch 265, Loss: 0.4473148584365845\n",
      "Batch 266, Loss: 0.09196101874113083\n",
      "Batch 267, Loss: -1.4460511207580566\n",
      "Batch 268, Loss: 0.44578850269317627\n",
      "Batch 269, Loss: -1.0906925201416016\n",
      "Batch 270, Loss: -1.090689778327942\n",
      "Batch 271, Loss: -1.4443957805633545\n",
      "Batch 272, Loss: 0.09179165959358215\n",
      "Batch 273, Loss: 0.4445733428001404\n",
      "Batch 274, Loss: -1.4442412853240967\n",
      "Batch 275, Loss: -1.4442557096481323\n",
      "Batch 276, Loss: 0.09122059494256973\n",
      "Batch 277, Loss: 0.09176695346832275\n",
      "Batch 278, Loss: -1.0908763408660889\n",
      "Batch 279, Loss: 0.0908232033252716\n",
      "Batch 280, Loss: 0.44509023427963257\n",
      "Batch 281, Loss: 0.4448593258857727\n",
      "Batch 282, Loss: 0.4443732798099518\n",
      "Batch 283, Loss: -1.0901026725769043\n",
      "Batch 284, Loss: 0.44309645891189575\n",
      "Batch 285, Loss: -1.0896989107131958\n",
      "Batch 286, Loss: 0.08967990428209305\n",
      "Batch 287, Loss: 0.08892093598842621\n",
      "Batch 288, Loss: 0.08910033106803894\n",
      "Batch 289, Loss: -1.0885756015777588\n",
      "Batch 290, Loss: -1.439131736755371\n",
      "Batch 291, Loss: 0.0875903069972992\n",
      "Batch 292, Loss: -1.4379589557647705\n",
      "Batch 293, Loss: -1.43812096118927\n",
      "Batch 294, Loss: -1.0869221687316895\n",
      "Batch 295, Loss: 0.0874590128660202\n",
      "Batch 296, Loss: -1.4398295879364014\n",
      "Batch 297, Loss: -1.4403165578842163\n",
      "Batch 298, Loss: 0.44100335240364075\n",
      "Batch 299, Loss: 0.08729821443557739\n",
      "Batch 300, Loss: -1.0870795249938965\n",
      "Batch 301, Loss: -1.4418115615844727\n",
      "Batch 302, Loss: -1.442414402961731\n",
      "Batch 303, Loss: 0.443168044090271\n",
      "Batch 304, Loss: -1.0870414972305298\n",
      "Batch 305, Loss: -1.087592363357544\n",
      "Batch 306, Loss: -1.0879809856414795\n",
      "Batch 307, Loss: -1.4450500011444092\n",
      "Batch 308, Loss: 0.08908199518918991\n",
      "Batch 309, Loss: 0.4463654160499573\n",
      "Batch 310, Loss: 0.08961610496044159\n",
      "Batch 311, Loss: -1.089633584022522\n",
      "Batch 312, Loss: 0.44663727283477783\n",
      "Batch 313, Loss: -1.0898752212524414\n",
      "Batch 314, Loss: 0.44692283868789673\n",
      "Batch 315, Loss: 0.09025578200817108\n",
      "Batch 316, Loss: 0.4458840489387512\n",
      "Batch 317, Loss: 0.4457928538322449\n",
      "Batch 318, Loss: 0.4443869888782501\n",
      "Batch 319, Loss: -1.4439799785614014\n",
      "Batch 320, Loss: -1.442531704902649\n",
      "Batch 321, Loss: 0.08925674110651016\n",
      "Batch 322, Loss: 0.08902103453874588\n",
      "Batch 323, Loss: -1.088629126548767\n",
      "Batch 324, Loss: -1.4420392513275146\n",
      "Batch 325, Loss: -1.0881487131118774\n",
      "Batch 326, Loss: -1.0882681608200073\n",
      "Batch 327, Loss: 0.088548943400383\n",
      "Batch 328, Loss: -1.4417780637741089\n",
      "Batch 329, Loss: -1.442254662513733\n",
      "Batch 330, Loss: -1.0893149375915527\n",
      "Batch 331, Loss: 0.089727982878685\n",
      "Batch 332, Loss: 0.44436997175216675\n",
      "Batch 333, Loss: 0.4449631869792938\n",
      "Batch 334, Loss: 0.08965650945901871\n",
      "Batch 335, Loss: -1.4446353912353516\n",
      "Batch 336, Loss: 0.4444146752357483\n",
      "Batch 337, Loss: 0.4445202350616455\n",
      "Batch 338, Loss: 0.4440844655036926\n",
      "Batch 339, Loss: -1.4434382915496826\n",
      "Batch 340, Loss: -1.4427980184555054\n",
      "Batch 341, Loss: 0.44278910756111145\n",
      "Batch 342, Loss: -1.4424946308135986\n",
      "Batch 343, Loss: 0.08847101032733917\n",
      "Batch 344, Loss: 0.08824127167463303\n",
      "Batch 345, Loss: 0.08755892515182495\n",
      "Batch 346, Loss: 0.4420376718044281\n",
      "Batch 347, Loss: -1.4415407180786133\n",
      "Batch 348, Loss: 0.44163936376571655\n",
      "Batch 349, Loss: 0.4412263035774231\n",
      "Batch 350, Loss: 0.08540371805429459\n",
      "Batch 351, Loss: -1.0847506523132324\n",
      "Batch 352, Loss: 0.08436578512191772\n",
      "Batch 353, Loss: 0.08384034782648087\n",
      "Batch 354, Loss: 0.4383118152618408\n",
      "Batch 355, Loss: 0.4372842609882355\n",
      "Batch 356, Loss: 0.08162080496549606\n",
      "Batch 357, Loss: -1.0810612440109253\n",
      "Batch 358, Loss: -1.0804766416549683\n",
      "Batch 359, Loss: -1.0799880027770996\n",
      "Batch 360, Loss: 0.43343451619148254\n",
      "Batch 361, Loss: -1.4330755472183228\n",
      "Batch 362, Loss: 0.079617440700531\n",
      "Batch 363, Loss: 0.07954507321119308\n",
      "Batch 364, Loss: 0.07915019243955612\n",
      "Batch 365, Loss: -1.0784653425216675\n",
      "Batch 366, Loss: -1.4305169582366943\n",
      "Batch 367, Loss: 0.43094462156295776\n",
      "Batch 368, Loss: -1.077738881111145\n",
      "Batch 369, Loss: -1.0778248310089111\n",
      "Batch 370, Loss: 0.07780750840902328\n",
      "Batch 371, Loss: -1.4301848411560059\n",
      "Batch 372, Loss: 0.43030494451522827\n",
      "Batch 373, Loss: 0.43014460802078247\n",
      "Batch 374, Loss: 0.42973223328590393\n",
      "Batch 375, Loss: 0.42864227294921875\n",
      "Batch 376, Loss: -1.0770928859710693\n",
      "Batch 377, Loss: 0.42754635214805603\n",
      "Batch 378, Loss: -1.0768494606018066\n",
      "Batch 379, Loss: -1.4253005981445312\n",
      "Batch 380, Loss: -1.0769827365875244\n",
      "Batch 381, Loss: -1.0773603916168213\n",
      "Batch 382, Loss: -1.4252122640609741\n",
      "Batch 383, Loss: -1.0781135559082031\n",
      "Batch 384, Loss: 0.4250722825527191\n",
      "Batch 385, Loss: -1.4249815940856934\n",
      "Batch 386, Loss: 0.4257882535457611\n",
      "Batch 387, Loss: 0.4257154166698456\n",
      "Batch 388, Loss: -1.4253721237182617\n",
      "Batch 389, Loss: -1.4247859716415405\n",
      "Batch 390, Loss: -1.4256056547164917\n",
      "Batch 391, Loss: -1.4261126518249512\n",
      "Batch 392, Loss: -1.4268321990966797\n",
      "Batch 393, Loss: 0.42773836851119995\n",
      "Batch 394, Loss: -1.0824997425079346\n",
      "Batch 395, Loss: -1.4285939931869507\n",
      "Batch 396, Loss: -1.0835260152816772\n",
      "Batch 397, Loss: -1.430392861366272\n",
      "Batch 398, Loss: -1.0850647687911987\n",
      "Batch 399, Loss: -1.4322367906570435\n",
      "Training [90%]\tLoss: -0.5014\n",
      "Batch 0, Loss: -1.4333837032318115\n",
      "Batch 1, Loss: -1.087485432624817\n",
      "Batch 2, Loss: -1.4358031749725342\n",
      "Batch 3, Loss: 0.4371481239795685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4, Loss: 0.4382621943950653\n",
      "Batch 5, Loss: -1.0903211832046509\n",
      "Batch 6, Loss: -1.439213752746582\n",
      "Batch 7, Loss: 0.0920366421341896\n",
      "Batch 8, Loss: -1.0921566486358643\n",
      "Batch 9, Loss: -1.0927395820617676\n",
      "Batch 10, Loss: 0.4420716166496277\n",
      "Batch 11, Loss: 0.44274473190307617\n",
      "Batch 12, Loss: 0.09439259767532349\n",
      "Batch 13, Loss: -1.0949528217315674\n",
      "Batch 14, Loss: -1.095314860343933\n",
      "Batch 15, Loss: -1.4423739910125732\n",
      "Batch 16, Loss: 0.44330894947052\n",
      "Batch 17, Loss: -1.4428396224975586\n",
      "Batch 18, Loss: 0.09719782322645187\n",
      "Batch 19, Loss: 0.09697868674993515\n",
      "Batch 20, Loss: -1.4441574811935425\n",
      "Batch 21, Loss: 0.09748490899801254\n",
      "Batch 22, Loss: 0.09737443178892136\n",
      "Batch 23, Loss: -1.4446626901626587\n",
      "Batch 24, Loss: 0.4451324939727783\n",
      "Batch 25, Loss: -1.0963430404663086\n",
      "Batch 26, Loss: -1.445446252822876\n",
      "Batch 27, Loss: -1.0963841676712036\n",
      "Batch 28, Loss: -1.4463754892349243\n",
      "Batch 29, Loss: -1.0972038507461548\n",
      "Batch 30, Loss: -1.0976470708847046\n",
      "Batch 31, Loss: -1.097945213317871\n",
      "Batch 32, Loss: -1.098931908607483\n",
      "Batch 33, Loss: -1.0994517803192139\n",
      "Batch 34, Loss: 0.4506070017814636\n",
      "Batch 35, Loss: -1.1013904809951782\n",
      "Batch 36, Loss: 0.45146995782852173\n",
      "Batch 37, Loss: 0.1025581955909729\n",
      "Batch 38, Loss: 0.4518802762031555\n",
      "Batch 39, Loss: 0.45167276263237\n",
      "Batch 40, Loss: 0.4512251019477844\n",
      "Batch 41, Loss: -1.450560450553894\n",
      "Batch 42, Loss: 0.4502207040786743\n",
      "Batch 43, Loss: -1.1039657592773438\n",
      "Batch 44, Loss: 0.10417834669351578\n",
      "Batch 45, Loss: 0.10383053869009018\n",
      "Batch 46, Loss: 0.4482381343841553\n",
      "Batch 47, Loss: 0.44699233770370483\n",
      "Batch 48, Loss: -1.1034221649169922\n",
      "Batch 49, Loss: 0.44513219594955444\n",
      "Batch 50, Loss: -1.444021463394165\n",
      "Batch 51, Loss: 0.44413644075393677\n",
      "Batch 52, Loss: -1.4424636363983154\n",
      "Batch 53, Loss: -1.4428510665893555\n",
      "Batch 54, Loss: 0.44199174642562866\n",
      "Batch 55, Loss: -1.4423733949661255\n",
      "Batch 56, Loss: -1.442321538925171\n",
      "Batch 57, Loss: 0.4425826668739319\n",
      "Batch 58, Loss: 0.4425121545791626\n",
      "Batch 59, Loss: -1.4426400661468506\n",
      "Batch 60, Loss: 0.4412837028503418\n",
      "Batch 61, Loss: -1.1029151678085327\n",
      "Batch 62, Loss: -1.4414111375808716\n",
      "Batch 63, Loss: -1.441498041152954\n",
      "Batch 64, Loss: 0.4419061541557312\n",
      "Batch 65, Loss: -1.44194757938385\n",
      "Batch 66, Loss: -1.4414031505584717\n",
      "Batch 67, Loss: -1.4421501159667969\n",
      "Batch 68, Loss: -1.104664921760559\n",
      "Batch 69, Loss: 0.10522549599409103\n",
      "Batch 70, Loss: -1.1050814390182495\n",
      "Batch 71, Loss: 0.4460661709308624\n",
      "Batch 72, Loss: -1.4460984468460083\n",
      "Batch 73, Loss: -1.1062228679656982\n",
      "Batch 74, Loss: -1.1067471504211426\n",
      "Batch 75, Loss: -1.107966661453247\n",
      "Batch 76, Loss: -1.1081801652908325\n",
      "Batch 77, Loss: 0.4493180513381958\n",
      "Batch 78, Loss: -1.4493356943130493\n",
      "Batch 79, Loss: 0.11050035059452057\n",
      "Batch 80, Loss: -1.1116604804992676\n",
      "Batch 81, Loss: -1.111579179763794\n",
      "Batch 82, Loss: -1.1123210191726685\n",
      "Batch 83, Loss: 0.45202505588531494\n",
      "Batch 84, Loss: -1.45159912109375\n",
      "Batch 85, Loss: 0.45205140113830566\n",
      "Batch 86, Loss: -1.4527804851531982\n",
      "Batch 87, Loss: 0.11571040004491806\n",
      "Batch 88, Loss: -1.1169307231903076\n",
      "Batch 89, Loss: 0.4537322521209717\n",
      "Batch 90, Loss: 0.4528893232345581\n",
      "Batch 91, Loss: 0.4533412456512451\n",
      "Batch 92, Loss: -1.451559066772461\n",
      "Batch 93, Loss: 0.45240792632102966\n",
      "Batch 94, Loss: 0.45056918263435364\n",
      "Batch 95, Loss: -1.4508094787597656\n",
      "Batch 96, Loss: 0.11901077628135681\n",
      "Batch 97, Loss: -1.1189053058624268\n",
      "Batch 98, Loss: -1.4471036195755005\n",
      "Batch 99, Loss: -1.1191898584365845\n",
      "Batch 100, Loss: 0.11955656856298447\n",
      "Batch 101, Loss: 0.44830039143562317\n",
      "Batch 102, Loss: 0.4478260278701782\n",
      "Batch 103, Loss: -1.446716070175171\n",
      "Batch 104, Loss: 0.11979340016841888\n",
      "Batch 105, Loss: 0.4479796290397644\n",
      "Batch 106, Loss: 0.44718390703201294\n",
      "Batch 107, Loss: -1.4458613395690918\n",
      "Batch 108, Loss: -1.118952751159668\n",
      "Batch 109, Loss: -1.1177490949630737\n",
      "Batch 110, Loss: 0.11933764070272446\n",
      "Batch 111, Loss: 0.11935696005821228\n",
      "Batch 112, Loss: 0.4435184895992279\n",
      "Batch 113, Loss: -1.4424036741256714\n",
      "Batch 114, Loss: -1.1187026500701904\n",
      "Batch 115, Loss: -1.1187797784805298\n",
      "Batch 116, Loss: -1.4418379068374634\n",
      "Batch 117, Loss: 0.44251322746276855\n",
      "Batch 118, Loss: 0.4423220753669739\n",
      "Batch 119, Loss: 0.11988504230976105\n",
      "Batch 120, Loss: 0.11983200162649155\n",
      "Batch 121, Loss: 0.4395377039909363\n",
      "Batch 122, Loss: 0.43498679995536804\n",
      "Batch 123, Loss: 0.43491047620773315\n",
      "Batch 124, Loss: 0.11746378242969513\n",
      "Batch 125, Loss: 0.4236035943031311\n",
      "Batch 126, Loss: 0.11837532371282578\n",
      "Batch 127, Loss: 0.41696637868881226\n",
      "Batch 128, Loss: 0.41059672832489014\n",
      "Batch 129, Loss: -1.402872085571289\n",
      "Batch 130, Loss: -1.1152622699737549\n",
      "Batch 131, Loss: 0.3935566842556\n",
      "Batch 132, Loss: -1.1153558492660522\n",
      "Batch 133, Loss: 0.38346046209335327\n",
      "Batch 134, Loss: 0.11619231849908829\n",
      "Batch 135, Loss: -1.1163209676742554\n",
      "Batch 136, Loss: -1.3702367544174194\n",
      "Batch 137, Loss: 0.37000608444213867\n",
      "Batch 138, Loss: 0.3677486777305603\n",
      "Batch 139, Loss: -1.367063045501709\n",
      "Batch 140, Loss: 0.11829449236392975\n",
      "Batch 141, Loss: -1.1211659908294678\n",
      "Batch 142, Loss: -1.3664524555206299\n",
      "Batch 143, Loss: -1.1191234588623047\n",
      "Batch 144, Loss: 0.11996784806251526\n",
      "Batch 145, Loss: -1.1237672567367554\n",
      "Batch 146, Loss: 0.1247638538479805\n",
      "Batch 147, Loss: 0.12510579824447632\n",
      "Batch 148, Loss: 0.12483871728181839\n",
      "Batch 149, Loss: 0.12051981687545776\n",
      "Batch 150, Loss: -1.3705779314041138\n",
      "Batch 151, Loss: 0.3692149519920349\n",
      "Batch 152, Loss: 0.36906805634498596\n",
      "Batch 153, Loss: -1.3683539628982544\n",
      "Batch 154, Loss: 0.36828339099884033\n",
      "Batch 155, Loss: -1.115759015083313\n",
      "Batch 156, Loss: 0.11823412775993347\n",
      "Batch 157, Loss: -1.1174278259277344\n",
      "Batch 158, Loss: -1.3693121671676636\n",
      "Batch 159, Loss: -1.3685240745544434\n",
      "Batch 160, Loss: -1.369584083557129\n",
      "Batch 161, Loss: 0.11530299484729767\n",
      "Batch 162, Loss: 0.37265050411224365\n",
      "Batch 163, Loss: -1.1151458024978638\n",
      "Batch 164, Loss: 0.3737156093120575\n",
      "Batch 165, Loss: -1.3734607696533203\n",
      "Batch 166, Loss: 0.11542829126119614\n",
      "Batch 167, Loss: -1.1179254055023193\n",
      "Batch 168, Loss: 0.372794508934021\n",
      "Batch 169, Loss: -1.3724795579910278\n",
      "Batch 170, Loss: 0.37297171354293823\n",
      "Batch 171, Loss: -1.1154658794403076\n",
      "Batch 172, Loss: 0.3725722134113312\n",
      "Batch 173, Loss: 0.1158025711774826\n",
      "Batch 174, Loss: 0.3707801103591919\n",
      "Batch 175, Loss: 0.36921989917755127\n",
      "Batch 176, Loss: -1.3672267198562622\n",
      "Batch 177, Loss: 0.11376073956489563\n",
      "Batch 178, Loss: 0.3649533689022064\n",
      "Batch 179, Loss: 0.11506811529397964\n",
      "Batch 180, Loss: 0.3620752692222595\n",
      "Batch 181, Loss: -1.3617297410964966\n",
      "Batch 182, Loss: 0.3606705665588379\n",
      "Batch 183, Loss: -1.1106185913085938\n",
      "Batch 184, Loss: -1.110111951828003\n",
      "Batch 185, Loss: 0.3579661250114441\n",
      "Batch 186, Loss: 0.10983492434024811\n",
      "Batch 187, Loss: 0.10681058466434479\n",
      "Batch 188, Loss: 0.3556867241859436\n",
      "Batch 189, Loss: 0.10530009120702744\n",
      "Batch 190, Loss: 0.35404640436172485\n",
      "Batch 191, Loss: -1.1034165620803833\n",
      "Batch 192, Loss: 0.10503268986940384\n",
      "Batch 193, Loss: 0.3524630069732666\n",
      "Batch 194, Loss: 0.10333012044429779\n",
      "Batch 195, Loss: 0.3509198725223541\n",
      "Batch 196, Loss: -1.101288080215454\n",
      "Batch 197, Loss: 0.34904205799102783\n",
      "Batch 198, Loss: 0.10015510022640228\n",
      "Batch 199, Loss: -1.3461477756500244\n",
      "Batch 200, Loss: 0.34620219469070435\n",
      "Batch 201, Loss: -1.0980616807937622\n",
      "Batch 202, Loss: -1.0977805852890015\n",
      "Batch 203, Loss: 0.09624642878770828\n",
      "Batch 204, Loss: 0.34325093030929565\n",
      "Batch 205, Loss: 0.3424223065376282\n",
      "Batch 206, Loss: -1.0953402519226074\n",
      "Batch 207, Loss: 0.09529779106378555\n",
      "Batch 208, Loss: -1.3397724628448486\n",
      "Batch 209, Loss: -1.096657156944275\n",
      "Batch 210, Loss: 0.09498489648103714\n",
      "Batch 211, Loss: 0.33889034390449524\n",
      "Batch 212, Loss: 0.33840271830558777\n",
      "Batch 213, Loss: -1.33750319480896\n",
      "Batch 214, Loss: -1.0960783958435059\n",
      "Batch 215, Loss: 0.3368273079395294\n",
      "Batch 216, Loss: 0.09418441355228424\n",
      "Batch 217, Loss: 0.33585140109062195\n",
      "Batch 218, Loss: 0.3351669907569885\n",
      "Batch 219, Loss: -1.3346796035766602\n",
      "Batch 220, Loss: -1.333801031112671\n",
      "Batch 221, Loss: -1.092837929725647\n",
      "Batch 222, Loss: -1.3334319591522217\n",
      "Batch 223, Loss: 0.0932193174958229\n",
      "Batch 224, Loss: 0.09317459166049957\n",
      "Batch 225, Loss: 0.33347123861312866\n",
      "Batch 226, Loss: -1.092508316040039\n",
      "Batch 227, Loss: 0.09486541152000427\n",
      "Batch 228, Loss: -1.333277702331543\n",
      "Batch 229, Loss: -1.0918995141983032\n",
      "Batch 230, Loss: 0.09434869885444641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 231, Loss: 0.09409158676862717\n",
      "Batch 232, Loss: -1.093536376953125\n",
      "Batch 233, Loss: 0.09273087978363037\n",
      "Batch 234, Loss: 0.3325875997543335\n",
      "Batch 235, Loss: 0.33280155062675476\n",
      "Batch 236, Loss: -1.3317205905914307\n",
      "Batch 237, Loss: -1.3314708471298218\n",
      "Batch 238, Loss: 0.33185505867004395\n",
      "Batch 239, Loss: 0.3312509059906006\n",
      "Batch 240, Loss: 0.3308316171169281\n",
      "Batch 241, Loss: 0.08974922448396683\n",
      "Batch 242, Loss: 0.08755651861429214\n",
      "Batch 243, Loss: 0.08817166835069656\n",
      "Batch 244, Loss: -1.0858452320098877\n",
      "Batch 245, Loss: -1.085231900215149\n",
      "Batch 246, Loss: 0.32752224802970886\n",
      "Batch 247, Loss: -1.0856174230575562\n",
      "Batch 248, Loss: 0.08547370880842209\n",
      "Batch 249, Loss: -1.3259515762329102\n",
      "Batch 250, Loss: -1.325758934020996\n",
      "Batch 251, Loss: -1.084733486175537\n",
      "Batch 252, Loss: 0.08485546708106995\n",
      "Batch 253, Loss: -1.325976848602295\n",
      "Batch 254, Loss: -1.0847049951553345\n",
      "Batch 255, Loss: -1.3265477418899536\n",
      "Batch 256, Loss: -1.3272477388381958\n",
      "Batch 257, Loss: -1.0855151414871216\n",
      "Batch 258, Loss: -1.3303000926971436\n",
      "Batch 259, Loss: 0.08570725470781326\n",
      "Batch 260, Loss: 0.3299541771411896\n",
      "Batch 261, Loss: 0.33033430576324463\n",
      "Batch 262, Loss: 0.33045631647109985\n",
      "Batch 263, Loss: -1.3304635286331177\n",
      "Batch 264, Loss: -1.3305779695510864\n",
      "Batch 265, Loss: -1.0875375270843506\n",
      "Batch 266, Loss: -1.333425760269165\n",
      "Batch 267, Loss: 0.3318253457546234\n",
      "Batch 268, Loss: 0.3321143388748169\n",
      "Batch 269, Loss: -1.332111120223999\n",
      "Batch 270, Loss: -1.088150978088379\n",
      "Batch 271, Loss: -1.3328731060028076\n",
      "Batch 272, Loss: 0.08913691341876984\n",
      "Batch 273, Loss: 0.08938649296760559\n",
      "Batch 274, Loss: -1.3338743448257446\n",
      "Batch 275, Loss: 0.3343367874622345\n",
      "Batch 276, Loss: -1.0906925201416016\n",
      "Batch 277, Loss: 0.33480116724967957\n",
      "Batch 278, Loss: -1.33481764793396\n",
      "Batch 279, Loss: 0.3355315923690796\n",
      "Batch 280, Loss: -1.0913686752319336\n",
      "Batch 281, Loss: 0.3356122374534607\n",
      "Batch 282, Loss: 0.09047935158014297\n",
      "Batch 283, Loss: -1.0903759002685547\n",
      "Batch 284, Loss: -1.3349908590316772\n",
      "Batch 285, Loss: 0.09075428545475006\n",
      "Batch 286, Loss: -1.334705114364624\n",
      "Batch 287, Loss: 0.3363588750362396\n",
      "Batch 288, Loss: 0.09229037910699844\n",
      "Batch 289, Loss: -1.3351480960845947\n",
      "Batch 290, Loss: 0.0917913019657135\n",
      "Batch 291, Loss: -1.3352117538452148\n",
      "Batch 292, Loss: -1.335145115852356\n",
      "Batch 293, Loss: 0.33587998151779175\n",
      "Batch 294, Loss: 0.0894571989774704\n",
      "Batch 295, Loss: 0.09028944373130798\n",
      "Batch 296, Loss: -1.3357776403427124\n",
      "Batch 297, Loss: -1.089157223701477\n",
      "Batch 298, Loss: -1.0878981351852417\n",
      "Batch 299, Loss: 0.33667075634002686\n",
      "Batch 300, Loss: -1.336728811264038\n",
      "Batch 301, Loss: 0.3369496166706085\n",
      "Batch 302, Loss: 0.33698928356170654\n",
      "Batch 303, Loss: -1.3368659019470215\n",
      "Batch 304, Loss: -1.0878396034240723\n",
      "Batch 305, Loss: 0.33708852529525757\n",
      "Batch 306, Loss: -1.0891377925872803\n",
      "Batch 307, Loss: 0.08945268392562866\n",
      "Batch 308, Loss: -1.3369425535202026\n",
      "Batch 309, Loss: -1.3369522094726562\n",
      "Batch 310, Loss: 0.33747705817222595\n",
      "Batch 311, Loss: 0.33740174770355225\n",
      "Batch 312, Loss: 0.08890554308891296\n",
      "Batch 313, Loss: -1.337046504020691\n",
      "Batch 314, Loss: -1.3372166156768799\n",
      "Batch 315, Loss: -1.337469458580017\n",
      "Batch 316, Loss: -1.337770938873291\n",
      "Batch 317, Loss: 0.338398277759552\n",
      "Batch 318, Loss: -1.088985562324524\n",
      "Batch 319, Loss: -1.0902903079986572\n",
      "Batch 320, Loss: -1.3396081924438477\n",
      "Batch 321, Loss: -1.340255856513977\n",
      "Batch 322, Loss: 0.09195680916309357\n",
      "Batch 323, Loss: 0.3420707881450653\n",
      "Batch 324, Loss: 0.3420374095439911\n",
      "Batch 325, Loss: 0.09131746739149094\n",
      "Batch 326, Loss: 0.3423740863800049\n",
      "Batch 327, Loss: -1.3417911529541016\n",
      "Batch 328, Loss: 0.09173735976219177\n",
      "Batch 329, Loss: -1.341678500175476\n",
      "Batch 330, Loss: -1.3419580459594727\n",
      "Batch 331, Loss: -1.34243643283844\n",
      "Batch 332, Loss: -1.090050458908081\n",
      "Batch 333, Loss: -1.0902762413024902\n",
      "Batch 334, Loss: 0.09158731997013092\n",
      "Batch 335, Loss: -1.3443596363067627\n",
      "Batch 336, Loss: -1.091935396194458\n",
      "Batch 337, Loss: -1.3456672430038452\n",
      "Batch 338, Loss: -1.0920045375823975\n",
      "Batch 339, Loss: -1.0934908390045166\n",
      "Batch 340, Loss: -1.3482022285461426\n",
      "Batch 341, Loss: 0.09516017138957977\n",
      "Batch 342, Loss: 0.3500227630138397\n",
      "Batch 343, Loss: -1.0960407257080078\n",
      "Batch 344, Loss: -1.3514204025268555\n",
      "Batch 345, Loss: 0.3522164821624756\n",
      "Batch 346, Loss: 0.09771159291267395\n",
      "Batch 347, Loss: -1.352927565574646\n",
      "Batch 348, Loss: -1.0981296300888062\n",
      "Batch 349, Loss: -1.3540606498718262\n",
      "Batch 350, Loss: 0.0981299951672554\n",
      "Batch 351, Loss: 0.09949284046888351\n",
      "Batch 352, Loss: -1.3561517000198364\n",
      "Batch 353, Loss: 0.35611873865127563\n",
      "Batch 354, Loss: -1.3564667701721191\n",
      "Batch 355, Loss: 0.35780632495880127\n",
      "Batch 356, Loss: -1.3573009967803955\n",
      "Batch 357, Loss: 0.09980865567922592\n",
      "Batch 358, Loss: -1.3581368923187256\n",
      "Batch 359, Loss: -1.0996071100234985\n",
      "Batch 360, Loss: 0.3603067696094513\n",
      "Batch 361, Loss: -1.0999009609222412\n",
      "Batch 362, Loss: -1.1002562046051025\n",
      "Batch 363, Loss: 0.0996696874499321\n",
      "Batch 364, Loss: -1.3608582019805908\n",
      "Batch 365, Loss: -1.3614964485168457\n",
      "Batch 366, Loss: 0.36353182792663574\n",
      "Batch 367, Loss: -1.3629186153411865\n",
      "Batch 368, Loss: 0.3648824989795685\n",
      "Batch 369, Loss: 0.3651484251022339\n",
      "Batch 370, Loss: 0.1014782190322876\n",
      "Batch 371, Loss: -1.3644436597824097\n",
      "Batch 372, Loss: 0.3645728528499603\n",
      "Batch 373, Loss: 0.10208997875452042\n",
      "Batch 374, Loss: -1.1016075611114502\n",
      "Batch 375, Loss: 0.3626255989074707\n",
      "Batch 376, Loss: 0.36284497380256653\n",
      "Batch 377, Loss: -1.1007484197616577\n",
      "Batch 378, Loss: -1.361299753189087\n",
      "Batch 379, Loss: -1.3610830307006836\n",
      "Batch 380, Loss: -1.36068594455719\n",
      "Batch 381, Loss: 0.36107826232910156\n",
      "Batch 382, Loss: -1.361159324645996\n",
      "Batch 383, Loss: 0.3615036606788635\n",
      "Batch 384, Loss: 0.3615405261516571\n",
      "Batch 385, Loss: 0.10051307827234268\n",
      "Batch 386, Loss: 0.10024937987327576\n",
      "Batch 387, Loss: -1.1000072956085205\n",
      "Batch 388, Loss: 0.3607615530490875\n",
      "Batch 389, Loss: 0.36024266481399536\n",
      "Batch 390, Loss: -1.0999276638031006\n",
      "Batch 391, Loss: -1.0998241901397705\n",
      "Batch 392, Loss: -1.3584551811218262\n",
      "Batch 393, Loss: -1.0991679430007935\n",
      "Batch 394, Loss: 0.3586333394050598\n",
      "Batch 395, Loss: -1.3582868576049805\n",
      "Batch 396, Loss: 0.3584787845611572\n",
      "Batch 397, Loss: -1.3581974506378174\n",
      "Batch 398, Loss: -1.1018338203430176\n",
      "Batch 399, Loss: -1.101132869720459\n",
      "Training [100%]\tLoss: -0.4890\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.006199120020028204 s\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: -0.4912\n",
      "\tAccuracy: 50.0%\n",
      "Test time: -0.0009709650184959173 s\n"
     ]
    }
   ],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
