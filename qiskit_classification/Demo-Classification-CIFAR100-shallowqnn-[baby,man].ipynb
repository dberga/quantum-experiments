{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from qiskit import Aer, QuantumCircuit, BasicAer\n",
    "from qiskit.utils import QuantumInstance, algorithm_globals\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "from qiskit.aqua.algorithms import QSVM\n",
    "from qiskit.aqua.components.multiclass_extensions import AllPairs\n",
    "from qiskit.aqua.utils.dataset_helper import get_feature_dimension\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from quantic.data import DatasetLoader\n",
    "    \n",
    "# Additional torch-related imports\n",
    "from torch import no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.optim as optim # Adam, SGD, LBFGS\n",
    "from torch.nn import NLLLoss # CrossEntropyLoss, MSELoss, L1Loss, BCELoss\n",
    "from qnetworks import HybridQNN_Shallow\n",
    "from qnetworks import HybridQNN\n",
    "\n",
    "#time\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae932ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'cat']\n"
     ]
    }
   ],
   "source": [
    "# network args\n",
    "n_classes = 2\n",
    "n_qubits = 2\n",
    "n_features = None\n",
    "if n_features is None:\n",
    "    n_features = n_qubits\n",
    "network = \"hybridqnn_shallow\" #hybridqnn_shallow\n",
    "# train args\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "LR = 0.001\n",
    "n_samples_train = 200 #128\n",
    "n_samples_test = 50 #64\n",
    "# plot args\n",
    "n_samples_show = batch_size\n",
    "# dataset args\n",
    "shuffle = True\n",
    "dataset = \"CIFAR100\" # MNIST / CIFAR10 / CIFAR100, any from pytorch\n",
    "dataset_cfg = \"\"\n",
    "specific_classes_names = [\"baby\",\"man\"] # ['0','1'] # selected (filtered) classes\n",
    "print(specific_classes_names)\n",
    "use_specific_classes = len(specific_classes_names)>=n_classes\n",
    "# preprocessing\n",
    "input_resolution = (28,28) #(28,28) # please check n_filts required on fc1/fc2 to input to qnn\n",
    "resize_interpolation = transforms.functional.InterpolationMode.BILINEAR \n",
    "\n",
    "# Set seed for random generators\n",
    "rand_seed = np.random.randint(50)\n",
    "algorithm_globals.random_seed = rand_seed\n",
    "manual_seed(rand_seed) # Set train shuffle seed (for reproducibility)\n",
    "\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ce65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset partitions: ['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "######## PREPARE DATASETS\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "if dataset_cfg:\n",
    "    # Load a dataset configuration file\n",
    "    dataset = DatasetLoader.load(from_cfg=dataset_cfg,framework='torchvision')\n",
    "else:\n",
    "    # Or instantate dataset manually\n",
    "    dataset = DatasetLoader.load(dataset_type=dataset,\n",
    "                                   num_classes=n_classes,\n",
    "                                   specific_classes=specific_classes_names,\n",
    "                                   num_samples_class_train=n_samples_train,\n",
    "                                   num_samples_class_test=n_samples_test,\n",
    "                                   framework='torchvision'\n",
    "                                   )\n",
    "print(f'Dataset partitions: {dataset.get_partitions()}')\n",
    "\n",
    "X_train = dataset['train']\n",
    "X_test = dataset['test']\n",
    "\n",
    "\n",
    "# Get channels (rgb or grayscale)\n",
    "if len(X_train.data.shape)>3: # 3d image (rgb+)\n",
    "    n_channels = X_train.data.shape[3]\n",
    "else: # 2d image (grayscale)\n",
    "    n_channels = 1\n",
    "\n",
    "if network == 'hybridqnn_shallow' or network == 'QSVM':\n",
    "    # Set preprocessing transforms\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "else:\n",
    "    list_preprocessing = [\n",
    "        transforms.Resize(input_resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "    ] #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    \n",
    "X_train.transform= transforms.Compose(list_preprocessing)\n",
    "X_test.transform = transforms.Compose(list_preprocessing)           \n",
    "\n",
    "# set filtered/specific class names\n",
    "classes_str = \",\".join(dataset.specific_classes_names)\n",
    "classes2spec = {}\n",
    "specific_classes = dataset.specific_classes\n",
    "for idx, class_idx in enumerate(specific_classes):\n",
    "    classes2spec[class_idx]=idx\n",
    "\n",
    "classes_list = dataset.classes\n",
    "n_samples = n_samples_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b33c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders elapsed time: 0.00048730603884905577 s\n"
     ]
    }
   ],
   "source": [
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=shuffle)\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Dataloaders elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d23142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### VISUALIZE LABELS\n",
    "data_iter = iter(train_loader)\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    try:\n",
    "        images, targets = data_iter.__next__()\n",
    "    except:\n",
    "        break\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, image in enumerate(images):\n",
    "        axes[idx].imshow(np.moveaxis(images[idx].numpy().squeeze(),0,-1))\n",
    "        axes[idx].set_xticks([])\n",
    "        axes[idx].set_yticks([])\n",
    "        class_label = classes_list[targets[idx].item()]\n",
    "        axes[idx].set_title(\"Labeled: {}\".format(class_label))\n",
    "        if idx > n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_labeled_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            break\n",
    "    plt.show()\n",
    "    n_samples_show_alt -= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79126128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComposedOp([\n",
      "  OperatorMeasurement(1.0 * ZZ),\n",
      "  CircuitStateFn(\n",
      "       ┌──────────────────────────┐┌──────────────────────────────────────┐\n",
      "  q_0: ┤0                         ├┤0                                     ├\n",
      "       │  ZZFeatureMap(x[0],x[1]) ││  RealAmplitudes(θ[0],θ[1],θ[2],θ[3]) │\n",
      "  q_1: ┤1                         ├┤1                                     ├\n",
      "       └──────────────────────────┘└──────────────────────────────────────┘\n",
      "  )\n",
      "])\n",
      "HybridQNN_Shallow(\n",
      "  (conv1): Conv2d(3, 2, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(2, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (qnn): TorchConnector()\n",
      "  (fc3): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Network init elapsed time: -0.00020319299073889852 s\n"
     ]
    }
   ],
   "source": [
    "##### DESIGN NETWORK\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "\n",
    "\n",
    "# init network\n",
    "if network == \"hybridqnn_shallow\":\n",
    "    ## predefine number of input filters to fc1 and fc2\n",
    "    # examples: 13456 for (128x128x1), 59536 for (256x256x1), 256 for (28x28x1), 400 for (28x28x3) or (35x35x1) \n",
    "    if n_channels == 1:\n",
    "        n_filts_fc1 = int(((((input_resolution[0]-4)/2)-4)/2)**2)*16\n",
    "    else:\n",
    "        #n_filts_fc1 = int(((((input_resolution[0])/2)-4)/2)**2)*16 # +7\n",
    "        n_filts_fc1 = 256\n",
    "    n_filts_fc2 = int(n_filts_fc1 / 4)\n",
    "\n",
    "    # declare quantum instance\n",
    "    qi = QuantumInstance(Aer.get_backend(\"aer_simulator_statevector\"))\n",
    "    # Define QNN\n",
    "    feature_map = ZZFeatureMap(n_features)\n",
    "    ansatz = RealAmplitudes(n_qubits, reps=1)\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = TwoLayerQNN(\n",
    "        n_qubits, feature_map, ansatz, input_gradients=True, exp_val=AerPauliExpectation(), quantum_instance=qi\n",
    "    )\n",
    "    print(qnn.operator)\n",
    "    qnn.circuit.draw(output=\"mpl\",filename=f\"plots/qnn{n_qubits}_{n_classes}classes.png\")\n",
    "    #from qiskit.quantum_info import Statevector\n",
    "    #from qiskit.visualization import plot_bloch_multivector\n",
    "    #state = Statevector.from_instruction(qnn.circuit)\n",
    "    #plot_bloch_multivector(state)\n",
    "    model = HybridQNN_Shallow(n_classes = n_classes, n_qubits = n_qubits, n_channels = n_channels, n_filts_fc1 = n_filts_fc1, n_filts_fc2 = n_filts_fc2, qnn = qnn)\n",
    "    print(model)\n",
    "\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "\n",
    "elif network == \"QSVM\":\n",
    "    backend = BasicAer.get_backend('qasm_simulator')\n",
    "\n",
    "    # todo: fix this transformation for QSVM\n",
    "    train_input = X_train.targets\n",
    "    test_input = X_test.targets\n",
    "    total_array = np.concatenate([test_input[k] for k in test_input])\n",
    "    #\n",
    "    feature_map = ZZFeatureMap(feature_dimension=get_feature_dimension(train_input),\n",
    "                               reps=2, entanglement='linear')\n",
    "    svm = QSVM(feature_map, train_input, test_input, total_array,\n",
    "               multiclass_extension=AllPairs())\n",
    "    quantum_instance = QuantumInstance(backend, shots=1024,\n",
    "                                       seed_simulator=algorithm_globals.random_seed,\n",
    "                                       seed_transpiler=algorithm_globals.random_seed)\n",
    "else:\n",
    "    model = HybridQNN(backbone=network,pretrained=True,n_qubits=n_qubits,n_classes=n_classes)\n",
    "    # Define model, optimizer, and loss function\n",
    "    optimizer = optim.Adam(model.network.parameters(), lr=LR)\n",
    "    loss_func = NLLLoss()\n",
    "    \n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Network init elapsed time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a19aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: -1.69571852684021\n",
      "Batch 1, Loss: -1.6487139463424683\n",
      "Batch 2, Loss: 0.7358797788619995\n",
      "Batch 3, Loss: -1.7580547332763672\n",
      "Batch 4, Loss: -1.778651237487793\n",
      "Batch 5, Loss: -1.7530670166015625\n",
      "Batch 6, Loss: 0.7879904508590698\n",
      "Batch 7, Loss: 0.755623459815979\n",
      "Batch 8, Loss: 0.7625733613967896\n",
      "Batch 9, Loss: 0.7713145613670349\n",
      "Batch 10, Loss: 0.7422940731048584\n",
      "Batch 11, Loss: 0.712405800819397\n",
      "Batch 12, Loss: 0.5321309566497803\n",
      "Batch 13, Loss: -1.751202940940857\n",
      "Batch 14, Loss: 0.7717148065567017\n",
      "Batch 15, Loss: 0.6668925285339355\n",
      "Batch 16, Loss: 0.6419951915740967\n",
      "Batch 17, Loss: 0.6644410490989685\n",
      "Batch 18, Loss: -0.25409021973609924\n",
      "Batch 19, Loss: -1.5768399238586426\n",
      "Batch 20, Loss: -1.0603878498077393\n",
      "Batch 21, Loss: -0.20601600408554077\n",
      "Batch 22, Loss: 0.22541527450084686\n",
      "Batch 23, Loss: -0.21384963393211365\n",
      "Batch 24, Loss: -1.363715648651123\n",
      "Batch 25, Loss: -0.04642084240913391\n",
      "Batch 26, Loss: -1.5241427421569824\n",
      "Batch 27, Loss: -0.2329661250114441\n",
      "Batch 28, Loss: 0.14078214764595032\n",
      "Batch 29, Loss: -0.1964758038520813\n",
      "Batch 30, Loss: -0.2674945294857025\n",
      "Batch 31, Loss: -1.1371221542358398\n",
      "Batch 32, Loss: -0.8890615701675415\n",
      "Batch 33, Loss: -1.3267552852630615\n",
      "Batch 34, Loss: -1.394484519958496\n",
      "Batch 35, Loss: -0.045005589723587036\n",
      "Batch 36, Loss: 0.48647546768188477\n",
      "Batch 37, Loss: -1.740983009338379\n",
      "Batch 38, Loss: -0.24166715145111084\n",
      "Batch 39, Loss: 0.7406561374664307\n",
      "Batch 40, Loss: 0.0016244947910308838\n",
      "Batch 41, Loss: -1.381995439529419\n",
      "Batch 42, Loss: -1.0552774667739868\n",
      "Batch 43, Loss: -1.7388932704925537\n",
      "Batch 44, Loss: -1.3277267217636108\n",
      "Batch 45, Loss: -1.6350221633911133\n",
      "Batch 46, Loss: -1.6340186595916748\n",
      "Batch 47, Loss: -0.3548184037208557\n",
      "Batch 48, Loss: -1.3681727647781372\n",
      "Batch 49, Loss: -1.65885591506958\n",
      "Batch 50, Loss: 0.37911078333854675\n",
      "Batch 51, Loss: 0.032735615968704224\n",
      "Batch 52, Loss: -1.4303126335144043\n",
      "Batch 53, Loss: -1.6019949913024902\n",
      "Batch 54, Loss: -0.33437180519104004\n",
      "Batch 55, Loss: 0.37646231055259705\n",
      "Batch 56, Loss: 0.5983296632766724\n",
      "Batch 57, Loss: 0.9033297300338745\n",
      "Batch 58, Loss: 0.0944494903087616\n",
      "Batch 59, Loss: -1.0654457807540894\n",
      "Batch 60, Loss: 0.9483746886253357\n",
      "Batch 61, Loss: -0.37015774846076965\n",
      "Batch 62, Loss: 0.5990244150161743\n",
      "Batch 63, Loss: -1.1581196784973145\n",
      "Batch 64, Loss: 0.2898726165294647\n",
      "Batch 65, Loss: -1.067671298980713\n",
      "Batch 66, Loss: 0.44034337997436523\n",
      "Batch 67, Loss: -1.121962547302246\n",
      "Batch 68, Loss: 0.3565918505191803\n",
      "Batch 69, Loss: -0.7680839896202087\n",
      "Batch 70, Loss: -1.0179609060287476\n",
      "Batch 71, Loss: -1.4667681455612183\n",
      "Batch 72, Loss: -1.297493577003479\n",
      "Batch 73, Loss: -1.0518430471420288\n",
      "Batch 74, Loss: -1.748978853225708\n",
      "Batch 75, Loss: -1.679403305053711\n",
      "Batch 76, Loss: -0.6936224699020386\n",
      "Batch 77, Loss: -1.3883945941925049\n",
      "Batch 78, Loss: -1.1101107597351074\n",
      "Batch 79, Loss: 0.49801909923553467\n",
      "Batch 80, Loss: -1.0866637229919434\n",
      "Batch 81, Loss: -1.0899412631988525\n",
      "Batch 82, Loss: -0.013982206583023071\n",
      "Batch 83, Loss: -1.1157851219177246\n",
      "Batch 84, Loss: -1.025397777557373\n",
      "Batch 85, Loss: -1.2387382984161377\n",
      "Batch 86, Loss: -0.004413574934005737\n",
      "Batch 87, Loss: 0.4439893960952759\n",
      "Batch 88, Loss: 0.43661126494407654\n",
      "Batch 89, Loss: 0.15682238340377808\n",
      "Batch 90, Loss: 0.3352842330932617\n",
      "Batch 91, Loss: -0.1861354112625122\n",
      "Batch 92, Loss: -1.443293571472168\n",
      "Batch 93, Loss: -0.9196141958236694\n",
      "Batch 94, Loss: 0.8164616823196411\n",
      "Batch 95, Loss: 0.024043530225753784\n",
      "Batch 96, Loss: -0.9465432167053223\n",
      "Batch 97, Loss: -0.1514020562171936\n",
      "Batch 98, Loss: -1.434935212135315\n",
      "Batch 99, Loss: -1.22838294506073\n",
      "Batch 100, Loss: -1.2346892356872559\n",
      "Batch 101, Loss: -1.5568289756774902\n",
      "Batch 102, Loss: -1.2470569610595703\n",
      "Batch 103, Loss: 0.27476149797439575\n",
      "Batch 104, Loss: 0.31440624594688416\n",
      "Batch 105, Loss: -1.3443312644958496\n",
      "Batch 106, Loss: 0.24632558226585388\n",
      "Batch 107, Loss: -1.0697591304779053\n",
      "Batch 108, Loss: -0.17511656880378723\n",
      "Batch 109, Loss: -1.7294411659240723\n",
      "Batch 110, Loss: 0.23508815467357635\n",
      "Batch 111, Loss: 0.3459126651287079\n",
      "Batch 112, Loss: 0.44004321098327637\n",
      "Batch 113, Loss: 0.1551765352487564\n",
      "Batch 114, Loss: -1.3093922138214111\n",
      "Batch 115, Loss: 0.3218711018562317\n",
      "Batch 116, Loss: 0.1467820107936859\n",
      "Batch 117, Loss: 0.5679841041564941\n",
      "Batch 118, Loss: -1.3434449434280396\n",
      "Batch 119, Loss: 0.2624095678329468\n",
      "Batch 120, Loss: 0.3704110383987427\n",
      "Batch 121, Loss: -0.24754855036735535\n",
      "Batch 122, Loss: -1.7620054483413696\n",
      "Batch 123, Loss: 0.3547670543193817\n",
      "Batch 124, Loss: 0.511859118938446\n",
      "Batch 125, Loss: 0.2257637232542038\n",
      "Batch 126, Loss: -1.1173882484436035\n",
      "Batch 127, Loss: 0.35417699813842773\n",
      "Batch 128, Loss: -1.665468692779541\n",
      "Batch 129, Loss: -1.4479025602340698\n",
      "Batch 130, Loss: -1.3415336608886719\n",
      "Batch 131, Loss: -1.4354287385940552\n",
      "Batch 132, Loss: 0.2863084077835083\n",
      "Batch 133, Loss: -0.6310462951660156\n",
      "Batch 134, Loss: -1.1090278625488281\n",
      "Batch 135, Loss: -0.1639300286769867\n",
      "Batch 136, Loss: -0.9486213326454163\n",
      "Batch 137, Loss: -1.0110825300216675\n",
      "Batch 138, Loss: -1.7656577825546265\n",
      "Batch 139, Loss: -1.7726163864135742\n",
      "Batch 140, Loss: -0.6534161567687988\n",
      "Batch 141, Loss: 0.22972208261489868\n",
      "Batch 142, Loss: -1.4586081504821777\n",
      "Batch 143, Loss: 0.1274614930152893\n",
      "Batch 144, Loss: 0.7518723011016846\n",
      "Batch 145, Loss: -1.5251264572143555\n",
      "Batch 146, Loss: 0.4044981300830841\n",
      "Batch 147, Loss: 0.3571859300136566\n",
      "Batch 148, Loss: -0.827316164970398\n",
      "Batch 149, Loss: 0.5586360096931458\n",
      "Batch 150, Loss: -0.11049351096153259\n",
      "Batch 151, Loss: -0.2549671232700348\n",
      "Batch 152, Loss: -0.021790236234664917\n",
      "Batch 153, Loss: -0.2047027349472046\n",
      "Batch 154, Loss: -0.23222297430038452\n",
      "Batch 155, Loss: -0.6690261363983154\n",
      "Batch 156, Loss: -0.6337003707885742\n",
      "Batch 157, Loss: -1.5448713302612305\n",
      "Batch 158, Loss: -1.4466865062713623\n",
      "Batch 159, Loss: -0.6758083701133728\n",
      "Batch 160, Loss: 0.7439022064208984\n",
      "Batch 161, Loss: -0.3437570035457611\n",
      "Batch 162, Loss: -0.0697295069694519\n",
      "Batch 163, Loss: -1.1026835441589355\n",
      "Batch 164, Loss: -0.670005202293396\n",
      "Batch 165, Loss: -0.2566312849521637\n",
      "Batch 166, Loss: -1.484924554824829\n",
      "Batch 167, Loss: 0.7465110421180725\n",
      "Batch 168, Loss: -1.6593084335327148\n",
      "Batch 169, Loss: -1.0784833431243896\n",
      "Batch 170, Loss: -1.4353022575378418\n",
      "Batch 171, Loss: -0.9728155136108398\n",
      "Batch 172, Loss: 0.7623783946037292\n",
      "Batch 173, Loss: -0.0329994261264801\n",
      "Batch 174, Loss: 0.49247586727142334\n",
      "Batch 175, Loss: -0.17809852957725525\n",
      "Batch 176, Loss: -1.5463181734085083\n",
      "Batch 177, Loss: 0.6232696175575256\n",
      "Batch 178, Loss: -1.3086645603179932\n",
      "Batch 179, Loss: -0.8493174314498901\n",
      "Batch 180, Loss: -1.5819923877716064\n",
      "Batch 181, Loss: 0.6624492406845093\n",
      "Batch 182, Loss: 0.6371296644210815\n",
      "Batch 183, Loss: -0.0874921977519989\n",
      "Batch 184, Loss: -0.9003459215164185\n",
      "Batch 185, Loss: -1.6081591844558716\n",
      "Batch 186, Loss: -1.787263035774231\n",
      "Batch 187, Loss: 0.49794167280197144\n",
      "Batch 188, Loss: -0.7245348691940308\n",
      "Batch 189, Loss: 0.43817204236984253\n",
      "Batch 190, Loss: -0.6560709476470947\n",
      "Batch 191, Loss: -1.3287408351898193\n",
      "Batch 192, Loss: -1.7732511758804321\n",
      "Batch 193, Loss: 0.3765612244606018\n",
      "Batch 194, Loss: -1.1093792915344238\n",
      "Batch 195, Loss: -0.9422118663787842\n",
      "Batch 196, Loss: -0.18707507848739624\n",
      "Batch 197, Loss: -1.7949970960617065\n",
      "Batch 198, Loss: 0.9129058122634888\n",
      "Batch 199, Loss: -1.549171805381775\n",
      "Training [10%]\tLoss: -0.5131\n",
      "Batch 0, Loss: 0.3738570809364319\n",
      "Batch 1, Loss: -1.7372448444366455\n",
      "Batch 2, Loss: 0.4295414686203003\n",
      "Batch 3, Loss: -1.2984087467193604\n",
      "Batch 4, Loss: -0.9826242923736572\n",
      "Batch 5, Loss: 0.11357200145721436\n",
      "Batch 6, Loss: 0.7490586042404175\n",
      "Batch 7, Loss: -1.7072954177856445\n",
      "Batch 8, Loss: 0.956407368183136\n",
      "Batch 9, Loss: -1.8020460605621338\n",
      "Batch 10, Loss: -1.3563560247421265\n",
      "Batch 11, Loss: -1.7948452234268188\n",
      "Batch 12, Loss: -1.7495461702346802\n",
      "Batch 13, Loss: 0.5619809031486511\n",
      "Batch 14, Loss: 0.37311840057373047\n",
      "Batch 15, Loss: -1.7064167261123657\n",
      "Batch 16, Loss: 0.5082004070281982\n",
      "Batch 17, Loss: -1.0846682786941528\n",
      "Batch 18, Loss: 0.15831759572029114\n",
      "Batch 19, Loss: -1.8504745960235596\n",
      "Batch 20, Loss: -0.31950947642326355\n",
      "Batch 21, Loss: 0.3392829895019531\n",
      "Batch 22, Loss: 0.0750817060470581\n",
      "Batch 23, Loss: 0.5718680620193481\n",
      "Batch 24, Loss: -0.2608509957790375\n",
      "Batch 25, Loss: 0.20699205994606018\n",
      "Batch 26, Loss: -0.17337891459465027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 27, Loss: -1.3753647804260254\n",
      "Batch 28, Loss: -0.8260995745658875\n",
      "Batch 29, Loss: 0.8442463874816895\n",
      "Batch 30, Loss: 0.7105922698974609\n",
      "Batch 31, Loss: -1.4181594848632812\n",
      "Batch 32, Loss: -0.041972219944000244\n",
      "Batch 33, Loss: -0.17629936337471008\n",
      "Batch 34, Loss: 0.7450137734413147\n",
      "Batch 35, Loss: -0.8418177962303162\n",
      "Batch 36, Loss: -0.23969551920890808\n",
      "Batch 37, Loss: -0.9491525888442993\n",
      "Batch 38, Loss: 0.31058913469314575\n",
      "Batch 39, Loss: -0.8611063957214355\n",
      "Batch 40, Loss: -0.004779160022735596\n",
      "Batch 41, Loss: -1.282116413116455\n",
      "Batch 42, Loss: 0.505250096321106\n",
      "Batch 43, Loss: 0.25320515036582947\n",
      "Batch 44, Loss: 0.44062453508377075\n",
      "Batch 45, Loss: -1.393799901008606\n",
      "Batch 46, Loss: 0.19396178424358368\n",
      "Batch 47, Loss: -0.8991267681121826\n",
      "Batch 48, Loss: -1.7257373332977295\n",
      "Batch 49, Loss: -1.1958811283111572\n",
      "Batch 50, Loss: -0.8517293334007263\n",
      "Batch 51, Loss: -0.08286809921264648\n",
      "Batch 52, Loss: 0.7371542453765869\n",
      "Batch 53, Loss: 0.46092522144317627\n",
      "Batch 54, Loss: 0.09227719902992249\n",
      "Batch 55, Loss: 0.6889408230781555\n",
      "Batch 56, Loss: 0.649091362953186\n",
      "Batch 57, Loss: -1.372347354888916\n",
      "Batch 58, Loss: 0.38536307215690613\n",
      "Batch 59, Loss: 0.3310248851776123\n",
      "Batch 60, Loss: -1.500855803489685\n",
      "Batch 61, Loss: 0.06911495327949524\n",
      "Batch 62, Loss: 0.4194146394729614\n",
      "Batch 63, Loss: -0.1408376395702362\n",
      "Batch 64, Loss: 0.12051963806152344\n",
      "Batch 65, Loss: -0.0846203863620758\n",
      "Batch 66, Loss: 0.5740436315536499\n",
      "Batch 67, Loss: -0.829059362411499\n",
      "Batch 68, Loss: 0.3076755702495575\n",
      "Batch 69, Loss: 0.3324507176876068\n",
      "Batch 70, Loss: -1.5686184167861938\n",
      "Batch 71, Loss: 0.0872003436088562\n",
      "Batch 72, Loss: -1.111220359802246\n",
      "Batch 73, Loss: 0.08816611766815186\n",
      "Batch 74, Loss: -0.21583595871925354\n",
      "Batch 75, Loss: -1.5677597522735596\n",
      "Batch 76, Loss: -0.8305802345275879\n",
      "Batch 77, Loss: -1.1814192533493042\n",
      "Batch 78, Loss: 0.9478280544281006\n",
      "Batch 79, Loss: -0.9782276749610901\n",
      "Batch 80, Loss: -1.7822915315628052\n",
      "Batch 81, Loss: 0.0842025876045227\n",
      "Batch 82, Loss: 0.5745503306388855\n",
      "Batch 83, Loss: 0.23873195052146912\n",
      "Batch 84, Loss: -1.4890215396881104\n",
      "Batch 85, Loss: 0.037553369998931885\n",
      "Batch 86, Loss: 0.2413625568151474\n",
      "Batch 87, Loss: -1.420150637626648\n",
      "Batch 88, Loss: 0.2935495376586914\n",
      "Batch 89, Loss: -1.8261759281158447\n",
      "Batch 90, Loss: -1.6207586526870728\n",
      "Batch 91, Loss: -1.506833791732788\n",
      "Batch 92, Loss: -1.861222267150879\n",
      "Batch 93, Loss: 1.0278379917144775\n",
      "Batch 94, Loss: 0.8119392395019531\n",
      "Batch 95, Loss: -1.4401439428329468\n",
      "Batch 96, Loss: -1.1306571960449219\n",
      "Batch 97, Loss: -0.11050796508789062\n",
      "Batch 98, Loss: -1.741363763809204\n",
      "Batch 99, Loss: -1.1862324476242065\n",
      "Batch 100, Loss: -1.5724529027938843\n",
      "Batch 101, Loss: 0.5719904899597168\n",
      "Batch 102, Loss: -0.832970380783081\n",
      "Batch 103, Loss: 0.14944243431091309\n",
      "Batch 104, Loss: -0.9691472053527832\n",
      "Batch 105, Loss: -0.619208574295044\n",
      "Batch 106, Loss: 0.3925263583660126\n",
      "Batch 107, Loss: 0.8854086399078369\n",
      "Batch 108, Loss: -0.816071629524231\n",
      "Batch 109, Loss: -1.2470016479492188\n",
      "Batch 110, Loss: -0.6893289089202881\n",
      "Batch 111, Loss: -1.9754774570465088\n",
      "Batch 112, Loss: 0.48330163955688477\n",
      "Batch 113, Loss: 0.1977500170469284\n",
      "Batch 114, Loss: -0.8217970132827759\n",
      "Batch 115, Loss: -1.1038120985031128\n",
      "Batch 116, Loss: 1.136057734489441\n",
      "Batch 117, Loss: -1.2396756410598755\n",
      "Batch 118, Loss: -1.308485746383667\n",
      "Batch 119, Loss: -0.8349287509918213\n",
      "Batch 120, Loss: 0.20592227578163147\n",
      "Batch 121, Loss: -0.18020153045654297\n",
      "Batch 122, Loss: -1.47433340549469\n",
      "Batch 123, Loss: -1.7083163261413574\n",
      "Batch 124, Loss: 0.3124913275241852\n",
      "Batch 125, Loss: -1.7429289817810059\n",
      "Batch 126, Loss: -1.8132351636886597\n",
      "Batch 127, Loss: 0.9519480466842651\n",
      "Batch 128, Loss: -0.7324233651161194\n",
      "Batch 129, Loss: 0.21629442274570465\n",
      "Batch 130, Loss: -0.36644887924194336\n",
      "Batch 131, Loss: 0.6076045036315918\n",
      "Batch 132, Loss: -1.5003994703292847\n",
      "Batch 133, Loss: -0.1146535575389862\n",
      "Batch 134, Loss: 0.5456064939498901\n",
      "Batch 135, Loss: -1.3697459697723389\n",
      "Batch 136, Loss: 0.2702637016773224\n",
      "Batch 137, Loss: -0.8669482469558716\n",
      "Batch 138, Loss: -1.146702766418457\n",
      "Batch 139, Loss: -0.7600938081741333\n",
      "Batch 140, Loss: 0.12692123651504517\n",
      "Batch 141, Loss: 0.6466207504272461\n",
      "Batch 142, Loss: -1.5991727113723755\n",
      "Batch 143, Loss: -2.1424319744110107\n",
      "Batch 144, Loss: -1.3932387828826904\n",
      "Batch 145, Loss: -1.384628415107727\n",
      "Batch 146, Loss: -0.5981999635696411\n",
      "Batch 147, Loss: -1.427542805671692\n",
      "Batch 148, Loss: 0.020787298679351807\n",
      "Batch 149, Loss: -1.6634528636932373\n",
      "Batch 150, Loss: 0.6893832683563232\n",
      "Batch 151, Loss: -1.9290403127670288\n",
      "Batch 152, Loss: -1.7831465005874634\n",
      "Batch 153, Loss: -1.2809230089187622\n",
      "Batch 154, Loss: -0.8505377769470215\n",
      "Batch 155, Loss: -0.7267120480537415\n",
      "Batch 156, Loss: -1.9586220979690552\n",
      "Batch 157, Loss: -2.1586532592773438\n",
      "Batch 158, Loss: -0.18010661005973816\n",
      "Batch 159, Loss: 0.48288923501968384\n",
      "Batch 160, Loss: -0.0017293691635131836\n",
      "Batch 161, Loss: 0.2781962752342224\n",
      "Batch 162, Loss: -1.6066102981567383\n",
      "Batch 163, Loss: -1.917048692703247\n",
      "Batch 164, Loss: 0.5934560298919678\n",
      "Batch 165, Loss: -0.0908271074295044\n",
      "Batch 166, Loss: -1.1668566465377808\n",
      "Batch 167, Loss: -1.012890338897705\n",
      "Batch 168, Loss: -0.05053877830505371\n",
      "Batch 169, Loss: -1.3542206287384033\n",
      "Batch 170, Loss: -0.19507011771202087\n",
      "Batch 171, Loss: -1.7211768627166748\n",
      "Batch 172, Loss: 0.34040647745132446\n",
      "Batch 173, Loss: -1.039750337600708\n",
      "Batch 174, Loss: 0.7772223949432373\n",
      "Batch 175, Loss: -1.5950465202331543\n",
      "Batch 176, Loss: -1.3739162683486938\n",
      "Batch 177, Loss: -1.543715476989746\n",
      "Batch 178, Loss: 0.2143336683511734\n",
      "Batch 179, Loss: -1.690422773361206\n",
      "Batch 180, Loss: -1.5452325344085693\n",
      "Batch 181, Loss: -0.1118684709072113\n",
      "Batch 182, Loss: 0.4395434558391571\n",
      "Batch 183, Loss: -1.261164665222168\n",
      "Batch 184, Loss: -1.3610138893127441\n",
      "Batch 185, Loss: -1.6219576597213745\n",
      "Batch 186, Loss: 0.2867995500564575\n",
      "Batch 187, Loss: 0.744097888469696\n",
      "Batch 188, Loss: -0.9289008378982544\n",
      "Batch 189, Loss: 0.7585678100585938\n",
      "Batch 190, Loss: 0.20701533555984497\n",
      "Batch 191, Loss: -1.4446074962615967\n",
      "Batch 192, Loss: -1.637933611869812\n",
      "Batch 193, Loss: 0.7143276929855347\n",
      "Batch 194, Loss: 0.6866329908370972\n",
      "Batch 195, Loss: 0.29305627942085266\n",
      "Batch 196, Loss: 0.05475500226020813\n",
      "Batch 197, Loss: -1.834637999534607\n",
      "Batch 198, Loss: -1.7138981819152832\n",
      "Batch 199, Loss: 0.33511948585510254\n",
      "Training [20%]\tLoss: -0.5228\n",
      "Batch 0, Loss: -1.3436274528503418\n",
      "Batch 1, Loss: -0.7968166470527649\n",
      "Batch 2, Loss: -1.0705286264419556\n",
      "Batch 3, Loss: -1.7016592025756836\n",
      "Batch 4, Loss: -1.4702801704406738\n",
      "Batch 5, Loss: -0.7246365547180176\n",
      "Batch 6, Loss: 0.0468883216381073\n",
      "Batch 7, Loss: 0.029944628477096558\n",
      "Batch 8, Loss: -0.88571697473526\n",
      "Batch 9, Loss: -0.6699380278587341\n",
      "Batch 10, Loss: 0.5654013752937317\n",
      "Batch 11, Loss: -0.6654962301254272\n",
      "Batch 12, Loss: -0.8518200516700745\n",
      "Batch 13, Loss: -1.0244684219360352\n",
      "Batch 14, Loss: 0.24252523481845856\n",
      "Batch 15, Loss: -1.7714896202087402\n",
      "Batch 16, Loss: 0.2891201376914978\n",
      "Batch 17, Loss: 0.8913268446922302\n",
      "Batch 18, Loss: 0.8444316387176514\n",
      "Batch 19, Loss: -0.9133970737457275\n",
      "Batch 20, Loss: 0.6058287024497986\n",
      "Batch 21, Loss: -0.9611326456069946\n",
      "Batch 22, Loss: -1.9860000610351562\n",
      "Batch 23, Loss: 0.7617397308349609\n",
      "Batch 24, Loss: 0.8884962797164917\n",
      "Batch 25, Loss: -0.6934342384338379\n",
      "Batch 26, Loss: -0.05686378479003906\n",
      "Batch 27, Loss: -1.475955843925476\n",
      "Batch 28, Loss: 0.21619483828544617\n",
      "Batch 29, Loss: 0.7143933773040771\n",
      "Batch 30, Loss: -1.0572006702423096\n",
      "Batch 31, Loss: -0.16383665800094604\n",
      "Batch 32, Loss: 0.9385465383529663\n",
      "Batch 33, Loss: -1.7849338054656982\n",
      "Batch 34, Loss: -1.2390269041061401\n",
      "Batch 35, Loss: 0.6490925550460815\n",
      "Batch 36, Loss: 0.7309889793395996\n",
      "Batch 37, Loss: -0.3524092435836792\n",
      "Batch 38, Loss: -0.26685985922813416\n",
      "Batch 39, Loss: -0.06044921278953552\n",
      "Batch 40, Loss: -1.2593770027160645\n",
      "Batch 41, Loss: -1.9156984090805054\n",
      "Batch 42, Loss: -0.8101280927658081\n",
      "Batch 43, Loss: -1.7036179304122925\n",
      "Batch 44, Loss: -1.5943371057510376\n",
      "Batch 45, Loss: -1.46586275100708\n",
      "Batch 46, Loss: -1.4453928470611572\n",
      "Batch 47, Loss: 0.5701577067375183\n",
      "Batch 48, Loss: -1.1484706401824951\n",
      "Batch 49, Loss: -0.02663072943687439\n",
      "Batch 50, Loss: -0.9560655951499939\n",
      "Batch 51, Loss: 0.5293190479278564\n",
      "Batch 52, Loss: 0.03528013825416565\n",
      "Batch 53, Loss: -1.724255084991455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 54, Loss: -1.8353852033615112\n",
      "Batch 55, Loss: -0.3489784896373749\n",
      "Batch 56, Loss: 0.8295214772224426\n",
      "Batch 57, Loss: -0.2310340404510498\n",
      "Batch 58, Loss: -1.401787281036377\n",
      "Batch 59, Loss: -1.731539011001587\n",
      "Batch 60, Loss: 0.20562191307544708\n",
      "Batch 61, Loss: -1.9310014247894287\n",
      "Batch 62, Loss: 0.256043016910553\n",
      "Batch 63, Loss: 0.5417416095733643\n",
      "Batch 64, Loss: -1.1439805030822754\n",
      "Batch 65, Loss: -0.964878261089325\n",
      "Batch 66, Loss: -0.7807037830352783\n",
      "Batch 67, Loss: -2.0155575275421143\n",
      "Batch 68, Loss: 0.11636525392532349\n",
      "Batch 69, Loss: -1.8148305416107178\n",
      "Batch 70, Loss: -0.0877552330493927\n",
      "Batch 71, Loss: -0.8314628601074219\n",
      "Batch 72, Loss: 0.13363522291183472\n",
      "Batch 73, Loss: -1.428256869316101\n",
      "Batch 74, Loss: -1.0614047050476074\n",
      "Batch 75, Loss: 0.9854392409324646\n",
      "Batch 76, Loss: -1.2170524597167969\n",
      "Batch 77, Loss: -1.8751415014266968\n",
      "Batch 78, Loss: 0.14634490013122559\n",
      "Batch 79, Loss: -0.8087342977523804\n",
      "Batch 80, Loss: -1.2962584495544434\n",
      "Batch 81, Loss: -0.8815497756004333\n",
      "Batch 82, Loss: -1.270316243171692\n",
      "Batch 83, Loss: -1.4865635633468628\n",
      "Batch 84, Loss: -1.1446411609649658\n",
      "Batch 85, Loss: -1.7201902866363525\n",
      "Batch 86, Loss: 0.7911219000816345\n",
      "Batch 87, Loss: 0.5861961841583252\n",
      "Batch 88, Loss: -0.021012097597122192\n",
      "Batch 89, Loss: -1.6783583164215088\n",
      "Batch 90, Loss: 0.41197633743286133\n",
      "Batch 91, Loss: -2.0538525581359863\n",
      "Batch 92, Loss: -0.9009979963302612\n",
      "Batch 93, Loss: 0.6979639530181885\n",
      "Batch 94, Loss: -1.787546992301941\n",
      "Batch 95, Loss: -1.0271611213684082\n",
      "Batch 96, Loss: 0.6199837327003479\n",
      "Batch 97, Loss: 0.9636939764022827\n",
      "Batch 98, Loss: 0.3521219491958618\n",
      "Batch 99, Loss: -0.6630422472953796\n",
      "Batch 100, Loss: -1.2236883640289307\n",
      "Batch 101, Loss: -0.9732205867767334\n",
      "Batch 102, Loss: 0.8153403997421265\n",
      "Batch 103, Loss: 0.5843955278396606\n",
      "Batch 104, Loss: -1.7268445491790771\n",
      "Batch 105, Loss: 0.3741926848888397\n",
      "Batch 106, Loss: 0.08443832397460938\n",
      "Batch 107, Loss: 0.3782992660999298\n",
      "Batch 108, Loss: -1.2541171312332153\n",
      "Batch 109, Loss: 0.6602363586425781\n",
      "Batch 110, Loss: -2.0903737545013428\n",
      "Batch 111, Loss: -1.719515323638916\n",
      "Batch 112, Loss: -1.2854077816009521\n",
      "Batch 113, Loss: -1.4252188205718994\n",
      "Batch 114, Loss: 0.17670677602291107\n",
      "Batch 115, Loss: -1.720503568649292\n",
      "Batch 116, Loss: -1.8645743131637573\n",
      "Batch 117, Loss: 1.020548939704895\n",
      "Batch 118, Loss: -0.8793931007385254\n",
      "Batch 119, Loss: 0.6774654984474182\n",
      "Batch 120, Loss: 0.19896933436393738\n",
      "Batch 121, Loss: -1.0322500467300415\n",
      "Batch 122, Loss: -1.0076996088027954\n",
      "Batch 123, Loss: 0.9263683557510376\n",
      "Batch 124, Loss: -1.0106664896011353\n",
      "Batch 125, Loss: -1.1257846355438232\n",
      "Batch 126, Loss: 0.10600128769874573\n",
      "Batch 127, Loss: -1.473130702972412\n",
      "Batch 128, Loss: -1.5648467540740967\n",
      "Batch 129, Loss: 0.7934192419052124\n",
      "Batch 130, Loss: -1.472402572631836\n",
      "Batch 131, Loss: 0.26922714710235596\n",
      "Batch 132, Loss: 0.2459370642900467\n",
      "Batch 133, Loss: -1.6150758266448975\n",
      "Batch 134, Loss: -2.1693601608276367\n",
      "Batch 135, Loss: 0.1760353147983551\n",
      "Batch 136, Loss: 0.5761779546737671\n",
      "Batch 137, Loss: 0.7325145602226257\n",
      "Batch 138, Loss: -1.716673493385315\n",
      "Batch 139, Loss: -0.044143110513687134\n",
      "Batch 140, Loss: -1.1605244874954224\n",
      "Batch 141, Loss: 0.2731386423110962\n",
      "Batch 142, Loss: 0.5982067584991455\n",
      "Batch 143, Loss: -1.0906651020050049\n",
      "Batch 144, Loss: 0.29081082344055176\n",
      "Batch 145, Loss: 0.15177059173583984\n",
      "Batch 146, Loss: 0.3810424506664276\n",
      "Batch 147, Loss: -1.7864789962768555\n",
      "Batch 148, Loss: 0.9127049446105957\n",
      "Batch 149, Loss: 0.07020479440689087\n",
      "Batch 150, Loss: -1.6927502155303955\n",
      "Batch 151, Loss: -0.273911714553833\n",
      "Batch 152, Loss: 0.11934986710548401\n",
      "Batch 153, Loss: 0.3011434078216553\n",
      "Batch 154, Loss: 0.7046107053756714\n",
      "Batch 155, Loss: -0.08241629600524902\n",
      "Batch 156, Loss: -1.33260977268219\n",
      "Batch 157, Loss: 0.7722926735877991\n",
      "Batch 158, Loss: 0.4471258521080017\n",
      "Batch 159, Loss: -0.07479444146156311\n",
      "Batch 160, Loss: -1.883191466331482\n",
      "Batch 161, Loss: -0.945805013179779\n",
      "Batch 162, Loss: 0.016366422176361084\n",
      "Batch 163, Loss: 0.716655969619751\n",
      "Batch 164, Loss: -1.2658017873764038\n",
      "Batch 165, Loss: -0.797393798828125\n",
      "Batch 166, Loss: -0.771510899066925\n",
      "Batch 167, Loss: 0.5427958965301514\n",
      "Batch 168, Loss: -1.4141530990600586\n",
      "Batch 169, Loss: 0.29808300733566284\n",
      "Batch 170, Loss: -0.8722022771835327\n",
      "Batch 171, Loss: -1.0211377143859863\n",
      "Batch 172, Loss: 0.7585063576698303\n",
      "Batch 173, Loss: 0.9929125905036926\n",
      "Batch 174, Loss: 0.3436205983161926\n",
      "Batch 175, Loss: 0.054056793451309204\n",
      "Batch 176, Loss: -1.0979936122894287\n",
      "Batch 177, Loss: -1.545210599899292\n",
      "Batch 178, Loss: -1.112646222114563\n",
      "Batch 179, Loss: 0.034753650426864624\n",
      "Batch 180, Loss: -1.2626991271972656\n",
      "Batch 181, Loss: -1.303638219833374\n",
      "Batch 182, Loss: 0.1256985068321228\n",
      "Batch 183, Loss: 0.6005851030349731\n",
      "Batch 184, Loss: 0.6407761573791504\n",
      "Batch 185, Loss: 1.1629562377929688\n",
      "Batch 186, Loss: -0.030092239379882812\n",
      "Batch 187, Loss: -0.19976305961608887\n",
      "Batch 188, Loss: 0.7829351425170898\n",
      "Batch 189, Loss: -0.7315488457679749\n",
      "Batch 190, Loss: 0.460470050573349\n",
      "Batch 191, Loss: 0.547809362411499\n",
      "Batch 192, Loss: -1.6142702102661133\n",
      "Batch 193, Loss: -0.8891099691390991\n",
      "Batch 194, Loss: -1.5244944095611572\n",
      "Batch 195, Loss: -0.012777775526046753\n",
      "Batch 196, Loss: 0.24999117851257324\n",
      "Batch 197, Loss: -0.2699027359485626\n",
      "Batch 198, Loss: -0.10874557495117188\n",
      "Batch 199, Loss: -0.015183687210083008\n",
      "Training [30%]\tLoss: -0.4736\n",
      "Batch 0, Loss: -1.4859905242919922\n",
      "Batch 1, Loss: -1.3755662441253662\n",
      "Batch 2, Loss: -1.3077061176300049\n",
      "Batch 3, Loss: -0.06743428111076355\n",
      "Batch 4, Loss: 0.20915398001670837\n",
      "Batch 5, Loss: 0.47079914808273315\n",
      "Batch 6, Loss: -0.9840853214263916\n",
      "Batch 7, Loss: -1.0710508823394775\n",
      "Batch 8, Loss: 0.9428247213363647\n",
      "Batch 9, Loss: 0.14748220145702362\n",
      "Batch 10, Loss: 0.2459234595298767\n",
      "Batch 11, Loss: -0.9556500911712646\n",
      "Batch 12, Loss: -1.5420595407485962\n",
      "Batch 13, Loss: 0.02772459387779236\n",
      "Batch 14, Loss: 0.7742242217063904\n",
      "Batch 15, Loss: -1.5535764694213867\n",
      "Batch 16, Loss: 0.2995029091835022\n",
      "Batch 17, Loss: -1.9010696411132812\n",
      "Batch 18, Loss: -0.19537296891212463\n",
      "Batch 19, Loss: -0.8001601696014404\n",
      "Batch 20, Loss: 0.6702301502227783\n",
      "Batch 21, Loss: 0.28173941373825073\n",
      "Batch 22, Loss: -1.2610383033752441\n",
      "Batch 23, Loss: -1.7037665843963623\n",
      "Batch 24, Loss: -1.1122922897338867\n",
      "Batch 25, Loss: 0.09382367134094238\n",
      "Batch 26, Loss: -1.7245298624038696\n",
      "Batch 27, Loss: -0.0987115204334259\n",
      "Batch 28, Loss: 0.1515873223543167\n",
      "Batch 29, Loss: 0.6108019351959229\n",
      "Batch 30, Loss: -1.7843711376190186\n",
      "Batch 31, Loss: -1.1907790899276733\n",
      "Batch 32, Loss: -1.1496403217315674\n",
      "Batch 33, Loss: -1.698668122291565\n",
      "Batch 34, Loss: -1.880958914756775\n",
      "Batch 35, Loss: 0.15558050572872162\n",
      "Batch 36, Loss: -1.1637967824935913\n",
      "Batch 37, Loss: 0.4032702147960663\n",
      "Batch 38, Loss: 0.7100602388381958\n",
      "Batch 39, Loss: -1.6624281406402588\n",
      "Batch 40, Loss: -1.831018328666687\n",
      "Batch 41, Loss: -0.19657418131828308\n",
      "Batch 42, Loss: -1.816821575164795\n",
      "Batch 43, Loss: -1.6348025798797607\n",
      "Batch 44, Loss: 0.7199783325195312\n",
      "Batch 45, Loss: -0.7993040084838867\n",
      "Batch 46, Loss: 0.2800828516483307\n",
      "Batch 47, Loss: 0.5119377374649048\n",
      "Batch 48, Loss: -1.1192083358764648\n",
      "Batch 49, Loss: 0.9698748588562012\n",
      "Batch 50, Loss: 0.5481586456298828\n",
      "Batch 51, Loss: -1.8034634590148926\n",
      "Batch 52, Loss: -0.06993868947029114\n",
      "Batch 53, Loss: -0.9951955080032349\n",
      "Batch 54, Loss: -1.6187469959259033\n",
      "Batch 55, Loss: -1.278521180152893\n",
      "Batch 56, Loss: -1.768752932548523\n",
      "Batch 57, Loss: -1.7981832027435303\n",
      "Batch 58, Loss: 0.40989696979522705\n",
      "Batch 59, Loss: -1.825443983078003\n",
      "Batch 60, Loss: 0.9009749889373779\n",
      "Batch 61, Loss: -1.8226370811462402\n",
      "Batch 62, Loss: 0.800776481628418\n",
      "Batch 63, Loss: -1.8213131427764893\n",
      "Batch 64, Loss: 0.4845427870750427\n",
      "Batch 65, Loss: 0.7481992244720459\n",
      "Batch 66, Loss: 0.7297893762588501\n",
      "Batch 67, Loss: -1.8311553001403809\n",
      "Batch 68, Loss: -1.7151970863342285\n",
      "Batch 69, Loss: -1.6463724374771118\n",
      "Batch 70, Loss: -1.6698179244995117\n",
      "Batch 71, Loss: 0.3158995509147644\n",
      "Batch 72, Loss: -1.311067819595337\n",
      "Batch 73, Loss: 0.11465921998023987\n",
      "Batch 74, Loss: 0.06002527475357056\n",
      "Batch 75, Loss: -1.716159701347351\n",
      "Batch 76, Loss: -1.5932018756866455\n",
      "Batch 77, Loss: 0.10600918531417847\n",
      "Batch 78, Loss: -1.986374020576477\n",
      "Batch 79, Loss: 0.3462012708187103\n",
      "Batch 80, Loss: -1.4175809621810913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 81, Loss: -1.4794447422027588\n",
      "Batch 82, Loss: 0.2652294337749481\n",
      "Batch 83, Loss: 0.44308552145957947\n",
      "Batch 84, Loss: -1.0082870721817017\n",
      "Batch 85, Loss: -0.23818683624267578\n",
      "Batch 86, Loss: 0.5827975869178772\n",
      "Batch 87, Loss: -0.9172096252441406\n",
      "Batch 88, Loss: -1.5343999862670898\n",
      "Batch 89, Loss: -0.8796464204788208\n",
      "Batch 90, Loss: 0.7560418844223022\n",
      "Batch 91, Loss: 0.7535854578018188\n",
      "Batch 92, Loss: -1.310606837272644\n",
      "Batch 93, Loss: 1.1439883708953857\n",
      "Batch 94, Loss: -1.009643793106079\n",
      "Batch 95, Loss: -1.6795353889465332\n",
      "Batch 96, Loss: -1.336714744567871\n",
      "Batch 97, Loss: 0.5228626728057861\n",
      "Batch 98, Loss: 0.5595590472221375\n",
      "Batch 99, Loss: -1.5475386381149292\n",
      "Batch 100, Loss: -1.0366723537445068\n",
      "Batch 101, Loss: -1.3349239826202393\n",
      "Batch 102, Loss: 0.4576987326145172\n",
      "Batch 103, Loss: -0.6696393489837646\n",
      "Batch 104, Loss: -1.2967066764831543\n",
      "Batch 105, Loss: -1.354691505432129\n",
      "Batch 106, Loss: -0.08176225423812866\n",
      "Batch 107, Loss: 0.01366385817527771\n",
      "Batch 108, Loss: 0.49919116497039795\n",
      "Batch 109, Loss: 0.8928321599960327\n",
      "Batch 110, Loss: -1.4130496978759766\n",
      "Batch 111, Loss: -1.4445480108261108\n",
      "Batch 112, Loss: -1.815335750579834\n",
      "Batch 113, Loss: 0.568192720413208\n",
      "Batch 114, Loss: 0.5344303846359253\n",
      "Batch 115, Loss: -0.05577173829078674\n",
      "Batch 116, Loss: -1.007606029510498\n",
      "Batch 117, Loss: -1.7550508975982666\n",
      "Batch 118, Loss: 0.8048000335693359\n",
      "Batch 119, Loss: 0.4768108129501343\n",
      "Batch 120, Loss: -0.005702614784240723\n",
      "Batch 121, Loss: -0.8768374919891357\n",
      "Batch 122, Loss: 0.7099654674530029\n",
      "Batch 123, Loss: -1.0024683475494385\n",
      "Batch 124, Loss: 0.9657031297683716\n",
      "Batch 125, Loss: -2.1673269271850586\n",
      "Batch 126, Loss: 0.8078017830848694\n",
      "Batch 127, Loss: -1.795954942703247\n",
      "Batch 128, Loss: -1.142784595489502\n",
      "Batch 129, Loss: -1.1016780138015747\n",
      "Batch 130, Loss: 0.16129717230796814\n",
      "Batch 131, Loss: -0.7000322341918945\n",
      "Batch 132, Loss: -1.7833473682403564\n",
      "Batch 133, Loss: -1.786451816558838\n",
      "Batch 134, Loss: -1.4930106401443481\n",
      "Batch 135, Loss: 0.06444898247718811\n",
      "Batch 136, Loss: -1.5672588348388672\n",
      "Batch 137, Loss: -1.6632949113845825\n",
      "Batch 138, Loss: 0.36633750796318054\n",
      "Batch 139, Loss: -0.09180670976638794\n",
      "Batch 140, Loss: 0.744672954082489\n",
      "Batch 141, Loss: 0.8987191915512085\n",
      "Batch 142, Loss: 0.5247307419776917\n",
      "Batch 143, Loss: -0.017781436443328857\n",
      "Batch 144, Loss: -0.16696080565452576\n",
      "Batch 145, Loss: 0.2157576084136963\n",
      "Batch 146, Loss: 0.5387478470802307\n",
      "Batch 147, Loss: -1.1915616989135742\n",
      "Batch 148, Loss: -1.7612558603286743\n",
      "Batch 149, Loss: -0.9930492043495178\n",
      "Batch 150, Loss: -1.083463191986084\n",
      "Batch 151, Loss: 0.8161481022834778\n",
      "Batch 152, Loss: -1.1128660440444946\n",
      "Batch 153, Loss: 0.644775927066803\n",
      "Batch 154, Loss: 0.922924280166626\n",
      "Batch 155, Loss: -0.21958360075950623\n",
      "Batch 156, Loss: 0.48105376958847046\n",
      "Batch 157, Loss: -1.7612640857696533\n",
      "Batch 158, Loss: 0.5814396142959595\n",
      "Batch 159, Loss: 0.4376784563064575\n",
      "Batch 160, Loss: 0.28873544931411743\n",
      "Batch 161, Loss: -0.20989593863487244\n",
      "Batch 162, Loss: -1.4178661108016968\n",
      "Batch 163, Loss: 0.023496568202972412\n",
      "Batch 164, Loss: -1.612021565437317\n",
      "Batch 165, Loss: 0.14031651616096497\n",
      "Batch 166, Loss: -2.1756865978240967\n",
      "Batch 167, Loss: 0.5583933591842651\n",
      "Batch 168, Loss: -1.5463142395019531\n",
      "Batch 169, Loss: 0.5150049924850464\n",
      "Batch 170, Loss: 0.5060743093490601\n",
      "Batch 171, Loss: -1.3901801109313965\n",
      "Batch 172, Loss: -1.5028494596481323\n",
      "Batch 173, Loss: 0.34458667039871216\n",
      "Batch 174, Loss: 0.47639936208724976\n",
      "Batch 175, Loss: -1.2384673357009888\n",
      "Batch 176, Loss: -1.5638095140457153\n",
      "Batch 177, Loss: 0.06787315011024475\n",
      "Batch 178, Loss: -1.5453846454620361\n",
      "Batch 179, Loss: -1.6300841569900513\n",
      "Batch 180, Loss: -1.9984416961669922\n",
      "Batch 181, Loss: -0.8496798276901245\n",
      "Batch 182, Loss: 0.30086129903793335\n",
      "Batch 183, Loss: 0.7793326377868652\n",
      "Batch 184, Loss: -1.8472893238067627\n",
      "Batch 185, Loss: 0.7762250900268555\n",
      "Batch 186, Loss: 0.8140653371810913\n",
      "Batch 187, Loss: -1.6214196681976318\n",
      "Batch 188, Loss: 0.5536088943481445\n",
      "Batch 189, Loss: 0.28216516971588135\n",
      "Batch 190, Loss: -2.014284372329712\n",
      "Batch 191, Loss: 0.692579984664917\n",
      "Batch 192, Loss: -0.7810256481170654\n",
      "Batch 193, Loss: 0.17799411714076996\n",
      "Batch 194, Loss: -0.10571593046188354\n",
      "Batch 195, Loss: -0.9920485019683838\n",
      "Batch 196, Loss: -0.008241802453994751\n",
      "Batch 197, Loss: 0.9206026196479797\n",
      "Batch 198, Loss: -0.8705335855484009\n",
      "Batch 199, Loss: 0.6879984736442566\n",
      "Training [40%]\tLoss: -0.5153\n",
      "Batch 0, Loss: -1.7673134803771973\n",
      "Batch 1, Loss: 1.0773341655731201\n",
      "Batch 2, Loss: 1.1692707538604736\n",
      "Batch 3, Loss: 0.08011603355407715\n",
      "Batch 4, Loss: -1.270804524421692\n",
      "Batch 5, Loss: -1.037093162536621\n",
      "Batch 6, Loss: -2.0206961631774902\n",
      "Batch 7, Loss: -1.111448049545288\n",
      "Batch 8, Loss: -1.539260983467102\n",
      "Batch 9, Loss: -1.1676435470581055\n",
      "Batch 10, Loss: 0.6356450319290161\n",
      "Batch 11, Loss: -0.7077299356460571\n",
      "Batch 12, Loss: -1.020523190498352\n",
      "Batch 13, Loss: -1.7793817520141602\n",
      "Batch 14, Loss: 0.2104695588350296\n",
      "Batch 15, Loss: 0.11428460478782654\n",
      "Batch 16, Loss: 0.4419326186180115\n",
      "Batch 17, Loss: 0.706317126750946\n",
      "Batch 18, Loss: 0.7875430583953857\n",
      "Batch 19, Loss: 0.21425172686576843\n",
      "Batch 20, Loss: 0.708846926689148\n",
      "Batch 21, Loss: 0.5495253801345825\n",
      "Batch 22, Loss: -2.1411025524139404\n",
      "Batch 23, Loss: -1.299824833869934\n",
      "Batch 24, Loss: 0.3434104323387146\n",
      "Batch 25, Loss: 0.6498512029647827\n",
      "Batch 26, Loss: -0.16930070519447327\n",
      "Batch 27, Loss: 0.3948332369327545\n",
      "Batch 28, Loss: -1.1977423429489136\n",
      "Batch 29, Loss: -1.065985083580017\n",
      "Batch 30, Loss: -0.8132245540618896\n",
      "Batch 31, Loss: -1.9346461296081543\n",
      "Batch 32, Loss: -1.6306984424591064\n",
      "Batch 33, Loss: -0.30277830362319946\n",
      "Batch 34, Loss: -1.1179194450378418\n",
      "Batch 35, Loss: -1.2047882080078125\n",
      "Batch 36, Loss: -1.765629529953003\n",
      "Batch 37, Loss: -1.8279377222061157\n",
      "Batch 38, Loss: -0.8208194971084595\n",
      "Batch 39, Loss: 0.7160476446151733\n",
      "Batch 40, Loss: -1.264093041419983\n",
      "Batch 41, Loss: 0.8553594350814819\n",
      "Batch 42, Loss: 0.09021514654159546\n",
      "Batch 43, Loss: -0.7975693941116333\n",
      "Batch 44, Loss: 0.8080042600631714\n",
      "Batch 45, Loss: -1.0086828470230103\n",
      "Batch 46, Loss: -1.329891324043274\n",
      "Batch 47, Loss: 0.8055014610290527\n",
      "Batch 48, Loss: 0.46974748373031616\n",
      "Batch 49, Loss: -0.22678795456886292\n",
      "Batch 50, Loss: -1.0700933933258057\n",
      "Batch 51, Loss: -1.7488641738891602\n",
      "Batch 52, Loss: 1.1394357681274414\n",
      "Batch 53, Loss: 0.24931001663208008\n",
      "Batch 54, Loss: 0.6966467499732971\n",
      "Batch 55, Loss: 0.5554792284965515\n",
      "Batch 56, Loss: -2.006838083267212\n",
      "Batch 57, Loss: -0.07138270139694214\n",
      "Batch 58, Loss: 0.5612800121307373\n",
      "Batch 59, Loss: -1.7020504474639893\n",
      "Batch 60, Loss: -1.7335598468780518\n",
      "Batch 61, Loss: -1.3159985542297363\n",
      "Batch 62, Loss: 0.5330344438552856\n",
      "Batch 63, Loss: -0.1230640709400177\n",
      "Batch 64, Loss: -1.070967435836792\n",
      "Batch 65, Loss: 0.1564449816942215\n",
      "Batch 66, Loss: 0.10371717810630798\n",
      "Batch 67, Loss: -0.7866190671920776\n",
      "Batch 68, Loss: 0.09239557385444641\n",
      "Batch 69, Loss: 0.8847111463546753\n",
      "Batch 70, Loss: 0.41403451561927795\n",
      "Batch 71, Loss: -1.8826899528503418\n",
      "Batch 72, Loss: -1.7977519035339355\n",
      "Batch 73, Loss: -1.076089859008789\n",
      "Batch 74, Loss: -1.5494658946990967\n",
      "Batch 75, Loss: -1.5526124238967896\n",
      "Batch 76, Loss: 0.1883341670036316\n",
      "Batch 77, Loss: 0.1849600076675415\n",
      "Batch 78, Loss: -1.4138667583465576\n",
      "Batch 79, Loss: -1.5376923084259033\n",
      "Batch 80, Loss: -1.6357033252716064\n",
      "Batch 81, Loss: -0.16662150621414185\n",
      "Batch 82, Loss: -0.8418391942977905\n",
      "Batch 83, Loss: -0.19789689779281616\n",
      "Batch 84, Loss: 1.008983850479126\n",
      "Batch 85, Loss: 0.8907063007354736\n",
      "Batch 86, Loss: 0.8573615550994873\n",
      "Batch 87, Loss: 0.3060338497161865\n",
      "Batch 88, Loss: -0.8066186904907227\n",
      "Batch 89, Loss: -1.6773433685302734\n",
      "Batch 90, Loss: 0.8708203434944153\n",
      "Batch 91, Loss: -0.19919148087501526\n",
      "Batch 92, Loss: -1.786022663116455\n",
      "Batch 93, Loss: -1.4834301471710205\n",
      "Batch 94, Loss: -1.0801528692245483\n",
      "Batch 95, Loss: -1.6664460897445679\n",
      "Batch 96, Loss: -1.4267845153808594\n",
      "Batch 97, Loss: -0.042275428771972656\n",
      "Batch 98, Loss: 0.42043566703796387\n",
      "Batch 99, Loss: 0.8836462497711182\n",
      "Batch 100, Loss: -1.3775981664657593\n",
      "Batch 101, Loss: -0.3062903583049774\n",
      "Batch 102, Loss: -0.1373407542705536\n",
      "Batch 103, Loss: -0.7829065918922424\n",
      "Batch 104, Loss: -1.1823170185089111\n",
      "Batch 105, Loss: -0.8133569955825806\n",
      "Batch 106, Loss: 0.4373748004436493\n",
      "Batch 107, Loss: -1.3679611682891846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 108, Loss: 0.003951221704483032\n",
      "Batch 109, Loss: -1.5870296955108643\n",
      "Batch 110, Loss: 0.459808349609375\n",
      "Batch 111, Loss: -0.23414325714111328\n",
      "Batch 112, Loss: -1.534226894378662\n",
      "Batch 113, Loss: -1.5063676834106445\n",
      "Batch 114, Loss: -1.7980636358261108\n",
      "Batch 115, Loss: 1.042977213859558\n",
      "Batch 116, Loss: -1.9625848531723022\n",
      "Batch 117, Loss: 0.8671990633010864\n",
      "Batch 118, Loss: 0.7376227378845215\n",
      "Batch 119, Loss: -1.1883485317230225\n",
      "Batch 120, Loss: -1.8976057767868042\n",
      "Batch 121, Loss: -0.9463092088699341\n",
      "Batch 122, Loss: 0.9047070741653442\n",
      "Batch 123, Loss: 0.0563390851020813\n",
      "Batch 124, Loss: -0.2159554660320282\n",
      "Batch 125, Loss: -1.7182848453521729\n",
      "Batch 126, Loss: 0.464849054813385\n",
      "Batch 127, Loss: 0.3501140773296356\n",
      "Batch 128, Loss: 0.5913000106811523\n",
      "Batch 129, Loss: 0.060279011726379395\n",
      "Batch 130, Loss: -1.7970938682556152\n",
      "Batch 131, Loss: 0.7414095997810364\n",
      "Batch 132, Loss: -2.1526741981506348\n",
      "Batch 133, Loss: -2.0984320640563965\n",
      "Batch 134, Loss: 0.7962042093276978\n",
      "Batch 135, Loss: -1.5842409133911133\n",
      "Batch 136, Loss: 0.05734008550643921\n",
      "Batch 137, Loss: -1.5337311029434204\n",
      "Batch 138, Loss: -0.27295389771461487\n",
      "Batch 139, Loss: -0.21288666129112244\n",
      "Batch 140, Loss: -0.7892854809761047\n",
      "Batch 141, Loss: -1.3979185819625854\n",
      "Batch 142, Loss: 0.5755310654640198\n",
      "Batch 143, Loss: -1.7620576620101929\n",
      "Batch 144, Loss: -1.7875792980194092\n",
      "Batch 145, Loss: -1.4597374200820923\n",
      "Batch 146, Loss: -1.7222193479537964\n",
      "Batch 147, Loss: 0.08095508813858032\n",
      "Batch 148, Loss: -2.168635129928589\n",
      "Batch 149, Loss: 0.9384031295776367\n",
      "Batch 150, Loss: -0.26057127118110657\n",
      "Batch 151, Loss: -1.972047209739685\n",
      "Batch 152, Loss: -0.2007080316543579\n",
      "Batch 153, Loss: -0.7643253803253174\n",
      "Batch 154, Loss: -0.963558554649353\n",
      "Batch 155, Loss: -1.7426799535751343\n",
      "Batch 156, Loss: -1.7698724269866943\n",
      "Batch 157, Loss: 1.035812258720398\n",
      "Batch 158, Loss: 0.6228044033050537\n",
      "Batch 159, Loss: -1.2537014484405518\n",
      "Batch 160, Loss: 0.19863462448120117\n",
      "Batch 161, Loss: 0.5451341867446899\n",
      "Batch 162, Loss: -1.8155608177185059\n",
      "Batch 163, Loss: -2.0659339427948\n",
      "Batch 164, Loss: -0.22480249404907227\n",
      "Batch 165, Loss: -0.33688294887542725\n",
      "Batch 166, Loss: -1.8077614307403564\n",
      "Batch 167, Loss: 0.644845187664032\n",
      "Batch 168, Loss: 0.7138757705688477\n",
      "Batch 169, Loss: 0.25292667746543884\n",
      "Batch 170, Loss: -1.9866631031036377\n",
      "Batch 171, Loss: -0.8973027467727661\n",
      "Batch 172, Loss: -0.02002537250518799\n",
      "Batch 173, Loss: -1.1079425811767578\n",
      "Batch 174, Loss: 0.9532802104949951\n",
      "Batch 175, Loss: -0.27492791414260864\n",
      "Batch 176, Loss: -1.2156060934066772\n",
      "Batch 177, Loss: -1.9432774782180786\n",
      "Batch 178, Loss: -1.237962007522583\n",
      "Batch 179, Loss: -0.9924441576004028\n",
      "Batch 180, Loss: -1.173295259475708\n",
      "Batch 181, Loss: -0.30276185274124146\n",
      "Batch 182, Loss: 0.9877915978431702\n",
      "Batch 183, Loss: 0.28849366307258606\n",
      "Batch 184, Loss: -0.8925133943557739\n",
      "Batch 185, Loss: -1.091622233390808\n",
      "Batch 186, Loss: 0.05827385187149048\n",
      "Batch 187, Loss: 0.7350215911865234\n",
      "Batch 188, Loss: -0.3460083305835724\n",
      "Batch 189, Loss: 0.12259691953659058\n",
      "Batch 190, Loss: 0.5618810057640076\n",
      "Batch 191, Loss: -1.2441179752349854\n",
      "Batch 192, Loss: 0.17940716445446014\n",
      "Batch 193, Loss: -1.2010263204574585\n",
      "Batch 194, Loss: -1.9538244009017944\n",
      "Batch 195, Loss: 0.3932201564311981\n",
      "Batch 196, Loss: -2.0542750358581543\n",
      "Batch 197, Loss: 0.19990423321723938\n",
      "Batch 198, Loss: -1.5395911931991577\n",
      "Batch 199, Loss: 0.37786564230918884\n",
      "Training [50%]\tLoss: -0.5394\n",
      "Batch 0, Loss: -0.23801541328430176\n",
      "Batch 1, Loss: 0.29498547315597534\n",
      "Batch 2, Loss: -1.3208764791488647\n",
      "Batch 3, Loss: 0.007190018892288208\n",
      "Batch 4, Loss: -1.022352933883667\n",
      "Batch 5, Loss: 0.5678409337997437\n",
      "Batch 6, Loss: 1.1641062498092651\n",
      "Batch 7, Loss: -1.1525262594223022\n",
      "Batch 8, Loss: 0.128340482711792\n",
      "Batch 9, Loss: -1.2242772579193115\n",
      "Batch 10, Loss: -1.8444552421569824\n",
      "Batch 11, Loss: 0.4224622845649719\n",
      "Batch 12, Loss: -0.08710461854934692\n",
      "Batch 13, Loss: -0.37205109000205994\n",
      "Batch 14, Loss: 0.7519457340240479\n",
      "Batch 15, Loss: -0.7275413870811462\n",
      "Batch 16, Loss: 0.48237037658691406\n",
      "Batch 17, Loss: 0.5431128144264221\n",
      "Batch 18, Loss: -1.583858847618103\n",
      "Batch 19, Loss: 0.26668423414230347\n",
      "Batch 20, Loss: -1.127685546875\n",
      "Batch 21, Loss: 0.6796451807022095\n",
      "Batch 22, Loss: -1.4427790641784668\n",
      "Batch 23, Loss: -1.437155842781067\n",
      "Batch 24, Loss: 1.0560953617095947\n",
      "Batch 25, Loss: -0.8461118936538696\n",
      "Batch 26, Loss: 0.23986762762069702\n",
      "Batch 27, Loss: 0.7759705781936646\n",
      "Batch 28, Loss: -2.1638317108154297\n",
      "Batch 29, Loss: -0.7489018440246582\n",
      "Batch 30, Loss: -0.7550373077392578\n",
      "Batch 31, Loss: -0.11486369371414185\n",
      "Batch 32, Loss: 0.7923669815063477\n",
      "Batch 33, Loss: -2.073160409927368\n",
      "Batch 34, Loss: -0.09533581137657166\n",
      "Batch 35, Loss: -1.7942959070205688\n",
      "Batch 36, Loss: 0.7245670557022095\n",
      "Batch 37, Loss: -1.225987434387207\n",
      "Batch 38, Loss: -0.08362489938735962\n",
      "Batch 39, Loss: -1.016147255897522\n",
      "Batch 40, Loss: 0.26968997716903687\n",
      "Batch 41, Loss: -1.1618671417236328\n",
      "Batch 42, Loss: 0.1464046835899353\n",
      "Batch 43, Loss: 1.081613302230835\n",
      "Batch 44, Loss: 0.7104847431182861\n",
      "Batch 45, Loss: -1.1984732151031494\n",
      "Batch 46, Loss: -1.2572600841522217\n",
      "Batch 47, Loss: 0.9999990463256836\n",
      "Batch 48, Loss: -1.193518877029419\n",
      "Batch 49, Loss: 0.5980604290962219\n",
      "Batch 50, Loss: -1.4787276983261108\n",
      "Batch 51, Loss: -1.7956843376159668\n",
      "Batch 52, Loss: -0.8254835605621338\n",
      "Batch 53, Loss: -0.2889028489589691\n",
      "Batch 54, Loss: -1.1817325353622437\n",
      "Batch 55, Loss: 0.17647616565227509\n",
      "Batch 56, Loss: 0.1761084944009781\n",
      "Batch 57, Loss: 0.019827961921691895\n",
      "Batch 58, Loss: 0.4576311707496643\n",
      "Batch 59, Loss: -1.5264933109283447\n",
      "Batch 60, Loss: -0.14613211154937744\n",
      "Batch 61, Loss: 0.8295208215713501\n",
      "Batch 62, Loss: -0.7618666291236877\n",
      "Batch 63, Loss: 0.5354106426239014\n",
      "Batch 64, Loss: -0.7221810817718506\n",
      "Batch 65, Loss: -1.8244693279266357\n",
      "Batch 66, Loss: 0.09926199913024902\n",
      "Batch 67, Loss: -1.8237568140029907\n",
      "Batch 68, Loss: -0.7190817594528198\n",
      "Batch 69, Loss: -1.9280833005905151\n",
      "Batch 70, Loss: -1.601711630821228\n",
      "Batch 71, Loss: 0.5057048797607422\n",
      "Batch 72, Loss: -1.17786705493927\n",
      "Batch 73, Loss: 0.12667438387870789\n",
      "Batch 74, Loss: -1.3246830701828003\n",
      "Batch 75, Loss: -1.1000597476959229\n",
      "Batch 76, Loss: -0.12907597422599792\n",
      "Batch 77, Loss: -1.0687246322631836\n",
      "Batch 78, Loss: -1.4177892208099365\n",
      "Batch 79, Loss: -0.20746135711669922\n",
      "Batch 80, Loss: 0.2187785804271698\n",
      "Batch 81, Loss: -1.0942473411560059\n",
      "Batch 82, Loss: -1.625394582748413\n",
      "Batch 83, Loss: 0.35720884799957275\n",
      "Batch 84, Loss: -1.51823091506958\n",
      "Batch 85, Loss: 0.2958366870880127\n",
      "Batch 86, Loss: 0.16592292487621307\n",
      "Batch 87, Loss: 0.37570956349372864\n",
      "Batch 88, Loss: 0.5421432852745056\n",
      "Batch 89, Loss: -0.8148201704025269\n",
      "Batch 90, Loss: -0.7654051780700684\n",
      "Batch 91, Loss: -1.8343595266342163\n",
      "Batch 92, Loss: 0.12982970476150513\n",
      "Batch 93, Loss: 0.49923259019851685\n",
      "Batch 94, Loss: -1.4463884830474854\n",
      "Batch 95, Loss: 1.1011098623275757\n",
      "Batch 96, Loss: 0.058293819427490234\n",
      "Batch 97, Loss: -1.8330421447753906\n",
      "Batch 98, Loss: -1.2446682453155518\n",
      "Batch 99, Loss: -1.0137780904769897\n",
      "Batch 100, Loss: -1.832573413848877\n",
      "Batch 101, Loss: -1.5010781288146973\n",
      "Batch 102, Loss: 0.26515457034111023\n",
      "Batch 103, Loss: -1.1518350839614868\n",
      "Batch 104, Loss: -1.5027239322662354\n",
      "Batch 105, Loss: -1.3621201515197754\n",
      "Batch 106, Loss: 0.8525716662406921\n",
      "Batch 107, Loss: 0.9037266373634338\n",
      "Batch 108, Loss: 0.8419828414916992\n",
      "Batch 109, Loss: -1.5844061374664307\n",
      "Batch 110, Loss: -1.09466552734375\n",
      "Batch 111, Loss: 0.2449328750371933\n",
      "Batch 112, Loss: -1.5697166919708252\n",
      "Batch 113, Loss: 0.38357019424438477\n",
      "Batch 114, Loss: -1.1960688829421997\n",
      "Batch 115, Loss: 0.22748322784900665\n",
      "Batch 116, Loss: -0.26847201585769653\n",
      "Batch 117, Loss: 0.5317578911781311\n",
      "Batch 118, Loss: 0.7424551248550415\n",
      "Batch 119, Loss: 0.04648023843765259\n",
      "Batch 120, Loss: 0.43908217549324036\n",
      "Batch 121, Loss: 0.12094098329544067\n",
      "Batch 122, Loss: -2.054330348968506\n",
      "Batch 123, Loss: -1.0427343845367432\n",
      "Batch 124, Loss: 0.5324740409851074\n",
      "Batch 125, Loss: -1.1309080123901367\n",
      "Batch 126, Loss: -0.22165587544441223\n",
      "Batch 127, Loss: 0.5297114849090576\n",
      "Batch 128, Loss: -1.1685889959335327\n",
      "Batch 129, Loss: -1.0200235843658447\n",
      "Batch 130, Loss: -0.2877851128578186\n",
      "Batch 131, Loss: -0.9497435092926025\n",
      "Batch 132, Loss: -2.07968807220459\n",
      "Batch 133, Loss: 0.45132240653038025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 134, Loss: -0.7397053241729736\n",
      "Batch 135, Loss: -2.0785720348358154\n",
      "Batch 136, Loss: 0.15966098010540009\n",
      "Batch 137, Loss: -1.7426812648773193\n",
      "Batch 138, Loss: -1.8408770561218262\n",
      "Batch 139, Loss: -1.9866220951080322\n",
      "Batch 140, Loss: -1.2666515111923218\n",
      "Batch 141, Loss: -1.5469986200332642\n",
      "Batch 142, Loss: 0.8262220025062561\n",
      "Batch 143, Loss: -1.6971226930618286\n",
      "Batch 144, Loss: -1.7882331609725952\n",
      "Batch 145, Loss: -1.7380577325820923\n",
      "Batch 146, Loss: -1.786595106124878\n",
      "Batch 147, Loss: -0.24372515082359314\n",
      "Batch 148, Loss: -0.2918112874031067\n",
      "Batch 149, Loss: 0.19613435864448547\n",
      "Batch 150, Loss: -1.7360702753067017\n",
      "Batch 151, Loss: -1.7813435792922974\n",
      "Batch 152, Loss: -1.5262526273727417\n",
      "Batch 153, Loss: 0.1468021124601364\n",
      "Batch 154, Loss: 0.5856844186782837\n",
      "Batch 155, Loss: -1.3031295537948608\n",
      "Batch 156, Loss: -1.1895493268966675\n",
      "Batch 157, Loss: 0.5072306394577026\n",
      "Batch 158, Loss: 1.065739631652832\n",
      "Batch 159, Loss: 0.924910306930542\n",
      "Batch 160, Loss: -1.7783203125\n",
      "Batch 161, Loss: 0.07817625999450684\n",
      "Batch 162, Loss: 0.19974999129772186\n",
      "Batch 163, Loss: -1.3622592687606812\n",
      "Batch 164, Loss: -0.8276528716087341\n",
      "Batch 165, Loss: 0.7689347863197327\n",
      "Batch 166, Loss: -0.7630127668380737\n",
      "Batch 167, Loss: 0.47951310873031616\n",
      "Batch 168, Loss: -1.6974976062774658\n",
      "Batch 169, Loss: -0.7731059193611145\n",
      "Batch 170, Loss: 0.4375353157520294\n",
      "Batch 171, Loss: -0.05910637974739075\n",
      "Batch 172, Loss: -1.8349158763885498\n",
      "Batch 173, Loss: -2.023679733276367\n",
      "Batch 174, Loss: -0.18125373125076294\n",
      "Batch 175, Loss: -1.0236841440200806\n",
      "Batch 176, Loss: 0.3393940031528473\n",
      "Batch 177, Loss: -1.383905053138733\n",
      "Batch 178, Loss: 0.8410714268684387\n",
      "Batch 179, Loss: -0.2601932883262634\n",
      "Batch 180, Loss: 0.5522677898406982\n",
      "Batch 181, Loss: -1.7721503973007202\n",
      "Batch 182, Loss: -1.3601223230361938\n",
      "Batch 183, Loss: 0.520072877407074\n",
      "Batch 184, Loss: -0.8950125575065613\n",
      "Batch 185, Loss: 0.8137215375900269\n",
      "Batch 186, Loss: -1.8467202186584473\n",
      "Batch 187, Loss: -0.9836304187774658\n",
      "Batch 188, Loss: -1.2784855365753174\n",
      "Batch 189, Loss: 0.6319921016693115\n",
      "Batch 190, Loss: -1.4202094078063965\n",
      "Batch 191, Loss: 0.360443651676178\n",
      "Batch 192, Loss: 1.0477856397628784\n",
      "Batch 193, Loss: -2.1289141178131104\n",
      "Batch 194, Loss: 0.16186662018299103\n",
      "Batch 195, Loss: 0.4395209848880768\n",
      "Batch 196, Loss: 1.153092861175537\n",
      "Batch 197, Loss: -1.5389368534088135\n",
      "Batch 198, Loss: 0.40861964225769043\n",
      "Batch 199, Loss: 0.181682288646698\n",
      "Training [60%]\tLoss: -0.5086\n",
      "Batch 0, Loss: -0.33854639530181885\n",
      "Batch 1, Loss: -1.319162368774414\n",
      "Batch 2, Loss: 0.5666124224662781\n",
      "Batch 3, Loss: 0.4996725618839264\n",
      "Batch 4, Loss: -1.0745189189910889\n",
      "Batch 5, Loss: -1.1478770971298218\n",
      "Batch 6, Loss: -1.4929938316345215\n",
      "Batch 7, Loss: 0.8381451368331909\n",
      "Batch 8, Loss: -1.8503391742706299\n",
      "Batch 9, Loss: -1.5486643314361572\n",
      "Batch 10, Loss: 0.6467046737670898\n",
      "Batch 11, Loss: 0.16737569868564606\n",
      "Batch 12, Loss: -1.487318754196167\n",
      "Batch 13, Loss: -1.3204985857009888\n",
      "Batch 14, Loss: 0.8020877838134766\n",
      "Batch 15, Loss: 1.1749954223632812\n",
      "Batch 16, Loss: 1.0821027755737305\n",
      "Batch 17, Loss: -1.7198045253753662\n",
      "Batch 18, Loss: 0.2976943850517273\n",
      "Batch 19, Loss: 0.505972146987915\n",
      "Batch 20, Loss: 0.17427851259708405\n",
      "Batch 21, Loss: -0.1823093593120575\n",
      "Batch 22, Loss: -1.5032850503921509\n",
      "Batch 23, Loss: -1.486475944519043\n",
      "Batch 24, Loss: 0.7290352582931519\n",
      "Batch 25, Loss: 0.5753879547119141\n",
      "Batch 26, Loss: 0.4210577607154846\n",
      "Batch 27, Loss: 0.8308519124984741\n",
      "Batch 28, Loss: -0.30456826090812683\n",
      "Batch 29, Loss: 0.6373010277748108\n",
      "Batch 30, Loss: 0.7691864967346191\n",
      "Batch 31, Loss: -0.21768027544021606\n",
      "Batch 32, Loss: -0.850436806678772\n",
      "Batch 33, Loss: -1.303849220275879\n",
      "Batch 34, Loss: -1.2772676944732666\n",
      "Batch 35, Loss: 0.06358969211578369\n",
      "Batch 36, Loss: -1.209937334060669\n",
      "Batch 37, Loss: -0.27613162994384766\n",
      "Batch 38, Loss: 0.011042296886444092\n",
      "Batch 39, Loss: 0.5013662576675415\n",
      "Batch 40, Loss: 0.4986250102519989\n",
      "Batch 41, Loss: -1.542161464691162\n",
      "Batch 42, Loss: -1.2860805988311768\n",
      "Batch 43, Loss: -1.1013561487197876\n",
      "Batch 44, Loss: 0.7502467632293701\n",
      "Batch 45, Loss: -0.8333234190940857\n",
      "Batch 46, Loss: 0.9119347333908081\n",
      "Batch 47, Loss: 0.12887388467788696\n",
      "Batch 48, Loss: -0.7004294395446777\n",
      "Batch 49, Loss: -1.82859468460083\n",
      "Batch 50, Loss: -1.1820662021636963\n",
      "Batch 51, Loss: 0.15160919725894928\n",
      "Batch 52, Loss: 0.4624226689338684\n",
      "Batch 53, Loss: 0.2909964323043823\n",
      "Batch 54, Loss: -1.1464686393737793\n",
      "Batch 55, Loss: 0.8387757539749146\n",
      "Batch 56, Loss: 0.6414711475372314\n",
      "Batch 57, Loss: -1.0994117259979248\n",
      "Batch 58, Loss: -0.1658875048160553\n",
      "Batch 59, Loss: -1.8449856042861938\n",
      "Batch 60, Loss: -1.0928282737731934\n",
      "Batch 61, Loss: 0.8634244203567505\n",
      "Batch 62, Loss: 0.7074052095413208\n",
      "Batch 63, Loss: -0.9680918455123901\n",
      "Batch 64, Loss: 0.4381052255630493\n",
      "Batch 65, Loss: -1.8308813571929932\n",
      "Batch 66, Loss: 0.575634241104126\n",
      "Batch 67, Loss: -0.7540463209152222\n",
      "Batch 68, Loss: -1.6936516761779785\n",
      "Batch 69, Loss: 0.8077420592308044\n",
      "Batch 70, Loss: -1.026938796043396\n",
      "Batch 71, Loss: -1.5680036544799805\n",
      "Batch 72, Loss: -1.5124081373214722\n",
      "Batch 73, Loss: -1.4348292350769043\n",
      "Batch 74, Loss: 0.6877726912498474\n",
      "Batch 75, Loss: -1.8310712575912476\n",
      "Batch 76, Loss: 0.11077731847763062\n",
      "Batch 77, Loss: 0.4828810691833496\n",
      "Batch 78, Loss: -0.20456555485725403\n",
      "Batch 79, Loss: -0.827004075050354\n",
      "Batch 80, Loss: -1.4564125537872314\n",
      "Batch 81, Loss: -1.892188549041748\n",
      "Batch 82, Loss: 0.31851017475128174\n",
      "Batch 83, Loss: -0.03666427731513977\n",
      "Batch 84, Loss: 0.24556635320186615\n",
      "Batch 85, Loss: -1.0352065563201904\n",
      "Batch 86, Loss: -2.1745779514312744\n",
      "Batch 87, Loss: -1.1904304027557373\n",
      "Batch 88, Loss: -1.9891583919525146\n",
      "Batch 89, Loss: -1.3786336183547974\n",
      "Batch 90, Loss: 0.4923960864543915\n",
      "Batch 91, Loss: -1.582378625869751\n",
      "Batch 92, Loss: -0.7574167847633362\n",
      "Batch 93, Loss: -1.0734221935272217\n",
      "Batch 94, Loss: -1.6674988269805908\n",
      "Batch 95, Loss: -1.2616474628448486\n",
      "Batch 96, Loss: -0.24694538116455078\n",
      "Batch 97, Loss: 0.1609562337398529\n",
      "Batch 98, Loss: 0.7014031410217285\n",
      "Batch 99, Loss: 0.4640204608440399\n",
      "Batch 100, Loss: -1.529870867729187\n",
      "Batch 101, Loss: 0.01991620659828186\n",
      "Batch 102, Loss: -2.0729827880859375\n",
      "Batch 103, Loss: 0.700149655342102\n",
      "Batch 104, Loss: -1.5400378704071045\n",
      "Batch 105, Loss: 0.22718508541584015\n",
      "Batch 106, Loss: 0.6263275742530823\n",
      "Batch 107, Loss: 0.5291948914527893\n",
      "Batch 108, Loss: -1.4513497352600098\n",
      "Batch 109, Loss: 1.0614362955093384\n",
      "Batch 110, Loss: -0.959136962890625\n",
      "Batch 111, Loss: -1.6279712915420532\n",
      "Batch 112, Loss: 0.5359280109405518\n",
      "Batch 113, Loss: -1.2609765529632568\n",
      "Batch 114, Loss: -1.0573205947875977\n",
      "Batch 115, Loss: -1.533660888671875\n",
      "Batch 116, Loss: -1.0702104568481445\n",
      "Batch 117, Loss: -1.4441553354263306\n",
      "Batch 118, Loss: -0.7429782152175903\n",
      "Batch 119, Loss: 0.38682088255882263\n",
      "Batch 120, Loss: -1.8379874229431152\n",
      "Batch 121, Loss: 0.29954278469085693\n",
      "Batch 122, Loss: 0.3638753294944763\n",
      "Batch 123, Loss: -0.8093267679214478\n",
      "Batch 124, Loss: 0.20076484978199005\n",
      "Batch 125, Loss: -1.8089385032653809\n",
      "Batch 126, Loss: -0.7721984386444092\n",
      "Batch 127, Loss: -1.7146377563476562\n",
      "Batch 128, Loss: 0.7271283864974976\n",
      "Batch 129, Loss: 0.3656975328922272\n",
      "Batch 130, Loss: -0.8413296937942505\n",
      "Batch 131, Loss: 0.031205475330352783\n",
      "Batch 132, Loss: 0.047841548919677734\n",
      "Batch 133, Loss: -1.3035435676574707\n",
      "Batch 134, Loss: 0.08939328789710999\n",
      "Batch 135, Loss: 0.5269495248794556\n",
      "Batch 136, Loss: -1.5357906818389893\n",
      "Batch 137, Loss: -1.1714709997177124\n",
      "Batch 138, Loss: -0.2216237187385559\n",
      "Batch 139, Loss: -1.5526342391967773\n",
      "Batch 140, Loss: 0.40403077006340027\n",
      "Batch 141, Loss: -0.9105099439620972\n",
      "Batch 142, Loss: -1.6759583950042725\n",
      "Batch 143, Loss: -1.6225435733795166\n",
      "Batch 144, Loss: -1.0472618341445923\n",
      "Batch 145, Loss: -0.23797768354415894\n",
      "Batch 146, Loss: -1.632515549659729\n",
      "Batch 147, Loss: 0.7002826929092407\n",
      "Batch 148, Loss: -1.4313024282455444\n",
      "Batch 149, Loss: 0.4122859835624695\n",
      "Batch 150, Loss: -0.8416219353675842\n",
      "Batch 151, Loss: 0.8226252794265747\n",
      "Batch 152, Loss: 0.10716834664344788\n",
      "Batch 153, Loss: -0.05251777172088623\n",
      "Batch 154, Loss: -0.16054007411003113\n",
      "Batch 155, Loss: -0.2548832893371582\n",
      "Batch 156, Loss: -0.10280638933181763\n",
      "Batch 157, Loss: -1.4651798009872437\n",
      "Batch 158, Loss: 0.7502493858337402\n",
      "Batch 159, Loss: 0.6739085912704468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 160, Loss: 0.4069010615348816\n",
      "Batch 161, Loss: -0.27722716331481934\n",
      "Batch 162, Loss: -1.0661990642547607\n",
      "Batch 163, Loss: 0.26811617612838745\n",
      "Batch 164, Loss: -1.5040807723999023\n",
      "Batch 165, Loss: -0.8839774131774902\n",
      "Batch 166, Loss: -0.0659366250038147\n",
      "Batch 167, Loss: -0.8512389659881592\n",
      "Batch 168, Loss: -0.7088226079940796\n",
      "Batch 169, Loss: 0.4137575924396515\n",
      "Batch 170, Loss: -1.3767422437667847\n",
      "Batch 171, Loss: -1.0439181327819824\n",
      "Batch 172, Loss: -0.6724684238433838\n",
      "Batch 173, Loss: 0.6326903700828552\n",
      "Batch 174, Loss: -1.519978642463684\n",
      "Batch 175, Loss: 0.23624086380004883\n",
      "Batch 176, Loss: 0.0035800039768218994\n",
      "Batch 177, Loss: -0.04050725698471069\n",
      "Batch 178, Loss: -0.30053335428237915\n",
      "Batch 179, Loss: -0.02552485466003418\n",
      "Batch 180, Loss: -0.20070794224739075\n",
      "Batch 181, Loss: 0.22805574536323547\n",
      "Batch 182, Loss: -1.7598936557769775\n",
      "Batch 183, Loss: 0.39306387305259705\n",
      "Batch 184, Loss: -1.0676363706588745\n",
      "Batch 185, Loss: -1.379789113998413\n",
      "Batch 186, Loss: -1.4122852087020874\n",
      "Batch 187, Loss: 0.2792377471923828\n",
      "Batch 188, Loss: -1.2222659587860107\n",
      "Batch 189, Loss: 0.08998993039131165\n",
      "Batch 190, Loss: -1.8180534839630127\n",
      "Batch 191, Loss: -1.1320164203643799\n",
      "Batch 192, Loss: -1.3648658990859985\n",
      "Batch 193, Loss: -1.4444267749786377\n",
      "Batch 194, Loss: -0.9399157762527466\n",
      "Batch 195, Loss: -1.2576839923858643\n",
      "Batch 196, Loss: -0.7684786319732666\n",
      "Batch 197, Loss: -1.8438899517059326\n",
      "Batch 198, Loss: 0.34272822737693787\n",
      "Batch 199, Loss: 0.5818281769752502\n",
      "Training [70%]\tLoss: -0.4943\n",
      "Batch 0, Loss: -0.24475300312042236\n",
      "Batch 1, Loss: 0.002962082624435425\n",
      "Batch 2, Loss: -1.8995815515518188\n",
      "Batch 3, Loss: -0.04631265997886658\n",
      "Batch 4, Loss: 0.05572080612182617\n",
      "Batch 5, Loss: -2.0801525115966797\n",
      "Batch 6, Loss: -1.241283655166626\n",
      "Batch 7, Loss: 0.7539435029029846\n",
      "Batch 8, Loss: -1.5054411888122559\n",
      "Batch 9, Loss: -1.4557961225509644\n",
      "Batch 10, Loss: 0.5568912625312805\n",
      "Batch 11, Loss: -0.7653347849845886\n",
      "Batch 12, Loss: -1.6556693315505981\n",
      "Batch 13, Loss: -1.6643881797790527\n",
      "Batch 14, Loss: 0.33036279678344727\n",
      "Batch 15, Loss: 0.826995849609375\n",
      "Batch 16, Loss: -1.837934970855713\n",
      "Batch 17, Loss: 0.06619831919670105\n",
      "Batch 18, Loss: 0.83130943775177\n",
      "Batch 19, Loss: -1.629819631576538\n",
      "Batch 20, Loss: -1.55744206905365\n",
      "Batch 21, Loss: 0.48639917373657227\n",
      "Batch 22, Loss: -0.774150013923645\n",
      "Batch 23, Loss: -0.2167784571647644\n",
      "Batch 24, Loss: -1.5335102081298828\n",
      "Batch 25, Loss: 0.8254145979881287\n",
      "Batch 26, Loss: 0.494749516248703\n",
      "Batch 27, Loss: -1.639357089996338\n",
      "Batch 28, Loss: -0.760905921459198\n",
      "Batch 29, Loss: -1.6858131885528564\n",
      "Batch 30, Loss: 0.0042761266231536865\n",
      "Batch 31, Loss: 0.9506601691246033\n",
      "Batch 32, Loss: 0.0273057222366333\n",
      "Batch 33, Loss: -0.0768653154373169\n",
      "Batch 34, Loss: -0.15546739101409912\n",
      "Batch 35, Loss: -1.6222894191741943\n",
      "Batch 36, Loss: 0.828216552734375\n",
      "Batch 37, Loss: 0.7410003542900085\n",
      "Batch 38, Loss: -0.7723431587219238\n",
      "Batch 39, Loss: -0.25536590814590454\n",
      "Batch 40, Loss: -1.7918235063552856\n",
      "Batch 41, Loss: 0.7639675140380859\n",
      "Batch 42, Loss: -1.8055212497711182\n",
      "Batch 43, Loss: -1.0871846675872803\n",
      "Batch 44, Loss: 0.21832334995269775\n",
      "Batch 45, Loss: -1.5397839546203613\n",
      "Batch 46, Loss: -1.566426157951355\n",
      "Batch 47, Loss: -1.4434107542037964\n",
      "Batch 48, Loss: -0.6173936128616333\n",
      "Batch 49, Loss: -1.7115291357040405\n",
      "Batch 50, Loss: -0.7649219036102295\n",
      "Batch 51, Loss: 0.12359687685966492\n",
      "Batch 52, Loss: 0.3228982985019684\n",
      "Batch 53, Loss: -1.7989195585250854\n",
      "Batch 54, Loss: -2.0493764877319336\n",
      "Batch 55, Loss: -1.9855315685272217\n",
      "Batch 56, Loss: -0.7742487192153931\n",
      "Batch 57, Loss: -1.1718565225601196\n",
      "Batch 58, Loss: -0.8711229562759399\n",
      "Batch 59, Loss: 0.025026291608810425\n",
      "Batch 60, Loss: -1.0209925174713135\n",
      "Batch 61, Loss: -1.0742706060409546\n",
      "Batch 62, Loss: -1.7181564569473267\n",
      "Batch 63, Loss: -1.1665434837341309\n",
      "Batch 64, Loss: -1.0899286270141602\n",
      "Batch 65, Loss: 0.09356847405433655\n",
      "Batch 66, Loss: -0.050372689962387085\n",
      "Batch 67, Loss: -1.059081792831421\n",
      "Batch 68, Loss: 0.13694119453430176\n",
      "Batch 69, Loss: 0.9670989513397217\n",
      "Batch 70, Loss: -1.3791327476501465\n",
      "Batch 71, Loss: 0.8379404544830322\n",
      "Batch 72, Loss: 0.8558435440063477\n",
      "Batch 73, Loss: -0.2665281891822815\n",
      "Batch 74, Loss: -1.6602513790130615\n",
      "Batch 75, Loss: -1.814730167388916\n",
      "Batch 76, Loss: -1.841444969177246\n",
      "Batch 77, Loss: -0.793306291103363\n",
      "Batch 78, Loss: -1.8337489366531372\n",
      "Batch 79, Loss: 0.7921640872955322\n",
      "Batch 80, Loss: -1.4638515710830688\n",
      "Batch 81, Loss: -1.7932651042938232\n",
      "Batch 82, Loss: 0.8184802532196045\n",
      "Batch 83, Loss: -1.8250863552093506\n",
      "Batch 84, Loss: -0.23462137579917908\n",
      "Batch 85, Loss: -0.1414775550365448\n",
      "Batch 86, Loss: -1.0062419176101685\n",
      "Batch 87, Loss: -1.8576959371566772\n",
      "Batch 88, Loss: -1.5772836208343506\n",
      "Batch 89, Loss: 0.8561447858810425\n",
      "Batch 90, Loss: 0.2926955819129944\n",
      "Batch 91, Loss: 0.4768027663230896\n",
      "Batch 92, Loss: -1.0675787925720215\n",
      "Batch 93, Loss: -1.845089316368103\n",
      "Batch 94, Loss: -0.01640036702156067\n",
      "Batch 95, Loss: -1.3647834062576294\n",
      "Batch 96, Loss: 0.8451297283172607\n",
      "Batch 97, Loss: 0.9426909685134888\n",
      "Batch 98, Loss: -1.0691277980804443\n",
      "Batch 99, Loss: 0.7806271910667419\n",
      "Batch 100, Loss: -1.8763405084609985\n",
      "Batch 101, Loss: -1.5702061653137207\n",
      "Batch 102, Loss: -1.6540522575378418\n",
      "Batch 103, Loss: -1.3444842100143433\n",
      "Batch 104, Loss: 0.21623437106609344\n",
      "Batch 105, Loss: 0.8650650978088379\n",
      "Batch 106, Loss: -0.15324550867080688\n",
      "Batch 107, Loss: 0.4828116297721863\n",
      "Batch 108, Loss: 0.5912183523178101\n",
      "Batch 109, Loss: 0.8932793140411377\n",
      "Batch 110, Loss: -0.8303565382957458\n",
      "Batch 111, Loss: -1.8663311004638672\n",
      "Batch 112, Loss: -0.08448794484138489\n",
      "Batch 113, Loss: 0.7127460837364197\n",
      "Batch 114, Loss: -1.8887810707092285\n",
      "Batch 115, Loss: -1.8594337701797485\n",
      "Batch 116, Loss: -0.0822567343711853\n",
      "Batch 117, Loss: 0.8349432349205017\n",
      "Batch 118, Loss: -1.6889442205429077\n",
      "Batch 119, Loss: 0.7920064330101013\n",
      "Batch 120, Loss: 0.5517120957374573\n",
      "Batch 121, Loss: 0.31161558628082275\n",
      "Batch 122, Loss: -1.64565110206604\n",
      "Batch 123, Loss: -0.8119745850563049\n",
      "Batch 124, Loss: -1.696508526802063\n",
      "Batch 125, Loss: 0.9663184881210327\n",
      "Batch 126, Loss: 0.8628331422805786\n",
      "Batch 127, Loss: -0.07197245955467224\n",
      "Batch 128, Loss: -0.18334046006202698\n",
      "Batch 129, Loss: 0.056628257036209106\n",
      "Batch 130, Loss: -0.8539456129074097\n",
      "Batch 131, Loss: 0.8437670469284058\n",
      "Batch 132, Loss: -1.293081521987915\n",
      "Batch 133, Loss: -1.3913867473602295\n",
      "Batch 134, Loss: -1.3289541006088257\n",
      "Batch 135, Loss: 0.39317944645881653\n",
      "Batch 136, Loss: 0.5408385396003723\n",
      "Batch 137, Loss: -0.8792471289634705\n",
      "Batch 138, Loss: -1.0610148906707764\n",
      "Batch 139, Loss: -1.874061942100525\n",
      "Batch 140, Loss: -0.14184346795082092\n",
      "Batch 141, Loss: -0.763412356376648\n",
      "Batch 142, Loss: 0.292271226644516\n",
      "Batch 143, Loss: -1.1494804620742798\n",
      "Batch 144, Loss: -1.530466914176941\n",
      "Batch 145, Loss: -1.6127409934997559\n",
      "Batch 146, Loss: -0.8956263065338135\n",
      "Batch 147, Loss: -1.2690434455871582\n",
      "Batch 148, Loss: -0.07203352451324463\n",
      "Batch 149, Loss: -1.8026559352874756\n",
      "Batch 150, Loss: 0.7137012481689453\n",
      "Batch 151, Loss: -0.10190069675445557\n",
      "Batch 152, Loss: -1.5126677751541138\n",
      "Batch 153, Loss: 0.16898158192634583\n",
      "Batch 154, Loss: 0.8433530330657959\n",
      "Batch 155, Loss: 0.768073558807373\n",
      "Batch 156, Loss: -1.8205225467681885\n",
      "Batch 157, Loss: 0.12497559189796448\n",
      "Batch 158, Loss: 0.8981509804725647\n",
      "Batch 159, Loss: -0.9372479915618896\n",
      "Batch 160, Loss: -0.04491475224494934\n",
      "Batch 161, Loss: 0.2615596652030945\n",
      "Batch 162, Loss: 0.5684151649475098\n",
      "Batch 163, Loss: -0.17724961042404175\n",
      "Batch 164, Loss: 0.6118385195732117\n",
      "Batch 165, Loss: 0.5592638254165649\n",
      "Batch 166, Loss: -0.8410184383392334\n",
      "Batch 167, Loss: -1.4868886470794678\n",
      "Batch 168, Loss: -1.4853532314300537\n",
      "Batch 169, Loss: -1.3015048503875732\n",
      "Batch 170, Loss: 0.008695781230926514\n",
      "Batch 171, Loss: 0.8078560829162598\n",
      "Batch 172, Loss: 0.6557362079620361\n",
      "Batch 173, Loss: -1.8356266021728516\n",
      "Batch 174, Loss: 0.5802726149559021\n",
      "Batch 175, Loss: 0.8586724400520325\n",
      "Batch 176, Loss: 0.7370874881744385\n",
      "Batch 177, Loss: -1.442584753036499\n",
      "Batch 178, Loss: 0.8148114681243896\n",
      "Batch 179, Loss: -0.3182840347290039\n",
      "Batch 180, Loss: -1.8839555978775024\n",
      "Batch 181, Loss: 0.5335867404937744\n",
      "Batch 182, Loss: -1.575716257095337\n",
      "Batch 183, Loss: 0.2952689528465271\n",
      "Batch 184, Loss: -0.06470108032226562\n",
      "Batch 185, Loss: -1.8617820739746094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 186, Loss: 0.8749875426292419\n",
      "Batch 187, Loss: -1.7081812620162964\n",
      "Batch 188, Loss: -1.645730972290039\n",
      "Batch 189, Loss: -0.21863552927970886\n",
      "Batch 190, Loss: -0.16988927125930786\n",
      "Batch 191, Loss: -0.7460868954658508\n",
      "Batch 192, Loss: 0.3794518709182739\n",
      "Batch 193, Loss: 0.042885273694992065\n",
      "Batch 194, Loss: 0.5446254014968872\n",
      "Batch 195, Loss: -0.936163067817688\n",
      "Batch 196, Loss: 0.2750084698200226\n",
      "Batch 197, Loss: -1.879728078842163\n",
      "Batch 198, Loss: -1.820500135421753\n",
      "Batch 199, Loss: -1.6207411289215088\n",
      "Training [80%]\tLoss: -0.5284\n",
      "Batch 0, Loss: 0.6755095720291138\n",
      "Batch 1, Loss: 0.2590499222278595\n",
      "Batch 2, Loss: -0.8962280750274658\n",
      "Batch 3, Loss: -0.017961442470550537\n",
      "Batch 4, Loss: 0.5142378807067871\n",
      "Batch 5, Loss: -0.24418097734451294\n",
      "Batch 6, Loss: -0.31730368733406067\n",
      "Batch 7, Loss: -0.8203319311141968\n",
      "Batch 8, Loss: -1.3594214916229248\n",
      "Batch 9, Loss: -0.22130638360977173\n",
      "Batch 10, Loss: 0.2893306016921997\n",
      "Batch 11, Loss: 0.8330403566360474\n",
      "Batch 12, Loss: -1.833265781402588\n",
      "Batch 13, Loss: 0.7639590501785278\n",
      "Batch 14, Loss: -1.5247516632080078\n",
      "Batch 15, Loss: -1.6157691478729248\n",
      "Batch 16, Loss: -1.8025269508361816\n",
      "Batch 17, Loss: -0.3084081709384918\n",
      "Batch 18, Loss: 0.5783784985542297\n",
      "Batch 19, Loss: -1.8501957654953003\n",
      "Batch 20, Loss: -1.335494041442871\n",
      "Batch 21, Loss: -1.6136000156402588\n",
      "Batch 22, Loss: 0.885015070438385\n",
      "Batch 23, Loss: -0.9751074314117432\n",
      "Batch 24, Loss: -1.4508082866668701\n",
      "Batch 25, Loss: -1.732363224029541\n",
      "Batch 26, Loss: -1.8676705360412598\n",
      "Batch 27, Loss: 0.9266512393951416\n",
      "Batch 28, Loss: -1.4045602083206177\n",
      "Batch 29, Loss: 0.5503700375556946\n",
      "Batch 30, Loss: 0.895808219909668\n",
      "Batch 31, Loss: 0.2769876718521118\n",
      "Batch 32, Loss: -1.0356210470199585\n",
      "Batch 33, Loss: -0.051112234592437744\n",
      "Batch 34, Loss: 0.19747735559940338\n",
      "Batch 35, Loss: -1.5063526630401611\n",
      "Batch 36, Loss: -0.2863411605358124\n",
      "Batch 37, Loss: 0.8161354064941406\n",
      "Batch 38, Loss: -1.60732102394104\n",
      "Batch 39, Loss: -0.0769178569316864\n",
      "Batch 40, Loss: 0.8291839957237244\n",
      "Batch 41, Loss: 0.37234580516815186\n",
      "Batch 42, Loss: 0.44781947135925293\n",
      "Batch 43, Loss: -0.1895841658115387\n",
      "Batch 44, Loss: -0.4291999340057373\n",
      "Batch 45, Loss: 0.5723943710327148\n",
      "Batch 46, Loss: -0.015078037977218628\n",
      "Batch 47, Loss: -0.9563045501708984\n",
      "Batch 48, Loss: 0.8264120817184448\n",
      "Batch 49, Loss: -1.3462318181991577\n",
      "Batch 50, Loss: -0.787158727645874\n",
      "Batch 51, Loss: -0.6616237163543701\n",
      "Batch 52, Loss: -0.9908607006072998\n",
      "Batch 53, Loss: -1.5571883916854858\n",
      "Batch 54, Loss: 0.6154877543449402\n",
      "Batch 55, Loss: -1.7403347492218018\n",
      "Batch 56, Loss: -1.149437427520752\n",
      "Batch 57, Loss: 0.7999618053436279\n",
      "Batch 58, Loss: 0.31329914927482605\n",
      "Batch 59, Loss: 0.09905064105987549\n",
      "Batch 60, Loss: 0.4538065195083618\n",
      "Batch 61, Loss: 0.6716145277023315\n",
      "Batch 62, Loss: -1.6688144207000732\n",
      "Batch 63, Loss: -0.01150602102279663\n",
      "Batch 64, Loss: -1.0419530868530273\n",
      "Batch 65, Loss: -1.477418303489685\n",
      "Batch 66, Loss: -0.04533544182777405\n",
      "Batch 67, Loss: -1.7901742458343506\n",
      "Batch 68, Loss: 0.8298437595367432\n",
      "Batch 69, Loss: -1.713365077972412\n",
      "Batch 70, Loss: -0.22916275262832642\n",
      "Batch 71, Loss: 0.6961038708686829\n",
      "Batch 72, Loss: -1.7565418481826782\n",
      "Batch 73, Loss: -1.503406286239624\n",
      "Batch 74, Loss: 0.1609002947807312\n",
      "Batch 75, Loss: 0.8065460920333862\n",
      "Batch 76, Loss: -1.6639595031738281\n",
      "Batch 77, Loss: -1.819122314453125\n",
      "Batch 78, Loss: 0.2556796669960022\n",
      "Batch 79, Loss: -0.7774760127067566\n",
      "Batch 80, Loss: -1.9689579010009766\n",
      "Batch 81, Loss: 0.8097629547119141\n",
      "Batch 82, Loss: 0.6999675035476685\n",
      "Batch 83, Loss: -1.4699170589447021\n",
      "Batch 84, Loss: -0.2543582618236542\n",
      "Batch 85, Loss: -1.8044497966766357\n",
      "Batch 86, Loss: -1.9258499145507812\n",
      "Batch 87, Loss: 0.8434617519378662\n",
      "Batch 88, Loss: 0.6441764235496521\n",
      "Batch 89, Loss: -1.8353595733642578\n",
      "Batch 90, Loss: -0.6612544059753418\n",
      "Batch 91, Loss: -0.2991717457771301\n",
      "Batch 92, Loss: -1.8499643802642822\n",
      "Batch 93, Loss: -0.951576828956604\n",
      "Batch 94, Loss: -1.865574836730957\n",
      "Batch 95, Loss: 0.9699403047561646\n",
      "Batch 96, Loss: -0.7841671705245972\n",
      "Batch 97, Loss: -0.7393780946731567\n",
      "Batch 98, Loss: 0.45286014676094055\n",
      "Batch 99, Loss: -0.749713122844696\n",
      "Batch 100, Loss: -0.06415703892707825\n",
      "Batch 101, Loss: -2.1160812377929688\n",
      "Batch 102, Loss: 0.8275047540664673\n",
      "Batch 103, Loss: 0.8680788278579712\n",
      "Batch 104, Loss: -0.6969175338745117\n",
      "Batch 105, Loss: 0.41049855947494507\n",
      "Batch 106, Loss: -1.328387975692749\n",
      "Batch 107, Loss: -1.7992159128189087\n",
      "Batch 108, Loss: -0.24589261412620544\n",
      "Batch 109, Loss: 0.21965594589710236\n",
      "Batch 110, Loss: -0.9864912033081055\n",
      "Batch 111, Loss: -0.12394446134567261\n",
      "Batch 112, Loss: 0.4674009680747986\n",
      "Batch 113, Loss: -0.8930923938751221\n",
      "Batch 114, Loss: -1.7333292961120605\n",
      "Batch 115, Loss: 0.012071967124938965\n",
      "Batch 116, Loss: -2.1696057319641113\n",
      "Batch 117, Loss: -0.7180571556091309\n",
      "Batch 118, Loss: -1.3599188327789307\n",
      "Batch 119, Loss: 0.7514466047286987\n",
      "Batch 120, Loss: 1.026543378829956\n",
      "Batch 121, Loss: -0.24654588103294373\n",
      "Batch 122, Loss: 0.5868098139762878\n",
      "Batch 123, Loss: 0.8102322816848755\n",
      "Batch 124, Loss: 0.8293393850326538\n",
      "Batch 125, Loss: -0.693812370300293\n",
      "Batch 126, Loss: -0.31147119402885437\n",
      "Batch 127, Loss: -1.091139793395996\n",
      "Batch 128, Loss: -1.8827667236328125\n",
      "Batch 129, Loss: -0.11670133471488953\n",
      "Batch 130, Loss: 0.4478102922439575\n",
      "Batch 131, Loss: -0.39702308177948\n",
      "Batch 132, Loss: -0.682785153388977\n",
      "Batch 133, Loss: -0.66816246509552\n",
      "Batch 134, Loss: -2.021134376525879\n",
      "Batch 135, Loss: -1.6372294425964355\n",
      "Batch 136, Loss: 0.3268613815307617\n",
      "Batch 137, Loss: 0.27404871582984924\n",
      "Batch 138, Loss: -1.7664321660995483\n",
      "Batch 139, Loss: 0.42032039165496826\n",
      "Batch 140, Loss: -0.6834285855293274\n",
      "Batch 141, Loss: -1.820770025253296\n",
      "Batch 142, Loss: -1.5823094844818115\n",
      "Batch 143, Loss: -0.24628987908363342\n",
      "Batch 144, Loss: 0.5990855097770691\n",
      "Batch 145, Loss: -1.7960978746414185\n",
      "Batch 146, Loss: -0.16163888573646545\n",
      "Batch 147, Loss: -0.6519337892532349\n",
      "Batch 148, Loss: -1.87282133102417\n",
      "Batch 149, Loss: 0.888689398765564\n",
      "Batch 150, Loss: -0.0934307873249054\n",
      "Batch 151, Loss: -0.7487356662750244\n",
      "Batch 152, Loss: -0.6647753119468689\n",
      "Batch 153, Loss: -1.069153070449829\n",
      "Batch 154, Loss: -0.9758279323577881\n",
      "Batch 155, Loss: -1.664650797843933\n",
      "Batch 156, Loss: 0.3040010631084442\n",
      "Batch 157, Loss: -0.2539343535900116\n",
      "Batch 158, Loss: -0.7692936658859253\n",
      "Batch 159, Loss: -0.8252663612365723\n",
      "Batch 160, Loss: -1.799613118171692\n",
      "Batch 161, Loss: -1.6243914365768433\n",
      "Batch 162, Loss: -0.29515451192855835\n",
      "Batch 163, Loss: -1.3631210327148438\n",
      "Batch 164, Loss: -1.7077579498291016\n",
      "Batch 165, Loss: -1.853912591934204\n",
      "Batch 166, Loss: -0.7017761468887329\n",
      "Batch 167, Loss: 0.05409383773803711\n",
      "Batch 168, Loss: 0.5912124514579773\n",
      "Batch 169, Loss: -0.4204547703266144\n",
      "Batch 170, Loss: 0.275362104177475\n",
      "Batch 171, Loss: 0.7230923175811768\n",
      "Batch 172, Loss: 0.2643846273422241\n",
      "Batch 173, Loss: -1.8497347831726074\n",
      "Batch 174, Loss: 0.8611345887184143\n",
      "Batch 175, Loss: -1.5525448322296143\n",
      "Batch 176, Loss: -2.09730863571167\n",
      "Batch 177, Loss: -0.12629902362823486\n",
      "Batch 178, Loss: -0.6998065710067749\n",
      "Batch 179, Loss: -0.7356615662574768\n",
      "Batch 180, Loss: 0.41793128848075867\n",
      "Batch 181, Loss: -0.8178040385246277\n",
      "Batch 182, Loss: -1.912850260734558\n",
      "Batch 183, Loss: 0.09673672914505005\n",
      "Batch 184, Loss: -1.7154029607772827\n",
      "Batch 185, Loss: -0.7296034097671509\n",
      "Batch 186, Loss: -1.7066643238067627\n",
      "Batch 187, Loss: -1.8548173904418945\n",
      "Batch 188, Loss: 0.5275067090988159\n",
      "Batch 189, Loss: 1.1140964031219482\n",
      "Batch 190, Loss: 0.5691433548927307\n",
      "Batch 191, Loss: -1.6139514446258545\n",
      "Batch 192, Loss: 0.41438090801239014\n",
      "Batch 193, Loss: 0.25505349040031433\n",
      "Batch 194, Loss: -0.5842493176460266\n",
      "Batch 195, Loss: -1.5329058170318604\n",
      "Batch 196, Loss: -1.8303723335266113\n",
      "Batch 197, Loss: -0.21464520692825317\n",
      "Batch 198, Loss: 0.8330955505371094\n",
      "Batch 199, Loss: -0.031159251928329468\n",
      "Training [90%]\tLoss: -0.5227\n",
      "Batch 0, Loss: -1.85862135887146\n",
      "Batch 1, Loss: -0.9224611520767212\n",
      "Batch 2, Loss: -0.8769010305404663\n",
      "Batch 3, Loss: 0.8460406064987183\n",
      "Batch 4, Loss: -0.22726422548294067\n",
      "Batch 5, Loss: 0.8972165584564209\n",
      "Batch 6, Loss: -1.8832237720489502\n",
      "Batch 7, Loss: -0.42782002687454224\n",
      "Batch 8, Loss: 0.6478009223937988\n",
      "Batch 9, Loss: -0.3839919865131378\n",
      "Batch 10, Loss: 1.1459532976150513\n",
      "Batch 11, Loss: 0.8496527671813965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12, Loss: 0.7570005059242249\n",
      "Batch 13, Loss: -1.7917559146881104\n",
      "Batch 14, Loss: -1.9255471229553223\n",
      "Batch 15, Loss: -1.8000603914260864\n",
      "Batch 16, Loss: -0.3141798973083496\n",
      "Batch 17, Loss: -0.7988367676734924\n",
      "Batch 18, Loss: -1.5346646308898926\n",
      "Batch 19, Loss: -1.8004494905471802\n",
      "Batch 20, Loss: 0.8051635026931763\n",
      "Batch 21, Loss: -0.41062691807746887\n",
      "Batch 22, Loss: -1.7729671001434326\n",
      "Batch 23, Loss: -0.7693297266960144\n",
      "Batch 24, Loss: -0.9701888561248779\n",
      "Batch 25, Loss: -1.8290040493011475\n",
      "Batch 26, Loss: -0.5831260681152344\n",
      "Batch 27, Loss: -0.9049161076545715\n",
      "Batch 28, Loss: -0.29747986793518066\n",
      "Batch 29, Loss: 0.5455161333084106\n",
      "Batch 30, Loss: 0.9344378709793091\n",
      "Batch 31, Loss: -1.7135467529296875\n",
      "Batch 32, Loss: -2.1499226093292236\n",
      "Batch 33, Loss: -0.6777257919311523\n",
      "Batch 34, Loss: -2.2250118255615234\n",
      "Batch 35, Loss: -1.8608052730560303\n",
      "Batch 36, Loss: -1.0010570287704468\n",
      "Batch 37, Loss: -1.6849920749664307\n",
      "Batch 38, Loss: 0.22826530039310455\n",
      "Batch 39, Loss: -0.8050870895385742\n",
      "Batch 40, Loss: 0.7364251613616943\n",
      "Batch 41, Loss: -1.757388949394226\n",
      "Batch 42, Loss: -0.2826988101005554\n",
      "Batch 43, Loss: 0.39921489357948303\n",
      "Batch 44, Loss: 0.18245132267475128\n",
      "Batch 45, Loss: -0.22870057821273804\n",
      "Batch 46, Loss: 0.02009931206703186\n",
      "Batch 47, Loss: -1.776404857635498\n",
      "Batch 48, Loss: -0.2615794241428375\n",
      "Batch 49, Loss: 0.2994719445705414\n",
      "Batch 50, Loss: -1.4840786457061768\n",
      "Batch 51, Loss: 0.4453100562095642\n",
      "Batch 52, Loss: -0.8761674165725708\n",
      "Batch 53, Loss: 0.7181993126869202\n",
      "Batch 54, Loss: -1.311099886894226\n",
      "Batch 55, Loss: -1.8244051933288574\n",
      "Batch 56, Loss: -1.1615538597106934\n",
      "Batch 57, Loss: 0.1001628041267395\n",
      "Batch 58, Loss: -0.3067433536052704\n",
      "Batch 59, Loss: -1.0407583713531494\n",
      "Batch 60, Loss: 0.5252267122268677\n",
      "Batch 61, Loss: -0.06121757626533508\n",
      "Batch 62, Loss: -1.3508706092834473\n",
      "Batch 63, Loss: 0.33355236053466797\n",
      "Batch 64, Loss: -1.7248406410217285\n",
      "Batch 65, Loss: -1.363614559173584\n",
      "Batch 66, Loss: 0.08309108018875122\n",
      "Batch 67, Loss: 0.17219915986061096\n",
      "Batch 68, Loss: 0.1498691439628601\n",
      "Batch 69, Loss: -2.0957372188568115\n",
      "Batch 70, Loss: 0.5771230459213257\n",
      "Batch 71, Loss: 0.6279174089431763\n",
      "Batch 72, Loss: 0.11041456460952759\n",
      "Batch 73, Loss: 0.6208481788635254\n",
      "Batch 74, Loss: 0.5311833620071411\n",
      "Batch 75, Loss: -0.1854352056980133\n",
      "Batch 76, Loss: -0.7431908845901489\n",
      "Batch 77, Loss: -1.7706215381622314\n",
      "Batch 78, Loss: 1.1161901950836182\n",
      "Batch 79, Loss: -0.2646254897117615\n",
      "Batch 80, Loss: -0.5563837289810181\n",
      "Batch 81, Loss: -1.5756263732910156\n",
      "Batch 82, Loss: 0.05110180377960205\n",
      "Batch 83, Loss: -0.3929869830608368\n",
      "Batch 84, Loss: -0.269892156124115\n",
      "Batch 85, Loss: 0.7720857858657837\n",
      "Batch 86, Loss: -1.110825777053833\n",
      "Batch 87, Loss: 0.5662946105003357\n",
      "Batch 88, Loss: -1.7874771356582642\n",
      "Batch 89, Loss: -1.8368550539016724\n",
      "Batch 90, Loss: -1.4805588722229004\n",
      "Batch 91, Loss: -0.8584682941436768\n",
      "Batch 92, Loss: 0.5953912138938904\n",
      "Batch 93, Loss: -1.1203293800354004\n",
      "Batch 94, Loss: -0.24866750836372375\n",
      "Batch 95, Loss: 0.7800043225288391\n",
      "Batch 96, Loss: 0.06328684091567993\n",
      "Batch 97, Loss: 0.6444305777549744\n",
      "Batch 98, Loss: -0.8176229000091553\n",
      "Batch 99, Loss: -1.139086365699768\n",
      "Batch 100, Loss: -1.1159515380859375\n",
      "Batch 101, Loss: 0.7978377938270569\n",
      "Batch 102, Loss: 0.45357710123062134\n",
      "Batch 103, Loss: -0.039874881505966187\n",
      "Batch 104, Loss: 0.9909584522247314\n",
      "Batch 105, Loss: -0.21545252203941345\n",
      "Batch 106, Loss: -0.1698843538761139\n",
      "Batch 107, Loss: -0.1638396978378296\n",
      "Batch 108, Loss: -0.9065237045288086\n",
      "Batch 109, Loss: 0.42079344391822815\n",
      "Batch 110, Loss: -0.5968602895736694\n",
      "Batch 111, Loss: -0.8716040849685669\n",
      "Batch 112, Loss: -1.159486174583435\n",
      "Batch 113, Loss: -0.10078901052474976\n",
      "Batch 114, Loss: -1.5429980754852295\n",
      "Batch 115, Loss: -0.7573208808898926\n",
      "Batch 116, Loss: -1.5833370685577393\n",
      "Batch 117, Loss: -0.1288793981075287\n",
      "Batch 118, Loss: -1.3179364204406738\n",
      "Batch 119, Loss: 0.1858590841293335\n",
      "Batch 120, Loss: -1.2146320343017578\n",
      "Batch 121, Loss: -0.2785153388977051\n",
      "Batch 122, Loss: -1.721708059310913\n",
      "Batch 123, Loss: -1.4862340688705444\n",
      "Batch 124, Loss: 0.6416174173355103\n",
      "Batch 125, Loss: -0.2619988024234772\n",
      "Batch 126, Loss: -1.2025340795516968\n",
      "Batch 127, Loss: 0.09811881184577942\n",
      "Batch 128, Loss: -0.7232271432876587\n",
      "Batch 129, Loss: 0.734748125076294\n",
      "Batch 130, Loss: -0.6889954805374146\n",
      "Batch 131, Loss: 0.5948384404182434\n",
      "Batch 132, Loss: -1.6281321048736572\n",
      "Batch 133, Loss: -1.5787851810455322\n",
      "Batch 134, Loss: 0.5094699263572693\n",
      "Batch 135, Loss: 0.43355897068977356\n",
      "Batch 136, Loss: -0.6416604518890381\n",
      "Batch 137, Loss: -0.31905776262283325\n",
      "Batch 138, Loss: 0.08223986625671387\n",
      "Batch 139, Loss: -0.8930741548538208\n",
      "Batch 140, Loss: -1.4894824028015137\n",
      "Batch 141, Loss: 0.5343753099441528\n",
      "Batch 142, Loss: -2.050940990447998\n",
      "Batch 143, Loss: -0.8812861442565918\n",
      "Batch 144, Loss: 0.8402307033538818\n",
      "Batch 145, Loss: -0.761966347694397\n",
      "Batch 146, Loss: 0.13096976280212402\n",
      "Batch 147, Loss: -0.22787684202194214\n",
      "Batch 148, Loss: 0.689030647277832\n",
      "Batch 149, Loss: 0.6551944017410278\n",
      "Batch 150, Loss: -0.7314263582229614\n",
      "Batch 151, Loss: -2.123825788497925\n",
      "Batch 152, Loss: -1.8542757034301758\n",
      "Batch 153, Loss: -1.2210304737091064\n",
      "Batch 154, Loss: 0.5776809453964233\n",
      "Batch 155, Loss: 0.6748713254928589\n",
      "Batch 156, Loss: 0.4226585626602173\n",
      "Batch 157, Loss: -1.9759756326675415\n",
      "Batch 158, Loss: -1.068910002708435\n",
      "Batch 159, Loss: -1.2561910152435303\n",
      "Batch 160, Loss: -1.5573291778564453\n",
      "Batch 161, Loss: -1.2527953386306763\n",
      "Batch 162, Loss: 0.42277950048446655\n",
      "Batch 163, Loss: 0.5502790212631226\n",
      "Batch 164, Loss: -1.4976180791854858\n",
      "Batch 165, Loss: 0.073140949010849\n",
      "Batch 166, Loss: 0.8876673579216003\n",
      "Batch 167, Loss: 0.37914103269577026\n",
      "Batch 168, Loss: 0.32205021381378174\n",
      "Batch 169, Loss: -0.918489933013916\n",
      "Batch 170, Loss: -0.9756588935852051\n",
      "Batch 171, Loss: -1.3403574228286743\n",
      "Batch 172, Loss: -1.8095873594284058\n",
      "Batch 173, Loss: 0.2181779146194458\n",
      "Batch 174, Loss: -0.8235271573066711\n",
      "Batch 175, Loss: -1.8828318119049072\n",
      "Batch 176, Loss: 0.14816318452358246\n",
      "Batch 177, Loss: -0.8297653794288635\n",
      "Batch 178, Loss: -0.1846066117286682\n",
      "Batch 179, Loss: -1.442876935005188\n",
      "Batch 180, Loss: 0.08792859315872192\n",
      "Batch 181, Loss: 0.8515713214874268\n",
      "Batch 182, Loss: 1.067751169204712\n",
      "Batch 183, Loss: -0.844990074634552\n",
      "Batch 184, Loss: -1.3086479902267456\n",
      "Batch 185, Loss: 0.7284941673278809\n",
      "Batch 186, Loss: -1.460615634918213\n",
      "Batch 187, Loss: -0.13279423117637634\n",
      "Batch 188, Loss: -1.1222813129425049\n",
      "Batch 189, Loss: 0.1434324085712433\n",
      "Batch 190, Loss: -1.2510135173797607\n",
      "Batch 191, Loss: 0.8768688440322876\n",
      "Batch 192, Loss: -1.1824849843978882\n",
      "Batch 193, Loss: -0.19252091646194458\n",
      "Batch 194, Loss: -0.29295122623443604\n",
      "Batch 195, Loss: -1.5613956451416016\n",
      "Batch 196, Loss: -0.9063841104507446\n",
      "Batch 197, Loss: -1.2737319469451904\n",
      "Batch 198, Loss: -1.9461860656738281\n",
      "Batch 199, Loss: -1.6193997859954834\n",
      "Training [100%]\tLoss: -0.5186\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: -0.000651123933494091 s\n"
     ]
    }
   ],
   "source": [
    "################# TRAIN\n",
    "# Start training\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.train()  # Set model to training mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "    loss_list = []  # Store loss history\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "            output = model(data)  # Forward pass\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            loss = loss_func(output, target)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize weights\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "            print(f\"Batch {batch_idx}, Loss: {total_loss[-1]}\")\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    # Plot loss convergence\n",
    "    plt.clf()\n",
    "    plt.plot(loss_list)\n",
    "    plt.title(\"Hybrid NN Training Convergence\")\n",
    "    plt.xlabel(\"Training Iterations\")\n",
    "    plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "    #plt.savefig(f\"plots/{dataset}_classification{classes_str}_hybridqnn_q{n_qubits}_{n_samples}samples_lr{LR}_bsize{batch_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "elif network == \"QSVM\":\n",
    "    result = svm.run(quantum_instance)\n",
    "    for k,v in result.items():\n",
    "        print(f'{k} : {v}')\n",
    "\n",
    "time_end = timeit.timeit()\n",
    "time_elapsed = time_end - time_start\n",
    "print(f\"Training time: {time_elapsed} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ce04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: -0.5115\n",
      "\tAccuracy: 50.0%\n",
      "Test time: -0.00019305397290736437 s\n"
     ]
    }
   ],
   "source": [
    "######## TEST\n",
    "time_start = timeit.timeit()\n",
    "\n",
    "if network != \"QSVM\":\n",
    "    if \"vgg\" in network or \"resnet\" in network:\n",
    "        model.network.eval()  # Set model to eval mode\n",
    "    if network == \"hybridqnn_shallow\":\n",
    "        model.eval()  # Set model to eval mode\n",
    "    with no_grad():\n",
    "        correct = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            if len(output.shape) == 1:\n",
    "                output = output.reshape(1, *output.shape)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # change target class identifiers towards 0 to n_classes\n",
    "            for sample_idx, value in enumerate(target):\n",
    "                target[sample_idx]=classes2spec[target[sample_idx].item()]\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            loss = loss_func(output, target)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        print(\n",
    "            \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "                sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time_end = timeit.timeit()\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(f\"Test time: {time_elapsed} s\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416d08bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted labels\n",
    "n_samples_show_alt = n_samples_show\n",
    "while n_samples_show_alt > 0:\n",
    "    plt.clf()\n",
    "    count = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(n_samples_show*2, batch_size*3))\n",
    "    if n_samples_show == 1:\n",
    "        axes = [axes]\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):    \n",
    "        if count == n_samples_show:\n",
    "            #plt.savefig(f\"plots/{dataset}_classification{classes_str}_subplots{n_samples_show_alt}_lr{LR}_pred_q{n_qubits}_{n_samples}samples_bsize{batch_size}_{epochs}epoch.png\")\n",
    "            plt.show()\n",
    "            break\n",
    "        output = model(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for sample_idx in range(batch_size):\n",
    "            try:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()]]\n",
    "            except:\n",
    "                class_label = classes_list[specific_classes[pred[sample_idx].item()-1]]\n",
    "            axes[count].imshow(np.moveaxis(data[sample_idx].numpy().squeeze(),0,-1))\n",
    "            axes[count].set_xticks([])\n",
    "            axes[count].set_yticks([])\n",
    "            axes[count].set_title(class_label)\n",
    "            count += 1\n",
    "    n_samples_show_alt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d31d1c",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
